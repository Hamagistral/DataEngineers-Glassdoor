company,company_rating,location,job_title,job_description,salary_estimate,company_size,company_type,company_sector,company_industry,company_founded,company_revenue
"Fuge Technologies Inc
4.7",4.7,"Jersey City, NJ",Big Data Engineer,"Big Data Engineering Technical Lead
Mandatory Skills
Big Data – Spark, Kafka, Databricks, Hive, Nifi
AWS, Java, Scala
Client Interview Needed for Selection (yes / No)
Yes
Detailed JD (Pl share the Detailed Description, 1 liner JD will not work)
10+ yrs experience as Data Engineer
Extensive experience in spark on data processing
Kafka design experience like monitoring & parallelism.
Comfortable with Python/Java/Scala coding experience
Crisp / clear cut communication
Flexible with learning new technology
Flink experience is a plus
AWS experience is nice to have
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Jersey City, NJ 07302: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",$72.50 /hr (est.),1 to 50 Employees,Contract,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Titan America
3.8",3.8,"Roanoke, VA",Data Engineer,"Data Engineer
We are seeking a motivated and detail-oriented Data Engineer to join our team based in Roanoke, Virginia. As a Data Engineer, you will play a crucial role in designing, developing and maintaining scalable data pipelines and transformations for business-critical use cases. In addition, you may take on various Software Development and Data Science responsibilities as we frequently share responsibilities and knowledge across our growing team.
Responsibilities:
Design, develop and maintain scalable data pipelines for extracting, transforming, and loading data from various sources
Collaborate with cross-functional teams to identify data needs and determine the best data solutions
Develop and implement data models to support business requirements and ensure data quality
Ensure the security and privacy of sensitive data by implementing appropriate access controls
Monitor and optimize data pipeline performance to ensure timely and accurate data delivery
Document data pipeline processes, data dictionaries, and data storage solutions
Requirements:
Bachelor's degree in Computer Science, Computer Engineering, or a related technical field
Minimum of five years of professional experience working as a Data Engineer or Software Developer
Strong hands-on experience with data warehouse and transformation solutions, i.e. Domo, Snowflake or similar
Proficient in at least one scripting language such as Python, JavaScript, or R
Understanding of data modeling, data integration and data quality processes
Familiarity with cloud platforms such as AWS, Azure, or Google Cloud Platform
Strong analytical and problem solving skills
Full Stack Software Development experience in a professional setting is highly desired, but not required
This is an excellent opportunity for a driven and collaborative individual to make a significant impact in a dynamic and growing team. If you have a passion for data and a desire to work in a fast-paced and dynamic environment, we want to hear from you!
Success begins with hiring the right people to partner with us as we grow and develop our businesses. People are central to everything we do. It is through their efforts and talents that Titan has been successful for over 100 years.
Titan America is an Equal Employment Opportunity (EEO) / Affirmative Action employer and welcomes all qualified applicants. Applicants will receive fair and impartial consideration without regard to race, sex, color, religion, national origin, age, disability, veteran status, genetic data, or other legally protected status.
Job Type: Full-time
Pay: $75,000.00 - $85,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Referral program
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Roanoke, VA 24019: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Do you currently or will you in the future need sponsorship to work in the United States?
This position requires the successful candidate to be in the office in Roanoke, VA at least 3 days a week. Can you meet this requirement?
This position requires the successful candidates to be a full time, W2 employee. Can you meet this requirement?
Education:
Bachelor's (Preferred)
Experience:
Data Engineering: 5 years (Preferred)
Work Location: Hybrid remote in Roanoke, VA 24019","$80,000 /yr (est.)",1001 to 5000 Employees,Company - Public,"Energy, Mining & Utilities",Mining & Metals,1902,Unknown / Non-Applicable
"RelMap Consulting
4.8",4.8,"Addison, TX",Sr. Data Engineer (Hybrid),"The data engineer entails:
Advanced SQL Server Development
Ability to create and modify views
Ability to create and modify custom functions
Ability to write complex queries, including joins, temp table processing, and common table expressions (CTE’s)
Ability to create and modify stored procedures using TSQL
Ability to analyze and optimize query performance using indices
Ability to use Red Gate’s SQL Compare to perform a schema (AKA “do a manual roll”) between one server an another, and between revision control and a database
Ability to stand-up new database server VM’s in Linux and Windows using VMWare
Ability to backup and restore a database in a Linux and Windows environment
Ability to manage and script automated database jobs
Ability to use PowerShell / Bash to interact with databases from the command line
Addition skills that would be desired but not required
· Experience with Redis
· Experience with MongoDB / CosmosDB
· Experience with Azure and Azure SQL
· C#, JavaScript (NodeJS / ES6), or Python experience
Job Type: Contract
Pay: $70.00 - $80.00 per hour
Compensation package:
Hourly pay
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Addison, TX 75001: Reliably commute or planning to relocate before starting work (Required)
Work Location: Hybrid remote in Addison, TX 75001",$75.00 /hr (est.),1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2010,$1 to $5 million (USD)
Proits Hub LLC,#N/A,Remote,Data Analytics Engineer,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote
Speak with the employer
+91 9417599933",$60.00 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Atika Technologies,#N/A,"Pittsburgh, PA",Data Engineer,"Role: Data Engineer/PySpark Developer, Pittsburgh, PA - Day One Onsite
Fulltime and Contract(W2)
LinkedIn Must
USC/GC/EADs
Three round of interview: First two on Zoom Video Call, End Client Round will be Onsite on the Client's Location (Pittsburgh, PA)
Job Description:
Python(PySpark), Jupiter hub, airflow, Java
Database knowledge - Oracle / SQL / Data Analytics
Regulatory reporting knowledge preferred
Good communication skills
Open / Ability to learn new technologies
Very good Analytical Skills
Experience in ETL is preferable
Job Types: Full-time, Contract
Salary: $120,000.00 - $130,000.00 per year
Benefits:
Dental insurance
Health insurance
Life insurance
Compensation package:
Bonus pay
Performance bonus
Yearly pay
Experience level:
10 years
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Pittsburgh, PA 15259: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Required)
Spark: 4 years (Required)
Regulatory reporting: 3 years (Required)
SQL: 4 years (Required)
IT: 10 years (Required)
Airflow: 4 years (Required)
Jupiter: 4 years (Required)
Work Location: One location","$125,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"spar information systems
3.5",3.5,Remote,Azure Data Engineer,"Role: Sr Azure Data Engineer
Location: Remote
Duration: 3 Months Contract to hire Full Time (W2 Only)
Must have 11+ IT Experience
Required Skills:
3+ years of experience in data software development, programming languages and developing with big data technologies
2+ years of experience designing and building on existing and new data applications
2+ years of experience in Cloud DevOps concepts, Cloud Services and Architecture, and Azure/AWS/GCP DevOps Operational Framework
1+ years of experience in open-source data tools and frameworks, or one of the following: .net Core, asp.Net, Angular, or Express.
Thanks & Regards,
Arvind Kumar Bind
Cell: 732 716 7403 (Text)
Direct Number:- 469-750-0607
Email : Arvind.B@sparinfosys.com
Job Types: Full-time, Contract, Permanent
Pay: $120,555.79 - $150,000.00 per year
Benefits:
401(k)
Health insurance
Paid time off
Experience level:
11+ years
Schedule:
8 hour shift
Experience:
Big data: 3 years (Required)
SQL: 1 year (Required)
Data lake: 3 years (Preferred)
Work Location: Remote","$135,278 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2012,$25 to $100 million (USD)
"Appsintegration INC
4.5",4.5,"California, KY",Data Engineer,"Role:- Data Engineer
Position:- Onsite (W2 position)
Experience:- 10+ Years
Responsibilities
Working closely with business teams to understand their requirements and develop reporting or analytic solutions.
Producing and automating the delivery of key metrics and KPIs to the business.
In some cases, this will focus on making data available and others will be developing full reports for end users.
Managing stakeholder demand and a backlog of reporting requirements and needs.
Actively working with users to understand data issues, tracing back data lineage, and helping the business put appropriate data cleansing and quality processes in place.
Managing secure access to data sets for the business.
Requirements
Bachelor's degree in Computer Science or equivalent.
Proficient in creating Data Factory pipelines for on-cloud ETL processing.
Experience implementing data pipelines using the latest technologies.
Experience developing using SQL, relational databases, and data warehousing technologies.
Extensive ETL experience.
Ability to scale systems in Cloud as per requirement.
Experience in handling structured and unstructured datasets.
Good to have experience in data modeling and Advanced SQL techniques.
Must-Have Skills: Azure Data Factory (ADF), SQL, Cloud-based ETL.
Good to Have Skills: Azure File Storage Services, Databricks, Azure Synapse.
you can reach me out on LinkedIn
here is my link :- linkedin.com/in/shravanthi-thakur-705a81231
Job Type: Contract
Salary: $42.00 - $45.00 per hour
Ability to commute/relocate:
California, KY 41007: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",$43.50 /hr (est.),1001 to 5000 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"DSMH LLC
5.0",5.0,Remote,"Data Engineer (AWS, Python)","Title: Data Engineer
Duration: 12+ Months
Technical Skills
(Required)
Four years or more of experience in data engineering and/or software
development.
Three years or more of experience with development, operations, or
architecture in AWS using Python.
Experience with development and delivery of microservices using serverless
AWS services (S3, RDS, Aurora, DynamoDB, Lambda, SNS, SQS, Kinesis,
IAM)
Strong competency in testing, including unit testing to coverage standards and
integration testing.
(Desired)
Software development experience with object-oriented development and design
patterns
Experience with serverless design in AWS
AWS technical certifications (Developer Associate, Solution Architect Associate)
Familiarity with machine learning and data design to support machine learning
Experience with productionizing machine learning workloads
Experience with ElasticSearch/ELK, Kibana, Grafana, and/or Prometheus
Experience with OpenTelemetry
Soft Skills
(Required)
Verbal and written communication skills, problem solving skills, customer
service and interpersonal skills.
Basic ability to work independently and manage one’s time.
(Desired)
Ability to work collaboratively in a complex, rapidly changing, and culturally
diverse environment.
Ability to clearly communicate complex technical ideas.
Comfortable working in a dynamic environment where digital is still evolving as
a core offering.
Job Type: Contract
Pay: $50.00 - $55.00 per hour
Experience level:
6 years
Schedule:
Monday to Friday
Experience:
AWS: 5 years (Required)
Python: 5 years (Required)
Work Location: Remote",$52.50 /hr (est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,$1 to $5 million (USD)
"Apolis
3.9",3.9,"Saint Louis, MO",Data Engineer (Python Focused),"Role: Data Engineer (Python Focused) (W2 ONLY)
Location : St. Louis, MO 63102
Duration: 18 Months
The primary responsibilities of this role will be to work as a part of the data and analytics team of the client. The ideal candidate would have experience using ETL and other data engineering tools in a cloud environment. Additionally, this candidate would have coding and scripting experience creating code to move and manipulate data using tools such in Python such as Spark or Pandas. Microsoft Azure experience is a plus but relevant experience with other cloud-based environments will be considered.
This team uses Microsoft Azure to build and productionize data sets and processes to the product owners and product users and this role will be expected to contribute to those objectives. As a member of this team, you will work in conjunction with other data engineers, platform engineers, solution developers and others to execute and deliver working solutions to the end users.
Technical Skills
High focus on Python
More Programmer/Coder experience, less Cloud experience needed.
Must have experience with data analytics/engineering and cloud technologies and an in-depth working knowledge and understanding of data engineering, design, and integration.
Intermediate or higher level of proficiency with Python Must have the proven ability to learn quickly and apply cloud data engineering principles and skills to new tools and use cases.
Ability to define and take technical ownership of detailed requirements and collaborate effectively with various team members such as Business Analysts and Project Managers and end users.
Excellent communication (verbal and written) and problem-solving skills are essential for this position.
Initiative, Proactive Cooperation, and curiosity are mandatory behavioral skills required for this position.
Experience with several of the following technologies is preferred:
Experience with Microsoft Azure Platform. In particular, the data processing toolsets including Azure Data Factory Pipelines, Azure Databricks, Azure Data Lake Storage.
Experience using Notebooks (Such as Jupyter, DataBricks) also desired.
Experience using code languages such as Python and C#
Experience using query languages such as SQL, MDX and DAX
Experience using scripting languages such as PowerShell, M-Query (Power Query), and Windows batch commands.
Understanding of Agile and DevOps principles for software development and ownership.
Familiar with compliance and controls for software development and deployment.
Education Required
A bachelor's in computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field [or equivalent work experience] is not required but is preferred.
Job Type: Contract
Salary: $60.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Vision insurance
Compensation package:
Overtime pay
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
St. Louis, MO: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Preferred)
ETL: 5 years (Preferred)
Cloud infrastructure: 4 years (Preferred)
Work Location: One location",$65.00 /hr (est.),501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,1996,$25 to $100 million (USD)
"shaped.ai Inc.
4.0",4.0,"New York, NY",Data Engineer,"Hello! We’re building the future of content discovery infrastructure with AI — come join us!

Shaped (YC W22) is an API for developers to seamlessly add personalized ranking and recommendation to their products. These frictionless discovery experiences help end-users find what they want faster, and, consequently, grow conversion and engagement business metrics. Behind the scenes, we’re building state-of-the-art ML infrastructure for training and deploying large AI models to power our personalization engine.
We’re a seed stage start-up backed by top-tier investors (e.g. Y-Combinator, Susa Ventures, Tribe Capital etc…) and executives (e.g. Google, Amazon, Uber, Dropbox etc…). Our team comes from Meta, Google, Apple and Uber. We’re a remote team but have a small office in Brooklyn, New York.
We’ve been lucky to have a huge amount of success since our initial launch earlier this year and now looking to expand our real-time data engineering capabilities. We are looking for a data engineer to design, build and optimize Shaped's real-time streaming data ingestion endpoints. You will be a founding engineer working on state-of-the-art infrastructure. As one of Shaped’s early employees you will help shape our product, culture and vision.
This role is for a full-time data engineer at $120k - $200k a year and 0.1 - 0.5% equity.

Responsibilities:
Manage the unification of our batch and real-time data processing pipelines.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Requirements:
Bachelor's in computer science, data science or mathematics related field. Master's degree or PhD will be advantageous.
5+ years of data engineering experience, ideally applied to machine learning use-cases.
Experience with AWS or common cloud technologies
Proficient with Python, SQL.
Extensive knowledge of both offline and online large scale data processing frameworks: Spark, Beam, Flink.
Extensive knowledge with stream technologies like Kakfa or Kinesis.
Excellent written and verbal communication skills.
We’re excited to work with you. Come build the future of AI and discovery with us!","$160,000 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
WellTrust Medical Group,#N/A,"Charlotte, NC",Data Engineer,"The Data Engineer will assist the senior analyst in complex modeling and projects to ensure their completion within the allotted timeline. Helps track measures of success and develop reports that measure operational and customer metrics. Demonstrates ability to troubleshoot data consumption and data quality problems.

Responsibilities:
Build and maintain reports using My SQL Server
Identify optimal data sources, structure data validation, and cache optimization.
Performs business problem modeling and quantitative analytics including regression analysis, time-series, hierarchical cluster analysis, and data mining.
Helps track measures of success and develop reports that measure operational and customer metrics.
Demonstrates ability to troubleshoot data consumption and data quality problems.
Evaluates and recommend improvements to current models, metrics, and analytic processes to improve operational efficiency and insight.
Design, document, and communicate effective technical solutions meeting end-user requirements
Perform analyses with large amounts of complex data, and uses data visualization tools to communicate results.
Requirements:

Bachelor's degree or equivalent experience in a quantitative field (Mathematics, Statistics, Computer Science, Economics, Finance, Physics, Engineering)
3+ years of professional working experience as a Data Engineer or Analyst
1-3 years of data warehouse experience
Build and maintain reports using My SQL Server
Strong analytical and creative problem-solving skills
Design, document, and communicate effective technical solutions meeting end-user
SQL and Excel Macro development
Strong understanding of various data environments such as SQL, Oracle, Big Query, etc.
Experience with data visualization tools such as Looker Studio, Tableau, D3.js, etc.
Proficient in R, Python, MATLAB, Java, or another programming language.",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Fracsys Inc,#N/A,"Washington, DC",Data Engineer,"Fracsys Inc is hiring a Data Engineer position. The ideal candidate must have at least 12+ years of industry experience. He or she must be responsible for successful technical delivery of Data Warehousing and Business Intelligence projects using a formal project management methodology.
Responsibilities
Directs, motivates and develops key technical personnel
Provides technical direction across the full lifecycle system development process
Provides strategic planning and comprehensive senior level technical consulting to IT senior management and senior technical staff
Working independently within guidelines
Provides detailed inputs for plans and schedules, including goals, risks, and resource allocation
Conduct diagnostic, troubleshooting steps and present conclusions to a varied audience
comfortable slinging some code to solve a problem
Build Data Marts and Data warehouses to facilitate analytical capabilities across the varied sources of data
Design and implement data patterns on cloud using cloud Data warehouse and data virtualization tools
Evaluate new tools and technologies for target state on cloud
Learn and implement new ETL, BI tools and data warehousing technologies
Qualifications
Minimum twelve (12) years in a Technical Lead or Consulting role leading Information Management related initiatives (system integration, data warehouse build, data mart build, or similar)
MUST have proven leadership experience for team of at least 3 technical resources.
Experience creating Data Marts, Data warehouses on-premise and on cloud for real time and batch processing frameworks
Experience building and optimizing big-data data pipelines and data sets including Postgres, AWS Relational Data Service
extensive hands-on experience building and optimizing data structures for data analytics, data science and business intelligence
Experience building self-service data consumption patterns and knowledge of cloud-based data Lake platforms
Experience wrangling data (structured and unstructured), in-depth knowledge of database architecture
Experience utilizing or leading implementations leveraging ETL tools (Informatica / Talend), BI Reporting tools such as MicroStrategy, Microsoft Power BI, data modeling tools such as Erwin, Oracle, SQL server, NoSQL, JDBC, UNIX shell scripting, PERL and JAVA, XML/JSON files, SAS , Python, AWS cloud-native technologies, S3, Athena, Redshift
Familiarity with the following technologies: Hadoop, Kafka, Airflow, Hive, Presto, Athena, S3, Aurora, EMR, Spark
Ability to drive, contribute to, and communicate solutions to technical product challenges
Ability to roll-up your sleeves and work in process or resource gaps and fill it in yourself
Excellent written and oral communication skills in English
Education
Bachelor’s degree in computer science, Data Analytics, Information Systems, or a related degree or equivalent experience. Master’s degree is preferred.
Job Type: Full-time
Pay: From $96,336.72 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Experience level:
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Washington, DC: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person","$96,337 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Infosoft,#N/A,"Peachtree Corners, GA",Data Pipeline Engineer - 2173,"Here are the job position details for your review:
Job Title: Data Pipeline Engineer (GStreamer/ DeepStream)
Pay Rate: $90 to 120/Hr.
Duration: 3 Months
Location: Peachtree Corners, GA
This position is for the Building Automation division.
We are looking for Data Pipeline Engineer (GStreamer) to participate in the development of Streaming data pipelines using NVIDIA Deepstream, GStreamer, Python programming, messaging queues, and related tools.
Preference is for candidates located in Atlanta, GA metro area. However, ok with non-local candidates who are open to 75% travel to Atlanta, GA.
Understanding the basics of our architecture/setup to be able to do setup/support for demos, testing, etc.
Basic computer hardware assembly and set-up
Possible set-up/installation of the application(s)
Possible development tasks depending on what the rest of the development team has for them
Develops business software applications, system software, and/or IT solutions on the basis of functional specifications.
Scope of position embraces both the customization or reuse/modification of system software and/or solutions as well as their development from scratch.
Consults with customers in creating functional specifications and work estimates, and supports Make-or-Buy decisions.
Analyses performance gaps of existing software environments and proposes measures to address efficiency problems.
Is also responsible for the implementation of data-driven components supporting data modeling and engineering in the context of data analytics (Data Stewardship).
Skills:
Python
Linux
Docker and Docker Swarm
Zero MQ / a good understanding of messaging
Desired Skills:
NVIDIA Deepstream SDK
Web Sockets
Flask
Protobuf
Possible:
GStreamer (if different than RTSP / H264 / MJPEG)
Job Type: Contract
Salary: $90.00 - $120.00 per hour
Ability to commute/relocate:
Peachtree Corners, GA: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
""Please share your Email ID""
Work Location: One location",$105.00 /hr (est.),Unknown,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Fulcrum Analytics
4.3",4.3,Remote,Data Engineer,"At Fulcrum, we provide cutting-edge data science consulting and software solutions to help companies solve challenging business problems. Through the utilization of our innovative products and dedicated team, clients are able to operate more efficiently, manage risk, and make informed business decisions every day. Fulcrum Analytics has stood at the forefront of data, analytics, and innovative software for over 25 years, offering sophisticated solutions, groundbreaking technologies, and winning strategies that help companies achieve their targeted results.
Role Description:
We are seeking a highly-skilled data engineer with experience in data analytics and data science. The ideal candidate will be responsible for building and maintaining data pipelines, designing and implementing databases and data models, and developing data-driven solutions to drive business growth for our clients.
Responsibilities:
Design, build, and maintain efficient, scalable, and reliable data pipelines using SQL, or programming languages such as Python or Java.
Develop and maintain databases and data models to meet clients’ analytical needs.
Identify data quality issues and develop processes to ensure data accuracy, completeness, and consistency.
Work with cross-functional teams to define and implement data integration strategies for new and existing systems.
Develop and maintain automated testing and deployment of analytics code and data pipelines.
Collaborate with data scientists to develop and implement machine learning models, statistical models, and other advanced analytics techniques.
Communicate analytical results and recommendations to technical and non-technical stakeholders through presentations and visualizations.
Stay up to date with emerging technologies and industry best practices, and recommend tools and processes that can improve data engineering and analytics processes.
Qualifications:
Bachelor's or master's degree in computer science, statistics, data science, or a related field.
2+ years of experience in data engineering, data analytics, and data science.
Strong programming skills in Python, with experience in building data pipelines using frameworks such as Airflow, Glue, Lambda, Athena, and BigQuery as well as data platforms such as Hadoop, Snowflake, and GCP.
Experience working with SOAP or RESTful web services.
Familiarity with version control, in particular Git.
Experience in designing, building, and optimizing relational and NoSQL databases.
Familiarity with data visualization tools, such as Tableau or Power BI.
Ability to understand and transform complex data into actionable insights.
Strong analytical and problem-solving skills, with a focus on delivering practical solutions to business challenges.
Ability to learn and solve problems independently.
Excellent communication and collaboration skills, with the ability to explain complex concepts to both technical and non-technical audiences.
If you are passionate about data analytics and want to work in a dynamic environment with talented colleagues, we encourage you to apply for this role. We offer competitive salary and benefits packages, as well as opportunities for professional growth and development.


Position Type: Full-time salaried
Reports to: Senior Data Engineer
Base Compensation: $110,000 - $140,000
Benefits: Medical, Dental, Vision, 401k with Match, Bonus, Unlimited PTO
Location: Remote within the United States of America; Eastern time zone preferred","$125,000 /yr (est.)",1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,1993,$5 to $25 million (USD)
"iManage
4.5",4.5,Remote,Senior Data Engineer (Azure),"We offer a flexible working policy that supports the health and well-being of our iManage employees. As an organization, we value collaborating and learning from our peers in person, while providing the necessary flexibility for our employees to have a meaningful work-life balance. Please reach out to learn more.
Being a Senior Data Engineer at iManage Means…
You are excited about data and believe in the democratization of data to support data driven decision-making. You will partner with our Information Technology team to implement, support, and extend our Enterprise Data Lake hosted on Azure and built using Azure Synapse. You will gather requirements from iManage business units and craft solutions which provide access to critical business data. You will develop data models and data pipelines for our Enterprise Data Lake, and provide integration with BI platforms and tools such as Totango and Power BI. You are passionate about lakehouse architecture and have experience using Delta Lake and bronze, silver, and gold data lake design.
Here is what one of our leaders, Cloud Services Director (Jacqueline Toepfer), has to say about the role: “As a Senior Data Engineer on our team, you will get the opportunity to showcase your expertise and make a real difference across the organization. You will be part of a truly collaborative team that is passionate about delivering quality solutions. You will be the in-house expert in the data models of multiple, disparate enterprise SaaS systems and utilize your wealth of knowledge to provide recommendations and solutions for consolidation, transformation, and integration of the disparate data sources.”
iM Responsible For…
Modeling, managing, and reporting of data stored in Azure Data Lake.
Gathering data requirements from various business units and translating these requirements into data models.
Using Python, PySpark, and system specific APIs to extract, transform, store and analyze data from a variety of systems.
Data modeling, defining data pipelines, and integrations necessary to present data in BI platforms such as Totango, or BI tools like Power BI.
Identifying and modeling all current disparate data sources and the data flows between these data sources.
Analyzing current repositories and proposing changes to data repositories and data flows to better support company objectives for the measurement of user experience and customer success.
Understanding the business needs of data integration and governance from disparate systems to drive the enhancement of the enterprise data lake.
Applying best practices to ensure the security and privacy of the data repositories.
Ensuring data repositories meet company standards for storage of PII.
Developing proficiency with the iManage product APIs for all iManage Cloud services.
iM Qualified Because I Have...
A Bachelor’s degree or higher in Computer Science or equivalent field.
3-5 years of experience working with data in a business setting.
Proficiency in data extraction, manipulation, and subsequent reporting with Spark and Python.
Experience designing data pipelines with a cloud-native mindset using Azure or AWS.
Knowledge and experience with architecting a data lake with Azure Synapse or adjacent technologies like Databricks.
Experience ingesting data from SaaS solutions and other services via API or other related technologies.
A passion to be a thought leader and work collaboratively within a team.
Commitment to understanding data requirements and delivering scalable, robust solutions that meet those requirements.
A creative mindset with a desire to explore new technologies and create innovative solutions.
Bonus Points If I Have…
Familiarity with Delta Lake.
A background with relational databases and data warehouse design using star schemas.
Experience with cloud-based data models for business solutions like Salesforce, Zendesk, and NetSuite.
Don't meet every qualification listed above? Studies show that women and people of color are less likely to apply to jobs unless they meet all qualifications. At iManage, we are committed to building a diverse and inclusive environment and encourage everyone to show up as their full authentic selves. We welcome those that come with a growth mindset and a hunger for learning; so, if you are excited about this role but your past experience doesn't align perfectly with every qualification, we encourage you to apply anyways!
iM Getting To…
Join a supportive, experienced team with an inclusive, encouraging, and vibrant culture.
Have flexible work hours that allow me to balance my ‘me time’ with my work commitments.
Collaborate in a modern open plan workspace, with a gaming area, free snacks, drinks and regular social events.
Focus on impactful work, solving complex, real challenges utilizing the latest technologies and protocols.
Own my career path with our internal development framework. Ask us more about this!
Learn new skills and earn certifications with access to unlimited courses in LinkedIn Learning.
Join an innovative, industry leading SaaS company that is continuing to grow & scale!
iManage Is Supporting Me By...
Creating an inclusive environment where I can help shape the culture not just by fitting in, but by adding to it.
Providing a market competitive salary that is applied through a consistent process, equitable for all our employees, and regularly reviewed based on industry data.
Rewarding me with an annual performance-based bonus.
Offering comprehensive Health/Vision/Dental/Life Insurance, and a 401k Retirement Savings Plan with a company match up to 4%.
Giving access to HealthJoy, a healthcare concierge service, to help me maximize my health benefits.
Granting enhanced leave for expecting parents; 20 weeks 100% paid for primary leave, and 10 weeks 100% paid for secondary leave.
Providing me with a flexible time off policy to take the time off that I need. Be it for vacation, volunteering, celebrating holidays, spending time with family, or simply taking time to recharge and reset.
Caring for my mental health and well-being with multiple company wellness days and free access to the Healthy Minds app for mindfulness, meditation and more.
About iManage…
iManage is dedicated to Making Knowledge WorkTM. Over one million professionals across 65+ countries rely on our intelligent, cloud-enabled, secure knowledge work platform to uncover and activate the knowledge that exists inside their business content and communications.
We are continuously innovating to solve the most complex professional challenges and enable better business outcomes; Our work is not always easy but it is ambitious and rewarding.
So we’re looking for people who love a challenge. People who are happiest when they’re solving problems and collaborating with the industry’s best and brightest. That’s the iManage way. It’s how we do things that might appear impossible. How we develop our employees’ strengths and unlock their potential. How we find meaning in everything we do.
Whoever you are, whatever you do, however you work. Make it mean something at iManage.
iManage provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.
Learn more at: www.imanage.com
Please see our privacy statement for more information on how we handle your personal data: https://imanage.com/privacy-policy/
#LI-LM1
#LI-Remote
V436F7WSwa",#N/A,501 to 1000 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,$100 to $500 million (USD)
"MyCare Medical Group
4.1",4.1,"Lutz, FL",Data Engineer,"Job Summary
The Data Engineer will design, develop, optimize, and maintain data architecture and pipelines that adhere to ELT principles and business goals.
Job Responsibilities:
Designs, develops, optimizes, and maintains data architecture and pipelines that adhere to ELT principles and business goals.
Solves complex data problems to deliver insights that helps business achieve its goals.
Creates data products for engineer, analyst, and data scientist team members to accelerate their productivity.
Engineers effective features for modelling in close collaboration with business analysts and stakeholders.
Leads the evaluation, implementation and deployment of emerging tools and process for analytics data engineering in order to improve productivity as a team and quality.
Partners with engineers, BI, and solutions architects to develop technical architectures for strategic enterprise projects and initiatives.
Fosters a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions.
Advises, consults, mentors, and coaches other data and analytic professionals on data standards and practices.
Develops and delivers communication and education plans on analytic data engineering capabilities, standards, and processes.
Job Qualifications:
Education and Work Experience: Bachelor’s degree in computer science, information systems, engineering, or a related field; Masters preferred.
3+ years of experience in data engineering role.
Expertise in ELT and data analysis, SQL and at least one programming language (e.g., SQL, Python).
Conceptual knowledge of data and analytics, such as dimensional modelling, reporting tools, data governance, and structured and unstructured data.
Skills: Strong understanding of agile methodologies and experience as a Data Engineer on a cross-functional agile team preferred.
Experience with designing and maintaining data warehouses and/or data lakes with technologies, such as Hadoop, Spark, or distributed databases, like Redshift and Snowflake, and experience with housing, accessing, and transforming data in a variety of relational databases.
Experience with cloud database technologies (e.g., Azure) and developing solutions on cloud computing services and infrastructure in the data and analytics space.
Experience in building data pipelines and deploying/maintaining them with best-in-class tools (e.g., Git and Jenkins).
Familiarity with the Linux operating system.
Basic familiarity with BI tools (e.g., Tableau, Power BI, Looker).
Experience working in data engineering or data architect role.
Candidate Qualities: Focuses on business agility and agile delivery with a “fail-fast, succeed early” mindset and emphasis on measurable outcomes.
Brings a high-energy and passionate outlook to the job and can influence those around them.
Able to build a sense of trust and rapport that creates a comfortable & effective workplace.
Passion for innovation and “can do” attitude.
BENEFITS
Comprehensive benefits package, including Health, Vision, Dental, and Life insurancesFSA and Life Assistance Program (EAP)
401(k) Retirement Plan
Health Advocacy, Travel Assistance, and My Secure Advantage
PTO Accrual and Holidays
#ZIP
Job Type: Full-time
Pay: $125,000.00 - $150,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Employee discount
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Referral program
Relocation assistance
Retirement plan
Tuition reimbursement
Vision insurance
Compensation package:
Performance bonus
Yearly bonus
Experience level:
3 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Lutz, FL 33558: Reliably commute or planning to relocate before starting work (Preferred)
Experience:
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person","$137,500 /yr (est.)",501 to 1000 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2018,Unknown / Non-Applicable
"Take Command Health
4.9",4.9,"Dallas, TX",Data Engineer,"Company: Take Command
Position: Data Engineer
Salary: $120,000-$130,000 plus bonus
Location: Dallas or Austin, TX
About Take Command
Take Command is a start-up on a mission to improve the healthcare system, starting with health insurance. Pragmatically speaking, we help employers reimburse employees for individual insurance instead of offering a traditional one-size-fits-all group plan. We believe this model can empower employees (when they have the right support) to be savvy healthcare consumers and have a transformative impact on the entire healthcare system.
Let’s be honest—health insurance is usually a confusing, frustrating, and even emotional experience for people. We want to fix that with a new model, great technology, and a superior user experience. We have made a great start, but we need your help to fully realize our long-term vision.
As a part of the dev team, you'll build the framework for both our clients and our team to succeed. You will be a pillar in our company as we grow and make our product the best product on the market!
In this role, you will:
Take ownership for delivering multiple features, their quality and assist team members in delivering their features
Recommend improvements to data quality, processes and code to make it easier to use, manage and maintain
Experience designing and developing data models and data migrations
Experience managing and maintaining ETL or ELT processes (Hevo, BigQuery, Looker, etc.)
Hands-on development experience with SQL, relational database design (ERDs)
Must be able to write SQL & contribute code
Be product-minded; focus not only on what needs to be built but why something needs to be built
You may be a great fit for this role if you have:
3 or more years of professional experience as a data analyst or data engineer
Cloud-Based Data Tools e.g. Hevo, BigQuery, Looker, etc.
Database technologies: SQL, RDBMS / Relational databases (Postgres, Amazon RDS / Aurora, etc.)
Nice to have experience: Python, Java, Typescript or another modern OOP language
Cloud infrastructure: AWS
Source control: Git
Working at Take Command
We’re excited to build a team and culture that reflects our values! We offer competitive pay, health benefits, and equity options to share with this position. We want you to be an owner along with us in revolutionizing healthcare.
A generously funded ICHRA for medical, dental, and vision premiums and medical expenses. You get to use our own product and we think that’s so exciting and rare!
Unlimited personal vacation in addition to regular company holidays.
401(k): 90-day eligibility for 2% match that vests immediately!
Access to LinkedIn Learning.
We have two beautiful offices conveniently located near The Hill (Dallas) and Holly Commons (Austin). The kitchen is well-stocked and we've designed the space to have lots of different areas to work!
Paid parental leave for new parents.
Flexible on where you work – we hope to see you around the office on average 3 days a week especially when white-boarding or building relationships with your colleagues, but you also have the ability to work from home or wherever you'd like when you need to get focus work done.
More About Us
We secured our Series A funding in 2021 and are thrilled to be able to expand our team. Despite being a small startup in a land of health insurance giants, we’re the recognized industry leader for what we do (health insurance reimbursements) and passionate about bringing it to market because we know we can help fix a broken system and improve our clients’ wellbeing and health outcomes.
We’ve been featured in The New York Times, The Wall Street Journal, The Dallas Morning News, and other national healthcare publications and are excited about our growth opportunities.
Take Command knows diversity and inclusion among our teammates is integral to our company’s success and growth. Our vision is to recruit, develop, and retain the best team from a diverse candidate pool.
This has mostly been about us, but we’d love to hear from you-we can’t wait to hear your story!
Take Command is an equal opportunity employer! We celebrate diversity and are committed to creating an inclusive environment for all employees.
Xrygn4NXMr","$125,000 /yr (est.)",1 to 50 Employees,Company - Private,Insurance,Insurance Agencies & Brokerages,2014,Unknown / Non-Applicable
"Intersec
4.5",4.5,Remote,Senior Data Engineer,"Architect and implement highly scalable data pipeline to streamline migration of data from source data repositories to the centralized data platform
Implement agreed upon data models (e.g., data marts) to enable faster and efficient consumption of data by downstream consumers
Conduct performance tuning to optimize data migration techniques and ensure adherence to identified service-level agreements (SLAs)
Define and implement automated validation checks to ensure data integrity between source and target
Assist in onboarding consumers with existing technology to connect to the platform
Setup predictive alert mechanism for pipeline degradation for IMF pipelines.
Enhance data quality framework for data pipelines and derived datasets
Enhance Data Audit Balance and Control mechanism for data pipelines
Enhance pipelines to be certified copies of authoritative source
Populate metadata for the Enterprise with particular focus on the Data Sources being onboarded to EDP (Technical and Business Glossaries) related to IMF Modernization Programs
Validate correct lineage for datasets
Required:
*8+ years of Databricks, Data Modeling, and/or Metadata management experience
* Familiarity with architecting and developing real and near-real-time solutions
* Bachelor's Degree in Engineering, Mathematics, computer Science, Statistics or other quantitative discipline, or 4 years equivalent professional experience
* US Citizenship and ability to gain a public trust
Preferred:
* Familiarity with IRS systems and ETL pipelines
* Ability to work independently and manage multiple tasks
* IRS MBI Clearance with IRS Laptop and Informatica Installed
Job Type: Full-time
Salary: $70.00 - $75.00 per hour
Schedule:
Monday to Friday
Application Question(s):
Are you a US/GC card holder?
How many years of Databricks, Data Modeling, and/or Metadata management experience do you have?
Work Location: Remote",$72.50 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,Unknown / Non-Applicable
"Cureatr
4.1",4.1,Remote,Data Engineer,"About Us
We are a thriving, medication management solutions company, headquartered in Manhattan, but largely virtually distributed, committed to tackling the $528B suboptimal medication management problem. Patient medication information is often incomplete for healthcare providers and systems – it’s our mission to fill in the gaps. We provide real-time, universal access to accurate medication data for over 265 million patients and high-quality interventions by board certified telepharmacists. Through our solutions, healthcare providers can reduce preventable hospitalizations, avoid readmissions, and ultimately, improve patient outcomes. We’re already used by dozens of hospital systems, with tens of thousands of clinicians and supporting staff using us to care for millions of patients.
Here at Cureatr, we are driven by a core set of values and strive to incorporate them into everything we do. We care deeply about our users, patients, and employees, and we aim to live and operate compassionately. We take pride in creating a diverse, inclusive, and equitable company culture, where our open and honest approach to work fosters innovation and encourages employees to achieve professional and personal growth.

The Role
We’re looking for a Data Engineer to join our growing Engineering team. Use the power of healthcare data to find insights that will demonstrate the value Cureatr is delivering to our customers, help improve patient outcomes, and expand the scope of Cureatr’s mission. As a Data Engineer at Cureatr, you will be responsible for designing, implementing and maintaining our data infrastructure, pipelines, automation, and ETL processes. You will work closely with our Data Science and Analytics teams to ensure that our data is clean, accurate, and readily available for analysis. You will also be responsible for ensuring that our data meets industry standards for security, privacy, and compliance.
This is a remote, work from home position, but candidates must be located within the U.S. Ideal candidates must be able to travel to our New York City office, or TBD company-wide offsite locations, up to 4x per year.

You’ll be part of a team working to solve problems like:
Increasing the number of patients who receive the opportunity to do a medication reconciliation, which can help keep them out of the hospital.
Streamlining and automating processes so that our pharmacists can serve as many patients as possible.
Predicting patient events likely to occur so that our clinical team can adequately prevent them before it’s too late.
Helping our pharmacists make the most of data and insights that will help them be as effective as possible in serving patients.
Evaluating the effectiveness of our clinical services impact at the population level.

What You’ll Do:
Design, implement, and maintain our data infrastructure, pipelines, automation, and ETL processes to support our data needs.
Build and maintain scalable data models and databases.
Collaborate with Data Science and Analytics teams to understand their data needs and provide data solutions accordingly.
Ensure data quality and integrity by implementing data validation and cleansing processes.
Ensure data security and compliance with healthcare regulations.
Monitor and optimize data performance and scalability.
Design and implement data governance policies and procedures.
Evaluate and implement new data technologies and tools to improve our data capabilities.

Your Skills:
3+ years of experience as a Data Engineer or similar role.
Strong programming skills in languages such as Python and SQL.
Experience using git to track changes.
Experience with data modeling and database design.
Experience with the “modern data stack” preferred
Experience with ETL and data pipeline development.
Knowledge of healthcare data and related standards (e.g., HL7, FHIR) is a plus.
Familiarity with AWS.
Strong problem-solving and analytical skills.
Excellent communication and collaboration skills.

Technology Stack:
Python
RedShift
dbt
Dagster
MongoDB
Looker
AWS

Educational Requirement: Bachelor’s Degree or relevant work experience

Salary: $110,000 - $140,000 plus full benefits

Why work with us?
Make a measurable impact on patients’ lives
Be part of a company dedicated to living its core values:
Empathy
Honesty
Integrity
Impact
Fun
Equity in Healthcare
Work in the company without micromanagement
Share your vision and make an impact on the product
Opportunities for career growth
Remote work
Competitive salary
Premium benefits (Health, Dental, Vision, 401k, unlimited PTO, more)

Equal Employment Opportunity Policy
Cureatr is an equal opportunity/affirmative action employer. All qualified applicants will receive consideration for employment without regard to sex, pregnancy, race, religion or religious creed, color, gender, gender identity, gender expression, national origin, ancestry, physical or mental disability, medical condition, genetic information, marital status, registered domestic partner status, age, sexual orientation, military or veteran status, protected veteran status, or any other basis protected by federal, state, local law, ordinance, or regulation and will not be discriminated against on these bases
Disclaimer: we are aware of a scam targeting applicants using a cureatrcareers.com email address. Please block and report this sender, as they are not affiliated with Cureatr. If you have any questions, please contact us at jobs@cureatr.com. Thank you!
uBajh3OfeT","$125,000 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2012,Less than $1 million (USD)
"SECURE RPO
4.3",4.3,Manhattan,Senior data engineer,"Must have skills:
8+ years of experience building high performance scalable enterpris`e analytics or data centric solutions
8+ or 5+ years of experience in the design and build of data extraction, transformation, and loading processes by writing custom data pipelines
At least 5 years of experience implementing complex ETL pipelines preferably in connection with Glue and/or Spark
Exceptional coding and design skills in Python or Java/Scala
Hands-on experience with AWS (i.e. Glue, Aurora Postgres, Lambda, EMR, EKS, Redshift, etc.)
Experience with visualization tools like QuickSight, PowerBI, Looker or Tableau
Experience with Talend (ETL) is a big plus
Roles and Responsibility:
Drive a high impact and high visibility project that enables data availability, encompasses data analytics, machine learning, and petabyte scale datasets, and provides reliable and timely access to thousands of data sources
Design, architect and support systems for collecting, storing, and analyzing data at scale
Recommend improvements and modifications on new and existing data and ETL pipelines. Create optimal data pipeline architecture and systems using Apache Airflow
Create data analytics for d ata scientists to innovate, build and optimize our ecosystem
Assemble large, complex data sets that meet functional and non-functional business requirements
Be responsible for ingesting data into our data lake and providing frameworks and services for operating on that data including the use of Spark
Analyze, debug and correct issues with data pipelines
Operate on or build solution required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, AWS and Spark technologies
Job Type: Full-time
Salary: $56.80 - $80.16 per hour
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Informatica: 7 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: On the road",$68.48 /hr (est.),1 to 50 Employees,Unknown,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Atalan Tech,#N/A,Remote,Data Engineer,"Atalan Tech is a start-up with a mission to mitigate the impact of clinician burnout and turnover in our nation’s health system. We are expanding our team and looking to hire the critical role of Data Engineer. The Data Engineer will work closely with the Chief Technology Officer, Chief Scientific Officer (UCLA Ph.D.), Chief Product Officer (Oxford Ph.D.), and Chief Executive Officer to deliver actionable insights from data in a variety of situations.
We are a small, close-knit team. This is an excellent opportunity for someone to join a mission-oriented, early-stage company at the ground floor and grow with us. There will be plenty of room for growth and creativity, and opportunities to tackle challenges that present one of the toughest problems in our society.
Requirements
Strong hands-on experience of designing, developing and managing data pipelines
Strong coding skills in Python (required) and SQL (required) and R (strongly preferred)
Experience building and maintaining data pipelines on AWS using EC2, EBS, Redshift, Postgres, S3, Data Pipeline, Kinesis, Lambda, etc.
Thorough understanding and experience with task management tools (Airflow, NiFi, etc.)
Knowledge of data management best practices for ETL, data governance, data warehousing, data modeling and data science
Basic knowledge of Linux (Redhat preferred) administration
Minimum B.S in Computer Science, software engineering or related field (or equivalent in related professional experience)
Strong communication skills
Strong problem-solving/critical thinking skills
Ability to work in a fast-paced, dynamic start-up environment
Some knowledge of healthcare, health tech, or healthcare data analytics (preferred)
Typical Tasks and Responsibilities include:
Working closely with CTO and CSO to build data pipelines for machine learning
Assisting the CPO in managing and supporting analysis of enterprise data to drive better products and solutions
Managing company’s data infrastructure on AWS (Postgres/RDS, Redshift, S3, etc.)
Synthesizing multiple data sources, such as internal medical records and HR data
Integrating with client systems including EHR (Epic, Cerner), HRIS, flat files, etc.
Working closely with other key members on other projects
Developing and designing data security standards and policies in line with industry best practices
About Us:
Atalan Tech’s mission is to improve the wellness of clinicians and their patients. We blend labor economics, machine learning, and clinical psychology with a team from Harvard, Yale, and UCLA. Atalan Tech is looking to contribute innovative and sustainable solutions to one of the most pressing problems in the healthcare industry. Building upon our proof-of-concept success, we have recently secured multiple engagements with nationwide healthcare organizations and have successfully raised institutional funding to expand our reach.
Contact Us:
If you’re interested in this position, please send us your CV/resume and cover letter.
Job Type: Full-time
Pay: $108,448.20 - $130,604.27 per year
Experience level:
2 years
Schedule:
Monday to Friday
Application Question(s):
Do you know the role is full time?
Can you start within the next month?
What are your salary expectations?
Have you held the title of a data engineer? If not, what experience do you have as a data engineer? Please be very specific.
Have you released Python code to a production environment in the last year?
Experience:
Data analytics: 2 years (Required)
Work Location: Remote","$119,526 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Radiant System,#N/A,"Pleasanton, CA",Data Engineer,"First Priority: Pleasanton, CA –
Second Priority for a superstar: Plano, TX
Experience in developing and deploying data pipelines, preferably in the Cloud
Experience working with Spark, Databricks, and other relevant technologies in Microsoft Azure cloud
Experience in productionizing and deploying Big Data platforms and applications
Expert in writing SQL statements
Job Type: Contract
Application Question(s):
Are you willing to work on W2?
Work Location: In person",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Integrated Technology Strategies, Inc.
4.0",4.0,"New York, NY",Data Engineer,"Integrated Technology Strategies is a provider of information technology consulting. Digital transformation, improving business performance, developing strategies and enhancing value is at the core of what we are.

Integrated Technology Strategies is looking to hire a Data Engineer to be a part of our Consulting team.

We are looking for someone passionate about data is focused on developing the platform for critical data products, including real time business metrics and analytics capabilities. The role requires supporting and collaborating with groups including Data Analytics/BI/Product, as well as our core backend API team. The individual will not be afraid to think out of the box and will play a key role in technical decision making. We are highly focused on giving ownership and responsibility to autonomous teams, using the right tools for the job, and building flexible architectures.

Responsibility:
Continue to evolve the internal Reporting and Analytics platform on top of Snowflake on AWS infrastructure.
Experience in Architect, design and implementing scalable ETL and data processing systems to handle the big data ecosystem including data collection, processing, ETL and Data warehouse.
Build soft real time capabilities and insight into product metrics to help product managers and BI/Analytics understand and optimize product features and guide product decisions.
Participate and contribute to the capabilities and engineering priorities across the organization.
Contribute to the codebase and participate in code review.
Build analytics tools that utilize the data pipeline to
provide actionable insights into operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product,
Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Reporting to the Senior Director, Software Engineering, you’ll be responsible for overseeing engineering product quality and delivery and setting and overseeing technical standards for teams who are working on everything from customer-facing applications.

Skill Requirements:
Solid understanding of real time data processing with Kafka, Spark and Flink and batch data processing frameworks on EMR and Snowflake.
Passion for building world-class data platforms that support a global customer base
Solid engineering background and understanding of programming languages such as Python, Java or equivalent
5+ years of progressive experience in data infrastructure development, with a track record of successful high-quality deliveries
Experience of working in an agile environment and embracing engineering best practices
Ability to apply both technical competence and interpersonal skills to achieve business outcomes
High emotional intelligence, sound temperament, and professional attitude
Strong understanding of SQL, experience with key databases such as Snowflake, MS-SQL and Postgres
Knowledge of the internals of how database systems work to design models for varied use cases.
Experience with CI and CD in an AWS environment with Terraforms
Experience with key Data technologies, such as Sqoop. Kafka will be a plus
Proven experience in building secure data platforms
Bachelor’s degree in Computer Science or equivalent","$110,128 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$5 to $25 million (USD)
"Egrove Systems Corporation
4.4",4.4,"Dallas, TX",Data Engineer Flink,"Java, Bigdata, Azure & Flink
Flink Sr. Developer, who has implemented and dealt with failure scenarios of processing data through Flink.
A senior who can help his Flink development team, guiding and helping them implement custom solutions through Flink.
Seasoned Java developer who knows about all aspects of SDLC.
Worked on integrations of other technologies with Flink, eg: Kafka, MongoDB, etc
Azure experience in the areas of Messaging, Data processing, preferably on Flink or on Databricks.
Job Type: Contract
Salary: From $51.83 per hour
Compensation package:
Monthly bonus
Experience level:
11+ years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
Work Location: On the road
Speak with the employer
+91 732)-860-7733",$51.83 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Internet & Web Services,#N/A,Unknown / Non-Applicable
"CDNetworks Inc.
2.9",2.9,"Monrovia, CA",Jr. Data Center Engineer,"CDNetworks is one of the top leading CDN & Edge Service Providers with global offices in Korea, Japan, Singapore, Malaysia, China, Russia, London, and Canada. We focus on delivering integrated cloud and edge computing solutions with unparalleled speed, ultra-low latency, rigorous security, and reliability so that our clients can focus on what’s most important – growing their business.
The Jr. Data Center Engineer is a Full-Time position based in Monrovia, CA. Job is performed indoors in a Data Center or Warehouse environment. 75% Travel required for this position.
Job Responsibilities
Ensure all incidents are logged and resolved, gather all relevant data, and ensure all incidents and tasks follow the appropriate procedures.
Support data center activities and work closely with our system and network team to complete tasks/projects.
First responder to all alerts and problem reports while managing communications between departments and handling crisis documentation and dissemination after the fact.
Utilize internal systems such as JIRA/Wiki to manage project plans and progress.
Performing general system administration duties including OS patching and upgrades, batch job monitoring, system and hardware diagnostics, and other activities to ensure optimal health and performance of all systems as required.
Resolve complex problems related to Server and H/W areas.
Assisting/working closely with Network, System Engineers to configure customer requirements.
Physical deployment of network devices, servers, cables, etc.
Assembling/dissembling server hardware for deployment and OS installation and network equipment testing.
Maintain existing department and system documentation (update workflow, process, training documentation).
Other duties as assigned.
Abilities Required
Good verbal and written communication skills, and ability to work independently with minimum instruction.
Basic degree of mentorship, training, and direction team members skills.
Knowledge of IDC industry.
Intermediate degree of analytical and project management skills.
Other Features of Job
Job is performed indoors in a Data Center or Warehouse environment.
Language Skills
Excellent communication skills (English) – written and verbal. Bilingual Chinese or Korean is a plus!!
Job Type: Full-time
Salary: $20.00 - $25.00 per hour
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Health insurance
Life insurance
Paid time off
Professional development assistance
Referral program
Vision insurance
Schedule:
10 hour shift
8 hour shift
Evening shift
Monday to Friday
Night shift
On call
Overtime
Supplemental pay types:
Bonus pay
Ability to commute/relocate:
Monrovia, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Computer networking: 1 year (Preferred)
Shift availability:
Overnight Shift (Required)
Night Shift (Required)
Day Shift (Required)
Willingness to travel:
75% (Preferred)
Work Location: In person",$22.50 /hr (est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2000,$10+ billion (USD)
"Integration Developer Network LLC
4.9",4.9,Remote,Data Engineer with PL/SQL Background,"Requirements:
Recent PL/SQL development work experience with Oracle 19.
Must have working experience with tools like Airflow, Git.
Must have python coding experience and REST API development experience.
Plus would be experience with microservices, docker, Kubernetes.
Plus would be experience with accounting and financial data
Experience in event driven and cloud based architectures and working knowledge of containerization and streaming technologies.
Detailed work experience on Oracle, SingleStore or IBM I DB2 database, SQL & PL/SQL, including ANSI SQL.
Experience in database analysis, development and maintenance of business applications specific to oil & gas back-office operations. Expertise in various ETL tools such as SQL*Loader, Equalum, Python scripts.
Familiarity with No-SQL databases and pipelines (Elasticsearch, MongoDB).
2+ years’ experience in the design, implementation and support of full life-cycle data engineering projects.
Bachelor's degree in Computer Science, Engineering or other equivalent degree, with 5+ years data engineering experience or Master's degree in Computer Science, Engineering or other equivalent degree with 2+ years data engineering preferred.
High school diploma or GED is required.
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Experience level:
9 years
Schedule:
8 hour shift
Experience:
Python: 8 years (Required)
PL/SQL: 5 years (Required)
ETL: 5 years (Required)
Work Location: Remote",$62.50 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2011,$1 to $5 million (USD)
DocsInk LLC,#N/A,"Wrightsville Beach, NC",Data Engineer,"DocsInk is on a mission to provide a collaborative and exciting alternative to the traditional healthcare model. Through the complete virtualization of healthcare organizations, we are equipping providers with the ability to deliver better patient care with speed, efficiency, and accuracy. With the widespread focus on providing value-based care, DocsInk is making it possible to provide superior care while also maximizing reimbursements. In today’s environment, shifting the way we approach healthcare is essential and DocsInk is on the cutting edge of where the industry is headed. We are excited to be growing our team and are looking for driven individuals with a passion for making a difference in the healthcare industry.

As our Data Engineer, you will be in the driver’s seat as our team creates elegant and forward-thinking software solutions for our healthcare customers. You will work with fellow developers, designers, training & support, quality assurance, and management in a fast-paced environment to take our products to the next level and exceed expectations. We are looking for someone who is incredibly talented to do what most cannot and yet humble enough to easily work with peers, accept guidance from more experienced staff, and mentor more junior staff. If designing and developing complex integration efforts is what gets you out of bed each morning, then we want you to help us reshape healthcare.

People are our most important asset and the number one reason we all love working at DocsInk. As a team, we value accountability, transparency and collaboration. We have agile teams, with clear, outcomes-focused goals. Delivering innovative solutions and superior service are at the forefront of what we do, and the collective ideas of the team allow us to continue to excel in a fun and energetic environment.

Professional Requirements and Responsibilities:

Role / Responsibilities:

Implement robust ETL solutions that integrate healthcare data feeds using tools such as Apache NiFi, NextGen Connect (JavaScript), PostgreSQL, MySQL, and Linux shell scripting
Configuring and managing deployments to AWS infrastructure consisting of both traditional servers and serverless components. CI/CD tools include Bitbucket and AWS Code Deploy
Building direct interfaces with hospitals and EMR vendors utilizing industry standards such as HL7 2.x, C-CDA, and FHIR as well as implementing interfaces that consume formats such as XML, JSON, and delimited files
Utilizing APIs and web services to permit bi-directional flows of data to and from our partner organizations
Monitoring operational ETL processes, triaging and resolving issues as they arise to meet SLAs
Writing PL/SQL functions and packages to automate workflows in a Postgres environment linked with web technologies
Maintaining data standards (including adherence to HIPAA), documenting work, and championing process improvements
Work as a project team member to jointly deliver high-quality business solutions consistent with project objectives and constraints

Knowledge and Skills Required:

4+ years experience with HL7 2.x
2+ years experience with Apache Nifi
2+ years experience with NextGen Connect
Advanced knowledge of PostgreSQL, MySQL, AWS Code Deploy, HL7 2.x, C-CDA, FHIR, and HIPAA data compliance

Qualifications and Other Attributes Required:

Degree in Computer Science or equivalent field, or equivalent work experience
Highly self-motivated team player who takes initiative
Strong sense of personal accountability; contributes beyond job role and responsibilities
Excellent problem-solving skills; meticulous & methodical","$85,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"N9 it solutions
4.6",4.6,Remote,Data Engineer,"Job title: Data Engineer
Visa's: CPT, OPT-EAD, H1B transfer
Employment: W2 position ( should be ok with Marketing)
Location: Hybrid Or Remote
(Authorized to work anywhere in the USA and for only those who are staying in the USA)
It's a long term project
Job description
Data Engineer (primary technologies: Azure Data Factory, Synapse, Synapse Pipelines, ADLS Gen 2, understanding of DataWarehouse concepts, ELT, Azure DevOps, Azure resource groups)
Develop architectural strategies for data modeling, design and implementation to meet stated requirements for metadata management, operational data stores and Extract Transform Load environments
Work with business leaders and teams to collect and translate information requirements into data to develop data-centric solutions
Apply industry-accepted data architecture principles and standards for modeling, stored procedures, replication, regulations, and security, among others, to meet technical and business goals.
Work to streamline data flows and models; improve consistency, quality, accessibility, and security; unify data architecture; remove unnecessary costs; and, optimize database activity across company needs.
Analyze and understand Data sources & APIs
Design and Develop methods to connect & collect data from different data sources
Design and Develop methods to filter/cleanse the data
Work closely with Data Scientists to ensure the source data is aggregated and cleansed
Work with Cloud and Data architects to define robust architecture in cloud setup pipelines and workflows
Benefits:
H1B and GC filling
Free training and Placement
E-verified
On-job technical support
Guesthouse facilities are also available
Skill Enhancement
Opportunity to work with Fortune 500 Companies
Job Type: Full-time
Salary: $40.26 - $56.00 per hour
Experience level:
6 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote
Speak with the employer
+91",$48.13 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2016,$5 to $25 million (USD)
Staff Bees Solutions,#N/A,"Dallas, TX",Data Engineer,"We are looking for OPT/CPT individuals, Helping Them to Train, Providing knowledge, and Placing Them in Fortune companies with the help of our Direct Clients in Big Data, Machine Learning, And Data Engineering Suitable Positions. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross-functional teams.
Responsibilities for Data Engineer
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics..
Qualifications for Data Engineer
An individual who has valid visa, Opt/Cpt are Applicable
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Strong project management and organizational skills.
Job Types: Full-time, Part-time, Contract
Salary: $70,000.00 - $80,000.00 per year
Benefits:
Employee assistance program
Health insurance
Professional development assistance
Relocation assistance
Compensation package:
Yearly pay
Experience level:
1 year
No experience needed
Under 1 year
Schedule:
10 hour shift
4 hour shift
8 hour shift
Monday to Friday
Ability to commute/relocate:
Dallas, TX 75243: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$75,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Quadrant Resource
4.1",4.1,Remote,Data Engineer,"Job Description:
4-6 years of experience as a Data Engineer in the Snowflake-Fivetran-dbt stack
• Possess a solid understanding of end to end Data ingestion, ETL, data analysis, Reporting processes
• Expert in ETL development using Fivetran and experienced in implementing data pipelines for Snowflake Cloud DWH
•Experienced in using dbt for data transformation
•Expertise on Tableau/Metabase for building reports/dashboards
•Expert in troubleshooting and resolving ETL issues, data load failures/problems and transformation translation problems
•Excellent communicator with the ability to work effectively with distributed teams
Job Type: Full-time
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Data Engineer: 9 years (Required)
Fivetran: 8 years (Required)
DBT: 7 years (Required)
Snowflake: 6 years (Required)
Work Location: Remote",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,2004,Unknown / Non-Applicable
OpenSecrets.org,#N/A,Remote,Snowflake Data Engineer,"About us
Our goal is to is to track the flow of money in American politics and provide the data and analysis to strengthen democracy. Our vision is for Americans to use this knowledge to create a more vibrant, representative and accountable democracy..
Our work environment includes:
Modern office setting
Work-from-home days
Wellness programs
Data Engineer
OpenSecrets is seeking to add a new member to our data team. The Data Engineer will be responsible for helping build and maintain our data systems.
Responsibilities include:
Implementing methods to improve data reliability and quality
Analyzing raw data and implementing that data into our system using data pipelines
Troubleshooting data errors and pipeline issues
REQUIRED QUALIFICATIONS:
Experience working with and analyzing large datasets, equivalent to 2-3 years in a similar position
Proven ability to build complex data pipelines using Snowpipe and SSIS
The ability to work with diverse populations
Experience with MSSQL Server and Snowflake
Advanced knowledge of ANSI SQL
PREFERRED QUALIFICATIONS:
Excellent written and verbal communication skills.
The ability to manage multiple priorities and projects both independently and as a member of a team
Snowflake SnowPro Data Engineer certification
Experience with Apache Spark
Experience with machine learning matching models
Knowledge of and interest in American government and money in politics.
SALARY AND BENEFITS:
We are a fully remote organization and this person can be based anywhere in the U.S.
OpenSecrets offers an excellent benefits package, including full medical, dental, 403(b) match, generous vacation and sick leave, and all federal holidays paid.
TO APPLY:
Please visit this link: https://airtable.com/shrGe2bugeapKVr8a
People of diverse backgrounds are strongly encouraged to apply. OpenSecrets is an equal opportunity employer committed to creating a diverse and inclusive organizational culture. We do not discriminate against candidates and employees because of their disability, sex, race, gender identity, sexual orientation, religion, national origin, age, veteran status, or any other protected status under the law.
ABOUT US: OpenSecrets is the nation's premier research group tracking money in U.S. politics and its effect on elections and public policy. Our mission is to track the flow of money in American politics and provide the data and analysis to strengthen democracy. Our vision is for Americans to use this knowledge to create a more vibrant, representative and accountable democracy.
Job Type: Full-time
Pay: $83,874.82 - $100,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Retirement plan
Vision insurance
Compensation package:
Yearly pay
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Experience:
SQL: 3 years (Required)
Snowflake: 1 year (Preferred)
ETL: 2 years (Required)
Work Location: Remote","$91,937 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Gridiron IT
4.5",4.5,Remote,Data Engineer,"GridironIT is seeking a Data Engineer.
Responsibilities:
We are looking for seasoned Data Engineer to work with our team and our clients to develop enterprise grade data platforms, services, and pipelines. We are looking for a more than just a ""Data Engineer"", but a technologist with excellent communication and customer service skills and a passion for data and problem solving.
Lead and architect migration of data environments with performance and reliability.
Assess and understand the ETL jobs, workflows, BI tools, and reports
Address technical inquiries concerning customization, integration, enterprise architecture and general feature / functionality of data products
Experience in crafting database / data warehouse solutions in cloud (Preferably AWS. Alternatively Azure, GCP).
Key must have skill sets – Python, AWS
Support an Agile software development lifecycle
You will contribute to the growth of our Data Exploitation Practice!
Qualifications:
US Citizen Only
Ability to hold a position of public trust with the US government.
8+ years industry experience coding commercial software and a passion for solving complex problems.
8+ years direct experience in Data Engineering with experience in tools such as:Big data tools: Hadoop, Spark, Kafka, etc.
Relational SQL and NoSQL databases, including Postgres and Cassandra
Data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
AWS cloud services: EC2, EMR, RDS, Redshift
Data streaming systems: Storm, Spark-Streaming, etc.
Search tools: Solr, Lucene, Elasticsearch
Object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Amazon S3, Athena, Redshift Spectrum, AWS Glue, AWS Glue Catalog, AWS Functions, and Amazon EC2 with SQL Server Developer
Advanced working SQL knowledge and experience working with relational databases, query authoring and optimization (SQL) as well as working familiarity with a variety of databases.
Experience with message queuing, stream processing, and highly scalable ‘big data’ data stores.
Experience manipulating, processing, and extracting value from large, disconnected datasets.
Experience manipulating structured and unstructured data for analysis
Experience constructing complex queries to analyze results using databases or in a data processing development environment
Experience with data modeling tools and process
Experience architecting data systems (transactional and warehouses)
Experience aggregating results and/or compiling information for reporting from multiple datasets
Experience working in an Agile environment
Experience supporting project teams of developers and data scientists who build web-based interfaces, dashboards, reports, and analytics/machine learning models
Experience with Informix and Data Stage
Job Type: Full-time
Pay: $140,000.00 - $150,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Hourly pay
Yearly pay
Experience level:
10 years
11+ years
7 years
8 years
9 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Due to the nature of the role, US Citizenship is required. Do you possess US Citizenship?
Education:
Bachelor's (Preferred)
Experience:
Data Engineering: 7 years (Required)
SQL: 7 years (Required)
AWS: 4 years (Required)
Big data: 5 years (Preferred)
NoSQL: 5 years (Preferred)
Work Location: Remote","$145,000 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable
"Iron Bow Technologies
3.5",3.5,"Washington, DC",Sr. Data Center Engineer,"Iron Bow Technologies is seeking a Sr. Data Center Engineer to support our Federal customer, The Department of Justice (DOJ), Criminal Division (CRM), in Washington, DC. The selected candidate will support our flexible and scalable solution with the DOJ CRM to extend cloud-native features to existing legacy and next-generation applications while also future proofing the infrastructure to easily accommodate future requirements, such as transitioning to a public cloud.
This position is hybrid and requires 2 days a week onsite at the customer location in NW Washington, DC and support at the customer data center as needed in Sterling, VA.
Essential Duties and Responsibilities:
Assess and provide VMware solutions to include vCenter, vSphere, SRM, ESXi
Primary storage solutions and data file storage management, NAS file storage solutions
Data Center blade servers and rack servers
Server virtualization
Experience:
10 years of related experience providing system administration support in a virtualized environment, including at least 6 years of VMWare solutioning, including vCenter, vSphere, SRM, and ESXi
Data Center operations experience, with SAN and NAS
Experience supporting primary storage solutions and data file storage management, NAS file storage solutions
Hands on experience with Data Center blade and rack servers
Ability to work staggered shift hours when needed to support critical coverage and/or time-sensitive needs
Be a member of our Team providing coordinated 24x7 IaaS on-call support
U.S. Citizen
Currently hold or obtain/maintain a Public Trust High clearance
Education:
BS in Computer Science, Information Science, or other relevant discipline is preferred – additional years of experience may be considered in lieu of the degree
Preferred Certifications:
VMware Certified Professional
Primary storage certifications such as HPE, Dell-EMC, NetApp, or Pure Storage
VCP-DCV
Travel Requirements:
No travel
PRINCIPALS ONLY. NO AGENCIES PLEASE. MUST BE A US CITIZEN ABLE TO OBTAIN A PUBLIC TRUST SECURITY CLEARANCE.
Iron Bow Technologies is a leading IT solution provider dedicated to successfully transforming technology investments into business capabilities for government, commercial and healthcare clients. Our depth of technical expertise, global reach and strategic partnerships with industry leaders as well as disruptive technology partners uniquely positions Iron Bow to target the right solution to ensure your successful business outcome. We partner with our clients from planning and implementation through ongoing maintenance and management to deliver solutions that are strong, flexible and on target .
Iron Bow Technologies is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.
#LI-TM1",#N/A,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,1983,$500 million to $1 billion (USD)
"Kastech Software Solutions Group
4.0",4.0,"Dallas, TX",Data Engineer,"In KastechSSG we are are seeking for a Cloud DB Platform Engineer with Data Enablement Services experience ($50/hour), to work in a Hybrid Schedule (3 days on-site, 2 days home office) for one of our partner companies!
Hybrid Schedule info: onsite 3 days per week from list of locations: Charlotte NC, Dallas TX, Chandler AZ, Newark, NJ - Must be local to one of these locations.
** Contract would be $50 per hour on W2 (18 months). American Citizens or GC Holders Only**
3-5 years experience in:
Terraform, Azure/GCP, DevOps Engineering, ETL, Client/AI, Data lake, Data architecture.
GCP Key Services Experience (ie Big Query, Composer, Vertex AI, DataProc, Data Catalog, Notebooks).
Azure Key Service Experience (ie Synapse, Purview, Data Lake, Logic Apps, Cognitive Services).
Job Type: Full-time
Salary: $50.00 per hour
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you American? If not, please specify your visa type
Would you be willing to work under W2?
Experience:
Terraform: 3 years (Preferred)
Azure Data Lake: 3 years (Preferred)
GCP Key Service Exp: 3 years (Preferred)
DevOps: 3 years (Preferred)
Work Location: In person",$50.00 /hr (est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2008,$25 to $100 million (USD)
"R1 RCM, Inc.
3.4",3.4,Remote,Data Engineer III,"The Software Engineer III serves as a Data Engineer focused on building next generation of healthcare products and services. As part of a collaborative, motivated Agile-based team, the Software Engineer III is responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.
Responsibilities
Create and maintain optimal data pipeline architecture to assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Work with stakeholders to optimize the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using ‘big data’ technologies.
Work with stakeholders including the Product, Data and Engineering teams to assist with data-related technical issues and support their data infrastructure needs.
Adhere to and ensure engineering best practices using modern SDLC that enables CI/CD and favors automation, auditability, automated testing, infrastructure, and policy as code.
Ability to collaborate effectively with other architects and engineers delivering end to end, high-performing, highly reliable, scalable, and operable solutions.
Required Qualifications
Bachelor’s degree in Computer Science, Engineering, or a related technical field; or equivalent experience.
3+ years of experience delivering data engineering applications and services.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets .
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Experience in any one of the big data technologies (Hadoop, MapReduce , Spark, Data Lake etc. )
Experience in any one of the stream-processing systems ( Kafka, Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages : Python, Java, Scala, etc.
Understanding of modern software development including version control, unit testing, and continuous integration and deployment.
Strong communication skills, with proven ability to present complex ideas and to document concisely.
Preferred Qualifications
Working knowledge of Cloud providers and services such as Amazon AWS or Microsoft Azure.
Experience with relational SQL and NoSQL databases, including MongoDB and ElasticSearch.
Knowledge of healthcare data standards such as HL7, FHIR, EDI X12.
Knowledge of the healthcare revenue cycle, EMRs, practice management systems.
Working in an evolving healthcare setting, we use our shared expertise to deliver innovative solutions. Our fast-growing team has opportunities to learn and grow through rewarding interactions, collaboration and the freedom to explore professional interests.

Our associates are given valuable opportunities to contribute, to innovate and create meaningful work that makes an impact in the communities we serve around the world. We also offer a culture of excellence that drives customer success and improves patient care. We believe in giving back to the community and offer a competitive benefits package including:
Comprehensive Medical, Dental, Vision & RX Coverage
Paid Time Off, Volunteer Time & Holidays
401K with Company Match
Company-Paid Life Insurance, Short-Term Disability & Long-Term Disability
Tuition Reimbursement
Parental Leave
R1 RCM Inc. (“the Company”) is dedicated to the fundamentals of equal employment opportunity. The Company’s employment practices , including those regarding recruitment, hiring, assignment, promotion, compensation, benefits, training, discipline, and termination shall not be based on any person’s age, color, national origin, citizenship status, physical or mental disability, medical condition, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status or any other characteristic protected by federal, state or local law. Furthermore, the Company is dedicated to providing a workplace free from harassment based on any of the foregoing protected categories.
If you have a disability and require a reasonable accommodation to complete any part of the job application process, please contact us at 312-496-7709 for assistance.
CA PRIVACY NOTICE: California resident job applicants can learn more about their privacy rights California Consent",#N/A,10000+ Employees,Company - Public,Healthcare,Health Care Services & Hospitals,2003,Unknown / Non-Applicable
"DiNi Communications, Inc.
3.0",3.0,"New York, NY",Data Center Engineer,"DiNi Communications, Inc. is seeking a Data Center Engineer. This person needs to be a team player with exceptional communication skills. Maintaining the confidentiality of the client's information is of utmost importance.
Required Education and Experience
· At least 8 years of experience as a large data center technician
· Knowledge of Networking,
· Ability to lift equipment at least (50lbs) Rack and Stack (Network Switches, Servers, Storage,
Appliances)
· Knowledge of structured Cabling in a Data Center.
· General knowledge of Operating Systems, (Windows, Unix).
· General knowledge of networking in N Tier Environment (App, Web, DMZ,).
· Level 1 and Level 2 troubleshooting in a N Tier Environment.
· Ability to follow and execute commands to start up and shutdown systems.
· Physically travel to and be available at site B to perform technical changes that require a
person at Site B.
· Performing backup and restore activities.
· Asset Management.
· Maintaining document.
· Work with vendor on circuit issues, (WAN and Internet).
· Cable Patch the equipment in the environment.
· Participate in Site B fall-over testing
Preferred Network Engineering Experience
· Knowledge of N Tier networking.
· Certification in Networking (CISCO).
· Certification in various Operating systems (Linux and Windows), Storage, Messaging
(Microsoft).
· Certification in IT Security.
· Knowledge of Databases.
· Experience working with CISCO, EMC, Symantec, HP, Oracle, Bluecoat, MacAfee, and other vendors' equipment and software in a networked environment.
Responsibilities include but are not limited to:
Adding equipment to support agency applications in site B Environment,
Coordinating Site B testing with Site A technical resources.
Monitoring the B environment.
Rack and Stack new and replacement equipment
Troubleshooting and resolving incidents and problems in the
Physically travel to and be available at Site B to perform technical changes that require a person at Site B.
Upgrading and expanding the environment
Performing backup and restore activities
Manage Site B assets
Maintaining document
Cable Patch the equipment in the environment
Participate in Site B fall-over testing.
Job Type: Full-time
Pay: $60.00 - $70.00 per hour
Schedule:
Monday to Friday
Experience:
large data center technician: 8 years (Required)
Networking in N Tier Environment (App, Web, DMZ,): 3 years (Required)
Ability to lift equipment at least (50lbs): 1 year (Required)
structured Cabling in a Data Center: 3 years (Required)
Level 1 and 2 troubleshooting in a N Tier Environment: 3 years (Required)
Performing backup and restore activities: 3 years (Required)
Cable Patch the equipment: 3 years (Required)
Certification in Networking (CISCO): 1 year (Preferred)
Certification in IT Security: 3 years (Preferred)
CISCO, EMC, Symantec, HP, Oracle, Bluecoat, MacAfee: 3 years (Preferred)
Work Location: In person",$65.00 /hr (est.),Unknown,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"Virtualan Software LLC
4.5",4.5,"Atlanta, GA","Sr. Data Engineer (Python,Spark & Databricks)","Role : Senior Data Engineer (Python, Spark & Databricks)
Location : Hybrid (Atlanta, GA)
Pay: 60-70$ on C2C / 1099
RESPONSIBILITIES:
Create or modify the conceptual, logical and physical data models.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of sources like Hadoop, Spark, AWS Lambda, etc.
Lead and/or mentor a small team of data engineers.
Design, develop, test, deploy, maintain and improve data integration pipeline.
Develop pipeline objects using Apache Spark , PySpark and/or Python.
Design and develop data pipeline architectures using Spark and related AWS Services.
Communicate effectively with client leadership and business stakeholders.
Participate in proposal and/or SOW development.
REQUIREMENTS:
5+ years of professional work experience designing and implementing data pipelines in a cloud environment is required.
2+ years of experience migrating/developing data solutions in the AWS cloud is required.
1+ years of experience building/implementing data pipelines using DataBricks is required.
Expert level knowledge of using SQL to write complex, highly-optimized queries across large volumes of data.
Hands-on object-oriented programming experience using Python is required.
Professional work experience building real-time data streams using Spark.
Knowledge or experience in architectural best practices in building data lakes.
Bachelor or Master degree in Computer Science, Engineering, Information Systems or relevant degree.
Hybrid work schedule, and must live in the Atlanta, GA metro area. Must be open to up to 25% national travel to client locations when on engagements outside of the Atlanta, GA area.
Job Type: Contract
Pay: $60.00 - $70.00 per hour
Schedule:
Monday to Friday
Application Question(s):
Apply only if you are interested in Hybrid Role in Atlanta, GA. Interested ?
Experience:
Data Engineer: 4 years (Required)
Databricks: 1 year (Required)
Willingness to travel:
25% (Required)
Work Location: On the road",$65.00 /hr (est.),1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,2019,Less than $1 million (USD)
"GOBankingRates
3.3",3.3,"North, SC",Staff Data Engineer,"GOBankingRates™ is unique in the digital marketing and media industry - we combine marketing, digital, content and fintech. Our performance based approach increases brand awareness and generates targeted audience engagement on our internal web properties and partner sites.
Learn More About What We Do

What's interesting about this role?
GOBankingRates has big growth plans ahead and is looking for a strong Staff Data Engineer experienced in Data warehousing and Python to join our Data Engineering team. The GOBankingRates Data Engineering team is responsible for designing and developing the Data lake, enterprise database, data warehouse, reporting solutions, and pipelines for data processing. If you are a critical thinker with a solid track record of developing data solutions and solving complex problems with SQL and Python, we want you to join our team! You will play a vital role in designing and developing our next generation data pipelines and data platform. Join our team and prototype new data product ideas and concepts!
How will you make an impact?
Build and maintain multiple data pipelines to ingest new data sources (API and file-based) and support products used by external users and internal teams.
Optimize by building tools to evaluate and automatically monitor data quality and develop automated scheduling, testing, and distribution of feeds.
Work with data engineers, data scientists, and product managers to design, rapid prototype, and productize new data product ideas and capabilities.
Design and build cloud-based data lakes and data warehouses.
Conquer complex problems by finding new ways to solve them with simple, efficient approaches focusing on our platforms' reliability, scalability, quality, and cost.
Collaborate with the team to perform root cause analysis and audit internal and external data and processes to help answer specific business questions.
What will you bring to us?
Master's Degree (or a B.S. degree with relevant industry experience) in math, statistics, computer science, or equivalent technical field
Experience with dimensional data modeling and schema design in a database or data warehouse
Expertise with scripting languages such as Python and writing efficient and optimized SQL.
Working experience in building data warehouses and data lakes.
Experience working directly with data analytics to bridge business requirements with data engineering.
Experience with AWS infrastructure
Ability to operate in an agile, entrepreneurial start-up environment and prioritize
Excellent communication and teamwork, and a passion for learning
Curiosity and passion for data, visualization, and solving problems
Willingness to question the validity, accuracy of data, and assumptions
Preferred Qualifications:
Experience building data warehouse, data lake, and data pipeline using Snowflake/Redshift and other AWS Technologies.
Experience with large-scale distributed systems with large datasets.
Experience with event streams and stream processing (e.g., Kafka, Spark, Kinesis)
Hands-on experience with event streaming with modern event streaming tools like Pulsar, Kafka, and Kinesis. Understanding when streaming vs. batch processing is appropriate, and tradeoffs in a given context
Knowledge of advertising platforms.

Benefits
Competitive salary with excellent growth opportunity; we pride ourselves in having a team that exudes leadership, high initiative, creativity and passion.
Awesome medical, dental and vision plans with heavy employer contribution
Paid maternity leave and paternity leave programs
Paid vacation, sick days and holidays
Company funding for outside classes and conferences to help you improve your skills
Contribution to student loan debt payments after the first year of employment
401(k) - employees can start contributing immediately. After the first year, GOBankingRates matches your contribution up to 4% of your salary
A note about our response to COVID -19 and our new norm: The world has changed and we know it's important to adapt and to do our part to take care of our teams in this global pandemic. Our number one priority is to have our team feel safe, balanced and connected. We're committed to providing our teams with the best resources and tools to navigate this new virtual world that we're living in. We've also reinvented the ways in which we recognize, celebrate, and engage with each other to keep our culture strong!
Here's a peek into our world at GOBankingRates -
Our teams are working remotely 100% for the foreseeable future and have flex time. We're in the digital media space so we're mobile and flexible!
Option to work from an office (if you need to get away!)
Tools & resources are available to keep our team connected across North America. (JIRA, Trello, Airtable, Slack, Zoom and so much more!)
To keep our community engaged and connected, virtual team building events are held weekly and monthly.
For wellness and balance, weekly virtual fitness classes such as yoga are available.
To care for the local communities that we're a part of across the U.S our team members host socially distanced philanthropic events every quarter.
And most importantly, we've committed to consistent and transparent communication to help us all stay informed, engaged and to keep us on our path to success and #greatness.
We are an equal-opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.","$134,519 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Internet & Web Services,#N/A,$25 to $100 million (USD)
"MoneyDolly
5.0",5.0,"Salt Lake City, UT",General Data Engineer,"Position Title: General Data Engineer
Location: Remote (US Based only)
Commitment: Full-Time

Job Brief

About Us
MoneyDolly is a fast growing Fintech Saas tech company, headquartered in Sandy, Utah. We are leading innovation in supporter relationship management, wherein teams meet all their fundraising goals. Teams can create their own page for your supporters to visit, offer products and incentives for their contributions, then simply invite their team to spread the word and watch the money roll in. MoneyDolly has already helped thousands of organizations, groups and teams nationwide raise over $20 million. We are still private, and our best work is still ahead of us. This is a massive industry with antiquated methods and no clear market leader. This is the spot for a qualified team player looking to build something new, make a real impact, and actually change the world.

Job Description:
We are looking for a highly motivated and talented Generalist Data Engineer to join our development team. The ideal candidate will be someone who loves to wear multiple hats and thrives in an environment of high autonomy to accomplish our business goals. You will be responsible for collaborating with our business team, creating flexible data models, and maintaining data pipelines to ensure the smooth flow of data in our organization.
Responsibilities:
Interact with our business team to gather requirements and understand data needs
Design and create flexible data models that allow for easy report generation and ad-hoc analysis
Write and maintain DBT transformations to generate flexible data models from our production database
Develop and maintain required data pipelines in Python to collect and process data from various sources
Develop and maintain Tableau and Hex.Tech dashboards to analyze the collected data
Ensure data quality, integrity, and security in all data processes
Collaborate with other team members to implement data solutions and integrate them into the existing infrastructure
Continuously monitor and optimize data models and pipelines to meet changing business requirements
Requirements:
Bachelor's degree in Computer Science, Engineering, or a related field
Strong experience in data modeling, ETL processes, and data pipeline development
Proficiency in Python and SQL
Familiarity with DBT transformations and best practices
Experience working with relational databases and big data technologies
Excellent problem-solving skills and attention to detail
Strong communication and collaboration skills
Ability to work independently and adapt to a fast-paced, dynamic environment
Nice to have:
Experience with data visualization tools (e.g., Tableau, Power BI)
Experience with AWS and GCP (RDS Postgres and BigQuery are our current stack)
Knowledge of the fundraising industry or a strong interest in learning more about it
What We Offer:
A competitive salary ($100k-$140k range) and benefits package
A supportive and collaborative work environment
Opportunities for professional growth and development
The chance to make a significant impact in a growing startup
If you are passionate about data engineering and excited about the prospect of revolutionizing the fundraising industry, we'd love to hear from you. Apply now to join the MoneyDolly team!","$120,000 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Redzara.us,#N/A,"Atlanta, GA",Azure Data Engineer,"Responsible for developing new and modernizing existing applications to Azure Cloud Platform.
The ideal candidate must demonstrate in-depth knowledge and understanding of Database concepts.
Requirements:
7+ years of experience in Data Warehousing and Business Intelligence
5+ Years in BI development leveraging the Microsoft SQL Server. Needs to be an expert in T-SQL
5+ Years in SSIS and use of Stored Procedures.
1-2 Years in Azure Data Factory, SQL VMs, build and maintain ETL
Ingest and aggregate data from both internal and external data sources to build our data lake
Strong understanding data modeling (i.e., conceptual, logical, and physical model design, experience with Operation Data Stores, Enterprise Data warehouses and Data Marts).
Job Type: Contract
Pay: $43.49 - $57.00 per hour
Compensation package:
1099 contract
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Atlanta, GA 30334: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure Data Factory: 3 years (Required)
Azure: 3 years (Required)
Data warehouse: 1 year (Preferred)
Work Location: Hybrid remote in Atlanta, GA 30334",$50.25 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Leadstack Inc
4.3",4.3,Remote,Lead Data Engineer with Sql/Python,"LeadStack Inc. is an award winning, one of the nation's fastest growing, certified minority owned (MBE) staffing services provider of contingent workforce. As a recognized industry leader in contingent workforce solutions and Certified as a Great Place to Work, we're proud to partner with some of the most admired Fortune 500 brands in the world.
Title: Lead Data Engineer with Sql/Python
Location: Remote
Duration: 6+months
Direct Client
Immediate interview
Top Skills: Azure, SQL/Python Developer.
Job Description:
Design and develop data system integrations and data models for reporting and alerting.
Essential Job Functions:
Drive digital innovation by leveraging innovative new technologies and approaches to renovate, extend, and transform the existing core data assets, including SQL-based, NoSQL-based, and Cloud-based data platforms
Define high-level migration plans to address the gaps between the current and future state
Present opportunities with cost/benefit analysis to leadership to shape sound architectural decisions
Lead the analysis of the technology environment to detect critical deficiencies and recommend solutions for improvement
Interpreting data, analyzing results using statistical techniques
Developing and implementing data analyses, data collection systems and other strategies that optimize statistical efficiency and quality
Acquiring data from primary or secondary data sources and maintaining databases
Promote the reuse of data assets, including the management of the data catalog for reference
Key Responsibilities:
Innovate, develop, and drive the development and communication of data strategy and roadmaps across the technology organization to support project portfolio and business strategy
Drive the development and communication of enterprise standards for data domains and data solutions, focusing on simplified integration and streamlined operational and analytical uses
Drive digital innovation by leveraging innovative new technologies and approaches to renovate, extend, and transform the existing core data assets, including SQL-based, NoSQL-based, and Cloud-based data platforms
Define high-level migration plans to address the gaps between the current and future state, typically in sync with the budgeting or other capital planning processes Lead the analysis of the technology environment to detect critical deficiencies and recommend solutions for improvement
Mentor team members in data principles, patterns, processes and practices
Promote the reuse of data assets, including the management of the data catalog for referenceDraft and review architectural diagrams, interface specifications and other design documents
To know more about current opportunities at LeadStack , please visit us on https://leadstackinc.com/careers/
Job Type: Contract
Salary: $70.00 - $80.00 per hour
Experience level:
11+ years
Schedule:
8 hour shift
Experience:
Azure: 5 years (Preferred)
Data warehouse: 1 year (Preferred)
SQL developer: 6 years (Preferred)
Work Location: Remote",$75.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2016,Unknown / Non-Applicable
"Tipico - North America
4.0",4.0,"Denver, CO",Data Engineer,"Company Description

Tipico is an energetic, passionate, bold tech company, and we love sports! Tipico has recently expanded to the US market, establishing offices in Hoboken, New Jersey and Denver, Colorado. We are driven by our core values: passion, progress and trust. Our goal is to build the best possible mobile sports betting product in the industry, in order to amplify the emotions of millions of sports fans every day. Let’s Make This Interesting ™
Please note: this role is located in our Denver, CO office; we work off a hybrid model and come into the office 2 days per week.

Job Description

The Data Engineer will report directly to the Data Engineering Manager and work together with rest of the data engineers, BI analysts, and product teams. The position holder ensures this is done by doing the following:
Create and maintain both batch (ETL/ELT) and real-time data pipelines and architecture
Ensure all data provided by Data Platform is of the highest quality and is delivered in a timely manner and in line with agreed SLAs
Assemble large and complex data sets that meet functional and non-functional business requirements
Identify, design, and implement internal process improvements, including automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies
Collaborate with stakeholders including data architects, Executive, Product, Data and IT team members from the beginning to the delivery of a project
Build a scalable, modern, and trusted data ecosystem, making sure it is responsive, available and in line with business requirements
Take part in sprints, workloads, and deliverables through agile methodologies
Manage quality assurance, verification of accuracy and consistency of data, specifically KPI measures sent to the business
Thrive working in a fast-paced and challenging environment
Open to continuous learning individually and with the team
Comfortable with being challenged / seeking challenge
Willing to work flexible hours if needed

Qualifications
3+ years in Data Platforms / Datawarehouse / BI experience
Must have experience in successfully implementing data-centric applications, such as data warehouses, operational data stores, and data integration projects
Strong analytical and problem-solving skills that balance with creative approaches
Ability to learn new and complex concepts quickly
Strong experience with large-scale production databases and SQL
Solid understanding of cloud data services: AWS services such as S3, Athena, EC2, RedShift, EMR, EKS, RDS and Lambda
Proficient with one or more big data & new data technologies such as DBT, NIFI, Airflow, Kafka
Experience with data modelling
Experience with ETL development (extractions, data load, aggregation, transformation)
Knowledge in working with time-series/analytics databases such as Elasticsearch
Understanding of containerization and orchestration technologies like Docker/Kubernetes
Must be able to take ownership of projects with a high level of attention to detail and priority
Team player with a hands-on approach and attitude
Must be comfortable with on call rotation
Nice to have:
Familiarity with Marketing tooling and/or CRM systems is preferred
Familiarity with using business intelligence tools such as Domo, Tableau or Looker
Knowledge about and / or experience with sports betting
Background in digital companies, online, gambling, gaming, e-commerce, marketing, or mobile apps
Familiarity with one scripting language, preferably Python

Additional Information

What's in it for you:
Work in an environment where you, your work and ideas matter and have an impact.
You will be a be part of the newest and up and coming USA online iGaming and sports betting companies in the market.
Work in a new and young business with high growth potential.
Build your own success story together with us.
Work with self-organized, self-responsible and entrepreneurial employees.
Start-up feeling, backed-up by a leading European sports betting house. We are a high-volume business and are taking off in the US!
Competitive salary, Medical, Vision, Dental Benefits, Unlimited PTO, and more
Salary for this position is $120k+ & annual performance bonus","$120,000 /yr (est.)",201 to 500 Employees,Company - Private,"Arts, Entertainment & Recreation",Gambling,2004,Unknown / Non-Applicable
National Group Corporation,#N/A,"Dallas, TX",Data Engineer,"Who are we?
ReAlign Insurance Holdings is a leading primary underwriter of program administrator managed specialty insurance programs in the U.S. With the extensive knowledge and experience our team has in building and managing carriers and programs we are able to reach decisions on program opportunities quicker than our competition.
Why choose ReAlign?
At ReAlign, we believe that we are only as strong as our people. We strive to create an inclusive and welcoming culture where employees feel comfortable and empowered to be themselves. We support a diverse and inclusive work environment, with a focus on a healthy work-life balance. We understand that comprehensive benefits are important which is why we offer a benefit package that includes: medical, dental, vision, company paid short and long-term disability, life insurance, health spending account, 401(k) match every pay period, PTO, paid holidays, floating holidays, tuition reimbursement, and referral bonuses.
Job Summary
The Data Engineer position will involve data exchange process analysis, design, development, unit testing, and support. This role would work with other data team members on database ETL processes and reporting needs in both the on-premise database environment as well as the Azure data environment. This position requires strong technical and analytical skills to review business processes and data, recommend solutions, conduct functional and code analysis, and lead and perform database development tasks. This position will also involve production support and require experience within a DevOps environment.
Essential Functions
· Design, development, and support data ETL processing for both external and internal customers
· Support the internal and Azure cloud-based data warehouse platforms
· Work with DBA/System Admin to develop and maintain automated data processing
· Design and analyze queries and stored procedures
· Write and support SSRS and PowerBI reports
· Work in a data development life cycle environment
Work with stakeholders and data analysts to understand business requirements
Work with end users and the testing team to perform unit, functional, and integration testing and validation prior to production implementation
Troubleshoot and resolve operational problems with a focus on delivery
· Communicate solutions and architectural design to IT leadership
· Work in both Kanban and Scrum Agile environments
· Continually improve processes and solutions, seeking ways to use new tools, features, and capabilities
Job Requirements
3-10 years of experience as a Database Developer
Highly proficient in writing advanced SQL queries, stored procedures, SSRS reports, and ETL processes
Strong knowledge of relational and transactional database architecture
Strong technical and analytical skills
Experience with the Microsoft Azure and PowerBI platforms and Agile development methodology a plus
Bachelor’s Degree in computer science, database development, or related field preferred
Detail oriented with the objective of delivering a solution to a problem in a timely and efficient manner
Self-driven, willing to take initiative when an opportunity presents itself
Strong collaboration and communication skills and the ability to work in a team environment a must
Ability and willingness to work in a partial on-prem and partial remote office environment
Experience in the Property and Casualty Insurance industry a plus
Proficient with standard desktop software used in a business environment (MS Word, Excel, Visio, etc.)
Must be able to work in a hybrid in-office and remote work environment
Must be authorized to work in the United States
Job Type: Full-time
Pay: $90,000.00 - $125,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Health savings account
Life insurance
Paid time off
Retirement plan
Vision insurance
Experience level:
3 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you currently local to the Dallas area and able to commute to the office 2 to 3 times per week? (Deal breaker)
Experience:
writing advanced SQL queries and ETL processes: 3 years (Required)
cloud database platform (i.e. Azure, AWS): 1 year (Preferred)
Azure Data Factory: 1 year (Preferred)
Work Location: In person","$107,500 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Stonehenge Technology Labs
5.0",5.0,"Bentonville, AR",Data Engineer (ETL),"At Stonehenge Technology Labs, we help leading CPG teams win in the omni-commerce space by bringing together data that powers insights and drives autonomous actions. The faster STOPWATCH™ Members identify opportunities and command gaps across their total online and in-store assortment, the faster they win! Learn more here: https://stopwatch.tech
The team at Stonehenge Technology Labs operates off three distinct principles: 1) Empower Unique Talent; 2) Operate as a World-Class Team; 3) Solve Complex Problems with Speed & Simplicity. Stonehenge recruits, employs, trains, compensates, and promotes regardless of race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.
Visa sponsorship is not available for this role.
Purpose:
As the Data Engineer at STOPWATCH™, you will be a data engineering powerhouse for the organization, tasked with pulling in data from a variety of sources, storing them in their raw format for long term accessibility, and then ingesting and transforming the data for easy consumption by teams downstream.
You will:
· Develop, monitor, maintain, optimize, and orchestrate end-to-end data pipelines while leveraging best practices.
· Develop data solutions to provide relevant, timely, and insightful information supporting both upstream and downstream teams.
· Leverage APIs to their full potential and for the betterment of our platform offerings.
· You may be tasked with deploying cloud resources as needed within the processes and procedures established.
· Leverage our stack, including but not limited to Databricks and SQL, in the most optimal way, making recommendations and executing on them in close partnership with the Lead Data Engineer and other functions.
· Take on projects hands-on with a high level of professionalism and varying levels autonomy.
· Be able to independently make decisions around tradeoffs between different methods for building pipelines and data extraction, with a core emphasis on data safety, cost optimization, and data accuracy.
· Partner closely with security engineer, cloud architect, product teams, and Power BI engineers to achieve business goals.
We expect you to operate with speed, simplicity, and store your work on the group system with a focus on process mapping and documentation enabling rapid scale and interconnected efficiencies between work groups.
As part of the application process, all candidates are required to complete a skills test.
SKILLSET/EXPERIENCES:
· Expert level understanding data engineering, specifically in end-to-end pipeline solutions is a must
· 2+ Years of data engineering experience is a must
· 2+ Years of cloud experience, preferably in an Azure environment is a must
· 2+ Years of SQL experience is a must
· 1+ Years of Python or Scala experience is a must
· 1+ Years of owning data pipeline orchestration is a must
· 1+ Years of DevOps experience or comparable is a must
· 1+ Years of Databricks experience is a must
· 1+ Years of experience with Azure Data Factory or equivalent is a plus
· Experience working with Typescript/Node.js is a plus
· Extreme attention to detail including self-monitoring QA
· Penchant for documentation, reproducibility, and standardization especially in terms of standard data model matrices
· Familiarity with retail/retailer/supply chain data is a plus; experience working for a major retailer or CPG brand even bigger plus
· Scrappy mindset; resourceful and relentless in finding answers
This role (and every role at Stonehenge Technology Labs) is expected to read, notate and arrive ready to discuss THE GOAL by Eliyahu Goldratt on Day 1.
Here’s what we offer in addition to competitive base pay:
Stock incentives based on individual and team performance
5 weeks paid vacation, 11 paid holidays
Flexible (albeit intense) work schedule
100% Company Paid Health Care for Employee and Spouse/Partner
Available Dental and Vision Plans
Available 401K Program
Contribution matching to qualified 501c3 organization (team member’s choice),
Kid/pet/partner focused culture
Paid maternity/paternity leave and financial assistance in support of adoptive & fostering activities
Executive coaching (certain roles)
Job Type: Full-time
Pay: $110,000.00 - $135,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Health savings account
Paid time off
Parental leave
Retirement plan
Vision insurance
Experience level:
2 years
Schedule:
Monday to Friday
Application Question(s):
Do you now or in the future require visa sponsorship to continue working in the United States?
Experience:
Databricks: 1 year (Required)
Python: 1 year (Required)
Data Engineering: 2 years (Required)
Cloud infrastructure: 2 years (Required)
SQL: 2 years (Required)
Work Location: Hybrid remote in Bentonville, AR 72712","$122,500 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2019,Unknown / Non-Applicable
#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"DataArt
4.6",4.6,Remote,Data Engineer,"CLIENT
Our client is a pioneer in US schools’ education since 2000, it is leading the way in next-generation curriculum and formative assessment. They develop a number of solutions and interactive web products for teachers, students and their parents.

The company is technology-driven with a huge number of software engineers involved in product development. They have a very solid approach to technology, employ best practices and processes, with a focus on cutting-edge frameworks, languages and tools.

We invite to the company, not a project
TEAM
There are multiple autonomous and empowered teams who work with their own part of the product and choose their own way of doing things and they expect people to be proactive and good team players.
Responsibilities
Building well-tested and optimized ETL data pipelines for both full and delta extraction
Collaborating with data analysts and learning scientists to store, aggregate, and calculate captured students’ work
Contributing to leading industry data standards, such as Caliper Analytics or xAPI
Improving our deployment and testing automation data pipelines
Requirements
3+ years of professional software development or data engineering experience
Strong CS and data engineering fundamentals
Proven fluency in SQL and a development language such as Python
Experience with the following storages: Snowflake, AWS Storage Services (S3, RDS, Glacier, DynamoDB)
Experience with ETL/BI: Airflow, DBT, Matillion, Looker
Experience with Cloud Infrastructure: AWS Kinesis, Lambda, API Gateway, Terraform
Understanding of ETL/ELT pipelines and Data Warehousing design, tooling, and support
Understanding of different data formatting (JSON, CSV, XML) and data storage techniques (3NF, EAV Model, Star Schema, Data Lake)
BS in Computer Science, Data Science, or equivalent
Experience in education or ed-tech
Proven passion and talent for teaching fellow engineers and non-engineers
Proven passion for building and learning: open source contributions, pet projects, self-education, Stack Overflow
Strong communication skills in writing, conversation",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,1997,$100 to $500 million (USD)
"ECI - Sacramento
4.4",4.4,"Sacramento, CA",Lead Data Quality Engineer (C),"Senior Lead Data Quality Engineer with Snowflake experience:
12-24 months.
Public sector experience is preferred.
Must have these specific experiences in data management, data integration, data qualify and Lead experience.
Must have Snowflake platform experience.
Project will start end of June.

Mandatory Qualifications:
Must have a bachelor's degree in computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field. Please attach degree(s).
Must have 10 or more years of experience working in Information Technology.
Must have at least seven (7) years or more of work experience in data management disciplines including data integration, modeling, optimization, and data quality, and/or other areas directly relevant to data engineering responsibilities and tasks.
Must have at least five (5) years of experience working in cross-functional teams and collaborating with business stakeholders in support of a departmental and/or multi-departmental data management and analytics initiative.
Must have at least three (3) years of experience in data integration, data warehouse, big-data-related initiatives, development, and implementation.
Must have at least three (3) years of experience in architecture patterns and data integration design principles.
Must have at least three (3) years of experience with database technologies (e.g., SQL, NoSQL, SQL Server).
Must have at least three (3) years of experience with the Snowflake Data Warehouse Platform.
Must have at least three (3) years of experience with extract, transform, and load (ETL) and other business intelligence tools (e.g., SSIS, Azure Data Factory, Power BI, Tableau).

Here are the desired which is best to get as many as possible:
Desired Qualifications:
Possess an advanced degree in computer science (MS), statistics, applied mathematics (Ph.D.), information science (MIS), data management, information systems, information science (post-graduation diploma or related) or a related quantitative field.
Possess Azure Cloud and Snowflake certifications.
Three (3) years working experience in the Azure Cloud platform using Synapse, Azure BLOB Storage/ADLS, Azure Data Factory, and Purview.
One (1) year experience with Azure DevOps and CI/CD pipelines.



About ECI - Sacramento:

Estrada Consulting, Inc. (ECI) delivers technology-enabled services and solutions to clients all over the USA and British Columbia. We provide system integration, custom application development, data warehouse and business intelligence, project management, custom reporting solutions and consulting services to mid-size and large enterprises in all major industries. The Company headquarters is in Sacramento, California, and was established in year 2000. Visit http://www.estradaci.com/ to learn about our projects, managed services, awards and certifications delivering value for a range of businesses and government agencies.",$75.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2000,Unknown / Non-Applicable
"NexTek, LLC",#N/A,"Shoreview, MN",Sr. Data Engineer,"Nextek, LLC is a software development company in the financial services industry that creates web applications and solutions for financial services professionals. We are seeking a motivated Senior Data Engineer to join the team.
What we provide:
A laptop and other equipment/software necessary for work
Health Insurance, Dental Insurance, Vision Insurance
Paid time off (PTO), Volunteer time off (VTO)
Hybrid remote work options
401K
Life Insurance
Flexible schedules
Monthly Social Activities
Casual dress code
Professional development
Here is what we are looking for:
Experience in database administration, security, backup, and recovery
Preferred Knowledge:4+ years of experience in data/software engineering
Experience in the financial services industry
Experience with a variety of relational databases (particularly MySQL & MSSQL)
Experience with collecting and presenting analytics
Here is what you will be doing:
Display proficiency in relevant technical skills.
Maintain a deep understanding of the data structure.
Provide evaluations and recommendations for our data environment based on potential issues, tradeoffs, and risks.
Scope and implement solutions.
Make good estimates and set realistic expectations for delivery.
Ask for guidance when necessary.
Maintain and monitor the health of databases and associated solutions.
Make use of the same environments setup by the Software Engineering team
Thoroughly test code for bugs and security issues before deploying to production environments
Maintain and monitor actively deployed solutions
Follow and help define best practices
Peer reviews
Maintain awareness of industry trends
Share experience and insights to help your team grow
Increase understanding of impact on other teams
Increase understanding of the customer
All other duties as assigned
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Shoreview, MN: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"GTA (Global Technology Associates)
4.6",4.6,"Plano, TX",Data Logging Engineer,"Looking for a Data Logging Engineer
What you will be doing as a Data Logging Engineer
· Perform system level log captures/analysis of several Network elements in the 4G/5G Network architecture to identify root causes of product and key performance issues.
· Document and maintain the troubleshooting steps including updating technical documentation for primary areas of responsibility
· Identify automation opportunities & implement scripts for log analysis, for discussions with external customers/internal customers
· Strong on customer support as well as data analysis of logs, etc.
· Must be able to drive for issue resolution with other Engineering team
· Initial analysis of Network KPI degradation to quantify and qualify the issue
· Be able to proactively perform Ad-hoc/periodic parameter audit, issue signature audit and SW/FW audit
· Ensure the latest ticket status ticket status is provided to the customers
· Software and patch upgrades on network element to support feature testing and issue resolution verification
· Provide 24X7 support as needed for commercial Network outages and network performance issues. Also provide technical reviews and provide support to customers as needed
What you will bring to the table as a Data Logging Engineer:
· Bachelor’s Degree in Telecommunication or Network Engineering with at least 2 years of experience in Wireless Telecom environment.
· Good knowledge of UNIX/LINUX operating systems
· Good Scripting skills using but not limited to Python, Shell or Perl
· Strong knowledge of MS office tools like Excel, power point and Word.
· Good analytical skills to investigate and evaluate trending data for issue resolution
· Good knowledge of 4G and 5G wireless network architecture concepts, RAN protocols and eNB interfaces S1-MME, S1-U is a plus
· Good knowledge of trouble ticket and knowledge management systems
· Willing to work during night to accommodate debugging involving offshore R&D teams
What you didn’t know about us:
· Competitive salary
· Health, Dental and Vision Benefits
· Short/Long Term Disability and Critical Care/Illness Protection
· Life Insurance and Retirement Plans
· Employee Assistance Program
· With this position, you will get the opportunity to work with our game changing clients and further advance your already valuable experience in the telecom industry!
We are Connectors. We thrive on ‘quality over quantity’ and put in the work building strong relationships. We create connections, discover qualities, uncover skills, and place people with accuracy. We are your true partner!
We are Collaborators. You’ll be working with a wholly-owned subsidiary of Kelly and part of the Kelly Telecom division. It allows us to be as nimble and fiercely competitive as a startup while having the backing of a multibillion dollar publicly traded company which has been in business for 75 years. With direct access to hiring managers, services don’t stop at standard recruiting processes. We use our expertise to improve your application skills and provide ongoing career support.
We give 24/7 Support. We are in this together. We provide around the clock availability, competitive employee benefits, and continuously check-in to make sure things are going smoothly. Check out our Glassdoor page!
Kelly Telecom is an equal opportunity employer and will consider all applications without regard to race, genetic information, sex, age, color, religion, national origin, veteran status, disability, or any other characteristic protected by law. For more information click Equal Employment Opportunity is the law.
You should know: Your safety matters! Vaccination against COVID-19 may be a requirement for this job in compliance with current client and governmental policies. A recruiter will confirm and share more details with you during the interview process.
#JobsAtKellyTelecom
Job Types: Full-time, Contract
Salary: From $50.00 per hour
Schedule:
8 hour shift
Work Location: In person",$50.00 /hr (est.),Unknown,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Staffbee Solutions Pvt Ltd
5.0",5.0,"Dallas, TX",Data Engineer,"Dear OPT/CPT candidates,
We currently hiring OPT/CPT Individuals , we also specialized in training ( data engineering course , How to handle the interviews etc...) and placement for Data Engineering positions, With many benefits like
Free accomodation untill you get free accomodation
100% placement
Modification of resumes according to the market standards
Best Package
H1b sponsership
Travel allowances
+1469 902 8976 (wapp/call)
Or share resume at vijay.komma@staffbees.com
Job Type: Full-time
Salary: $60,000.00 - $70,000.00 per year
Application Question(s):
This requirment is for only OPT/CPT visa status candidates , Do you aware?
Are you sure you have read the JD?
Work Location: One location","$65,000 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2018,Unknown / Non-Applicable
"Braintrust
4.6",4.6,"San Francisco, CA",Data Engineer,"ABOUT US:
Braintrust is a user-owned talent network that connects you with great jobs with no fees or membership costs—so you keep 100% of what you earn.

JOB TYPE: Freelance, Contract Position (no agencies/C2C - see notes below)
LOCATION: Remote - United States only (TimeZone: EST | Partial overlap)
HOURLY RANGE: Our client is looking to pay $100 – $105/hr
ESTIMATED DURATION: 40h/week - Long term

ABOUT THE HIRING PROCESS:
When you join Braintrust, you will be invited to a screening process for Braintrust to learn more about your previous work experiences. Once completed, you will have access to the employer for this role and other top companies that seek high-quality talent. Apply to this job to kick off the process.
THE OPPORTUNITY
Requirements
Summary:
Advanced analytics SQL - minimum 2 years, preferable 5
Python - minimum 1 year, preferable 2
Ability to work in cloud platform
Qualities that will help you thrive in this role:
You understand that being an effective engineer is about communicating with people as much as it is about writing code.
You are willing to work with and improve code you did not originally write, primarily in SQL and Python.
You are generous with your time and experience and can mentor and learn from other engineers.
You are comfortable with best practices for traditional data warehousing.
You love SQL and writing efficient and optimized ETL pipelines.
You are familiar with building and monitoring cloud services and infrastructure.
What you’ll be working on
What’s the role?
As a member of our client's Data Applications, Data Warehouse team, you’ll help us improve the stability, performance, and usability of their BigQuery data warehouse while advising their stakeholders on best practices and optimizations. Your work will enable other developers, data scientists, and analysts to write the high-performing pipelines that power data science, machine learning, and product development.
In addition to BigQuery SQL, our client's toolset includes Looker, Java, Python, and Spark, as well as Airflow, Terraform, and Kubernetes, and GCP services like Dataproc and Dataflow.
About The Team
They build highly-performant systems and data warehouses that are maintainable and cost effective.
They develop robust, highly available, well-monitored data infrastructure.
They stay in close communication with the internal customers and make strategic improvements to ensure those that depend on us have a great experience using data
What does the day-to-day look like?
You should have experience building data warehouses, data marts, and aggregate tables - supporting them at scale, and collaborating with other teams that depend on them.
Experience building applications and managing infrastructure using one of the major cloud providers is preferred but not required. (Our client uses Google Cloud).
Our client values curiosity, passion, responsibility, and generosity of spirit.
Apply Now!
Braintrust Job ID: 6590


C2C Candidates: This role is not available to C2C candidates working with an agency. If you are a professional contractor who has created an LLC/corp around their consulting practice, this is well aligned with Braintrust and we’d welcome your application.

Braintrust values the multitude of talents and perspectives that a diverse workforce brings. All qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status.

This is a remote position.",$102.50 /hr (est.),51 to 200 Employees,Company - Private,Management & Consulting,Business Consulting,2018,$5 to $25 million (USD)
"Quirch Foods
3.0",3.0,"Coral Gables, FL",Data Engineer,"About Quirch Foods
In business for over 50 years, Quirch Foods is a global distributor and exporter of fresh and frozen food products with a unique focus on ethnic cultures. We service our customers from 5 distribution centers strategically located in the United States. We have consistently been ranked among the top 50 exporters in the US and are one of the largest importers of seafood. Our customers include independent grocers, chain supermarkets, foodservice distributors, cruise lines, restaurants and food processors/manufacturers. We have an extensive product list with over 8,000 SKU's and carry niche brands such as Panamei, Mambo and Chiquita along with national brands like Tyson, IBP, Excel, JBS, Iowa Premium, Smithfield, Dietz & Watson, along with many others. We are headquartered in Coral Gables, FL.
Quirch is a portfolio company of Palladium Equity Partners, a leading mid-market private equity fund based in New York City, which partnered with the Executive Team to capitalize on Quirch Foods’ record of excellence and to grow it decisively beyond its current business.
Position Summary
At Quirch Foods we believe in leveraging technology as a source of competitive advantage. This includes customized ERP Solutions, Business Intelligence, EDI, Web and Mobile Solutions. We are currently looking for a Data Engineer to join our development efforts.
Essential Duties and Responsibilities:
Develop, test and maintain custom business solutions by leveraging state of the art software development practices using an agile methodology
Evaluate business requirement and help define problems and develop solutions
Participate in design and architecture discussions with business leaders, end users and IT team members
Document business requirements and solution, document code and provide support for creation of end user documentation
Update business knowledge, technical skills and soft skills, leverage educational opportunities, participate in professional organizations
Experience:
MS SQL Programming
Visual Studio\C#\WPS Experience
Microsoft SSAS, SSIS, SSRS desired but not required
Biztalk experience desired but not required
Skills and Requirements:
Bachelor’s degree in Computer Science, Engineering, Math or equivalent and\or related experience and training preferred
Strong analytical and problem-solving skills
Strong collaborator and team player
Great organizational skills, attention to detail and follow thru
Effective oral and written communication skills
Extensive familiarity with data management principles
Benefits:
Professional growth and developmental opportunities to grow your skills using state of the art technology
Comprehensive benefits package that includes: Medical, Dental, Prescription Drug Plan, Disability Plan, Life insurance Plan
401K savings Plan
Paid Holidays
Personal Time off
Employee Discounts
Quirch Foods is an Equal Opportunity Employer (EOE). Qualified applicants are considered for employment without regard to age, race, color, religion, sex, national origin, sexual orientation, disability, or veteran status. All applicants must be eligible to work in the United States.
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Vision insurance
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Coral Gables, FL 33134: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
What is your desired compensation?
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Data Engineering: 1 year (Preferred)
Work Location: In person","$89,340 /yr (est.)",201 to 500 Employees,Company - Private,Retail & Wholesale,Wholesale,1967,$500 million to $1 billion (USD)
Vibrant Planet,#N/A,California,Data Engineer,"About Vibrant Planet
We are a team of leaders in science, forestry, policy, and tech, building a cloud-based, data-driven platform to increase the pace and scale of forest restoration and reduce catastrophic wildfire, tree mortality, forest degradation, and deforestation. Our current software modernizes land management planning and monitoring with AI-driven data development, user friendly scenario building and decision support, and forest resilience trends and treatment outcome detection. The system quantifies potential and actual treatment benefits, helping to grow markets for ecosystem services (carbon, water, biodiversity, sustainable forest-derived products).
Our initial platform (launched in September 2021) is focused on temperate, “fire adapted” landscapes in California and the Western US. Our mission is global; our roadmap expands into other geographies and forest types accordingly.
Driven by our sense of urgency to protect vital forests and the services they provide, we use our expertise to build sophisticated, AI/ML-driven, user friendly products to democratize the use of data and accelerate the process of stabilizing and restoring our forests.
Vibrant Planet is backed by climate and ecosystem resilience solutions leaders, including Grantham Foundation, Earthshot, Elemental Excelerator, Ecosystem Integrity Fund, Chris Cox (CPO at FB), Neil Hunt (ex CPO of Netflix), Cisco, Valia Ventures, and Halogen Ventures.
For further information please visit: VibrantPlanet.net
Equal Opportunity Employer: Vibrant Planet is committed to diversity. We encourage applicants from all cultures, races, colors, religions, sexes, national or regional origins, ages, disability status, sexual orientation, gender identity, military, or other status protected by law to apply.
We are most interested in finding the best candidate for the job, and that candidate may come from a less traditional background, but have capacity to grow into and thrive in the position after some mentoring. We do not require that you have experience with every job description task. We will consider any equivalent combination of knowledge, skills, education, and experience to meet minimum qualifications. We encourage each candidate to think broadly about their unique background and skill set and how it may relate to the role. This is important to us. We aren’t just saying this, we mean it.
For further information please visit: https://vibrantplanet.net
About the Role
We are looking for a versatile, hands-on data focused engineer to help us scale and build our geospatial pipelines and tooling. You will be part of a small team of engineers and scientists, tackling some of the most important climate change related issues facing the world today. This is a remote role (and company) so being comfortable and effective working in a distributed team is crucial.
Key Responsibilities:
Design, develop, scale, and maintain data pipelines that ingest, transform, and store large volumes of geospatial data from multiple sources.
Optimize data processing and storage performance and cost efficiency by leveraging cloud-based technologies and services.
Collaborate with engineers, scientists and other stakeholders to share knowledge and build expertise.
Lead and participate in development life cycle activities like design, coding, testing, and production release.
Contribute to our evolving engineering culture, standards, tooling, and processes.
Mentor and support other engineers and deeply review code.
Technical Qualifications
Strong software engineering skill set.
2+ years experience in data engineering or a related field.
Experience with distributed data processing frameworks and tools (e.g., Airflow, Hadoop, Spark).
Proficiency with Python or an equivalent language. Ability to write clean, high-quality code and tests to keep our system fast, reliable, and monitorable.
Bonus Qualifications - Absolutely not required, but nice to have
Airflow
Pandas / Geopandas
Jupyter notebooks
Geospatial processing
Lidar data
Satellite imagery
Experience with ArcGIS/QGIS
Other Expectations:
Strong communication skills; discussing complex technical concepts to engineers and non-engineers is no problem to you.
Collaborative and supportive team player, with a desire to enrich our engineering culture.
Eagerness to learn, think creatively, and share knowledge with others.
Ability to write understandable, testable code with an eye towards maintainability.
Proactive and empathetic mindset - you love to roll up your sleeves to fix problems
Location - We are a remote first company with the majority of the team in the US west coast time zone. We also have a small and growing presence in New Zealand. Location can be flexible, but we are primarily targeting those two time zones and anything in between.",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Loyal
3.9",3.9,"Atlanta, GA",Senior Cloud Data Operations Engineer,"Loyal is an organization centered on experience and building a platform that allows consumers to make meaningful decisions when it comes to healthcare. We deeply understand providers, locations, services, appointments, business rules, and moreover, we understand patients - who they are, the preferred method of communication, upcoming appointments, lapsed appointments, outstanding bills, health risks, and more. With this intelligence, our platform fuels highly relevant and personalized experiences across all mediums (website, email, voice…) allowing patients to get healthy, stay healthy, and have a better relationship with the health care provider.
**This is a remote role**
Summary
The Senior Cloud Data Operations Engineer is responsible for delivering Loyal's cloud-based data warehouse. They interface with groups across the organization to ensure we are collecting and storing data in a way that makes it easy to create and consume. They identify best practices for big data management, select the right tooling, create a workable data architecture, and drive an implementation roadmap others can depend on. The Senior Cloud Data Operations Engineer also works closely with our security group to ensure data security and privacy. They also mentor others on the team, providing technical leadership to those they work with.
You will
Create data streams and consolidate data sources into a single cloud-based data warehouse
Architect and catalog data warehouse storage schema
Work with product teams to enable easy data aggregation
Work with data consumers (analysts, data scientists, and AI/ML engineers) to ensure data is easily accessible and consumable
Create and execute a roadmap that meets data storage and availability goals
Ensure data is secure through data analysis, access control, and encryption
You have
Bachelor's degree in computer science, information systems, applied math or related field, or equivalent work experience
Minimum 5 years of experience working with cloud data processing services (AWS, Azure, or GCP).
Minimum 3 year experience with Big Data technologies, such as Databricks, Snowflake, Spark, Hadoop, etc.
Strong experience in creating data architecture and storage schemas required
Significant working knowledge of SQL and other query languages and tools
Significant experience with building relational database systems strongly preferred
Experience with ETL tools a plus
Knowledge of Python preferred
Software development, data analytics, or data science experience is a plus.
Experience working within a software as a service (SaaS) company preferred.
Experience working within a high growth and/or ambiguous environment, with proven experience to be dynamic preferred.
#LI-REMOTE
We know that potential candidates are often less likely to apply to a position if they don't match 100% of the job qualifications. Don't let that be why you miss out on this opportunity! We encourage you to apply if you can demonstrate many of these skills and competencies.
Loyal to our employees
We are a remote-friendly company! We encourage you to apply from anywhere in the United States. We also believe in a work/life balance that fulfills you while you're here and supports you when you're not. We built our benefits package to prove that we're committed to you having everything you need (including a little fun). Here is what we offer full-time employees:
Flexible paid time off, sick and personal days
At least one holiday per month (sometimes, more!)
Full health, dental, and vision insurance - Loyal pays the premium for all employees!
One Time Home Office Setup Stipend For Remote & Hybrid Roles
Monthly Internet Stipend for Remote & Hybrid Roles
Long term & short term disability
401[k] plan
16 Weeks Paid Parental Leave
2 Volunteer days per year
Matching Gift Program
Participation Grant Program
Annual Travel/Team Events up to twice per year (post-COVID)
Our Commitment
We believe that the key to Loyal's success is you. Your unique background, life experience, knowledge, self-expression, and talent make you uniquely you. Who you are, what you have experienced, and how you think inspires us to be innovative and bold.
Loyal is an equal opportunity employer. We hire great people from a wide variety of backgrounds, not just because it's the right thing to do, but because it makes our company stronger. We welcome the unique contributions that you can bring in terms of your education, opinions, culture, ethnicity, race, ancestry, sex, gender identity and expression, national origin, citizenship, marital status, age, languages spoken, veteran status, color, religion, disability, sexual orientation, and beliefs.
We consider qualified applicants regardless of criminal histories, consistent with legal requirements.
Further, consistent with applicable federal and state law, Loyal provides reasonable accommodations when requested by qualified applicants or employees with disabilities, unless doing so would cause an undue hardship. Loyal's policy regarding requests for reasonable accommodation applies to all aspects of employment, including the application process. If you require a reasonable accommodation in connection with the application process, please contact the Talent Acquisition Department at talentacquisition@loyalhealth.com.
E-Verify
This employer participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S. If E-Verify cannot confirm that you are authorized to work, this employer is required to give you written instructions and an opportunity to contact Department of Homeland Security (DHS) or Social Security Administration (SSA) so you can begin to resolve the issue before the employer can take any action against you, including terminating your employment. Employers can only use E-Verify once you have accepted a job offer and completed the I-9 Form.
COVID-19 Vaccinations
Consistent with Loyal policy, candidates performing in-person work will be required to show proof of being fully vaccinated against COVID-19 upon commencing employment. Reasonable accommodations will be considered on a case-by-case basis for exemptions to this requirement in accordance with applicable law. If you require a reasonable accommodation to Loyal's vaccination requirement, please contact the People Department at people@loyalhealth.com.",#N/A,51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2015,$5 to $25 million (USD)
Siri info solutions inc,#N/A,"Beaverton, OR",Sr. Data Engineer,"Must Have Skills
PySpark
Python on AWS
Responsibilities:
Understanding of advanced data warehousing concepts, data modeling and extract, transform and load development
Advanced SQL skills with experience querying large datasets from multiple sources and developing automated reporting
Python skills for scripting, data manipulation, custom extract, transform and loads and statistical/regression analysis particularly, as they apply to sales and marketing operations and performance
Job Type: Full-time
Salary: $47.00 - $50.00 per hour
Experience level:
7 years
Work Location: On the road",$48.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
United Digestive,#N/A,"Atlanta, GA",Data Engineer,"GENERAL SUMMARY OF DUTIES: United Digestive’s Data Engineer will be responsible for developing and maintaining data pipelines, data warehouses, and data models to support the company's Business Intelligence reporting initiatives.
REPORTS TO: Director of Business Intelligence & Analytics
RESPONSIBILITIES:
Duties may include but are not limited to:
Implement, maintain, and improve data integration processes to collect, process, and manage large datasets
Collaborate with stakeholders to understand business requirements and design data solutions that meet their needs
Implement and maintain data quality checks and validations to ensure data accuracy and completeness
Optimize data processing and storage to ensure high availability, scalability, and performance
Develop and maintain automated data workflows and scheduling tools
Code ETL process using Azure Data Factory workflows and other technologies as appropriate
Troubleshoot and resolve data-related issues and incidents
Stay up to date with the latest technologies and trends in data engineering, specifically the Azure suite, and recommend improvements and optimizations to the data architecture and infrastructure
Ability to develop solutions and manage existing Azure DevOps pipelines
Maintains a high level of confidentiality with sensitive information
Performs any other duties and/or special tasks as assigned by United Digestive leadership
REQUIRED EDUCATION, SKILLS & EXPERIENCE:
Bachelor’s degree in Computer Science, Software Engineering, Information Technology, or in a related field required; healthcare-related Data Analytics experience preferred; 2+ years of data pipeline management required. Proficiency in SQL and experience with relational databases (e.g., Oracle, SQL Server, PostgreSQL); experience with data modeling, data warehousing, and ETL processes, proficiency in various programming language (e.g., Python, SQL, Java), experience with cloud platforms (e.g., AWS, Azure, GCP) and big data technologies (e.g., Databricks, Hadoop, Spark, Kafka, Cassandra). Must have knowledge of data visualization and BI tools (e.g., Tableau, Power BI) and how to support these tools in an efficient manner.
ADDITIONAL SKILLS AND EXPERIENCE:
Data Engineer must have:
Excellent verbal and written communication skills.
Excellent organizational skills and attention to detail.
Ability to multi-task and prioritize job needs
Ability to deal with patients, visitors, co-workers, and physicians with courtesy and respect
Work under pressure; assess, respond to, and communicate issues in a timely manner.
Self-motivation and be a proactive individual with a professional can-do attitude
PHYSICAL/MENTAL/ENVIRONMENTAL DEMANDS
Requires sitting and standing associated with a normal office environment. Travel as business needs dictate.
DRUG FREE WORKPLACE
United Digestive is a drug free workplace. All offers of employment are contingent upon passing a pre-employment drug screening.
EQUAL OPPORTUNITY EMPLOYER
United Digestive is an Equal Opportunity Employer and does not discriminate on the basis of race, religion, gender, color, or national origin in its employment practices.
Location: 1355 Peachtree Street NE STE 1600, Atlanta, GA 30309
Job Type: Full-time",#N/A,Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"American Power & Gas
2.5",2.5,"Largo, FL",Data Engineer,"Because of expansive growth American Power & Gas is seeking a Data Engineer to add to our technical team. This is a fulltime permanent on-site role.
We have been offering Green Energy solutions to both residential and small commercial customers for over 20 years and have won the award for fastest growing company in the Tampa Bay Business Journal as well as being featured in Forbes and the Huffington Post.
**
**
Key Responsibilities
Support operational executives in solving business problems by designing, developing, troubleshooting, and implementing data driven solutions to complex technical objectives.
Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes.
Deploy sophisticated analytics programs, machine learning and statistical models to predict business outcomes and continually optimize performance through data science.
Gather and summarize technical requirements associated with strategic business initiatives.
Document, maintain, support and enhance company technology platforms and analytics applications.
Look for opportunities to streamline and automate various reporting processes to help support internal teams.
Assist with the research and development of internal business case studies to support onboarding new data tools.
Extract, manipulate and cleanse raw data from various data sources – call center, web, and CRM systems.
Work independently as well as collaboratively with other team members and key stakeholders as required to troubleshoot and resolve data issues.
Provide comprehensive technical and consultative services to support development and maintenance of internal and third-party platforms.
Create functional test cases/criteria to verify all functionality adheres to specifications and create end user manuals.
Routinely represent the Analytics department in cross-functional status & data strategy meetings.
Assist business analysts in provision of regular performance trends reporting, forecasts, and insights for marketing and sales team leaders as well as senior executive management to maintain a tight finger on the pulse of emerging performance trends and opportunities.
Partner with internal cross functional teams to identify business needs and analytics opportunities, developing tools and techniques to analyze and provide performance-improving recommendations.
Partner with the Operations team to optimize data workflows from sourcing to storage to reporting to deliverables to maximize for value-added and time-efficiency.
Develop, enhance, and manage various analytical solutions in support of business objectives.
Determine what data is needed and how to consume and store this data to support reporting needs and ad hoc performance-improving analysis for internal stakeholders.
Ensure deliverables are adapted properly to stakeholder audience; adapting terminology and visuals as needed to “speak the stakeholder’s language”, thus communicating with maximum effectiveness.
Troubleshoot and QA data, reporting, and tracking anomalies as needed, with proactive communication to stakeholders.
Relentlessly challenge the status quo. Always be critical of how we can be more effective or more efficient as an individual, as a team, and as a business.
Provide ongoing and proactive client service to your internal customers as required to continue elevating the performance of the business.
Regularly work with and analyze data across marketing channels and the customer journey through website analytics, call center activity, and CRM systems.
Requirements
University degree or college graduate in Engineering, Computer Science, Mathematics, Statistics, related technical/programming discipline, or the equivalent hands on experience in Data Engineering or Software Development.
Proficiency with coding in SQL is required. Ability to also write in Python, R, Java, or similar programming languages is preferred.
Strong technical prowess, including an understanding of algorithms, systems architecture and end user experience.
Experience with modern source, build, and deploy tools such as Git, Grub, Maven, Yeoman, etc. is a plus.
Ability to think unconventionally to derive innovative and creative solutions.
Competency in accurately estimating development timelines.
Experience with data warehouse design, relational databases, SQL/NoSQL data modeling, RESTful API standards and large scale data processing solutions.
Demonstrated skill in database development with solid understanding of schema design, stored procedure development, query optimization and ETL processes.
Excellent troubleshooting ability. Must be able to resolve issues tied to capturing and processing data in a timely manner.
Excellent English written and verbal communication skills, especially explaining technical concepts to non-technical business leaders.
Exceptional critical thinking and problem-solving skills; able to distill overall objectives into the actionable steps required to achieve those objectives.
Capable of effectively managing projects, priorities, timelines, and working relationships.
Must possess the intellectual curiosity to succeed in a dynamic, entrepreneurial, fast paced, sales driven organizational culture.
Excited by the opportunity to disrupt the status quo and uncover the eureka moment insights that will take the business to the next level – proactively searching for problems to solve, knowing there is so much to learn.
We offer Health, Dental, Optical and Life Insurance, PTO (paid time off) and the opportunity for promotions and room to advance.
For immediate consideration please send a resume to Carl Schumacher the Manager of Recruiting CarlS@goapg.com
Job Type: Full-time","$92,733 /yr (est.)",201 to 500 Employees,Company - Private,"Energy, Mining & Utilities",Energy & Utilities,2010,$100 to $500 million (USD)
Kanini,#N/A,"North Druid Hills, GA",Senior Data Engineer,"About Kanini
Kanini provides Agile Software Development, Cloud Computing, Data Science, and Location Intelligence services to public and private organizations. We have successfully served our clients in government, finance, transportation, utility, and software industries since 2003.
Why you should join
Working at Kanini is flexible and personal. We are a highly motivated, collaborative team experimenting with the latest technologies. We are committed to everyone having a healthy work/life balance, and we provide extensive mentorship and training resources to help you succeed.
Kanini is looking for Data Engineer who has a deep experience in Data Engineering, Agile planning, Snowflake, SQL, MySQL, PostgreSQL, Git, AWS Data Tools, Python, Jira
Job Description
The Senior Data Engineer is responsible for processing structured and unstructured data, validating data quality, and developing and supporting data products. The Senior Data Engineer also plays a role in Agile planning, providing advice and guidance, and monitoring emerging technologies.
Responsibilities
Gathers and processes raw, structured, semi-structured, and unstructured data using batch and real-time data processing frameworks.
Understands and enforces appropriate data master management techniques.
Ensures data quality and implements tools and frameworks for automating the identification of data quality issues.
Work with internal and external data providers on data validation providing feedback and making customized changes to data feeds and data mappings.
Understands the challenges that the analytics organization faces in their day-to-day work and partner with them to design viable data solutions.
Recommends improvements to processes, technology, and interfaces that improve the effectiveness of the team and reduce technical debt.
Implements and optimizes data solutions in enterprise data warehouses and big data repositories.
Installs, maintains, monitors, and supports business intelligence, distributed computation, and big data analytics tools.
Provides ongoing support, monitoring, and maintenance of deployed products.
Actively works with less experienced data engineers providing technical guidance and oversight.
Actively participates in the engineering community, staying up to date on new data technologies and best practices and shares insights with others in the organization.
Qualifications:
Bachelor’s degree in computer science or related field and 4 years work experience
Working experience with batch and real-time data processing frameworks
Working experience with data modelling, data access, schemas, and data storage techniques within Snowflake
Working experience in design, development, and implementation of highly scalable, high-volume software systems and components, client-facing web applications, and major Internet-oriented applications and systems
Working experience working with relational databases such as SQL, MySQL, Postgres/PostgreSQL
Working experience with business intelligence tools and platforms
Working experience with data quality tools
Working experience with application lifecycle methodologies (e.g. waterfall, agile, iterative)
Experience with ETL processes and tools
Experience working with Git.
Desirable Qualifications
Scrum Developer Certification or equivalent
Working experience with Sisense platform
Working experience with SQL Server Integration Services
Working experience with AWS data tools (Database Migration Service)
Working experience with Python
Experience working with Jira, Rally or similar tools
Kanini Software Solutions, Inc. does not discriminate in employment matters based on race, gender, religion, age, national origin, citizenship, veteran status, family status, disability status, or any other protected class. We support workplace diversity. If you have a disability, please let us know if there is anything we can do to improve the interview process for you; we’re happy to accommodate.
Kanini Software Solutions, Inc., 25 Century Blvd., Ste. 602, Nashville, TN 37214
Job Types: Full-time, Contract
Pay: $60.00 - $65.00 per hour
Application Question(s):
Current Location?
Work Location: Remote",$62.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
KNN IT,#N/A,"Hillsboro, OH",Data Center Engineer,"KNN IT is looking for a Data Center Engineer for a role in Hillsboro, OR.
Applicant must have relevant experience
Must have an active VISA
Must have the basic tools like laptop etc
Candidates from near vicinity are encouraged to apply.
Job Types: Full-time, Contract
Salary: Up to $4,000.00 per month
Ability to commute/relocate:
Hillsboro, OH 45133: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data center: 1 year (Preferred)
Work Location: In person","$4,000 /mo (est.)",Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Contact Government Services, LLC
4.7",4.7,"Washington, DC",Data Engineer,"CGS is seeking a passionate and driven Data Engineer to support a rapidly growing Data Analytics and Business Intelligence platform focused on providing solutions that empower our Federal customers with the tools and capabilities needed to turn data into actionable insights. The ideal candidate is a critical thinker and perpetual learner; excited to gain exposure and build skillsets across a range of technologies while solving some of our clients’ toughest challenges. As a Data Engineer, you will be integral to data operations for the development and integration of multiple data types across a range of data sets and sources.

You will be responsible for the day-to-day operations of systems that depend on data, ensuring data is properly processed and securely transferred to its appropriate location, in a timely manner. Processing data will include managing, manipulating, storing and parsing data in a data pipeline for variety of target sources. You will also support maintenance of applications and tools that reside on these systems such as upgrades, patches, configuration changes, etc. The work is performed in a multidisciplinary team environment using agile methodologies. The candidate we seek must be highly motivated and enthusiastic about implementing new technologies and learning about new data in a small team environment where deadlines are important.
Responsibilities:
Complete development efforts across data pipeline to store, manage, store, and provision to data consumers
Being an active and collaborating member of an Agile/Scrum team and following all Agile/Scrum best practices
Write code to ensure the performance and reliability of data extraction and processing
Support continuous process automation for data ingest
Achieve technical excellence by advocating for and adhering to lean-agile engineering principles and practices such as API-first design, simple design, continuous integration, version control, and automated testing
Work with program management and engineers to implement and document complex and evolving requirements
Help cultivate an environment that promotes customer service excellence, innovation, collaboration, and teamwork
Collaborate with others as part of a cross-functional team that includes user experience researchers and designers, product managers, engineers, and other functional specialists
Required Skills:
Must be a US Citizen
Must be able to obtain a Public Trust Clearance
7+ years of IT experience including experience in design, management, and solutioning of large, complex data sets and models.
Experience with developing data pipelines from many sources from structured and unstructured data sets in a variety of formats
Proficiency developing ETL processes, and performing test and validation steps
Proficiency to manipulate data (Python, R, SQL, SAS)
Strong knowledge of big data analysis and storage tools and technologies
Strong understanding of the agile principles and ability to apply them
Strong understanding of the CI/CD pipelines and ability to apply them
Experience with relational database, such as, PostgreSQL
Work comfortably in version control systems, such as, Git Repositories
Contact Government Services strives to simplify and enhance government bureaucracy through the optimization of human, technical, and financial resources. We combine cutting edge technology with world class personnel to deliver customized solutions that fit specific needs and solve the most challenging, dynamic problems. Our team emphasizes technical expertise and an empathy to public service to provide unique top tier support for our government engagements.
SOME BENEFITS OF THIS POSITION INCLUDE:
Health, Dental, & Vision insurance
Life Insurance
401k
Flexible Spending Account (Health, Dependent Care, and Commuter)
Paid Time Off & Observance of State/Federal Holidays
For more information about CGS please visit: https://www.cgsfederal.com
39U0b7kJu8","$103,213 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Iron Bow Technologies
3.5",3.5,"Washington, DC",Sr. Data Center Engineer,"Iron Bow Technologies is seeking a Sr. Data Center Engineer to support our Federal customer, The Department of Justice (DOJ), Criminal Division (CRM), in Washington, DC. The selected candidate will support our flexible and scalable solution with the DOJ CRM to extend cloud-native features to existing legacy and next-generation applications while also future proofing the infrastructure to easily accommodate future requirements, such as transitioning to a public cloud.
This position is hybrid and requires 2 days a week onsite at the customer location in NW Washington, DC and support at the customer data center as needed in Sterling, VA.
Essential Duties and Responsibilities:
Assess and provide VMware solutions to include vCenter, vSphere, SRM, ESXi
Primary storage solutions and data file storage management, NAS file storage solutions
Data Center blade servers and rack servers
Server virtualization
Experience:
10 years of related experience providing system administration support in a virtualized environment, including at least 6 years of VMWare solutioning, including vCenter, vSphere, SRM, and ESXi
Data Center operations experience, with SAN and NAS
Experience supporting primary storage solutions and data file storage management, NAS file storage solutions
Hands on experience with Data Center blade and rack servers
Ability to work staggered shift hours when needed to support critical coverage and/or time-sensitive needs
Be a member of our Team providing coordinated 24x7 IaaS on-call support
U.S. Citizen
Currently hold or obtain/maintain a Public Trust High clearance
Education:
BS in Computer Science, Information Science, or other relevant discipline is preferred – additional years of experience may be considered in lieu of the degree
Preferred Certifications:
VMware Certified Professional
Primary storage certifications such as HPE, Dell-EMC, NetApp, or Pure Storage
VCP-DCV
Travel Requirements:
No travel
PRINCIPALS ONLY. NO AGENCIES PLEASE. MUST BE A US CITIZEN ABLE TO OBTAIN A PUBLIC TRUST SECURITY CLEARANCE.
Iron Bow Technologies is a leading IT solution provider dedicated to successfully transforming technology investments into business capabilities for government, commercial and healthcare clients. Our depth of technical expertise, global reach and strategic partnerships with industry leaders as well as disruptive technology partners uniquely positions Iron Bow to target the right solution to ensure your successful business outcome. We partner with our clients from planning and implementation through ongoing maintenance and management to deliver solutions that are strong, flexible and on target .
Iron Bow Technologies is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.
#LI-TM1",#N/A,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,1983,$500 million to $1 billion (USD)
National Group Corporation,#N/A,"Dallas, TX",Data Engineer,"Who are we?
ReAlign Insurance Holdings is a leading primary underwriter of program administrator managed specialty insurance programs in the U.S. With the extensive knowledge and experience our team has in building and managing carriers and programs we are able to reach decisions on program opportunities quicker than our competition.
Why choose ReAlign?
At ReAlign, we believe that we are only as strong as our people. We strive to create an inclusive and welcoming culture where employees feel comfortable and empowered to be themselves. We support a diverse and inclusive work environment, with a focus on a healthy work-life balance. We understand that comprehensive benefits are important which is why we offer a benefit package that includes: medical, dental, vision, company paid short and long-term disability, life insurance, health spending account, 401(k) match every pay period, PTO, paid holidays, floating holidays, tuition reimbursement, and referral bonuses.
Job Summary
The Data Engineer position will involve data exchange process analysis, design, development, unit testing, and support. This role would work with other data team members on database ETL processes and reporting needs in both the on-premise database environment as well as the Azure data environment. This position requires strong technical and analytical skills to review business processes and data, recommend solutions, conduct functional and code analysis, and lead and perform database development tasks. This position will also involve production support and require experience within a DevOps environment.
Essential Functions
· Design, development, and support data ETL processing for both external and internal customers
· Support the internal and Azure cloud-based data warehouse platforms
· Work with DBA/System Admin to develop and maintain automated data processing
· Design and analyze queries and stored procedures
· Write and support SSRS and PowerBI reports
· Work in a data development life cycle environment
Work with stakeholders and data analysts to understand business requirements
Work with end users and the testing team to perform unit, functional, and integration testing and validation prior to production implementation
Troubleshoot and resolve operational problems with a focus on delivery
· Communicate solutions and architectural design to IT leadership
· Work in both Kanban and Scrum Agile environments
· Continually improve processes and solutions, seeking ways to use new tools, features, and capabilities
Job Requirements
3-10 years of experience as a Database Developer
Highly proficient in writing advanced SQL queries, stored procedures, SSRS reports, and ETL processes
Strong knowledge of relational and transactional database architecture
Strong technical and analytical skills
Experience with the Microsoft Azure and PowerBI platforms and Agile development methodology a plus
Bachelor’s Degree in computer science, database development, or related field preferred
Detail oriented with the objective of delivering a solution to a problem in a timely and efficient manner
Self-driven, willing to take initiative when an opportunity presents itself
Strong collaboration and communication skills and the ability to work in a team environment a must
Ability and willingness to work in a partial on-prem and partial remote office environment
Experience in the Property and Casualty Insurance industry a plus
Proficient with standard desktop software used in a business environment (MS Word, Excel, Visio, etc.)
Must be able to work in a hybrid in-office and remote work environment
Must be authorized to work in the United States
Job Type: Full-time
Pay: $90,000.00 - $125,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Health savings account
Life insurance
Paid time off
Retirement plan
Vision insurance
Experience level:
3 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you currently local to the Dallas area and able to commute to the office 2 to 3 times per week? (Deal breaker)
Experience:
writing advanced SQL queries and ETL processes: 3 years (Required)
cloud database platform (i.e. Azure, AWS): 1 year (Preferred)
Azure Data Factory: 1 year (Preferred)
Work Location: In person","$107,500 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Hiscox
4.1",4.1,"Atlanta, GA",Senior Data Engineer,"job description:
You will be a senior member of our US IT Team data engineering practice. You will be an early member of this team and will work closely with the Director of Data Engineering to help establish it. We will have our work cut out for us delivering planned work, building the practice and evolving our technical landscape. This is an exciting opportunity for the right individual to join at the very beginning and grow with the new team.
As a senior you will be expected to lead, mentor others and deputise for the director of data engineering when required. Yoi will also continue to contribute as a hands-on engineer. You will have the knowledge, best earned through 5+ years of experience, to truly inhabit this senior role in an ambitious team.
What you’ll be doing:
Working with business stakeholders and our delivery team to understand high value business problems that can be solved through the application of data processing and analytical systems.
Developing, expanding and evolving our existing databases and ETL pipelines.
Working with architects to design, build and support a transformational cloud-based data platform for the business.
Being a core and professional member of the new data engineering practice
Understand business requirements and help refine into development tasks and estimate their complexity.
Research, evaluate and adopt new technologies with a right tool for the job mentality.
Focus on both speed of delivery and quality, with suitable pragmatism – ensuring your solutions are always appropriate and not too complex or over-engineered.
Quick progression of projects from proof-of-concept to post-production stage.
Communication and presentation of ideas to colleagues in all parts of the wider tech team.
Participating in code reviews for the data engineering practice.
Act as a servant leader in our practice – ensuring team members have
person specification:
Knowledge of, or ability to rapidly adopt our core languages for data engineering – SQL and/or Python.
Experience working using a modern DevOps approach. You should not just have written data code but also the CI/CD pipelines necessary to test and deploy that code in a professional environment.
A robust understanding of core data engineering topics – ETL vs ELT, structured and unstructured data, data quality and data governance.
Ability to contribute to all aspects of a solution – design, infrastructure, development, testing and maintenance.
The ability to design and advocate for technical solutions to business problems.
Effective collaboration with technical and non-technical team members through agile ceremonies – roadmap planning, feature workshops, backlog elaboration, code review.
Understanding of cloud technology and significant hands-on experience working with it. You must understand how cloud-native solutions must be built differently from traditional ones.
Track record of taking initiative and delivering projects end-to-end; clear evidence of being self-driven and motivated
Immense curiosity, high energy and desire to go the extra mile to make a difference.

Desirable Skills
Beyond the required skills we are open to individuals of diverse talents. Experience with additional technologies, data science knowledge or a business background are all valued. We want to know how your unique abilities can contribute to our team.
Our Technology
We operate in a diverse technical landscape and are looking for flexible engineers who can adapt to and use many different tools. We would not expect any engineer to be familiar with the entire tech stack – no engineer can. Instead, we seek people with a good understanding of data structures and algorithms and the ability to apply this knowledge in learning new tools.
We have an existing data landscape involving both MS SQL Server and Oracle databases. We use SSIS for ETL. We will also need to source data from our integration layer – webMethods.
If some of that sounds like it needs bringing up to date, we know! We hope to start building a modern cloud data platform to realise the full value of our data. The technologies we use for that platform are still to be decided. Likely it will be based on Snowflake with Azure Data Factory for ELT. Other parts of our business have had success using Prefect for orchestration and dbt for transformation and we will reuse where it is sensible.
We prepare data for use by analysts working with a variety of tools – Tableau, PowerBI, and even Excel.
We take a DevOps approach and strive for continuous integration / continuous deployment. We use Azure Pipelines to deploy our code. We deploy infrastructure the same way using Terraform and Docker.
Our technology landscape is not fixed. Our engineering and architecture teams drive the technology we adopt.


Salary range $140,000 - $170,000","$155,000 /yr (est.)",1001 to 5000 Employees,Company - Public,Insurance,Insurance Carriers,1901,$1 to $5 billion (USD)
"Petrolink International
2.9",2.9,"Houston, TX",Data Analysis Engineer,"Location:
Houston, Texas, U.S.A.
Job Type:
Full-time
Job Description:
Petrolink is a global company that specializes in user-driven technology for the Energy Industry.
We are looking for a Data Analysis Engineer to assist with creating and delivering algorithms primarily used by our engineering solutions.
This position is based in our U.S. office in Houston, Texas.
Duties & Responsibilities:
Research and provide logic for engineering solutions algorithms.
Collaborate with Subject Matter Experts (Drilling Engineers, Mud loggers, Pore Pressure Analysts, and other specialists from the Drilling Industry) to understand different challenges in the Drilling Industry and the practices in place to address/solve these challenges.
Code the algorithms using C# to run on Petrolink’s proprietary analytics engine.
Develop, test, and maintain algorithms.
Support escalations related to issues in algorithms.
Learn existing algorithms to provide support when required.
Prepare training material and sessions for stakeholders as part of the production release.
Skills & Qualifications:
Proficient in C#.
Experience with Python is an advantage.
Knowledge in drilling practices, and ability to speak to other drilling experts and capture requirements and logic.
Knowledge in drilling performance KPIs is an advantage.
Prior experience with WITSML is an advantage.
Function and communicate well in a team having diverse expertise.
Bi-lingual (English and Spanish) preferred.
At least 2 years’ relevant experience.
Benefits:
Health insurance
Dental insurance
401(k) with matching
Health savings account
Life insurance
Long-term disability
This job description is intended to explain the general nature and level of work being performed. Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.
Applicants must be legally authorized to work in the United States for any employer.
To apply for this position, applicants must meet the above criteria.
Interested applicants fulfilling the above criteria, please send your resume to careers.usa@petrolink.com.","$81,133 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"Gold Coast Health Plan
3.8",3.8,"Camarillo, CA",Sr. ETL DEV/Data Engineer,"Data Engineers will be responsible for transformation and modernization of enterprise data solutions on Cloud Platforms integrating Azure services and 3rd party data technologies. Data Engineer will work closely with a multidisciplinary agile team to build high quality data pipelines driving analytic solutions.
ESSENTIAL FUNCTIONS
Reasonable Accommodations Statement
To accomplish this job successfully, an individual must be able to perform, with or without reasonable accommodation, each essential function satisfactorily. Reasonable accommodations may be made to help enable qualified individuals with disabilities to perform the essential functions.
Essential Functions Statement
As a Data Engineer, you will be responsible for assisting our clients envision, design, and deploy data engineering workloads as part of our solutions. As part of a small, dynamic team, you will have the opportunity to contribute to multiple phases of the solution life cycle including designing and implementing models and processes for large-scale datasets used for descriptive, diagnostic, predictive, and prescriptive purposes
Work with the team to evaluate business needs and priorities, liaise with key business partners and address team needs related to data systems and management.
Translate business requirements into technical specifications; establish and define details, definitions, and requirements of applications, components and enhancements.
Generate design, development, test plans, detailed functional specifications documents, user interface design, and process flow charts for execution of programming.
Develop data pipelines / APIs using Python, SQL, potentially Spark and AWS, Azure or GCP Methods.
Build large-scale batch and real-time data pipelines with data processing frameworks in Azure cloud platform.
Assist in the migration from on-prem SQL Server data analytics platform to MS Azure cloud platform.
Work as part of a team to build upon ingestion framework to intake new data sources.
Analyze, design, code and test multiple components of application code across one or more clients.
Perform maintenance, enhancements and/or development work

Qualifications
BA/BS in computer science, mathematics, information management, business, or equivalent experience
6+ years of experience in SQL
4+ years of experience in Cloud Platforms: Azure or AWS or GCP
4+ years of experience in Python and Pyspark
4+ years of experience in Synapse highly preferred
Experience using SQL, dB Visualizer, AWS, Azure, Cloud technologies
Experience with Power BI or similar data visualization tools
knowledge of HL7 v2, HL7 CDA and FHIR interface mapping highly preferred
Exposure to non-relational databases and tools, such as Cassandra, JSON, JAVA, Python, and Spark
In-depth knowledge of healthcare interoperability and patient data aggregation
Ability to effectively communicate, at times in a non-technical language, with customers at all levels of the organization.","$127,500 /yr (est.)",Unknown,Self-employed,Insurance,Insurance Agencies & Brokerages,#N/A,Unknown / Non-Applicable
OpenSecrets.org,#N/A,Remote,Snowflake Data Engineer,"About us
Our goal is to is to track the flow of money in American politics and provide the data and analysis to strengthen democracy. Our vision is for Americans to use this knowledge to create a more vibrant, representative and accountable democracy..
Our work environment includes:
Modern office setting
Work-from-home days
Wellness programs
Data Engineer
OpenSecrets is seeking to add a new member to our data team. The Data Engineer will be responsible for helping build and maintain our data systems.
Responsibilities include:
Implementing methods to improve data reliability and quality
Analyzing raw data and implementing that data into our system using data pipelines
Troubleshooting data errors and pipeline issues
REQUIRED QUALIFICATIONS:
Experience working with and analyzing large datasets, equivalent to 2-3 years in a similar position
Proven ability to build complex data pipelines using Snowpipe and SSIS
The ability to work with diverse populations
Experience with MSSQL Server and Snowflake
Advanced knowledge of ANSI SQL
PREFERRED QUALIFICATIONS:
Excellent written and verbal communication skills.
The ability to manage multiple priorities and projects both independently and as a member of a team
Snowflake SnowPro Data Engineer certification
Experience with Apache Spark
Experience with machine learning matching models
Knowledge of and interest in American government and money in politics.
SALARY AND BENEFITS:
We are a fully remote organization and this person can be based anywhere in the U.S.
OpenSecrets offers an excellent benefits package, including full medical, dental, 403(b) match, generous vacation and sick leave, and all federal holidays paid.
TO APPLY:
Please visit this link: https://airtable.com/shrGe2bugeapKVr8a
People of diverse backgrounds are strongly encouraged to apply. OpenSecrets is an equal opportunity employer committed to creating a diverse and inclusive organizational culture. We do not discriminate against candidates and employees because of their disability, sex, race, gender identity, sexual orientation, religion, national origin, age, veteran status, or any other protected status under the law.
ABOUT US: OpenSecrets is the nation's premier research group tracking money in U.S. politics and its effect on elections and public policy. Our mission is to track the flow of money in American politics and provide the data and analysis to strengthen democracy. Our vision is for Americans to use this knowledge to create a more vibrant, representative and accountable democracy.
Job Type: Full-time
Pay: $83,874.82 - $100,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Retirement plan
Vision insurance
Compensation package:
Yearly pay
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Experience:
SQL: 3 years (Required)
Snowflake: 1 year (Preferred)
ETL: 2 years (Required)
Work Location: Remote","$91,937 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Barracuda Networks Inc.
3.6",3.6,"Alpharetta, GA",Data Engineer,"Job ID: 23-579

Come Join Our Passionate Team! At Barracuda, we make the world a safer place. We believe every business deserves access to cloud-enabled, enterprise-grade security solutions that are easy to buy, deploy, and use. We protect email, networks, data and applications with innovative solutions that grow and adapt with our customers’ journey. More than 200,000 organizations worldwide trust Barracuda to protect them — in ways they may not even know they are at risk — so they can focus on taking their business to the next level.
We know a diverse workforce adds to our collective value and strength as an organization. Barracuda Networks is proud to be an Equal Opportunity Employer, committed to equal employment opportunity and equitable compensation regardless of race, gender, religion, sex, sexual orientation, national origin, or disability.
Envision yourself at Barracuda
We currently have an exciting new position open for a Data Engineer. This person will assist in driving technology innovation for engineering projects and work on architecture of AWS, Elastic and Kubernetes. They will also work closely with the Technology Architect to create and drive the Architecture Review Board.
What you’ll be working on
Determine internal / external customer data needs and identify system specifications
Analyze the needs of large systems and breaking them down into smaller manageable parts
Responsible for designing the journey of data from various sources to the database and everything in between
Plan and design the structure of data, brainstorm with team
Analyze best technology products and systems for internal teams
Help provide build vs buy decisions on data architecture
Communicate data requirements to engineers; explain data architecture to them and provide assistance throughout SDLC
Choose / maintain suitable software, hardware and integration methods
Help resolve technical problems as and when they arise
Review testing procedures to ensure data performant and error free
Ensure that database satisfy quality standards and procedures. Setup such standards.
Work with senior IT personnel to devise plans for future IT requirements of the organization
Produce progress reports on key infrastructures - Elastic and Data Pipelines
What you bring to the role
Bachelor Degree or Post Graduate in Computer Science, or relevant work experience
5+ years of experience in an Engineering / Operations heavy organization
1+ years of experience as a Data Analysis and Data Design responsible for data architecture of a SaaS application
Experience in understanding and designing data sharding in a distributed database
Some Hands-on experience in software development and system administration
Knowledge of strategic IT solutions
Familiarity with programming languages like JavaScript or Java or Python
Familiarity with Linux OS and Windows alike
Experience in cloud computing and cloud Technologies (AWS, GCP, Azure)
Excellent communication skills – you should be adept at listening to, understanding, and explaining key concepts to managerial and technical resources.
Good Anticipation and problem-solving skills
Ability to prioritize and manage time – responsible for multiple streams of work at the same time
Relationship building - Ability to form a good rapport with internal and external stakeholders
In-depth knowledge about quality standards and security best practices

Preferred Skills:
Expert level Understanding of Elastic eco system - Elasticsearch, Logstash, Kibana, Filebeats
Understanding of secure tunnel communication using VPN
Experience designing data at scale
AWS experience is preferred in Cloud Technologies
What you’ll get from us
A team where you can voice your opinion, make an impact, and where you and your experience are valued. Internal mobility – there are opportunities for cross training and the ability to attain your next career step within Barracuda.
Equity, in the form of non-qualified options
High-quality health benefits
Retirement Plan with employer match
Career-growth opportunities
Flexible Time Off and Paid Time Off benefits
Volunteer opportunities
#LI-Remote","$98,161 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,$100 to $500 million (USD)
"Encore Technologies
4.4",4.4,"Atlanta, GA",Senior Data Engineer,"Encore Technologies is seeking a Senior Data Engineer to work for a client in Atlanta, GA (zip code 30339). This is a Direct Hire role that will be worked in a hybrid schedule, with 2-3 days a week onsite.
Summary:
The Senior Data Engineer is responsible for managing and organizing enterprise data. They will translate requirements and designs into functional data pipelines while ensuring the continued quality and completeness of the information. Senior Data Engineers will combine raw information from different sources to create consistent and machine-readable datasets that are easy to analyze and support company initiatives. They will support other Data Engineers and Data Analysts on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They will also implement methods to improve data reliability and quality, improve data visibility and reduce effort through automation.
Essential Duties:
Contribute to a team of data engineers through design, demand delivery, code reviews, release management, implementation, presentations, and meetings.
Mentor fellow data engineers and contribute to ongoing process improvements for the team
Evaluate business needs and objectives and align architecture/designs with business requirements
Build the data pipelines required for the optimal extraction, transformation, integration, and loading of raw data from a wide variety of data sources
Assemble large, complex data sets and model our data in a way that meets functional / non-functional business requirements
Create data tools for analytics team members that assist them in generating innovative industry insights that provide our business a competitive advantage
Implement data tagging mechanisms and metadata management so data is accurately classified and visible to the organization
Build processes to help identify and improve data quality, consistency, and effectiveness
Ensure our data is managed in a way that conforms to all information privacy and protection policies
Use agile software development processes to iteratively make improvements to our data management systems
Identify opportunities for automation
Be an advocate for best practices and continued learning
Requirements:
Bachelor's/Tech School degree in Computer Science, Information Systems, Engineering or equivalent and/or commensurate years of real-world experience in software engineering.
4+ years of relevant experience in data management
3+ years in data engineering with detailed knowledge of data warehouse technical architectures, infrastructure components, and ETL/ ELT.
Experience with performance analysis and optimization.
Experience in data acquisition, transformation, and storage design using design principles, patterns, and best practices.
Data engineering certification is a plus.
Experience with Informatica, Kafka, CDC, SQL, Irwin, Python, AWS (S3, Athena, Glue, Kinesis, Redshift), Spark, Scala, AI/ML, Modern data platforms, Snowflake, dbt, Fivetran, and Airflow.
Encore Technologies is an Equal Opportunity Employer. We respect and seek to empower each individual and support the diverse cultures, perspectives, skills, and experiences within our workforce.
Job Type: Full-time
Pay: $120,000.00 - $160,000.00 per year
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Atlanta, GA 30339: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Preferred)
Experience:
ETL/ELT: 3 years (Required)
data management: 4 years (Required)
Work Location: Hybrid remote in Atlanta, GA 30339","$140,000 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2014,$100 to $500 million (USD)
"NexTek, LLC",#N/A,"Shoreview, MN",Sr. Data Engineer,"Nextek, LLC is a software development company in the financial services industry that creates web applications and solutions for financial services professionals. We are seeking a motivated Senior Data Engineer to join the team.
What we provide:
A laptop and other equipment/software necessary for work
Health Insurance, Dental Insurance, Vision Insurance
Paid time off (PTO), Volunteer time off (VTO)
Hybrid remote work options
401K
Life Insurance
Flexible schedules
Monthly Social Activities
Casual dress code
Professional development
Here is what we are looking for:
Experience in database administration, security, backup, and recovery
Preferred Knowledge:4+ years of experience in data/software engineering
Experience in the financial services industry
Experience with a variety of relational databases (particularly MySQL & MSSQL)
Experience with collecting and presenting analytics
Here is what you will be doing:
Display proficiency in relevant technical skills.
Maintain a deep understanding of the data structure.
Provide evaluations and recommendations for our data environment based on potential issues, tradeoffs, and risks.
Scope and implement solutions.
Make good estimates and set realistic expectations for delivery.
Ask for guidance when necessary.
Maintain and monitor the health of databases and associated solutions.
Make use of the same environments setup by the Software Engineering team
Thoroughly test code for bugs and security issues before deploying to production environments
Maintain and monitor actively deployed solutions
Follow and help define best practices
Peer reviews
Maintain awareness of industry trends
Share experience and insights to help your team grow
Increase understanding of impact on other teams
Increase understanding of the customer
All other duties as assigned
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Shoreview, MN: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Cox powered by Atrium,#N/A,"Atlanta, GA",AWS Data Engineer- Hybrid,"Minimum Qualifications:
Bachelor’s degree or equivalent work experience
A minimum of 3+ years’ experience in Microsoft Windows/ SQL Server Technologies, .Net development, AWS Administration.
Experience working on 24x7 environments oriented towards a zero downtime target.
Working knowledge or previous administration of SQL 2016-SQL 2022 and Windows Server 2012+ preferred.
Ability to work with minimal direction, in a team environment.
Performance tuning for AWS/DataLake systems.
Some Experience with SQL in virtual, physical and cloud-based environments.
Experience with Athena and data modeling for cloud technologies.
Proven ability to quickly learn and implement new technologies.
Experience with Administration, Security/Identity Management and Terraforms in AWS.
Preferred Qualifications:
Experience with SentryOne, a plus.
Ability to code Powershell commands and maintain code in GitHub, a plus.
Some Experience with Metabase and Collibra, a plus.
Experience with ETL in AWS, a plus.
Pay Range:
$60-$68/hr
Requisition Disclaimer:
This job posting is for a temporary role as an employee of Atrium on assignment at Cox. The individual selected for this role will be offered the role as an employee of Atrium; compensation, medical benefits, fringe benefits and other terms and conditions of employment shall be presented by Atrium upon offer. The pay rate range provided is a reasonable estimate of the anticipated compensation range for this job at the time of posting. The actual pay rate will be based on a number of factors, including skills, competencies, experience, location and/or being pursued and other job-related factors permitted by law. In addition, this role will be eligible for overtime pay, in accordance with federal and state requirements

By applying for this position you agree to the Atrium Terms and Conditions. Agreeing to these terms, includes permission to use the email address and mobile phone number you provide during the application process or throughout the duration of your prospective or actual employees to notify you of job openings, profiles, articles, news, and other employment-related information, as well as to notify you of special promotions or additional products and services offered by us or our affiliates and partners (collectively, “Atrium Alerts”). Atrium Alerts may be sent by email, phone or text message. Your personal information will be safely stored in our database. Atrium does not sell your personal information to third parties. Text message and data rates may apply. To OPT OUT of text messaging or to modify your communication preferences for Atrium Alerts at any time, please contact us at privacyadministrator@atriumstaff.com.

If you do not agree with the Atrium Terms and Conditions, you can still complete your application for this position by emailing your resume to our team at coxrecruit@atriumworks.com. Please include the job title in the subject of your email.

As a woman-owned firm, Atrium values diversity. We are an equal opportunity employer and will consider all applications without regard to race, sex, age, color, religion, national origin, veteran status, disability, genetic information or any other characteristic protected by law. We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.
Posting: #zip",$64.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Loyal
3.9",3.9,"Atlanta, GA",Senior Cloud Data Operations Engineer,"Loyal is an organization centered on experience and building a platform that allows consumers to make meaningful decisions when it comes to healthcare. We deeply understand providers, locations, services, appointments, business rules, and moreover, we understand patients - who they are, the preferred method of communication, upcoming appointments, lapsed appointments, outstanding bills, health risks, and more. With this intelligence, our platform fuels highly relevant and personalized experiences across all mediums (website, email, voice…) allowing patients to get healthy, stay healthy, and have a better relationship with the health care provider.
**This is a remote role**
Summary
The Senior Cloud Data Operations Engineer is responsible for delivering Loyal's cloud-based data warehouse. They interface with groups across the organization to ensure we are collecting and storing data in a way that makes it easy to create and consume. They identify best practices for big data management, select the right tooling, create a workable data architecture, and drive an implementation roadmap others can depend on. The Senior Cloud Data Operations Engineer also works closely with our security group to ensure data security and privacy. They also mentor others on the team, providing technical leadership to those they work with.
You will
Create data streams and consolidate data sources into a single cloud-based data warehouse
Architect and catalog data warehouse storage schema
Work with product teams to enable easy data aggregation
Work with data consumers (analysts, data scientists, and AI/ML engineers) to ensure data is easily accessible and consumable
Create and execute a roadmap that meets data storage and availability goals
Ensure data is secure through data analysis, access control, and encryption
You have
Bachelor's degree in computer science, information systems, applied math or related field, or equivalent work experience
Minimum 5 years of experience working with cloud data processing services (AWS, Azure, or GCP).
Minimum 3 year experience with Big Data technologies, such as Databricks, Snowflake, Spark, Hadoop, etc.
Strong experience in creating data architecture and storage schemas required
Significant working knowledge of SQL and other query languages and tools
Significant experience with building relational database systems strongly preferred
Experience with ETL tools a plus
Knowledge of Python preferred
Software development, data analytics, or data science experience is a plus.
Experience working within a software as a service (SaaS) company preferred.
Experience working within a high growth and/or ambiguous environment, with proven experience to be dynamic preferred.
#LI-REMOTE
We know that potential candidates are often less likely to apply to a position if they don't match 100% of the job qualifications. Don't let that be why you miss out on this opportunity! We encourage you to apply if you can demonstrate many of these skills and competencies.
Loyal to our employees
We are a remote-friendly company! We encourage you to apply from anywhere in the United States. We also believe in a work/life balance that fulfills you while you're here and supports you when you're not. We built our benefits package to prove that we're committed to you having everything you need (including a little fun). Here is what we offer full-time employees:
Flexible paid time off, sick and personal days
At least one holiday per month (sometimes, more!)
Full health, dental, and vision insurance - Loyal pays the premium for all employees!
One Time Home Office Setup Stipend For Remote & Hybrid Roles
Monthly Internet Stipend for Remote & Hybrid Roles
Long term & short term disability
401[k] plan
16 Weeks Paid Parental Leave
2 Volunteer days per year
Matching Gift Program
Participation Grant Program
Annual Travel/Team Events up to twice per year (post-COVID)
Our Commitment
We believe that the key to Loyal's success is you. Your unique background, life experience, knowledge, self-expression, and talent make you uniquely you. Who you are, what you have experienced, and how you think inspires us to be innovative and bold.
Loyal is an equal opportunity employer. We hire great people from a wide variety of backgrounds, not just because it's the right thing to do, but because it makes our company stronger. We welcome the unique contributions that you can bring in terms of your education, opinions, culture, ethnicity, race, ancestry, sex, gender identity and expression, national origin, citizenship, marital status, age, languages spoken, veteran status, color, religion, disability, sexual orientation, and beliefs.
We consider qualified applicants regardless of criminal histories, consistent with legal requirements.
Further, consistent with applicable federal and state law, Loyal provides reasonable accommodations when requested by qualified applicants or employees with disabilities, unless doing so would cause an undue hardship. Loyal's policy regarding requests for reasonable accommodation applies to all aspects of employment, including the application process. If you require a reasonable accommodation in connection with the application process, please contact the Talent Acquisition Department at talentacquisition@loyalhealth.com.
E-Verify
This employer participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S. If E-Verify cannot confirm that you are authorized to work, this employer is required to give you written instructions and an opportunity to contact Department of Homeland Security (DHS) or Social Security Administration (SSA) so you can begin to resolve the issue before the employer can take any action against you, including terminating your employment. Employers can only use E-Verify once you have accepted a job offer and completed the I-9 Form.
COVID-19 Vaccinations
Consistent with Loyal policy, candidates performing in-person work will be required to show proof of being fully vaccinated against COVID-19 upon commencing employment. Reasonable accommodations will be considered on a case-by-case basis for exemptions to this requirement in accordance with applicable law. If you require a reasonable accommodation to Loyal's vaccination requirement, please contact the People Department at people@loyalhealth.com.",#N/A,51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2015,$5 to $25 million (USD)
The Data Sherpas,#N/A,Remote,Data Engineer - LATAM,"Who We Are:
Working at The Data Sherpas is like being part of a dynamic and collaborative team of talented individuals passionate about helping clients navigate the ever-changing information technology landscape. At The Data Sherpas, you can work with cutting-edge tools and technologies, tackle challenging data problems, and continuously develop your skills in a supportive environment. As a Data Sherpa, you'll be empowered to lead projects, take ownership of your work, and make meaningful contributions to our client's success.
What We Are Looking For:
We're seeking an ambitious and driven Data Engineer to join our team. As a Data Engineer, you can work on exciting, challenging projects supporting our client's needs. You will be part of a dynamic team of experts dedicated to improving business performance and driving data-driven results.
What You'll Do:
Design and implement data pipelines, ETL processes, and data warehousing solutions
Develop and maintain attribution and measurement models for ad campaigns
Perform data matching and segmentation techniques to create customer profiles and behavior patterns
Configure and maintain AWS infrastructure and services to support data engineering processes
Collaborate with cross-functional teams to identify data needs and develop data-driven solutions.
Stay up-to-date with the latest technologies and industry trends related to Data Engineering and Ad Tech.
What You Have:
Proficiency in data modeling, ETL development, and data warehousing
Ability to design and maintain data pipelines for large-scale data sets
Proven experience with Python programming language for data engineering solutions
Knowledge of data matching techniques for identifying duplicate data and inconsistencies
Experience with data deduplication techniques
Ability to work with large datasets to segment and group data
Experience with clustering algorithms and methodologies
Understanding of customer segmentation and persona development
Familiarity with data visualization tools to showcase segmentation results
Strong experience with data engineering in cloud-based environments, primarily in AWS
Experience using AWS data analytics services like EMR, Redshift, Kinesis, and Glue",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Tech Mahindra / Microsoft
3.2",3.2,Remote,Data Engineer - Cosmos,"Hi,
One of my direct client is looking for Data Engineer in Redmond, WA. If you are interested, please share me your updated resume.
Title: Data Engineer
Location: Redmond, WA
Direct Client: Microsoft
Job Description:
Primary Requirement:
1. Cosmos Scope Scripting
2. Azure Data Lake, Pipelines
3. ADLS, ADLA
4. SQL querying experience
5. Microsoft projects experience
Optional Requirement:
1. ADO
2. PowerShell or Python Scripting language
3. Project manager
Thanks & Regards
K. ManiMegalai
Phone: (206) 337-5702 Ext 241
Zen3 is now a Tech Mahindra Company.
Tech Mahindra is a strategic Microsoft Partner and is a proud partner in implementing the World’s largest Azure Migration program.
Job Type: Full-time
Schedule:
8 hour shift
Monday to Friday
Work Location: Remote",#N/A,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Alter Tech Solutions,#N/A,"Jersey City, NJ",Senior Data Engineer,"Job Title: Data Engineer
Location: Jersey City, NJ
Job Type: Hybrid
Job Description:
Position is more about scale - application monitoring, automation framework, test automation - data engineering - data pipelines - rest api's
system design - experience with distributed systems, design the right components, push out to the AWS cloud, data structures/algorithms
Experience:
10+ years of experience cross functional experience - data engineering and sw engineering background (working with risk, audit teams) need hands on coding but also design
Qualifications & Requirements:
TOP SKILLS java preferred language but would consider strong python - Apache Flink, Kafka, Cassandra, GraphQL, SPARK, MACHINE LEARNING.
Job Types: Full-time, Permanent
Salary: $60.00 - $75.00 per hour
Experience:
Java: 8 years (Preferred)
GraphQL: 8 years (Preferred)
Python: 5 years (Preferred)
Work Location: On the road",$67.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"American Power & Gas
2.5",2.5,"Largo, FL",Data Engineer,"Because of expansive growth American Power & Gas is seeking a Data Engineer to add to our technical team. This is a fulltime permanent on-site role.
We have been offering Green Energy solutions to both residential and small commercial customers for over 20 years and have won the award for fastest growing company in the Tampa Bay Business Journal as well as being featured in Forbes and the Huffington Post.
**
**
Key Responsibilities
Support operational executives in solving business problems by designing, developing, troubleshooting, and implementing data driven solutions to complex technical objectives.
Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes.
Deploy sophisticated analytics programs, machine learning and statistical models to predict business outcomes and continually optimize performance through data science.
Gather and summarize technical requirements associated with strategic business initiatives.
Document, maintain, support and enhance company technology platforms and analytics applications.
Look for opportunities to streamline and automate various reporting processes to help support internal teams.
Assist with the research and development of internal business case studies to support onboarding new data tools.
Extract, manipulate and cleanse raw data from various data sources – call center, web, and CRM systems.
Work independently as well as collaboratively with other team members and key stakeholders as required to troubleshoot and resolve data issues.
Provide comprehensive technical and consultative services to support development and maintenance of internal and third-party platforms.
Create functional test cases/criteria to verify all functionality adheres to specifications and create end user manuals.
Routinely represent the Analytics department in cross-functional status & data strategy meetings.
Assist business analysts in provision of regular performance trends reporting, forecasts, and insights for marketing and sales team leaders as well as senior executive management to maintain a tight finger on the pulse of emerging performance trends and opportunities.
Partner with internal cross functional teams to identify business needs and analytics opportunities, developing tools and techniques to analyze and provide performance-improving recommendations.
Partner with the Operations team to optimize data workflows from sourcing to storage to reporting to deliverables to maximize for value-added and time-efficiency.
Develop, enhance, and manage various analytical solutions in support of business objectives.
Determine what data is needed and how to consume and store this data to support reporting needs and ad hoc performance-improving analysis for internal stakeholders.
Ensure deliverables are adapted properly to stakeholder audience; adapting terminology and visuals as needed to “speak the stakeholder’s language”, thus communicating with maximum effectiveness.
Troubleshoot and QA data, reporting, and tracking anomalies as needed, with proactive communication to stakeholders.
Relentlessly challenge the status quo. Always be critical of how we can be more effective or more efficient as an individual, as a team, and as a business.
Provide ongoing and proactive client service to your internal customers as required to continue elevating the performance of the business.
Regularly work with and analyze data across marketing channels and the customer journey through website analytics, call center activity, and CRM systems.
Requirements
University degree or college graduate in Engineering, Computer Science, Mathematics, Statistics, related technical/programming discipline, or the equivalent hands on experience in Data Engineering or Software Development.
Proficiency with coding in SQL is required. Ability to also write in Python, R, Java, or similar programming languages is preferred.
Strong technical prowess, including an understanding of algorithms, systems architecture and end user experience.
Experience with modern source, build, and deploy tools such as Git, Grub, Maven, Yeoman, etc. is a plus.
Ability to think unconventionally to derive innovative and creative solutions.
Competency in accurately estimating development timelines.
Experience with data warehouse design, relational databases, SQL/NoSQL data modeling, RESTful API standards and large scale data processing solutions.
Demonstrated skill in database development with solid understanding of schema design, stored procedure development, query optimization and ETL processes.
Excellent troubleshooting ability. Must be able to resolve issues tied to capturing and processing data in a timely manner.
Excellent English written and verbal communication skills, especially explaining technical concepts to non-technical business leaders.
Exceptional critical thinking and problem-solving skills; able to distill overall objectives into the actionable steps required to achieve those objectives.
Capable of effectively managing projects, priorities, timelines, and working relationships.
Must possess the intellectual curiosity to succeed in a dynamic, entrepreneurial, fast paced, sales driven organizational culture.
Excited by the opportunity to disrupt the status quo and uncover the eureka moment insights that will take the business to the next level – proactively searching for problems to solve, knowing there is so much to learn.
We offer Health, Dental, Optical and Life Insurance, PTO (paid time off) and the opportunity for promotions and room to advance.
For immediate consideration please send a resume to Carl Schumacher the Manager of Recruiting CarlS@goapg.com
Job Type: Full-time","$92,733 /yr (est.)",201 to 500 Employees,Company - Private,"Energy, Mining & Utilities",Energy & Utilities,2010,$100 to $500 million (USD)
"Intertech, Inc
4.4",4.4,Minnesota,Sr. Data Engineer,"Sr. Data Engineer
US Citizenship Required
Contract to Hire Opportunity
Fully Remote

The Senior Data Engineer will oversee the department's data integration work, including developing data models, maintaining a data warehouse and analytics environment, and writing scripts for data integration and analysis. This role will work closely and collaboratively with members of other areas to define requirements, mine and analyze data, integrate data from a variety of sources, and deploy high quality data pipelines in support of the analytics needs.

Responsibilities
Maintain and build our data warehouse and analytics environment
Design, implement, test, deploy, and maintain stable, secure, and scalable data engineering solutions and pipelines in support of data and analytics projects, including integrating new sources of data into our central data warehouse, and moving data out to applications and affiliates as needed
Make data available for the reporting and analytics teams
Produce scalable, replicable code and engineering solutions that help automate repetitive data management tasks
Implement and monitor best in class security measures in our data warehouse and analytics environment, with an eye towards the evolving threat landscape
Maintain high quality documentation of data definitions, transformations, and processes to ensure data governance and security
Other duties as assigned
Provide technical assistance in an on-call rotation



Job Qualifications

Required:
Bachelor’s Degree in Computer Science or Management Information Systems (MIS) or Business, Finance or Accounting with an emphasis in MIS
Minimum 5 years experience of developing and supporting enterprise level data warehouse systems
Strong knowledge of relational databases and SQL. Extract, Transform, and Load (ETL) data into a relational database
General data manipulation skills: read in data, process and clean it, transform and recode it, merge different data sets together, reformat data between wide and long, etc.
Demonstrated ability to learn new techniques and troubleshoot code without support, ex. find answers to common programming challenges
Strong knowledge of T-SQL language as evidenced by ability to write complex SQL queries, Microsoft SQL Management Studio, SQL Analysis Services and SQL Server Integration Services
Demonstrated ability to work independently and be a self-starter
Demonstrated ability to work effectively in teams, in both a lead and support role
Strong analytical and problem-solving skills with the ability to work independently with minimal supervision

Preferred:
Experience working with Data Vault 2.0
Experience working with cloud infrastructure services like Amazon Web Services and Google Cloud
Experience with advanced data visualization and mapping",#N/A,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,1991,$5 to $25 million (USD)
Accentvision Technology Inc,#N/A,"Plano, TX",Senior Data Engineer - Confluent Kafka,"Job Description:
Apply Confluent Kafka API lifecycle development and management.
Administer and improve Kafka use throughout organization including Kafka Producers, Kafka Consumers, Kafka Connect, KsqlDB, KStreams, and custom implementations.
Work with multiple teams to ensure best use of Kafka and data-safe event streaming.
Understand and apply event-driven architecture patterns and Kafka standard processes.
Model system behaviors using standard process methods for communicating architecture and design.
Develop or assist developers in choosing correct patterns, event modeling, and ensuring data integrity.
Work with Kafka APIs to provide pro-active insights and automation.
Acquire a deep understanding of source and sink connector technical details for a variety of platforms including Snowflake, Sales Force, MongoDB, PostgreSQL, MS SQL Server, Azure, and others as required.
Experience:
Senior Data Engineer: 10+ years proven experience in data engineering.
Data Engineer: 5+ years of proven experience in data engineering.
Solid understanding of Kafka (Consumption, publishing, and streaming) architecture and integrations.
Familiar with 3rd Party Confluent Kafka Connectors, Kafka Connect and its connectors for integrating with external systems.
Development experience using Confluent Kafka producers, consumers, and streams.
Experience with building streaming applications with Confluent Kafka
Experience with Java coding, CI/CD processes, microservices is required.
Good exposure to various Azure cloud platforms to play a key role in Application Modernization
Experience in CI/CD, DevOps tool chain, GIT, docker, Jira, and a test-driven approach to agile delivery.
Ability to participate in and contribute to code management in GitHub including actively collaborating in peer-reviews, feature branches, and resolving impediments and commits.
Job Types: Full-time, Contract, Temporary, Permanent
Pay: $110,000.00 - $150,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Vision insurance
Compensation package:
1099 contract
Hourly pay
Yearly pay
Experience level:
6 years
Schedule:
Monday to Friday
Experience:
Data Engineering: 6 years (Required)
Confluent Kafka: 5 years (Required)
KSQL: 5 years (Required)
KStreams: 5 years (Required)
Work Location: Hybrid remote in Plano, TX 75024","$130,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"R1 RCM, Inc.
3.4",3.4,Remote,Data Engineer III,"The Software Engineer III serves as a Data Engineer focused on building next generation of healthcare products and services. As part of a collaborative, motivated Agile-based team, the Software Engineer III is responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.
Responsibilities
Create and maintain optimal data pipeline architecture to assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Work with stakeholders to optimize the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using ‘big data’ technologies.
Work with stakeholders including the Product, Data and Engineering teams to assist with data-related technical issues and support their data infrastructure needs.
Adhere to and ensure engineering best practices using modern SDLC that enables CI/CD and favors automation, auditability, automated testing, infrastructure, and policy as code.
Ability to collaborate effectively with other architects and engineers delivering end to end, high-performing, highly reliable, scalable, and operable solutions.
Required Qualifications
Bachelor’s degree in Computer Science, Engineering, or a related technical field; or equivalent experience.
3+ years of experience delivering data engineering applications and services.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets .
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Experience in any one of the big data technologies (Hadoop, MapReduce , Spark, Data Lake etc. )
Experience in any one of the stream-processing systems ( Kafka, Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages : Python, Java, Scala, etc.
Understanding of modern software development including version control, unit testing, and continuous integration and deployment.
Strong communication skills, with proven ability to present complex ideas and to document concisely.
Preferred Qualifications
Working knowledge of Cloud providers and services such as Amazon AWS or Microsoft Azure.
Experience with relational SQL and NoSQL databases, including MongoDB and ElasticSearch.
Knowledge of healthcare data standards such as HL7, FHIR, EDI X12.
Knowledge of the healthcare revenue cycle, EMRs, practice management systems.
Working in an evolving healthcare setting, we use our shared expertise to deliver innovative solutions. Our fast-growing team has opportunities to learn and grow through rewarding interactions, collaboration and the freedom to explore professional interests.

Our associates are given valuable opportunities to contribute, to innovate and create meaningful work that makes an impact in the communities we serve around the world. We also offer a culture of excellence that drives customer success and improves patient care. We believe in giving back to the community and offer a competitive benefits package including:
Comprehensive Medical, Dental, Vision & RX Coverage
Paid Time Off, Volunteer Time & Holidays
401K with Company Match
Company-Paid Life Insurance, Short-Term Disability & Long-Term Disability
Tuition Reimbursement
Parental Leave
R1 RCM Inc. (“the Company”) is dedicated to the fundamentals of equal employment opportunity. The Company’s employment practices , including those regarding recruitment, hiring, assignment, promotion, compensation, benefits, training, discipline, and termination shall not be based on any person’s age, color, national origin, citizenship status, physical or mental disability, medical condition, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status or any other characteristic protected by federal, state or local law. Furthermore, the Company is dedicated to providing a workplace free from harassment based on any of the foregoing protected categories.
If you have a disability and require a reasonable accommodation to complete any part of the job application process, please contact us at 312-496-7709 for assistance.
CA PRIVACY NOTICE: California resident job applicants can learn more about their privacy rights California Consent",#N/A,10000+ Employees,Company - Public,Healthcare,Health Care Services & Hospitals,2003,Unknown / Non-Applicable
"Contact Government Services, LLC
4.7",4.7,"Washington, DC",Data Engineer,"CGS is seeking a passionate and driven Data Engineer to support a rapidly growing Data Analytics and Business Intelligence platform focused on providing solutions that empower our Federal customers with the tools and capabilities needed to turn data into actionable insights. The ideal candidate is a critical thinker and perpetual learner; excited to gain exposure and build skillsets across a range of technologies while solving some of our clients’ toughest challenges. As a Data Engineer, you will be integral to data operations for the development and integration of multiple data types across a range of data sets and sources.

You will be responsible for the day-to-day operations of systems that depend on data, ensuring data is properly processed and securely transferred to its appropriate location, in a timely manner. Processing data will include managing, manipulating, storing and parsing data in a data pipeline for variety of target sources. You will also support maintenance of applications and tools that reside on these systems such as upgrades, patches, configuration changes, etc. The work is performed in a multidisciplinary team environment using agile methodologies. The candidate we seek must be highly motivated and enthusiastic about implementing new technologies and learning about new data in a small team environment where deadlines are important.
Responsibilities:
Complete development efforts across data pipeline to store, manage, store, and provision to data consumers
Being an active and collaborating member of an Agile/Scrum team and following all Agile/Scrum best practices
Write code to ensure the performance and reliability of data extraction and processing
Support continuous process automation for data ingest
Achieve technical excellence by advocating for and adhering to lean-agile engineering principles and practices such as API-first design, simple design, continuous integration, version control, and automated testing
Work with program management and engineers to implement and document complex and evolving requirements
Help cultivate an environment that promotes customer service excellence, innovation, collaboration, and teamwork
Collaborate with others as part of a cross-functional team that includes user experience researchers and designers, product managers, engineers, and other functional specialists
Required Skills:
Must be a US Citizen
Must be able to obtain a Public Trust Clearance
7+ years of IT experience including experience in design, management, and solutioning of large, complex data sets and models.
Experience with developing data pipelines from many sources from structured and unstructured data sets in a variety of formats
Proficiency developing ETL processes, and performing test and validation steps
Proficiency to manipulate data (Python, R, SQL, SAS)
Strong knowledge of big data analysis and storage tools and technologies
Strong understanding of the agile principles and ability to apply them
Strong understanding of the CI/CD pipelines and ability to apply them
Experience with relational database, such as, PostgreSQL
Work comfortably in version control systems, such as, Git Repositories
Contact Government Services strives to simplify and enhance government bureaucracy through the optimization of human, technical, and financial resources. We combine cutting edge technology with world class personnel to deliver customized solutions that fit specific needs and solve the most challenging, dynamic problems. Our team emphasizes technical expertise and an empathy to public service to provide unique top tier support for our government engagements.
SOME BENEFITS OF THIS POSITION INCLUDE:
Health, Dental, & Vision insurance
Life Insurance
401k
Flexible Spending Account (Health, Dependent Care, and Commuter)
Paid Time Off & Observance of State/Federal Holidays
For more information about CGS please visit: https://www.cgsfederal.com
39U0b7kJu8","$103,213 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"AMBE Engineering
4.4",4.4,"Rome, GA",Data Engineer,"DATA ENGINEER
FULL TIME
LOCATION: Rome, GA
Works with various stakeholders across the organization to create dashboards and interactive visual reports using Power BI.
Designs, extends and enhance Power BI data models and development of new cubes.
Maintains functionality and relevance of reports and problem solve as issues arise.
Interprets trends and patterns, conduct complex data analysis and report on results.
Develops profiling scripts, data quality reports. Collaborate with business to develop DQ rules to run profiling.
Links Power BI with ERP system for efficient and automated report generation.
Combines raw information from different sources.
Works with users and team members at all levels to elevate existing reports and reporting capabilities and distribution.
Develops, test, and deploy DAX based scripts, queries and functions in Power BI.
Drives continuous improvement in IT processes and increases operational efficiencies
Works closely with project teams in gathering requirements, standing up and configuring the platform.
Other duties can be assigned based on company needs and employee capabilities.
REQUIREMENT
Understands business requirements in BI context and convert data into meaningful insights.
Be able to write and execute DAX measures, queries, and functions in Power BI.
Strong SQL skills (complex queries on relational databases, stored procedures, automation).
Strong Excel skills (experience migrating data from Excel to Power BI strongly preferred)
Understands business requirements in BI context and convert data into meaningful insights
Benefits
Who we are:
Ambe Engineering is a dual-certified W/MBE (woman and minority owned business) Diversity Supplier continuing to grow its ecosystem and extend their expertise globally. We bring the right resource for supplier development, high-impact project management, holistic cost savings, lean manufacturing and quality systems/problem solving solutions.
Quality, Logistics & Production | Crisis Management / Critical Situations | Cost Reduction | HR Services
__
_www.ambeeng.com_
Job Type: Full-time
Pay: $130,000.00 - $160,000.00 per year
Schedule:
8 hour shift
Day shift
Experience:
Power BI: 2 years (Required)
Automotive: 2 years (Preferred)
Work Location: In person","$145,000 /yr (est.)",Unknown,Company - Private,Manufacturing,Transportation Equipment Manufacturing,#N/A,Unknown / Non-Applicable
"LatentView Analytics
4.0",4.0,"Dallas, TX",Data Engineer,"Role : Data Engineer
Experience : 6 - 8+ Years
Location : Dallas,Tx (Onsite)
Position: FullTime Only
Skills :Python,SQL Server ,Scala, Hadoop, HPCC, Storm, Cloudera, Cassandra,Excel, R,Docker,Kubernetes,Snowflake,Azure,Kafka,Redshift,Hadoop,AWS.
Job Type: Full-time
Pay: $80,000.00 - $1,100,000.00 per year
Compensation package:
Yearly pay
Experience level:
6 years
Schedule:
8 hour shift
Experience:
data engineer: 6 years (Preferred)
Work Location: In person
Speak with the employer
+91 9876543210","$109,012 /yr (est.)",1001 to 5000 Employees,Company - Public,Management & Consulting,Business Consulting,2006,$25 to $100 million (USD)
"Kroenke Sports Enterprises
3.4",3.4,"Denver, CO",Data Engineer,"Job Title: Data Engineer
Department: Hockey Operations
Business Unit: Colorado Avalanche
Location: Denver, CO or Remote
Reports To: Director of Analytics
Employment Type: Full Time – Salaried - Exempt
Supervisor Position: No
_____________________________________________________________________________________
Kroenke Sports & Entertainment (KSE) is an American Sports and Entertainment holding company based in Denver, Colorado. KSE is committed to providing world class sports and entertainment for both live and broadcast audiences. We are the employer of choice as the owner and operator of Ball Arena, DICK’S Sporting Goods Park, the Paramount Theatre, 1STBANK Center, Denver Nuggets (NBA), the Colorado Avalanche (NHL), Colorado Mammoth (NLL), Colorado Rapids (MLS), KIMN,KXKL, KKSE (FM/AM), Altitude Sports & Entertainment, Major League Fishing/Fishing League Worldwide (MLFLW), Winnercomm, Outdoor Sportsman Group and SkyCam.

Nature of Work:
The Colorado Avalanche are looking to hire a full-time Data Engineer to work within the team’s Hockey Operations Department. This person will be responsible for maintaining and expanding the Avalanche hockey operations database. They will be tasked with importing and integrating data from external providers and interacting with the rest of the hockey operations department to ensure optimal dissemination of information to the appropriate parties. This person will also have the opportunity to analyze data and share insights with members of the Analytics department as well as the broader Hockey Operations department if desired. They will report to the Director of Analytics.

Examples of work performed:
Manage and improve the organization’s data storage, structure, and ETL pipeline while optimizing query performance for large datasets
Design automated processes to oversee data integrity and query performance on a regular basis, as well as being available to spot and resolve data issues that may arise at any time
Ensure that our automated processes run on schedule without issue
Incorporate expansive new datasets from disparate sources into our structure in a seamless fashion

This description is a summary only and is describing the general level of work being performed, it is not intended to be all-inclusive. The duties of this position may change from time to time and/or based on business needs. We reserve the right to add or delete duties and responsibilities at the discretion of the supervisor and/or hiring authority.

Working Conditions & Physical Demands:
Typical Office Conditions
Travel may be required

Qualifications:
Required
Strong knowledge of ETL architecture and development in a cloud-based environment
Academic and/or industry experience in database architecture, back-end software design, and query optimization
Expertise in SQL and Spark
Excellent attention to detail, problem-solving abilities, and work ethic
Preferred
1-3 years of work experience in a database-related position
An advanced degree in software engineering, computer science, information technology, or a related field
Familiarity with R and Python, as well as R Studio / Posit
Experience with databricks and Linux operating system
Knowledge of NHL players and teams as well as a passion for hockey and hockey analytics
Some front-end development and/or data analysis experience is considered a plus

Competencies/Knowledge, Skills & Abilities:
Ability to maintain positive attitude and demonstrate professionalism
Ability to maintain a high level of confidentiality
Ability to complete work accurately and in a timely manner
Ability to work independently & in a group setting and demonstrate good judgment skills
Ability to communicate effectively orally and in writing
Possesses excellent interpersonal skills
Ability to multi-task, prioritize and adapt to changing environments

Compensation:
Salary Range $80,000 - $110,000 per annum

Benefits Include:
12 Paid Company Holidays
Health Insurance (Medical, Dental, Vision)
Paid Time Off (PTO)
Life Insurance
Short and Long-term Disability
Health Savings Account (HSA)
Flexible Spending plans (FSAs)
401K/Employer Match

Equal Employment Opportunity
Kroenke Sports & Entertainment (KSE) provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.","$95,000 /yr (est.)",1001 to 5000 Employees,Company - Private,"Arts, Entertainment & Recreation",Sports & Recreation,#N/A,Unknown / Non-Applicable
"Diverse Lynx
3.9",3.9,"Charlotte, NC",Azure Data Engineer,"Production support Engineer
Job summary required skill:
ITIL and Prod Support experience
Powershell • Scripting knowledge
Should be able to check server and service health
Should be able to check certificates
Good to have knowledge of SQL (preferred)
Development experience
Experience: 5+ years
Required Skills:
1. Powershell
2. Any Scripting knowledge
3. SQL
Preferred Skills: Good to have knowledge of SQL
Technical Skills: PowerShell Domain Skills, ITIL Domain Skills, Technology Infrastructure Services
Development experience required skill: PowerShell
ITIL and Prod Support experience
Powershell
Scripting knowledge
Should be able to check server and service health
Should be able to check certificates
Job Type: Full-time
Salary: $78,245.06 - $171,281.86 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Charlotte, NC 28203: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL (Preferred)
Informatica (Preferred)
Data warehouse (Preferred)
Work Location: In person","$124,763 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Software Development,2002,$100 to $500 million (USD)
"RevolutionParts
4.0",4.0,United States,Data Engineer,"RevolutionParts is dedicated to modernizing the auto industry through our parts e-commerce platform. And we are pretty great at it too! We have enabled thousands of dealerships to sell auto parts online by transforming the way buyers and sellers connect.

And not only are we dedicated to revolutionizing the auto industry; we are also passionate about building a revolutionary team. Our Revolutionaries (as we call ourselves) are talented humans who have a shared goal of delivering an exceptional product and customer experience. Plus, we have fun while doing it!

The Role
RevolutionParts is on a mission to take our data to the next level. In this key role, you will help design and implement the next generation of our ETL pipeline for our parts catalog, pricing, and inventory data, which powers all of our eCommerce solutions. You will also be involved in establishing an enterprise-grade data platform for our largest partners. This is a high-impact role where you will be driving initiatives affecting teams and decisions across the company and setting standards for all our data stakeholders. Does the idea of spearheading a data practice in a high-growth e-commerce business sound exciting? If so, read on.
Responsibilities
We’re pulling in diverse data sources. You’ll need to learn our data and bring a strong grasp of ETL & ELT, workflows, AWS Glue, and data organization via efficient data lake and relational designs.
You will help design and build all stages of data from access to transformation and modeling.
You will build quality into the pipeline from day one using automated tests and data validation.
You will work with stakeholders in Product and data science to run ad hoc analysis of our data to answer questions and help prototype solutions.
You must own business problems through to resolution both individually and as part of a data team.
You will support product engineering teams by performing query analysis and optimization, as well as work with product teams to implement data driven product features.
Requirements
You should have 4+ years experience as a data engineer; or at least 2 years experience as a Data Engineer and 3+ as a software engineer.
You need to show us that you know what good looks like. This means experience implementing automated tests in a multi-stage data pipeline to ensure quality.
You are highly analytical and curious by nature.
You must have the ability to own business problems and the design and solutions that drive business outcomes.
You must be a team player with the ability to work with others and know when to support and when to push.
This role requires strong communication and collaboration skills; comfortable discussing projects with anyone from end users up to executive leadership.
Fluency with the programming language of your trade. Our primary languages for data are Golang and Python, but we use others as well. You must be comfortable learning new skills on the job.
We require fluency with best practices in an object-oriented design and programming; experience as a backend software engineer is necessary. Demonstrable experience with a functional paradigm is also valuable.
The ability to write and optimize complex SQL statements is a base requirement.
Familiarity with ETL/ELT pipelines and modern tools is fundamental to this role. We are building workflows managed in Argo utilizing Docker containers deployed within Kubernetes.
You should have experience working in a cloud-based software development environment, preferably with AWS.
Familiarity with no-SQL databases such as ElasticSearch, DynamoDB or others is helpful.
Bachelor’s Degree in Computer Science or equivalent is required.
RevolutionParts is proud to provide all full-time Revolutionaries with a comprehensive employment package including competitive compensation, career development, benefits, 401K match, parental leave, and many more valuable perks. You can learn more about our core-value driven culture at our career page.

RevolutionParts is an Equal Opportunity Employer; we value diversity. We do not discriminate on the basis of race, religion, color, national origin, gender, gender orientation, gender identity or expression, sexual identity, sexual orientation, age, marital status, family status, genetic information, veteran status, or disability status.",#N/A,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2013,$5 to $25 million (USD)
"MoneyDolly
5.0",5.0,"Salt Lake City, UT",General Data Engineer,"Position Title: General Data Engineer
Location: Remote (US Based only)
Commitment: Full-Time

Job Brief

About Us
MoneyDolly is a fast growing Fintech Saas tech company, headquartered in Sandy, Utah. We are leading innovation in supporter relationship management, wherein teams meet all their fundraising goals. Teams can create their own page for your supporters to visit, offer products and incentives for their contributions, then simply invite their team to spread the word and watch the money roll in. MoneyDolly has already helped thousands of organizations, groups and teams nationwide raise over $20 million. We are still private, and our best work is still ahead of us. This is a massive industry with antiquated methods and no clear market leader. This is the spot for a qualified team player looking to build something new, make a real impact, and actually change the world.

Job Description:
We are looking for a highly motivated and talented Generalist Data Engineer to join our development team. The ideal candidate will be someone who loves to wear multiple hats and thrives in an environment of high autonomy to accomplish our business goals. You will be responsible for collaborating with our business team, creating flexible data models, and maintaining data pipelines to ensure the smooth flow of data in our organization.
Responsibilities:
Interact with our business team to gather requirements and understand data needs
Design and create flexible data models that allow for easy report generation and ad-hoc analysis
Write and maintain DBT transformations to generate flexible data models from our production database
Develop and maintain required data pipelines in Python to collect and process data from various sources
Develop and maintain Tableau and Hex.Tech dashboards to analyze the collected data
Ensure data quality, integrity, and security in all data processes
Collaborate with other team members to implement data solutions and integrate them into the existing infrastructure
Continuously monitor and optimize data models and pipelines to meet changing business requirements
Requirements:
Bachelor's degree in Computer Science, Engineering, or a related field
Strong experience in data modeling, ETL processes, and data pipeline development
Proficiency in Python and SQL
Familiarity with DBT transformations and best practices
Experience working with relational databases and big data technologies
Excellent problem-solving skills and attention to detail
Strong communication and collaboration skills
Ability to work independently and adapt to a fast-paced, dynamic environment
Nice to have:
Experience with data visualization tools (e.g., Tableau, Power BI)
Experience with AWS and GCP (RDS Postgres and BigQuery are our current stack)
Knowledge of the fundraising industry or a strong interest in learning more about it
What We Offer:
A competitive salary ($100k-$140k range) and benefits package
A supportive and collaborative work environment
Opportunities for professional growth and development
The chance to make a significant impact in a growing startup
If you are passionate about data engineering and excited about the prospect of revolutionizing the fundraising industry, we'd love to hear from you. Apply now to join the MoneyDolly team!","$120,000 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"GOBankingRates
3.3",3.3,"North, SC",Staff Data Engineer,"GOBankingRates™ is unique in the digital marketing and media industry - we combine marketing, digital, content and fintech. Our performance based approach increases brand awareness and generates targeted audience engagement on our internal web properties and partner sites.
Learn More About What We Do

What's interesting about this role?
GOBankingRates has big growth plans ahead and is looking for a strong Staff Data Engineer experienced in Data warehousing and Python to join our Data Engineering team. The GOBankingRates Data Engineering team is responsible for designing and developing the Data lake, enterprise database, data warehouse, reporting solutions, and pipelines for data processing. If you are a critical thinker with a solid track record of developing data solutions and solving complex problems with SQL and Python, we want you to join our team! You will play a vital role in designing and developing our next generation data pipelines and data platform. Join our team and prototype new data product ideas and concepts!
How will you make an impact?
Build and maintain multiple data pipelines to ingest new data sources (API and file-based) and support products used by external users and internal teams.
Optimize by building tools to evaluate and automatically monitor data quality and develop automated scheduling, testing, and distribution of feeds.
Work with data engineers, data scientists, and product managers to design, rapid prototype, and productize new data product ideas and capabilities.
Design and build cloud-based data lakes and data warehouses.
Conquer complex problems by finding new ways to solve them with simple, efficient approaches focusing on our platforms' reliability, scalability, quality, and cost.
Collaborate with the team to perform root cause analysis and audit internal and external data and processes to help answer specific business questions.
What will you bring to us?
Master's Degree (or a B.S. degree with relevant industry experience) in math, statistics, computer science, or equivalent technical field
Experience with dimensional data modeling and schema design in a database or data warehouse
Expertise with scripting languages such as Python and writing efficient and optimized SQL.
Working experience in building data warehouses and data lakes.
Experience working directly with data analytics to bridge business requirements with data engineering.
Experience with AWS infrastructure
Ability to operate in an agile, entrepreneurial start-up environment and prioritize
Excellent communication and teamwork, and a passion for learning
Curiosity and passion for data, visualization, and solving problems
Willingness to question the validity, accuracy of data, and assumptions
Preferred Qualifications:
Experience building data warehouse, data lake, and data pipeline using Snowflake/Redshift and other AWS Technologies.
Experience with large-scale distributed systems with large datasets.
Experience with event streams and stream processing (e.g., Kafka, Spark, Kinesis)
Hands-on experience with event streaming with modern event streaming tools like Pulsar, Kafka, and Kinesis. Understanding when streaming vs. batch processing is appropriate, and tradeoffs in a given context
Knowledge of advertising platforms.

Benefits
Competitive salary with excellent growth opportunity; we pride ourselves in having a team that exudes leadership, high initiative, creativity and passion.
Awesome medical, dental and vision plans with heavy employer contribution
Paid maternity leave and paternity leave programs
Paid vacation, sick days and holidays
Company funding for outside classes and conferences to help you improve your skills
Contribution to student loan debt payments after the first year of employment
401(k) - employees can start contributing immediately. After the first year, GOBankingRates matches your contribution up to 4% of your salary
A note about our response to COVID -19 and our new norm: The world has changed and we know it's important to adapt and to do our part to take care of our teams in this global pandemic. Our number one priority is to have our team feel safe, balanced and connected. We're committed to providing our teams with the best resources and tools to navigate this new virtual world that we're living in. We've also reinvented the ways in which we recognize, celebrate, and engage with each other to keep our culture strong!
Here's a peek into our world at GOBankingRates -
Our teams are working remotely 100% for the foreseeable future and have flex time. We're in the digital media space so we're mobile and flexible!
Option to work from an office (if you need to get away!)
Tools & resources are available to keep our team connected across North America. (JIRA, Trello, Airtable, Slack, Zoom and so much more!)
To keep our community engaged and connected, virtual team building events are held weekly and monthly.
For wellness and balance, weekly virtual fitness classes such as yoga are available.
To care for the local communities that we're a part of across the U.S our team members host socially distanced philanthropic events every quarter.
And most importantly, we've committed to consistent and transparent communication to help us all stay informed, engaged and to keep us on our path to success and #greatness.
We are an equal-opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.","$134,519 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Internet & Web Services,#N/A,$25 to $100 million (USD)
"National Conference of Bar Examiners
4.1",4.1,"Madison, WI",Data Engineer,"JOB SUMMARY:
The National Conference of Bar Examiners (NCBE) is a nonprofit organization that provides high-quality assessment products, services, and research for the benefit and protection of the public and the legal profession. We assist state courts and licensing authorities with fulfilling their responsibility to determine minimal competence for entry to the legal profession.

Diversity, fairness, and inclusion are central to NCBE’s mission and to our vision for a competent, ethical, and diverse legal profession. NCBE provides an inclusive and family-friendly environment, flexible schedules, remote work options, and competitive salary and benefits. NCBE’s headquarters is located in Madison, Wisconsin, a vibrant community with excellent municipal services and educational opportunities.

The Data Engineer is responsible for creating, editing, and maintaining SQL functionality in the relational database platforms. The data engineer is responsible for managing and developing code on Oracle, MySQL, and Postgres platforms both on-prem and in the cloud. The data engineer also works to support application developers with database expertise regarding data architecture, SQL construction, application/database integration, as well as developing and maintaining complex procedures for the software backend. The position is also responsible for providing business management information to business leaders via ad-hoc queries and reports. Some traditional DBA server management skills such as creating and restoring data, creating / updating development and test database environments, maintaining backups, tuning for optimal performance and security awareness are also used.
Work is performed under the supervision of the IT Manager of Infrastructure.

ESSENTIAL DUTIES and RESPONSIBILITIES:
Create databases with efficient structures.
Develop, test, maintain and document changes to databases, both internal and public facing.
Develop queries, views and triggers for integration with other applications.
Assist developers and others in the department with database related needs.
Perform customer support for applications as needed.
Monitor health and security of the databases. Maintain high standards of data quality and integrity.
Develop and maintain queries and complex reports as needed.
Develop, debug and maintain procedures, functions and other DDL as needed; understand issues related to performance and security.
Understand and apply the SDLC, specifically Scrum and Agile methodology.
Perform other duties as assigned.

QUALIFICATIONS:
S:
Education and Experience Required:
4+ years of experience working on progressively more complex systems.
PL/SQL, Transact SQL, SQL 92 or other database language required.
Familiarity with scripting languages.
Familiarity with Linux.
Familiarity with Agile development principles.
Strong work ethic, professional, organized, and flexible; able to work independently to solve issues through self-directed research and troubleshooting.
Ability to handle and prioritize multiple tasks.
Bachelor’s Degree or relevant work experience.

Preferred Qualifications:
Design and development of database functions, procedures and packages.
Experience with reporting tools such as Crystal Report.
Experience working in a team environment.
Experience with modern systems development model.

The Data Engineer position may work remotely.

Mission
NCBE promotes fairness, integrity, and best practices in admission to the legal profession for the benefit and protection of the public. We serve admission authorities, courts, the legal education community, and candidates by providing high-quality
assessment products, services, and research;
character investigations; and
informational and educational resources and programs.

EEO Statement
NCBE is proud to be an equal employment opportunity organization. We are committed to providing equal employment opportunity to all applicants and employees regardless of their race, color, religion, age, sex, national origin, disability, military service, protected veteran status, genetic information, sexual orientation, gender identity, or any other characteristic protected by federal, state or local law.

Please note that applicants may be contacted via email throughout the hiring process. We suggest that you add BambooHR (@bamboohr.com and @app.bamboohr.com) to your Approved/Safe Sender list so that email notifications are delivered to your inbox and not marked as spam.","$89,236 /yr (est.)",51 to 200 Employees,Nonprofit Organization,Government & Public Administration,State & Regional Agencies,#N/A,$5 to $25 million (USD)
United Digestive,#N/A,"Atlanta, GA",Data Engineer,"GENERAL SUMMARY OF DUTIES: United Digestive’s Data Engineer will be responsible for developing and maintaining data pipelines, data warehouses, and data models to support the company's Business Intelligence reporting initiatives.
REPORTS TO: Director of Business Intelligence & Analytics
RESPONSIBILITIES:
Duties may include but are not limited to:
Implement, maintain, and improve data integration processes to collect, process, and manage large datasets
Collaborate with stakeholders to understand business requirements and design data solutions that meet their needs
Implement and maintain data quality checks and validations to ensure data accuracy and completeness
Optimize data processing and storage to ensure high availability, scalability, and performance
Develop and maintain automated data workflows and scheduling tools
Code ETL process using Azure Data Factory workflows and other technologies as appropriate
Troubleshoot and resolve data-related issues and incidents
Stay up to date with the latest technologies and trends in data engineering, specifically the Azure suite, and recommend improvements and optimizations to the data architecture and infrastructure
Ability to develop solutions and manage existing Azure DevOps pipelines
Maintains a high level of confidentiality with sensitive information
Performs any other duties and/or special tasks as assigned by United Digestive leadership
REQUIRED EDUCATION, SKILLS & EXPERIENCE:
Bachelor’s degree in Computer Science, Software Engineering, Information Technology, or in a related field required; healthcare-related Data Analytics experience preferred; 2+ years of data pipeline management required. Proficiency in SQL and experience with relational databases (e.g., Oracle, SQL Server, PostgreSQL); experience with data modeling, data warehousing, and ETL processes, proficiency in various programming language (e.g., Python, SQL, Java), experience with cloud platforms (e.g., AWS, Azure, GCP) and big data technologies (e.g., Databricks, Hadoop, Spark, Kafka, Cassandra). Must have knowledge of data visualization and BI tools (e.g., Tableau, Power BI) and how to support these tools in an efficient manner.
ADDITIONAL SKILLS AND EXPERIENCE:
Data Engineer must have:
Excellent verbal and written communication skills.
Excellent organizational skills and attention to detail.
Ability to multi-task and prioritize job needs
Ability to deal with patients, visitors, co-workers, and physicians with courtesy and respect
Work under pressure; assess, respond to, and communicate issues in a timely manner.
Self-motivation and be a proactive individual with a professional can-do attitude
PHYSICAL/MENTAL/ENVIRONMENTAL DEMANDS
Requires sitting and standing associated with a normal office environment. Travel as business needs dictate.
DRUG FREE WORKPLACE
United Digestive is a drug free workplace. All offers of employment are contingent upon passing a pre-employment drug screening.
EQUAL OPPORTUNITY EMPLOYER
United Digestive is an Equal Opportunity Employer and does not discriminate on the basis of race, religion, gender, color, or national origin in its employment practices.
Location: 1355 Peachtree Street NE STE 1600, Atlanta, GA 30309
Job Type: Full-time",#N/A,Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Trellis
4.4",4.4,Remote,Data Engineer,"Overview
As a Data Engineer for Trellis, you will be responsible for designing, building, and maintaining scalable data pipelines and infrastructure to support the deployment of machine learning models for educational applications. You will be working with large volumes of text data, and will be responsible for ensuring data quality, performing data cleaning, and implementing data transformation processes.
Responsibilities
Build and maintain data pipelines for processing large volumes of text data
Perform data cleaning and preprocessing to ensure data quality and consistency
Implement data transformation processes to convert raw data into formats suitable for machine learning models
Collaborate with data scientists and other stakeholders to understand project requirements and provide data engineering support as needed
Develop and maintain data infrastructure to support machine learning workflows
Monitor and troubleshoot data pipelines to ensure high availability and reliability
Qualifications
Bachelor's or Master's degree in Computer Science, Data Science, or a related field
Strong programming skills in Python and experience with data processing frameworks such as Spark and Hadoop
Experience with text data processing and natural language processing (NLP) techniques
Familiarity with machine learning workflows and frameworks such as TensorFlow and PyTorch
Experience with cloud-based data storage and processing technologies such as AWS, Google Cloud, or Azure
Strong problem-solving skills and attention to detail
Excellent communication and collaboration skills
Benefits
SF office, but remote-friendly. Come into the office 60% of the time — you’ll want to! It’ll be built as a library in a way that’s anti-fatigue. We will also have offices in NYC and Montreal.
Health insurance with 100% premium covered
Generous PTO / sick leave
401(k) plan with employer match
Free lunch and snacks
Annual company retreat in Montreal
Bring your dog to work",#N/A,51 to 200 Employees,Company - Private,Media & Communication,Advertising & Public Relations,2012,$5 to $25 million (USD)
"BuzzClan Private Limited
4.3",4.3,"Dallas, TX",Data Platform Engineer,"Only w2 CANDIDATES
Job Overview
The Data Platform Engineer shall be responsible for maintaining current data platforms, troubleshooting issues, designing new data platforms, ensuring data platform availability, ensuring data integrity, implementing data platform changes, managing security, and providing on-call support. Current data platforms include – SQL Server, Azure SQL Database, Azure SQL Datawarehouse, Azure Data Lake, SQL Server Analysis Services (SSAS), SQL Server Integration Services (SSIS), SQL Server Reporting Services (SSRS), and PowerBI. Teams supported include: Application Engineering, Data Engineering, Decision Science, and Business Analytics.
For a successful candidate for this position, you must -
Have extensive experience of the setup, deployment and hardening of data-intensive environments using cloud-based database and storage technologies
Have experience in creating robust and automated pipelines to ingest and process structured and unstructured data sources into analytical platforms using batch and streaming mechanisms leveraging cloud native toolset
Be able to drive security paradigms for databases and related infrastructure resources
Leverage the right tools for the right job to deliver testable, maintainable, and modern data solutions
Be comfortable with researching data questions, identify root causes, and interact closely with business users and technical resources on various data related decisions
Understand how to profile code, queries, programming objects and optimize performance
Be able to advice on performance optimizations and best practices for scalable data models, pipelines and queries
Aspire to be efficient, thorough and proactive
Responsibilities and Duties
Provides and designs tools to assist in the management of the data platforms
Works to provide a working model of our transaction processing environment for capacity assessment and planning
Develops a methodology for the ongoing assessment of data platform performance and the identification of problem areas
Develops a security scheme for the data platform environment, as well as assisting in disaster recovery, if necessary
Provides leadership during the development and enhancement of the enterprise’s production applications including working with applications, technical support and operations during the design, development and implementation of applications
Recognizes and identifies potential areas where existing policies and procedures require change, or where new ones need to be developed, especially regarding future business expansion
Fulfills departmental requirements in terms of providing work coverage and administrative notification during periods of personnel illness, vacation, or education
Performs at or above the enterprise’s Information Technology performance standards
Participates with vendors in the assessment of advanced transaction processing and data platform productions including beta and field test participation
Special projects as requested
Performs other duties as assigned
Job Type: Full-time
Salary: $80,000.00 - $90,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Vision insurance
Experience level:
8 years
9 years
Schedule:
8 hour shift
Monday to Friday
Work Location: On the road","$85,000 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$5 to $25 million (USD)
OpenSecrets.org,#N/A,Remote,Snowflake Data Engineer,"About us
Our goal is to is to track the flow of money in American politics and provide the data and analysis to strengthen democracy. Our vision is for Americans to use this knowledge to create a more vibrant, representative and accountable democracy..
Our work environment includes:
Modern office setting
Work-from-home days
Wellness programs
Data Engineer
OpenSecrets is seeking to add a new member to our data team. The Data Engineer will be responsible for helping build and maintain our data systems.
Responsibilities include:
Implementing methods to improve data reliability and quality
Analyzing raw data and implementing that data into our system using data pipelines
Troubleshooting data errors and pipeline issues
REQUIRED QUALIFICATIONS:
Experience working with and analyzing large datasets, equivalent to 2-3 years in a similar position
Proven ability to build complex data pipelines using Snowpipe and SSIS
The ability to work with diverse populations
Experience with MSSQL Server and Snowflake
Advanced knowledge of ANSI SQL
PREFERRED QUALIFICATIONS:
Excellent written and verbal communication skills.
The ability to manage multiple priorities and projects both independently and as a member of a team
Snowflake SnowPro Data Engineer certification
Experience with Apache Spark
Experience with machine learning matching models
Knowledge of and interest in American government and money in politics.
SALARY AND BENEFITS:
We are a fully remote organization and this person can be based anywhere in the U.S.
OpenSecrets offers an excellent benefits package, including full medical, dental, 403(b) match, generous vacation and sick leave, and all federal holidays paid.
TO APPLY:
Please visit this link: https://airtable.com/shrGe2bugeapKVr8a
People of diverse backgrounds are strongly encouraged to apply. OpenSecrets is an equal opportunity employer committed to creating a diverse and inclusive organizational culture. We do not discriminate against candidates and employees because of their disability, sex, race, gender identity, sexual orientation, religion, national origin, age, veteran status, or any other protected status under the law.
ABOUT US: OpenSecrets is the nation's premier research group tracking money in U.S. politics and its effect on elections and public policy. Our mission is to track the flow of money in American politics and provide the data and analysis to strengthen democracy. Our vision is for Americans to use this knowledge to create a more vibrant, representative and accountable democracy.
Job Type: Full-time
Pay: $83,874.82 - $100,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Retirement plan
Vision insurance
Compensation package:
Yearly pay
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Experience:
SQL: 3 years (Required)
Snowflake: 1 year (Preferred)
ETL: 2 years (Required)
Work Location: Remote","$91,937 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
National Group Corporation,#N/A,"Dallas, TX",Data Engineer,"Who are we?
ReAlign Insurance Holdings is a leading primary underwriter of program administrator managed specialty insurance programs in the U.S. With the extensive knowledge and experience our team has in building and managing carriers and programs we are able to reach decisions on program opportunities quicker than our competition.
Why choose ReAlign?
At ReAlign, we believe that we are only as strong as our people. We strive to create an inclusive and welcoming culture where employees feel comfortable and empowered to be themselves. We support a diverse and inclusive work environment, with a focus on a healthy work-life balance. We understand that comprehensive benefits are important which is why we offer a benefit package that includes: medical, dental, vision, company paid short and long-term disability, life insurance, health spending account, 401(k) match every pay period, PTO, paid holidays, floating holidays, tuition reimbursement, and referral bonuses.
Job Summary
The Data Engineer position will involve data exchange process analysis, design, development, unit testing, and support. This role would work with other data team members on database ETL processes and reporting needs in both the on-premise database environment as well as the Azure data environment. This position requires strong technical and analytical skills to review business processes and data, recommend solutions, conduct functional and code analysis, and lead and perform database development tasks. This position will also involve production support and require experience within a DevOps environment.
Essential Functions
· Design, development, and support data ETL processing for both external and internal customers
· Support the internal and Azure cloud-based data warehouse platforms
· Work with DBA/System Admin to develop and maintain automated data processing
· Design and analyze queries and stored procedures
· Write and support SSRS and PowerBI reports
· Work in a data development life cycle environment
Work with stakeholders and data analysts to understand business requirements
Work with end users and the testing team to perform unit, functional, and integration testing and validation prior to production implementation
Troubleshoot and resolve operational problems with a focus on delivery
· Communicate solutions and architectural design to IT leadership
· Work in both Kanban and Scrum Agile environments
· Continually improve processes and solutions, seeking ways to use new tools, features, and capabilities
Job Requirements
3-10 years of experience as a Database Developer
Highly proficient in writing advanced SQL queries, stored procedures, SSRS reports, and ETL processes
Strong knowledge of relational and transactional database architecture
Strong technical and analytical skills
Experience with the Microsoft Azure and PowerBI platforms and Agile development methodology a plus
Bachelor’s Degree in computer science, database development, or related field preferred
Detail oriented with the objective of delivering a solution to a problem in a timely and efficient manner
Self-driven, willing to take initiative when an opportunity presents itself
Strong collaboration and communication skills and the ability to work in a team environment a must
Ability and willingness to work in a partial on-prem and partial remote office environment
Experience in the Property and Casualty Insurance industry a plus
Proficient with standard desktop software used in a business environment (MS Word, Excel, Visio, etc.)
Must be able to work in a hybrid in-office and remote work environment
Must be authorized to work in the United States
Job Type: Full-time
Pay: $90,000.00 - $125,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Health savings account
Life insurance
Paid time off
Retirement plan
Vision insurance
Experience level:
3 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you currently local to the Dallas area and able to commute to the office 2 to 3 times per week? (Deal breaker)
Experience:
writing advanced SQL queries and ETL processes: 3 years (Required)
cloud database platform (i.e. Azure, AWS): 1 year (Preferred)
Azure Data Factory: 1 year (Preferred)
Work Location: In person","$107,500 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Barracuda Networks Inc.
3.6",3.6,"Alpharetta, GA",Data Engineer,"Job ID: 23-579

Come Join Our Passionate Team! At Barracuda, we make the world a safer place. We believe every business deserves access to cloud-enabled, enterprise-grade security solutions that are easy to buy, deploy, and use. We protect email, networks, data and applications with innovative solutions that grow and adapt with our customers’ journey. More than 200,000 organizations worldwide trust Barracuda to protect them — in ways they may not even know they are at risk — so they can focus on taking their business to the next level.
We know a diverse workforce adds to our collective value and strength as an organization. Barracuda Networks is proud to be an Equal Opportunity Employer, committed to equal employment opportunity and equitable compensation regardless of race, gender, religion, sex, sexual orientation, national origin, or disability.
Envision yourself at Barracuda
We currently have an exciting new position open for a Data Engineer. This person will assist in driving technology innovation for engineering projects and work on architecture of AWS, Elastic and Kubernetes. They will also work closely with the Technology Architect to create and drive the Architecture Review Board.
What you’ll be working on
Determine internal / external customer data needs and identify system specifications
Analyze the needs of large systems and breaking them down into smaller manageable parts
Responsible for designing the journey of data from various sources to the database and everything in between
Plan and design the structure of data, brainstorm with team
Analyze best technology products and systems for internal teams
Help provide build vs buy decisions on data architecture
Communicate data requirements to engineers; explain data architecture to them and provide assistance throughout SDLC
Choose / maintain suitable software, hardware and integration methods
Help resolve technical problems as and when they arise
Review testing procedures to ensure data performant and error free
Ensure that database satisfy quality standards and procedures. Setup such standards.
Work with senior IT personnel to devise plans for future IT requirements of the organization
Produce progress reports on key infrastructures - Elastic and Data Pipelines
What you bring to the role
Bachelor Degree or Post Graduate in Computer Science, or relevant work experience
5+ years of experience in an Engineering / Operations heavy organization
1+ years of experience as a Data Analysis and Data Design responsible for data architecture of a SaaS application
Experience in understanding and designing data sharding in a distributed database
Some Hands-on experience in software development and system administration
Knowledge of strategic IT solutions
Familiarity with programming languages like JavaScript or Java or Python
Familiarity with Linux OS and Windows alike
Experience in cloud computing and cloud Technologies (AWS, GCP, Azure)
Excellent communication skills – you should be adept at listening to, understanding, and explaining key concepts to managerial and technical resources.
Good Anticipation and problem-solving skills
Ability to prioritize and manage time – responsible for multiple streams of work at the same time
Relationship building - Ability to form a good rapport with internal and external stakeholders
In-depth knowledge about quality standards and security best practices

Preferred Skills:
Expert level Understanding of Elastic eco system - Elasticsearch, Logstash, Kibana, Filebeats
Understanding of secure tunnel communication using VPN
Experience designing data at scale
AWS experience is preferred in Cloud Technologies
What you’ll get from us
A team where you can voice your opinion, make an impact, and where you and your experience are valued. Internal mobility – there are opportunities for cross training and the ability to attain your next career step within Barracuda.
Equity, in the form of non-qualified options
High-quality health benefits
Retirement Plan with employer match
Career-growth opportunities
Flexible Time Off and Paid Time Off benefits
Volunteer opportunities
#LI-Remote","$98,161 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,$100 to $500 million (USD)
"American Power & Gas
2.5",2.5,"Largo, FL",Data Engineer,"Because of expansive growth American Power & Gas is seeking a Data Engineer to add to our technical team. This is a fulltime permanent on-site role.
We have been offering Green Energy solutions to both residential and small commercial customers for over 20 years and have won the award for fastest growing company in the Tampa Bay Business Journal as well as being featured in Forbes and the Huffington Post.
**
**
Key Responsibilities
Support operational executives in solving business problems by designing, developing, troubleshooting, and implementing data driven solutions to complex technical objectives.
Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes.
Deploy sophisticated analytics programs, machine learning and statistical models to predict business outcomes and continually optimize performance through data science.
Gather and summarize technical requirements associated with strategic business initiatives.
Document, maintain, support and enhance company technology platforms and analytics applications.
Look for opportunities to streamline and automate various reporting processes to help support internal teams.
Assist with the research and development of internal business case studies to support onboarding new data tools.
Extract, manipulate and cleanse raw data from various data sources – call center, web, and CRM systems.
Work independently as well as collaboratively with other team members and key stakeholders as required to troubleshoot and resolve data issues.
Provide comprehensive technical and consultative services to support development and maintenance of internal and third-party platforms.
Create functional test cases/criteria to verify all functionality adheres to specifications and create end user manuals.
Routinely represent the Analytics department in cross-functional status & data strategy meetings.
Assist business analysts in provision of regular performance trends reporting, forecasts, and insights for marketing and sales team leaders as well as senior executive management to maintain a tight finger on the pulse of emerging performance trends and opportunities.
Partner with internal cross functional teams to identify business needs and analytics opportunities, developing tools and techniques to analyze and provide performance-improving recommendations.
Partner with the Operations team to optimize data workflows from sourcing to storage to reporting to deliverables to maximize for value-added and time-efficiency.
Develop, enhance, and manage various analytical solutions in support of business objectives.
Determine what data is needed and how to consume and store this data to support reporting needs and ad hoc performance-improving analysis for internal stakeholders.
Ensure deliverables are adapted properly to stakeholder audience; adapting terminology and visuals as needed to “speak the stakeholder’s language”, thus communicating with maximum effectiveness.
Troubleshoot and QA data, reporting, and tracking anomalies as needed, with proactive communication to stakeholders.
Relentlessly challenge the status quo. Always be critical of how we can be more effective or more efficient as an individual, as a team, and as a business.
Provide ongoing and proactive client service to your internal customers as required to continue elevating the performance of the business.
Regularly work with and analyze data across marketing channels and the customer journey through website analytics, call center activity, and CRM systems.
Requirements
University degree or college graduate in Engineering, Computer Science, Mathematics, Statistics, related technical/programming discipline, or the equivalent hands on experience in Data Engineering or Software Development.
Proficiency with coding in SQL is required. Ability to also write in Python, R, Java, or similar programming languages is preferred.
Strong technical prowess, including an understanding of algorithms, systems architecture and end user experience.
Experience with modern source, build, and deploy tools such as Git, Grub, Maven, Yeoman, etc. is a plus.
Ability to think unconventionally to derive innovative and creative solutions.
Competency in accurately estimating development timelines.
Experience with data warehouse design, relational databases, SQL/NoSQL data modeling, RESTful API standards and large scale data processing solutions.
Demonstrated skill in database development with solid understanding of schema design, stored procedure development, query optimization and ETL processes.
Excellent troubleshooting ability. Must be able to resolve issues tied to capturing and processing data in a timely manner.
Excellent English written and verbal communication skills, especially explaining technical concepts to non-technical business leaders.
Exceptional critical thinking and problem-solving skills; able to distill overall objectives into the actionable steps required to achieve those objectives.
Capable of effectively managing projects, priorities, timelines, and working relationships.
Must possess the intellectual curiosity to succeed in a dynamic, entrepreneurial, fast paced, sales driven organizational culture.
Excited by the opportunity to disrupt the status quo and uncover the eureka moment insights that will take the business to the next level – proactively searching for problems to solve, knowing there is so much to learn.
We offer Health, Dental, Optical and Life Insurance, PTO (paid time off) and the opportunity for promotions and room to advance.
For immediate consideration please send a resume to Carl Schumacher the Manager of Recruiting CarlS@goapg.com
Job Type: Full-time","$92,733 /yr (est.)",201 to 500 Employees,Company - Private,"Energy, Mining & Utilities",Energy & Utilities,2010,$100 to $500 million (USD)
"BuzzClan Private Limited
4.3",4.3,"Dallas, TX",Data Platform Engineer,"Only w2 CANDIDATES
Job Overview
The Data Platform Engineer shall be responsible for maintaining current data platforms, troubleshooting issues, designing new data platforms, ensuring data platform availability, ensuring data integrity, implementing data platform changes, managing security, and providing on-call support. Current data platforms include – SQL Server, Azure SQL Database, Azure SQL Datawarehouse, Azure Data Lake, SQL Server Analysis Services (SSAS), SQL Server Integration Services (SSIS), SQL Server Reporting Services (SSRS), and PowerBI. Teams supported include: Application Engineering, Data Engineering, Decision Science, and Business Analytics.
For a successful candidate for this position, you must -
Have extensive experience of the setup, deployment and hardening of data-intensive environments using cloud-based database and storage technologies
Have experience in creating robust and automated pipelines to ingest and process structured and unstructured data sources into analytical platforms using batch and streaming mechanisms leveraging cloud native toolset
Be able to drive security paradigms for databases and related infrastructure resources
Leverage the right tools for the right job to deliver testable, maintainable, and modern data solutions
Be comfortable with researching data questions, identify root causes, and interact closely with business users and technical resources on various data related decisions
Understand how to profile code, queries, programming objects and optimize performance
Be able to advice on performance optimizations and best practices for scalable data models, pipelines and queries
Aspire to be efficient, thorough and proactive
Responsibilities and Duties
Provides and designs tools to assist in the management of the data platforms
Works to provide a working model of our transaction processing environment for capacity assessment and planning
Develops a methodology for the ongoing assessment of data platform performance and the identification of problem areas
Develops a security scheme for the data platform environment, as well as assisting in disaster recovery, if necessary
Provides leadership during the development and enhancement of the enterprise’s production applications including working with applications, technical support and operations during the design, development and implementation of applications
Recognizes and identifies potential areas where existing policies and procedures require change, or where new ones need to be developed, especially regarding future business expansion
Fulfills departmental requirements in terms of providing work coverage and administrative notification during periods of personnel illness, vacation, or education
Performs at or above the enterprise’s Information Technology performance standards
Participates with vendors in the assessment of advanced transaction processing and data platform productions including beta and field test participation
Special projects as requested
Performs other duties as assigned
Job Type: Full-time
Salary: $80,000.00 - $90,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Vision insurance
Experience level:
8 years
9 years
Schedule:
8 hour shift
Monday to Friday
Work Location: On the road","$85,000 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$5 to $25 million (USD)
"DSFederal Inc
4.0",4.0,Remote,Data Engineer*,"Description:
We are seeking an experienced Data Engineer to design, build, and maintain our government client’s data infrastructure. The Data Engineer will work closely with cross-functional teams to develop scalable data solutions that support our client’s business needs.
Requirements:
Design, develop, and maintain data pipelines and data storage systems.
Collaborate with cross-functional teams to design and implement data-driven solutions.
Optimize and tune data systems for performance and scalability.
Implement and maintain data quality and validation processes.
Troubleshoot and debug data-related issues.
Participate in code reviews and contribute to best practices for data engineering.
Technical Skills Required:
Proficiency in one or more programming languages (Python, Java, etc.)
Experience with data modeling, database design, data marts, and data warehousing concepts
Knowledge of ETL tools and techniques
Experience with cloud-based data platforms such as AWS or Azure
Strong problem-solving and analytical skills
Desired Skills:
Familiarity with data visualization tools such as Tableau or Power BI.
Knowledge of machine learning concepts and frameworks.
Experience Required:
3+ years of experience in data engineering or related field
Education Required:
Bachelor's in engineering

Who We Are:
DSFederal is a leader in health IT and data analytics that delivers innovative solutions to improve healthcare delivery and address critical challenges, including cancer surveillance, HIV/AIDS, maternal and child health, and COVID-19 emergency response. Our highly experienced professionals serve over 50 different federal clients on over 80 projects. Our people are united by one mission – to improve human life through transformative solutions. www.dsfederal.com.
What We Can Offer You:
DSFederal attracts the best people in the business with our competitive benefits package that includes medical, dental and vision coverage, 401k plan with employer contribution, paid holidays, vacation, and more. If you enjoy being a part of a high performing, professional services, and technology focused organization, we encourage you to apply!
Our Mission: Improve human life through transformative solutions.
Our Core Values: Integrity, People First, Disciplined Business Practices, Customer Centric, Value Creation.
EEO Statement: DS Federal is an Equal Opportunity / Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, or protected veteran status.
E-Verify Statement
DS Federal utilizes the E-Verify program for every potential new hire. This makes it possible for us to confirm that every employee who works for DS Federal is eligible to work in the United States. To learn more about E-Verify you can call 1-800-255-7688 or visit their website by clicking the logo below. E-Verify® is a registered trademark of the United States Department of Homeland Security.
#LI-LP1",#N/A,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$25 to $100 million (USD)
Cox powered by Atrium,#N/A,"Atlanta, GA",AWS Data Engineer- Hybrid,"Minimum Qualifications:
Bachelor’s degree or equivalent work experience
A minimum of 3+ years’ experience in Microsoft Windows/ SQL Server Technologies, .Net development, AWS Administration.
Experience working on 24x7 environments oriented towards a zero downtime target.
Working knowledge or previous administration of SQL 2016-SQL 2022 and Windows Server 2012+ preferred.
Ability to work with minimal direction, in a team environment.
Performance tuning for AWS/DataLake systems.
Some Experience with SQL in virtual, physical and cloud-based environments.
Experience with Athena and data modeling for cloud technologies.
Proven ability to quickly learn and implement new technologies.
Experience with Administration, Security/Identity Management and Terraforms in AWS.
Preferred Qualifications:
Experience with SentryOne, a plus.
Ability to code Powershell commands and maintain code in GitHub, a plus.
Some Experience with Metabase and Collibra, a plus.
Experience with ETL in AWS, a plus.
Pay Range:
$60-$68/hr
Requisition Disclaimer:
This job posting is for a temporary role as an employee of Atrium on assignment at Cox. The individual selected for this role will be offered the role as an employee of Atrium; compensation, medical benefits, fringe benefits and other terms and conditions of employment shall be presented by Atrium upon offer. The pay rate range provided is a reasonable estimate of the anticipated compensation range for this job at the time of posting. The actual pay rate will be based on a number of factors, including skills, competencies, experience, location and/or being pursued and other job-related factors permitted by law. In addition, this role will be eligible for overtime pay, in accordance with federal and state requirements

By applying for this position you agree to the Atrium Terms and Conditions. Agreeing to these terms, includes permission to use the email address and mobile phone number you provide during the application process or throughout the duration of your prospective or actual employees to notify you of job openings, profiles, articles, news, and other employment-related information, as well as to notify you of special promotions or additional products and services offered by us or our affiliates and partners (collectively, “Atrium Alerts”). Atrium Alerts may be sent by email, phone or text message. Your personal information will be safely stored in our database. Atrium does not sell your personal information to third parties. Text message and data rates may apply. To OPT OUT of text messaging or to modify your communication preferences for Atrium Alerts at any time, please contact us at privacyadministrator@atriumstaff.com.

If you do not agree with the Atrium Terms and Conditions, you can still complete your application for this position by emailing your resume to our team at coxrecruit@atriumworks.com. Please include the job title in the subject of your email.

As a woman-owned firm, Atrium values diversity. We are an equal opportunity employer and will consider all applications without regard to race, sex, age, color, religion, national origin, veteran status, disability, genetic information or any other characteristic protected by law. We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.
Posting: #zip",$64.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Loyal
3.9",3.9,"Atlanta, GA",Senior Cloud Data Operations Engineer,"Loyal is an organization centered on experience and building a platform that allows consumers to make meaningful decisions when it comes to healthcare. We deeply understand providers, locations, services, appointments, business rules, and moreover, we understand patients - who they are, the preferred method of communication, upcoming appointments, lapsed appointments, outstanding bills, health risks, and more. With this intelligence, our platform fuels highly relevant and personalized experiences across all mediums (website, email, voice…) allowing patients to get healthy, stay healthy, and have a better relationship with the health care provider.
**This is a remote role**
Summary
The Senior Cloud Data Operations Engineer is responsible for delivering Loyal's cloud-based data warehouse. They interface with groups across the organization to ensure we are collecting and storing data in a way that makes it easy to create and consume. They identify best practices for big data management, select the right tooling, create a workable data architecture, and drive an implementation roadmap others can depend on. The Senior Cloud Data Operations Engineer also works closely with our security group to ensure data security and privacy. They also mentor others on the team, providing technical leadership to those they work with.
You will
Create data streams and consolidate data sources into a single cloud-based data warehouse
Architect and catalog data warehouse storage schema
Work with product teams to enable easy data aggregation
Work with data consumers (analysts, data scientists, and AI/ML engineers) to ensure data is easily accessible and consumable
Create and execute a roadmap that meets data storage and availability goals
Ensure data is secure through data analysis, access control, and encryption
You have
Bachelor's degree in computer science, information systems, applied math or related field, or equivalent work experience
Minimum 5 years of experience working with cloud data processing services (AWS, Azure, or GCP).
Minimum 3 year experience with Big Data technologies, such as Databricks, Snowflake, Spark, Hadoop, etc.
Strong experience in creating data architecture and storage schemas required
Significant working knowledge of SQL and other query languages and tools
Significant experience with building relational database systems strongly preferred
Experience with ETL tools a plus
Knowledge of Python preferred
Software development, data analytics, or data science experience is a plus.
Experience working within a software as a service (SaaS) company preferred.
Experience working within a high growth and/or ambiguous environment, with proven experience to be dynamic preferred.
#LI-REMOTE
We know that potential candidates are often less likely to apply to a position if they don't match 100% of the job qualifications. Don't let that be why you miss out on this opportunity! We encourage you to apply if you can demonstrate many of these skills and competencies.
Loyal to our employees
We are a remote-friendly company! We encourage you to apply from anywhere in the United States. We also believe in a work/life balance that fulfills you while you're here and supports you when you're not. We built our benefits package to prove that we're committed to you having everything you need (including a little fun). Here is what we offer full-time employees:
Flexible paid time off, sick and personal days
At least one holiday per month (sometimes, more!)
Full health, dental, and vision insurance - Loyal pays the premium for all employees!
One Time Home Office Setup Stipend For Remote & Hybrid Roles
Monthly Internet Stipend for Remote & Hybrid Roles
Long term & short term disability
401[k] plan
16 Weeks Paid Parental Leave
2 Volunteer days per year
Matching Gift Program
Participation Grant Program
Annual Travel/Team Events up to twice per year (post-COVID)
Our Commitment
We believe that the key to Loyal's success is you. Your unique background, life experience, knowledge, self-expression, and talent make you uniquely you. Who you are, what you have experienced, and how you think inspires us to be innovative and bold.
Loyal is an equal opportunity employer. We hire great people from a wide variety of backgrounds, not just because it's the right thing to do, but because it makes our company stronger. We welcome the unique contributions that you can bring in terms of your education, opinions, culture, ethnicity, race, ancestry, sex, gender identity and expression, national origin, citizenship, marital status, age, languages spoken, veteran status, color, religion, disability, sexual orientation, and beliefs.
We consider qualified applicants regardless of criminal histories, consistent with legal requirements.
Further, consistent with applicable federal and state law, Loyal provides reasonable accommodations when requested by qualified applicants or employees with disabilities, unless doing so would cause an undue hardship. Loyal's policy regarding requests for reasonable accommodation applies to all aspects of employment, including the application process. If you require a reasonable accommodation in connection with the application process, please contact the Talent Acquisition Department at talentacquisition@loyalhealth.com.
E-Verify
This employer participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S. If E-Verify cannot confirm that you are authorized to work, this employer is required to give you written instructions and an opportunity to contact Department of Homeland Security (DHS) or Social Security Administration (SSA) so you can begin to resolve the issue before the employer can take any action against you, including terminating your employment. Employers can only use E-Verify once you have accepted a job offer and completed the I-9 Form.
COVID-19 Vaccinations
Consistent with Loyal policy, candidates performing in-person work will be required to show proof of being fully vaccinated against COVID-19 upon commencing employment. Reasonable accommodations will be considered on a case-by-case basis for exemptions to this requirement in accordance with applicable law. If you require a reasonable accommodation to Loyal's vaccination requirement, please contact the People Department at people@loyalhealth.com.",#N/A,51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2015,$5 to $25 million (USD)
"NexTek, LLC",#N/A,"Shoreview, MN",Sr. Data Engineer,"Nextek, LLC is a software development company in the financial services industry that creates web applications and solutions for financial services professionals. We are seeking a motivated Senior Data Engineer to join the team.
What we provide:
A laptop and other equipment/software necessary for work
Health Insurance, Dental Insurance, Vision Insurance
Paid time off (PTO), Volunteer time off (VTO)
Hybrid remote work options
401K
Life Insurance
Flexible schedules
Monthly Social Activities
Casual dress code
Professional development
Here is what we are looking for:
Experience in database administration, security, backup, and recovery
Preferred Knowledge:4+ years of experience in data/software engineering
Experience in the financial services industry
Experience with a variety of relational databases (particularly MySQL & MSSQL)
Experience with collecting and presenting analytics
Here is what you will be doing:
Display proficiency in relevant technical skills.
Maintain a deep understanding of the data structure.
Provide evaluations and recommendations for our data environment based on potential issues, tradeoffs, and risks.
Scope and implement solutions.
Make good estimates and set realistic expectations for delivery.
Ask for guidance when necessary.
Maintain and monitor the health of databases and associated solutions.
Make use of the same environments setup by the Software Engineering team
Thoroughly test code for bugs and security issues before deploying to production environments
Maintain and monitor actively deployed solutions
Follow and help define best practices
Peer reviews
Maintain awareness of industry trends
Share experience and insights to help your team grow
Increase understanding of impact on other teams
Increase understanding of the customer
All other duties as assigned
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Shoreview, MN: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
United Digestive,#N/A,"Atlanta, GA",Data Engineer,"GENERAL SUMMARY OF DUTIES: United Digestive’s Data Engineer will be responsible for developing and maintaining data pipelines, data warehouses, and data models to support the company's Business Intelligence reporting initiatives.
REPORTS TO: Director of Business Intelligence & Analytics
RESPONSIBILITIES:
Duties may include but are not limited to:
Implement, maintain, and improve data integration processes to collect, process, and manage large datasets
Collaborate with stakeholders to understand business requirements and design data solutions that meet their needs
Implement and maintain data quality checks and validations to ensure data accuracy and completeness
Optimize data processing and storage to ensure high availability, scalability, and performance
Develop and maintain automated data workflows and scheduling tools
Code ETL process using Azure Data Factory workflows and other technologies as appropriate
Troubleshoot and resolve data-related issues and incidents
Stay up to date with the latest technologies and trends in data engineering, specifically the Azure suite, and recommend improvements and optimizations to the data architecture and infrastructure
Ability to develop solutions and manage existing Azure DevOps pipelines
Maintains a high level of confidentiality with sensitive information
Performs any other duties and/or special tasks as assigned by United Digestive leadership
REQUIRED EDUCATION, SKILLS & EXPERIENCE:
Bachelor’s degree in Computer Science, Software Engineering, Information Technology, or in a related field required; healthcare-related Data Analytics experience preferred; 2+ years of data pipeline management required. Proficiency in SQL and experience with relational databases (e.g., Oracle, SQL Server, PostgreSQL); experience with data modeling, data warehousing, and ETL processes, proficiency in various programming language (e.g., Python, SQL, Java), experience with cloud platforms (e.g., AWS, Azure, GCP) and big data technologies (e.g., Databricks, Hadoop, Spark, Kafka, Cassandra). Must have knowledge of data visualization and BI tools (e.g., Tableau, Power BI) and how to support these tools in an efficient manner.
ADDITIONAL SKILLS AND EXPERIENCE:
Data Engineer must have:
Excellent verbal and written communication skills.
Excellent organizational skills and attention to detail.
Ability to multi-task and prioritize job needs
Ability to deal with patients, visitors, co-workers, and physicians with courtesy and respect
Work under pressure; assess, respond to, and communicate issues in a timely manner.
Self-motivation and be a proactive individual with a professional can-do attitude
PHYSICAL/MENTAL/ENVIRONMENTAL DEMANDS
Requires sitting and standing associated with a normal office environment. Travel as business needs dictate.
DRUG FREE WORKPLACE
United Digestive is a drug free workplace. All offers of employment are contingent upon passing a pre-employment drug screening.
EQUAL OPPORTUNITY EMPLOYER
United Digestive is an Equal Opportunity Employer and does not discriminate on the basis of race, religion, gender, color, or national origin in its employment practices.
Location: 1355 Peachtree Street NE STE 1600, Atlanta, GA 30309
Job Type: Full-time",#N/A,Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Artint Knowledge Tech,#N/A,"Charlotte, NC",Azure Cloud Data Engineer,"Role: Azure Cloud Data Engineer (Onsite role)
Location: Mount Laurel, NJ / Charlotte, NC (day 1onsite)
Job Duration: Full Time
Below is the JD:
Keys Skills: Azure, Synapse, ADF, DataBricks, PySpark, Informatica Power Centre or SQL Server SSIS or DataStage
Must Have
* More than 12 years of IT experience in Datawarehouse
* Hands-on data experience on Cloud Technologies on Azure, Synapse, ADF, DataBricks, PySpark
* Prior Experience on any of the ETL Technologies like Informatica Power Centre, SSIS, DataStage
* Ability to understand Design, Source to target mapping (STTM) and create specifications documents
* Flexibility & willingness to work on non-cloud ETL technologies as per the project requirements, though main focus of this role is to work on cloud related projects
* Flexibility to operate from client office locations
* Able to mentor and guide junior resources, as needed
* Banking experience on RISK & Regulatory OR Commercial OR Credit Cards/Retail
Job Type: Full-time
Salary: $100.00 - $115.00 per year
Work Location: One location",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Dime Community Bank
3.4",3.4,"Hauppauge, NY",Data Engineer,"Summary: Dime Community Bank is looking for a Data Engineer based in Hauppauge, NY.
Salary commensurate with experience, ranging from $100,000 to $135,000 annually. The exact compensation may vary based on relevant experience, skills, education, training, licensure and certifications, and location.
All applicants need to attach a recent resume.
Responsibilities:
Work with relational databases and data warehousing. Add new and modify existing data models to ensure data warehouse is up to date for reporting needs.
Handle data visualization for extract transform and load (ETL).
Handle job pipelining and orchestration. Automate data loads and systems with Powershell scripting and Python. Orchestrate jobs with Dagster to ensure proper data processing, swift execution of tasks, and accurate monitoring. Build ETL pipelines to bring all data sources to the data lake, data warehouse and data marts. Work with python & SQL, and scripting in Powershell for automation.
Build python packages for use by the team for report building, accessing databases, etc.
Develop and maintain operational data stores (ODS) in our datalake and data warehouse architecture. Utilize ETL concepts and handle transformation with data build tool (DBT). 20468.3.6
**Telecommuting may be permitted.
Mail resume to Robin.Derin@dime.com. Must reference job 20468.3.6.
Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities
The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information. 41 CFR 60-1.35(c)","$117,500 /yr (est.)",501 to 1000 Employees,Company - Public,Financial Services,Banking & Lending,2021,$100 to $500 million (USD)
"Summit2Sea Consulting
5.0",5.0,"Tampa, FL",Data Engineer,"Summit2Sea Consulting, a cBEYONData company, is a technology consulting firm run by hands on technologists that combines people, process and technology to deliver innovative solutions to our clients. We have been named on The Washington Post's list of top work places for the past 3 years! We invest in our biggest asset - our people! You can be a part of a winning team that will contribute to your career growth.
Have you been looking to shift your career into high gear? This is your opportunity to take your ambitions and convert them into a solid career in a supportive and innovative environment!
Summit2Sea, a cBEYONData company is seeking a Data Engineer to support our federal customer's multi-domain technology platform which offers military and business decision makers, analysts, and users at all levels unprecedented access to authoritative enterprise data and structured analytics in a scalable, reliable, and secure environment.
Responsibilities:
Support data collection, ingestion, validation, and loading of optimized data in the appropriate data stores
Work on a team made up of analyst(s), developer(s), data scientist(s), and a product lead
Working directly with the analyst(s) and the product lead, the data engineer identifies and implements solutions for the data requirements, including building pipelines to collect data from disparate, external sources, implementing rules to validate that expected data is received, cleansed, transformed, massaged and in an optimized output format for the data store
Performs validation and analytics corresponding with client requirements and evolves solutions through automation, optimizing performance with minimal human involvement
As pipelines are executed, the data engineer monitors their status, performance, and troubleshoots issues while working on improvements to ensure the solution is the very best version to address the customer need
This role focuses specifically on the development and maintenance of scalable data stores that supply big data in forms needed for business analysis
Apply advanced consulting skills, extensive technical expertise and has full industry knowledge to develop innovative solutions to complex problems
Requirements:
Must Have:
4+ years of experience with SQL
4 years of experience working in a big data and cloud environment
4+ years of experience with a modern programming language such as Python or Java
4+ years of experience developing data pipelines using modern Big Data ETL technologies like NiFi or StreamSets.
Active Secret Clearance or higher
Nice to Have:
2 years of experience working in an agile development environment
Possession of excellent verbal and written communication skills
Preferred experience at the respective command with an understanding of analytical and data paint points and challenges across the J-Codes.
Ability to display a positive, can-do attitude to solve the challenges of tomorrow
Ability to quickly learn technical concepts and communicate with multiple functional groups
Summit2Sea Consulting, a cBEYONData company, is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.
Equal Employment Opportunity:
All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.",#N/A,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2003,$5 to $25 million (USD)
"LatentView Analytics
4.0",4.0,"Dallas, TX",Data Engineer,"Role : Data Engineer
Experience : 6 - 8+ Years
Location : Dallas,Tx (Onsite)
Position: FullTime Only
Skills :Python,SQL Server ,Scala, Hadoop, HPCC, Storm, Cloudera, Cassandra,Excel, R,Docker,Kubernetes,Snowflake,Azure,Kafka,Redshift,Hadoop,AWS.
Job Type: Full-time
Pay: $80,000.00 - $1,100,000.00 per year
Compensation package:
Yearly pay
Experience level:
6 years
Schedule:
8 hour shift
Experience:
data engineer: 6 years (Preferred)
Work Location: In person
Speak with the employer
+91 9876543210","$109,012 /yr (est.)",1001 to 5000 Employees,Company - Public,Management & Consulting,Business Consulting,2006,$25 to $100 million (USD)
"Purpose Financial
4.2",4.2,Remote,Data Engineer,"Address : 322 Rhett St, Greenville, South Carolina, United States - 29601
Purpose Financial, Inc. is an innovative consumer financial services company that offers a diverse suite of credit products, promoting financial inclusion and meeting consumers wherever they are. Through its brands, the company is committed to helping customers achieve their version of financial stability in the moment and in the future. Since 1997, Purpose Financial has been a pioneer in the consumer credit and financial services market offering money solutions in over 1,000 storefronts locations and online lending. Providing services in over 20 states, Purpose Financial employs over 2,700 team members.
At Purpose Financial, our people are our greatest asset. We are inclusive and encourage different points of view. We all belong here. We respect our colleagues. We value our customers, and we support our communities.
What do we do? Our sole purpose is to help each customer achieve their version of financial stability in the moment and in the future.
We have a history! Consumers know that when they choose Purpose, they’re backed by a trusted, reliable financial services company with 25 years in the business, 2,700+ employees, 1,000+ store locations, and operating in 24 states.
Benefits and Perks. We believe our employee benefits should make life better. Purpose Financial offers a comprehensive benefits package to everyone on our team.
Health & Welfare Benefits. When you take care of yourself, you’re a better you – which is better for everyone!
Health Savings & Flexible Spending Accounts. Pre-tax money for your family’s out-of-pocket expenses.
401k with Matching Contributions. Think bigger! You can build a better future.
Wellness & Employee Assistance Programs. Take care of yourself in every way that matters.
Generous Vacation Policy & Paid Time Off. Time off to take care of yourself, your family – and your bucket list!
Volunteer Days. We want you to care. Always. Take 2 days off each year to volunteer and make a difference in your community.
Tuition Reimbursement. Pursue your goals without fretting over all the costs.
Employee Discounts. Save on home and auto insurance, cell phone service, entertainment, shopping, pet insurance and even travel.
Performance Incentives & Awards. We want you to feel valued for all your contributions.
Hybrid Workforce Support. We believe in getting it done – in whatever way works best for you and your team.
Want to meet our executive team? Visit our home page at www.havepurpose.com.
Position Summary
The job function is responsible for building a scalable data infrastructure. The role will be part of the effort to enable Purpose Financial to democratize data by developing self-serving and curated data models for our single source of truth. The job function includes understanding of business requirements, defining data requirements, identifying enterprise data sources, planning, designing, and building data acquisition from sources, defining data model, designing data ingestion pipelines, data normalization, data transformation, and consumption using platforms such as Snowflake and AWS.
Job Responsibility
Take ownership of building/optimizing data pipelines to increase efficiency and performance
Develop and maintain data ingestion and integration processes that focus on data quality and monitoring
Use AWS and Snowflake tools such as Glue, Lambda, Snowpark, and Snowpipe to build data ingestion pipelines from various sources
Incorporate standards and best practices into data pipeline solutions
Participate in troubleshooting and problem-solving design discussions
Provide logical and physical data design, and database modeling
Solve complex data issues around data integration, unusable data elements, unstructured data sets, and other data processing incidents
Continuously design the future state of our data & reporting process by collaborating with IT and business partners.
Partner with enterprise teams, data scientist, architects to define development standards and solutions.
Manage cloud infrastructure as code (IaC), using Terraform.
Work closely with other members of the data team to better design and build metrics that enhance our analytical capabilities
Manage workflows and data build processes that help the BI team build key business metrics and design dashboard visualizations
Assess the effectiveness and accuracy of new data sources, data points, and data gathering techniques
Troubleshoot problems with data (missing, corrupted, invalid) and build data recovery strategy
Apply best practices for AWS & Snowflake architecture, ELT and data models
Be a leader for our data journey, act as SME for data and database related topics, and assist with development of data lakes and warehouses
Education Required
Bachelor’s degree (or foreign equivalent degree) in any Computer, Engineering, or other technical field.
Experience Required
5+ years experience in data warehousing
3+ years of experience in data architecture and design, with a good understanding of various data systems and structures (i.e. SQL, NoSQL, Key-Value, Streams)
3+ years of experience with building data pipelines and using data orchestration tools and implementing end to end Monitoring & Alerting solutions
3+ years of experience working with AWS or public cloud provider.
Strong understanding of SQL, data structures, data integrity, and schema design best practices
Programming experience required, preferably Python.
A strong advocate of data governance and data quality
Experience with database internals, database design, SQL and database programming
Experience in financial services (Banking and consumer lending preferred)
Experience of authoring CI/CD pipelines, automation elements related to infrastructure composition, deployment orchestration, and monitoring.
Familiarity and experience with Agile/SCRUM based development methodology.
Knowledge Required
Excellent written and verbal communications skills as would be needed to communicate in person, by phone, and through email; adaptability and flexibility to changing environment; and comfortable working in a dynamic, high volume, fast-paced environment. Ability to read, write, evaluate, and apply information. Ability to interact professionally and exhibit appropriate social skills. Ability to understand and ensure compliance with policies, procedures, and laws governing our industry/business and products. Ability to develop and maintain business relationships.
Physical Requirements
Sitting for long periods of time; standing occasionally; walking; bending; squatting; kneeling; pushing/pulling; reaching; twisting; frequent lifting of less than 10 lbs., occasional lifting of up to 20 lbs.; driving and having access during the workday to an insured and reliable transportation; typing; data entry; grasping; transferring items between hands and/or to another person or receptacle; use of office equipment to include computers; ability to travel to, be physically present at, and complete the physical requirements of the position at any assigned location.
Competencies
Integrity
Customer Advocacy
Interpersonal Skills
Results/Accountability
Inclusiveness
Technical Proficiency/Leadership
Big Data Trends
Data Management
Adaptability
Travel - None
Attire - Business Casual
Other - Must be eligible to work in the USA and able to pass a background check.
Job Type: Full-time
Compensation package:
Yearly pay
Experience level:
5 years
Schedule:
Day shift
Work Location: Remote",#N/A,Unknown,Company - Private,Financial Services,Financial Transaction Processing,#N/A,Unknown / Non-Applicable
"Petrolink International
2.9",2.9,"Houston, TX",Data Analysis Engineer,"Location:
Houston, Texas, U.S.A.
Job Type:
Full-time
Job Description:
Petrolink is a global company that specializes in user-driven technology for the Energy Industry.
We are looking for a Data Analysis Engineer to assist with creating and delivering algorithms primarily used by our engineering solutions.
This position is based in our U.S. office in Houston, Texas.
Duties & Responsibilities:
Research and provide logic for engineering solutions algorithms.
Collaborate with Subject Matter Experts (Drilling Engineers, Mud loggers, Pore Pressure Analysts, and other specialists from the Drilling Industry) to understand different challenges in the Drilling Industry and the practices in place to address/solve these challenges.
Code the algorithms using C# to run on Petrolink’s proprietary analytics engine.
Develop, test, and maintain algorithms.
Support escalations related to issues in algorithms.
Learn existing algorithms to provide support when required.
Prepare training material and sessions for stakeholders as part of the production release.
Skills & Qualifications:
Proficient in C#.
Experience with Python is an advantage.
Knowledge in drilling practices, and ability to speak to other drilling experts and capture requirements and logic.
Knowledge in drilling performance KPIs is an advantage.
Prior experience with WITSML is an advantage.
Function and communicate well in a team having diverse expertise.
Bi-lingual (English and Spanish) preferred.
At least 2 years’ relevant experience.
Benefits:
Health insurance
Dental insurance
401(k) with matching
Health savings account
Life insurance
Long-term disability
This job description is intended to explain the general nature and level of work being performed. Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.
Applicants must be legally authorized to work in the United States for any employer.
To apply for this position, applicants must meet the above criteria.
Interested applicants fulfilling the above criteria, please send your resume to careers.usa@petrolink.com.","$81,133 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"Longevity Holdings Inc.
3.9",3.9,"Minneapolis, MN",Associate Data Engineer (Temporary),"As a Associate Data Engineer, you will treat data as an asset to design, build, and execute high performance and data centric solutions by using the comprehensive big data capabilities for the company's data platform environment. In this role, you will build and optimize data products to bring data and analytics products and solutions to businesses.
Essential Job Responsibilities:
· Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions
· Leading data collection efforts and performing trend analysis to identify common performance challenges that require further attention
· Partner closely with our data scientists to ensure the right data is made available in a timely manner to deliver compelling and insightful solutions
· Building out scalable data pipelines and choosing the right tools for the right job. Manage, optimize, and monitor data pipelines
· Incorporate core data management competencies including data governance, data security, and data quality
Required Skills:
· Bachelors in a quantitative field
· 1+ years of data engineering or equivalent experience
· Proficient use of tools, techniques, and manipulation including Cloud platforms, programming languages, and an understanding of software engineering practices
· Demonstrated knowledge of relational data sets, structures, and SQL
· Familiarity with big data platforms such as Apache Spark, Hadoop, Kafka, etc.
· Experience leading data organization, dashboarding, & visualization efforts
· Inquisitive, proactive, and interested in learning new tools and techniques
· Excellent communication skills
Longevity Holdings Inc prohibits discrimination and harassment and will take affirmative action to employ and
advance in employment qualified individuals based on their status as protected veterans or individuals with
disabilities, race, color, religion, sex, national origin, sexual orientation, and gender identity.
Our privacy notice is available at https://longevity.inc/employment-privacy-notice
Job Type: Full-time
Pay: $20.00 - $30.00 per hour
Schedule:
Monday to Friday
Application Question(s):
Will you now or in the future require sponsorship for employment visa status (e.g., H-1B visa status)?
Work Location: Hybrid remote in Minneapolis, MN 55402",$25.00 /hr (est.),51 to 200 Employees,Company - Private,Management & Consulting,Research & Development,1998,$25 to $100 million (USD)
Resilience Consultancy Services,#N/A,Remote,Azure Data Engineer/Tech Lead,"Job Title: Azure Data Engineer and Tech Lead
12+ Months
Contract
Job Summary: We are looking for an experienced Azure Data Engineer and Tech Lead to join our team. The successful candidate will be responsible for designing and implementing data solutions using Azure services such as Data Factory, Databricks, and other related services. Additionally, the candidate will lead and manage the technical team responsible for developing and maintaining these solutions.
Responsibilities:
Design and implement scalable data solutions on Azure platform using Data Factory, Databricks, PySpark, Python and other related services.
Build data pipelines and workflows to ingest, transform, and load data from various sources.
Develop and maintain data models and schemas for efficient data storage and retrieval.
Manage and lead a technical team responsible for data engineering and analysis.
Develop and maintain CI/CD pipelines for automated deployment and testing of data solutions.
Collaborate with cross-functional teams to ensure data solutions meet business requirements.
Continuously evaluate and recommend new data technologies and approaches to improve data solutions.
Requirements:
Bachelor's or Master's degree in Computer Science, Information Systems, or related field.
5+ years of experience in data engineering with expertise in Azure Data Factory, Databricks, PySpark, Python and related services.
Proven experience in leading and managing technical teams in data engineering and analysis.
Experience in designing and developing data models and schemas.
Strong proficiency in programming languages such as Python.
Experience in developing and maintaining CI/CD pipelines for automated deployment and testing.
Good understanding of cloud computing and its services (e.g., Azure, AWS, GCP).
Excellent problem-solving skills and ability to work in a fast-paced environment.
Strong communication and interpersonal skills.
Job Type: Contract
Pay: $75.00 - $80.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Education:
Bachelor's (Preferred)
Experience:
Data Factory: 6 years (Preferred)
SQL: 8 years (Preferred)
Databricks: 6 years (Preferred)
PySpark: 5 years (Preferred)
Python: 5 years (Preferred)
CI/CD: 5 years (Preferred)
Azure: 5 years (Preferred)
Work Location: Remote",$77.50 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Express Global Solutions LLC
4.3",4.3,Remote,Data Engineer-Databricks Consultant,"5+ years' data engineering experience working with big data.
4+ years' experience developing in Databricks.
Job Types: Full-time, Contract
Pay: $80,631.44 - $97,104.52 per year
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote","$88,868 /yr (est.)",51 to 200 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Presidio
4.0",4.0,"Tampa, FL","Engineer, Data Center (FL)","SEIZE THE OPPORTUNITY TO BE A PART OF SOMETHING GREAT!
Presidio is on the leading edge of a technology-driven movement to transform the way business is done, for our customers and our customers' customers. Joining Presidio means immersing yourself in a culture of self-starters, collaborators and innovators who make real, lasting change in the marketplace via cutting-edge technology and business solutions. At Presidio, we know that it’s our people that make the connections happen.
WHY YOU SHOULD JOIN US? You will set your career on track for outstanding achievement with a company that knows no limits. Presidio is a leading a global digital services and solutions provider focused on Digital Infrastructure, Business Analytics, Cloud, Security & Emerging solutions.
THE ROLE: Engineer, Data Center
Job Summary: Presidio is looking for a Data Center Engineer to join our talented Staff Augmented team at the Hillsborough County Courthouse. Primary responsibilities include engineering, design, installation, monitoring and troubleshooting of a complex data center environment.
Travel Requirements: This role is an onsite role with up to 2 days remote work per week. There is no travel required for this opportunity.
Job Responsibilities:
Administration, maintenance and troubleshooting of all servers in the Courts. This includes: monitoring of the servers, verification of all services, upgrades to servers and applications, permission changes, drive shares and folders, and associated other tasks
Review application logs and server resources to identify any preventative maintenance required .
Daily verification of the previous backup jobs
Perform any user-oriented moves, adds and changes to the information in Active Directory. This includes usernames, full names, locations, phone number, accounts and associated information
Keeping up with expiration dates and quotes for all maintenance contracts.
Develop comprehensive graphical and text-based design documentation and effectively manage the implementation process from design to customer acceptance.
Administration, maintenance and troubleshooting of all storage devices in the Courts. This includes: EMC, NetApp, Nimble
Administration, maintenance and troubleshooting of all virtualization infrastructure. This includes: VMware, VCenter, Site Recovery Manager
Administer and Maintain the CrowdStrike server to deploy and monitor CrowdStrike on all servers at the Courts.
Research, testing and staging of the Windows Updates using the WSUS server
Verification and final installation of the server patches. This includes all servers in the Courts.
Required Skills:
Systems Administration
Switches, Routers, Firewalls
Build out, OS install, security scans and remediation of Enterprise servers in a Data Center solution
Build/configure VMware host machines with integration into vCenter
Build/configure windows 2003-2016 virtual servers, patching, security compliance, AD, GPOs and user management
Education and Experience: (List out)
Bachelor's degree or equivalent experience and/or military experience
Over all 5 years of experience in design, installation and management of all Data Center Operations
*****
ABOUT PRESIDIO
Presidio is committed to Diversity, Equity, and Inclusion at the highest levels and has strengthened its drive to build and drive systemic DEI change process across all levels of the organization. Cultivating a culture of inclusion where the expression of all our differences are valued, celebrated, and add to our collective achievements.
Presidio is a global digital services and solutions provider accelerating business transformation through secured technology modernization. Highly skilled teams of engineers and solutions architects with deep expertise across cloud, security, networking and modern data center infrastructure help customers acquire, deploy and operate technology that delivers impactful business outcomes. Presidio is a trusted strategic advisor with a flexible full life cycle model of professional, managed, and support and staffing services to help execute, secure, operationalize and maintain technology solutions. We serve as an extension of our clients' IT teams, providing deep expertise and letting them focus on their core business. Presidio operates in 40+ US offices and offices in Ireland, London, Singapore, and India.
For more information visit: http://presidio.com
*****
Presidio is an Equal Opportunity / Affirmative Action Employer / VEVRAA Federal Contractor. All qualified candidates will receive consideration for this position regardless of race, color, creed, religion, national origin, age, sex, citizenship, ethnicity, veteran status, marital status, disability, sexual orientation, gender identification or any other characteristic protected by applicable federal, state and local statutes, regulations and ordinances.
To read more about discrimination protections under Federal Law, please visit: https://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf
If you have any difficulty using our online system and need an accommodation in the job application process due to a disability, please send an email to recruitment@presidio.com for assistance.
Presidio is a VEVRAA Federal Contractor requesting priority referrals of protected veterans for its openings. State Employment Services, please provide priority referrals to recruitment@presidio.com.
RECRUITMENT AGENCIES PLEASE NOTE:
Agencies/3 Parties may not solicit to any employee of Presidio. Any candidate information received from any Agency/3 Party will be considered a gift and property of Presidio, unless the Agency/3 Party is an Authorized Vendor of Presidio with an up-to-date Presidio Contract in hand signed by Presidio Talent Acquisition. No payment will be made to any Agency/3 Party who is not an Authorized Vendor, nor has specific approval in writing from Presidio Talent Acquisition to engage in recruitment efforts for Presidio.","$77,293 /yr (est.)",1001 to 5000 Employees,Subsidiary or Business Segment,Information Technology,Information Technology Support Services,2006,Unknown / Non-Applicable
"Axos Bank
3.6",3.6,"San Diego, CA",Junior Business & Technology Analyst - Data Engineer,"Job Summary and Opportunity:
This is an exciting opportunity to join a unique and immersive rotational program as a first step in your career in technology. This full-time rotational program is geared toward providing multi-software platform exposure that focuses on the expansion of knowledge and real-life application within each. We are seeking innovative and energetic individuals who are excited about expanding their skillsets and accelerating their career path with immediate exposure to software applications.
For this position, you will be in the Data Engineer Rotational Program where you will be joining the Axos' Center of ExcellenceTeam. You will get to be a part of a team responsible for the implementation of cutting-edge software driven solutions. As you progress through the program, you will rotate into different complimentary areas within the Data program where roles and responsibilities will change. The final goal of the program is permanent placement within your area of focus. For those looking to make an impact this is where it begins.
In this role you will be focused on SQL related software or software built on direct interactions with SQL. Through the different rotations completed, you will gain the knowledge and skills database development, data quality, and business intelligence reporting to provide enterprise level solutions.
This position is on-site and will be located at our HQ in San Diego, California.
Responsibilities:
Define, prepare, execute and implement data validation and unit and integration testing methods to ensure data quality
Create SSIS packages for data transformation, cleansing, caching, aggregation, staging, and transfer
Analyze and define data flow requirements and prepare applicable system documentation and operation manuals as needed
Code, test and maintain new and existing SQL jobs, stored procedures and functions
Performance tune existing stored procedures, tables and indexes
Troubleshoot problems that may come up with database environments: performance issues, replication issues, or operational issues
Review SQL code written by other developers to ensure compliance to coding standards and best practices as well as maximum performance
Perform data analysis and data profiling tasks to provide support and recommendations for development and design decisions
Develop standardized reporting dashboards to meet the needs of the multiple business units across the Bank
Apply advanced modeling, data mining, machine learning and/or statistical techniques to data and dashboards to generate actionable insights enabling informed decision-making for optimized business and operational performance
Create mock-ups of reporting products, scorecards, dashboards, etc. to provide visualization to the end user
Work with teams within the organization to gather and document reporting requirements
Join client meetings to communicate status, give demos, provide timelines and offer insights
Participate in daily meetings that go over testing, and code reviews
Work with IT, Enterprise Data Management, Project Managers, Business Analysts, stakeholders across multiple business units to systematically plan the launch of new or enhanced dashboards, prepare launch collateral/documentation and work closely with users during through the different phases of a project
Develop deep understanding of the Bank's databases, identify appropriate data sources, relationships and logic needed to produce consistently reliable reports
Contribute to the overall strategy and quality of dashboarding
Document process steps of repetitive tasks performed
Partner with IT and other Infrastructure teams to tackle software upgrades, and coordinate testing
Perform any additional duties as assigned
Requirements:
Bachelor's degree in Information Technology, Computer Science, Business Administration, Mathematics or a related discipline
Customer Obsession: ""Good enough"" isn't good enough for you. You're obsessed with perfecting the customer experience
Leadership: A confident person with the ability to connect and inspire others to achieve success, whether or not they directly report to you
Results Oriented: A driver who possess the ability to take actions and implement effective solutions in a timely manner. Excuses aren’t in your vocabulary because you always find alternative solutions when issues arise
Ethics: Highest level of professional integrity and honesty as well as personal credibility. Your reputation for precedes you in this regard
Innovation: Dedication to maintaining cutting edge talent with the courage to implement new ideas, technology, and aggressively challenge the status quo. You don’t accept responses to new ideas like “That’s the way it’s always been done” because you use facts, data, and people skills to implement meaningful change
Immersion: A propensity to rapidly master the understanding and application of new technology
Excellent verbal and written communication skills, including ability to simplify complex concepts for technical and non-technical audience
Preferred:
Basic to intermediate knowledge in SQL server database development and testing
Working knowledge of Tableau
1+ year's working in an office environment or recent college graduate
APPLY DIRECTLY FOR CONSIDERATION:
Born digital, Axos Bank has reinvented the banking model and grown to over $18.4 billion in assets since our founding in 2000. With a broad and ever-growing range of financial products, Axos Bank is rated among the top 5 digital banks in the country! Axos Financial is our holding company and publicly traded on the New York Stock Exchange under the symbol ""AX"" (NYSE: AX).

We bring together human insight and digital expertise to anticipate the needs of our customers. Our team members are innovative, technologically sophisticated, and motivated to achieve.

Learn more about working here!

A targeted annual base salary range of USD $24/HR - $30/HR, based on the experience, skills, and education/certification required for this position. Eligibility for a discretionary semi-annual incentive compensation plan, based upon performance, payable in cash and/or share grants (RSU’s) that may vest over time. The annual discretionary target bonus percentage is up to 20%.

Axos benefits and perks include:
3 weeks’ Vacation, Sick leave, and Holidays (about 11 a year); Medical, Dental, Vision, Life insurance and more
HSA or FSA account and other voluntary benefits
401(k) Retirement Saving Plan with Employer Match Program and 529 Savings Plan
Employee Mortgage Loan Program and free access to Self-Directed Trading

Pre-Employment Drug Test:

All offers are contingent upon the candidate successfully passing a credit check, criminal background check, and pre-employment drug screening, which includes screening for marijuana. Axos Bank is a federally regulated banking institution. At the federal level, marijuana is an illegal schedule 1 drug; therefore, we will not employ any person who tests positive for marijuana, regardless of state legalization.

Equal Employment Opportunity:

Axos Bank is an Equal Opportunity employer. We are committed to providing equal employment opportunities to all employees and applicants without regard to race, religious creed, color, sex (including pregnancy, breast feeding and related medical conditions), gender, gender identity, gender expression, sexual orientation, national origin, ancestry, citizenship status, military and veteran status, marital status, age, protected medical condition, genetic information, physical disability, mental disability, or any other protected status in accordance with all applicable federal, state and local laws.

Job Functions and Work Environment:

While performing the duties of this position, the employee is required to sit for extended periods of time. Manual dexterity and coordination are required while operating standard office equipment such as computer keyboard and mouse, calculator, telephone, copiers, etc.

The work environment characteristics described here are representative of those an employee may encounter while performing the essential functions of this position. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions of this position.

#LI-Onsite",$27.00 /hr (est.),1001 to 5000 Employees,Company - Public,Financial Services,Banking & Lending,2000,$500 million to $1 billion (USD)
Alter Tech Solutions,#N/A,"Jersey City, NJ",Senior Data Engineer,"Job Title: Data Engineer
Location: Jersey City, NJ
Job Type: Hybrid
Job Description:
Position is more about scale - application monitoring, automation framework, test automation - data engineering - data pipelines - rest api's
system design - experience with distributed systems, design the right components, push out to the AWS cloud, data structures/algorithms
Experience:
10+ years of experience cross functional experience - data engineering and sw engineering background (working with risk, audit teams) need hands on coding but also design
Qualifications & Requirements:
TOP SKILLS java preferred language but would consider strong python - Apache Flink, Kafka, Cassandra, GraphQL, SPARK, MACHINE LEARNING.
Job Types: Full-time, Permanent
Salary: $60.00 - $75.00 per hour
Experience:
Java: 8 years (Preferred)
GraphQL: 8 years (Preferred)
Python: 5 years (Preferred)
Work Location: On the road",$67.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"FiberSense
5.0",5.0,"Oakland, CA",Data Engineer,"Why join us: Our benefits
Global team and remote role
Inclusive and flexible work culture
Motivated and vibrant team
Small team, large career growth opportunities
Opportunities and support to learn and receive mentoring
Fast growing start-up with unique, highly technical and exciting product
We are making the world safer and more sustainable
Our technology is one of a kind – there is no one else in the world doing what we're doing
About the opportunity: Data Engineer
Hybrid (from Oakland) or remote | Very flexible hours | $141,000 - $155,000 USD per annum
The software team works in a hybrid work environment from an office in Oakland, so ideally you would be based in Oakland too. We will consider candidates who are 100% remote within the USA. We encourage all types of flexible working arrangements.
As FiberSense's first dedicated Data Engineer, you will have a significant role in enhancing our existing data pipelines, while also taking charge of designing, developing, and maintaining new robust and scalable data infrastructure.
You will report directly to the head of ML/AI, located in Oakland, California. However, for the right candidate, we support remote work and offer flexible working hours.
Your responsibilities will include:
Design, build, and maintain data infrastructure focusing on reliability and scalability that can accommodate the growing volume of data from the sensor fleet, using AWS cloud technologies (AWS S3, Lambda, RDS, EMR, and Glue/EventBridge).
Extract and integrate data streaming from our rapidly growing fleet of sensors via RESTful APIs into storage buckets and databases
Ensure data quality, accuracy, and completeness by implementing data validation protocols and testing
Provide input on which APIs and data formats to expose for easier ingestion of streaming data from the sensors
Evaluate infrastructure options that fit the business's needs and optimize costs at scale
Stay up to date with emerging technologies and industry trends
Automate data security and develop data microservices
Build scalable and performant databases using SQL, such as PostgreSQL and MySQL.
Implement data monitoring and alerting, ensure the data infrastructure is running smoothly and that any issues are detected and resolved quickly
Within 90 days, you will:
Ensured the robustness of, or rebuild, the current event-triggered data extraction pipelines to S3/RDS
Have met with key stakeholders to scope needs and priorities both long and short-term
Explored existing data pipelines and contribute significantly to a roadmap for data infrastructure and pipelines
Begun implementing proposed infrastructure and pipelines
About you: The type of person who will be a good fit
If you are excited about this role but not sure if you meet all of the criteria, please apply. Research shows women and minority groups are less likely to apply for roles when they don't meet 100% of the criteria. We'd like to change this.
Essential:
Minimum of 3 years of experience in data engineering
Strong programming skills in relevant languages such as Python, SQL (required) and Golang (or other low-level language and a willingness to learn Go), Java (preferred)
Bachelor's or Master's degree in Computer Science, Information Systems, or a related field
Demonstrated proficiency in AWS cloud technologies, including AWS S3, EMR, EC2, RDS, Glue, and Lambda
Experience using workflow managers and building ETL processes
Experience with data pipeline development using tools such as Apache Airflow, AWS Glue, and AWS Data Pipeline
Expertise in data warehousing, database architecture, and SQL/NoSQL databases, such as PostgreSQL, MySQL, MongoDB, Cassandra, and DynamoDB.
Strong understanding of API development and implementation, including RESTful APIs using tools such AWS API Gateway
Hands-on experience with data ingestion tools such as Apache Flume and Kafka to collect, store and process large volumes of data in real-time or batch mode
Excellent problem-solving and analytical skills
Excellent communication and collaboration skills
Highly desired:
Experience working with IoT or sensor fleets particularly time-series data
Experience working with data stored in protobufs
Experience working with large and rapidly increasing data throughput (10s GB/day)
Experience serving data to be consumed by AI/ML teams
Experience building AI/ML models
Experience designing and building data infrastructure/pipelines end-to-end from inception
Experience working in a rapidly-growing start-up
About FiberSense:
FiberSense is a deep tech company with a remarkable set of capabilities and new service offerings that will transform the way we perceive and respond in public spaces. Our mission is to build the first centralized ubiquitous sensing fabric for all moving objects and events in public spaces in all cities around the world. Our expertise and technology sits at the intersection of optical fiber sensing networks, integrated photonics and machine learning. More than a sensor company, FiberSense is building out a global scale sensing platform using our patented VID+R™ (Vibration Detection and Ranging) technology. The company is Australian founded and already has employees across the globe, which is also true of our customers. FiberSense is implementing a giant vision and a culture built on two key pillars of massive innovation and effective execution. We offer an environment that is exciting, inclusive, challenging, fun and extremely rewarding.
FiberSense is an equal opportunity employer committed to providing a working environment that embraces and values diversity and inclusion. If you have any support or reasonable adjustment requirements, we encourage you to advise us at time of application. All qualified applicants will receive consideration for employment without regard to age, ancestry, colour, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances.
Apply now. Join us on our growth journey as we transform the way we perceive and respond in public spaces!","$148,000 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"1SEO Digital Agency
4.2",4.2,"Bristol, PA",Data Integration Engineer,"Based just outside Philadelphia, PA, 1SEO Digital Agency proudly stands as a full-service Digital Marketing Agency and a Google Premier Partner, placing us in the top 1% of all agencies in the US. We prioritize our clients' growth, protection, and inspiration by being solely client-centric. Our team operates on a hybrid schedule, with the majority of us working four days in our Bristol, PA office and one day remotely each week.

We are looking for a skilled Data Engineer to help us solve the problem of multiple software systems with redundant and inconsistent data. The ideal candidate will have experience in integrating disparate data sources and implementing data pipelines to automate data flow between systems. They will work closely with our internal teams to identify pain points and design solutions to streamline our data processes and establish a single source of truth.

As our Data Integration Engineer, you will be responsible for designing, building, and maintaining our data infrastructure, including data warehouses, ETL pipelines, and data integration processes. You will be working with a variety of third-party SaaS applications, as well as helping to integrate with our own SaaS applications (in the future).
Responsibilities:
Design, build, and maintain data infrastructure, including data warehouses, ETL pipelines, and data integration solutions to connect multiple software systems
Collaborate with other teams to ensure data quality, accuracy, and accessibility
Work with a variety of third-party SDKs and APIs to integrate data and streamline workflow processes
Develop and implement data and cybersecurity protocols and procedures
Participate in the design and development of our future SaaS application
Troubleshoot and resolve issues with data integration processes
Document data integration processes and maintain data dictionaries
Requirements:
Bachelor's or Master's degree in Computer Science or Information Systems
Strong knowledge of database architecture, data modeling, and SQL
Experience with ETL tools and data integration techniques
Experience integrating cloud platforms, such as Google WorkSpace, Salesforce.com, NetSuite, QuickBooks, and various SaaS applications
Familiarity with software development best practices, including version control, testing, and continuous integration
Ability to communicate effectively with non-technical stakeholders and users
What You Can Expect From 1SEO Digital Agency:
Open-floor office environment with NO cubicles whatsoever. Basketball, Foosball, Billiards, and ping-pong are in the employee lounge.
A fully-stocked kitchen provided by ownership with catered lunches multiple times per week. There is no shortage of snacks & you could almost eat breakfast, lunch & dinner here every day.
Access to the gym in our building with NO membership fee. Work out before or after work, or during your lunch break.
The office is open from 8 am to 6 pm Monday-Friday. We offer a flexible work schedule for BOTH early and late risers

After 90 days of Full-time employment, we offer our full-time employees:
50% funded healthcare benefits (Medical, Dental & Vision) for the employee. Dependents can be added to the plan AND we offer Supplemental healthcare insurance at a reduced cost
Earn up to 3 weeks of PTO with an additional week given at year 3 AND another week at year 5.
You can join the 401K after your 1st year of employment, with up to a 4% match.
Generous incentive program for each anniversary you celebrate.","$102,793 /yr (est.)",51 to 200 Employees,Company - Private,Media & Communication,Advertising & Public Relations,2009,Unknown / Non-Applicable
"Health Federation Of Philadelphia
3.9",3.9,"Philadelphia, PA",EHR Data Engineer,"Equal Opportunity Employer
The mission of the Health Federation of Philadelphia is to promote health equity for marginalized communities by advancing access to high-quality, integrated, and comprehensive health and human services. Health equity is at the heart of all our work. We believe in and are firmly committed to equal employment opportunity for employees and applicants. We do not discriminate on the basis of race, color, national or ethnic origin, ancestry, age, religion, disability, sex or gender, gender identity and/or expression, sexual orientation, military or veteran status. This commitment applies to all aspects of the Health Federation of Philadelphia’s employment practices, including recruiting, hiring, training, and promotion.
JOB SUMMARY
The Electronic Health Records (EHR) Data Engineer will be responsible for managing all aspects of EHR related data with a focus on database administration, data integration, and data flow design to deliver advanced clinical and operational analytics for the Philadelphia Department of Public Health, Ambulatory Health Services (AHS), a large community health organization. The EHR Data Engineer will utilize a systematic approach and advanced tools to design and support AHS data management services. The position requires skills and knowledge to connect, store, manage, secure, analyze, report, distribute and govern EHR related data. This position works directly with AHS Leadership, EHR Program Management Office (EHR PMO) Program staff, Health IT (HIT), City Office of Innovation and Technology (OIT) and respective partner personnel to support comprehensive planning and tactical delivery of data management services in production operations. In addition, responsibilities include Tier 2 and Tier 3 support for EHR production issues and Helpdesk tickets regarding data, database, and respective services.
Strategic direction, programmatic and operational prioritization will be specified through the authority of AHS Leadership. Accountability is managed through the EHR PMO with AHS oversight.
JOB SPECIFICATIONS
Responsibilities/Duties
Support the design, development, deployment, and ongoing management of AHS data. Work with EHR PMO to integrate data management strategies for projects, programs, and initiatives.
Assist in the design, development, deployment, and support of relational and non-relational database designs in a team-oriented environment using various data visualization tools in accordance with City OIT standards.
Responsible for overseeing patch management and maintenance on reporting infrastructure and data related systems. Develop version and release migration methodologies.
Design, implement and control access to data and databases with/for approved AHS partners and City entities.
Assure systems and data integrity.
Oversee functional configuration components to support monitoring and administering assigned systems and applications.
Serve as the technical intermediary between departments and vendors for various database solutions and datasets within on-premise and cloud hosted systems.
Assist to design, implement, and manage integration points for automated retrieval, sharing and delivery of incoming and outgoing data.
Assist with data conversion, migration, and validation of data to ensure quality and accuracy.
Develop Standard Operating Procedures (SOPs) for EHR data, database, and system administration establishing standards and procedures to maintain consistent practices.
Abide by the EHR PMO change control policies and procedures while ensuring change control methods are congruent with overall Health IT and OIT procedures; Submit and present various changes related to AHS data management.
Support the backup and recovery strategy for database environments. Oversee and administer business continuity and disaster recovery tests and methods.
Perform daily administration and performance monitoring of database related systems.
Monitor and proactively support capacity and performance planning and analysis.
Identify database sizing requirements based on AHS patient population, provider, user and legal retention requirements; monitor space usage and alert appropriate staff members to resolve sizing issues and conditions that may cause application failures.
Perform root cause analyses and troubleshoot internal and external data-related problems.
Aid in the support/writing/configuration of data transformation services (DTS) and extract, transform, load (ETL) packages, Web Services, and SQL database objects to both monitor and maintain the database and support applications.
Actively participate in the creative process to continuously improve architectural designs and implementation to ensure optimal efficiency and cost effectiveness.
Assist AHS in developing and conducting presentations related to data management.
Maintain working relationships with AHS, EHR, HIT, City OIT, and Partner staff including regular status communications using EHR and AHS methods of communication.
Provide off-hours support for upgrades, problem resolution, outages, or disaster recovery events as needed.
Stay current with data management tools as departmental infrastructure evolves.
Assist AHS leadership & staff as needed.
Other data analysis and management duties as required.
Education
Bachelors Degree in Computer Science/Information Systems, Data Analytics, or related experience in the field.
Database and/or Server Administration related Certification
Industry related certificates or certification a plus
Skills/Experience
Demonstrated knowledge of the Systems Development Life Cycle
5-7 years of experience working with SQL Server (2016 or later) and related products; special consideration given to Microsoft Business Intelligence Platform/Development Studio suite (SSRS, SSIS, and SSAS)
5-7 years of experience working with Windows Server (2016 or later) and related products
Five (5) years working in health services related field
HIPAA/HITECH trained
In depth understanding of conceptual, logical, and physical database structures, architectures, and integration
Ability to translate, integrate, and export data from various cross-platform software and databases
Comfortable leading the design of both relational and OLAP databases
Database related expertise working with SQL Server (2012 or later), Oracle SQL developer, and related products; special consideration given to Microsoft Business Intelligence Development Studio suite (SSRS, SSIS, and SSAS)
Experience with working in physical and virtual server environments
Experience with log shipping, SQL back-up restore, and SFTP
Strong analytical skills including experience with tools for: Business Intelligence; Analytics; Dashboards; Reporting
Well-developed computer skills including: Office365, Visio, entity relational diagrams
Effective organizational skills, detail-oriented
Excellent communication and presentation skills, both written and oral
Well-developed interpersonal skills including proper phone etiquette
Ability to work independently, be flexible and handle multiple tasks
Ability to work with a variety of cultures and diverse audiences
Additional proficiencies not required, but preferred
Experience with FQHC, ambulatory, municipal government and/or public health
Knowledge of healthcare, medical, lab and/or public health terminology
Experience with eClinicalWorks, i2i Tracks, Rhapsody and VMware
Experience with Microsoft SQL, Oracle database, Azure Synapse Analytics, and other multi-model database management or data warehouse systems
Experience with Microsoft PowerShell, SharePoint, and Visual Studio
Experience with data visualization software such as Tableau, Power BI, Red Cap, Quickbase, etc.
Web Analytics; batch processing development; HTML; XML; IIS server administration
Work Environment
Standard office setting with extended periods at work station and periodic use of office equipment
Position Type and Work Schedule
Full time position, typical hours are Monday through Friday 8:30 am to 5:00 pm. Flex office schedule options available with supervisor approval.
Travel
Local travel to multiple sites could be required for periodic implementations, upgrades, outages or onsite meetings.
Physical Demands
Ability to manage in an office environment located in a City health center.
Ability to work in a cubicle or shared space environment.
Ability to travel between city locations.
Ability to lift up to 50 pounds.
Ability to work in a fast paced, high pressure setting.
Ability to tolerate extensive use of keyboard, typing, computer.
Salary and Benefits Our employees are our most valuable resource, so we offer a competitive and comprehensive benefits package, which can include:

Medical with vision benefits
Dental insurance
Flexible spending accounts
Life, AD&D and long term care insurance
Short- and long-term disability insurance
403(b) Retirement Plan, with a company contribution
Paid time off including vacation, sick, personal and holiday
Employee Assistance Program
Eligibility and participation is consistent with the plan documents and HFP policy.
DISCLAIMER
The Health Federation reserves the right to modify, interpret, or apply this job description in any way the Company desires. The above statements are intended to describe the general nature and level of work being performed by an employee assigned to this position. This job description in no way implies that these are the only duties, including essential duties, responsibilities and/or skills to be performed by the employee occupying this position. This job description is not an employment contract, implied, or otherwise. The employment relationship remains “at will.” The aforementioned job requirements are subject to change to reasonably accommodate qualified disabled individuals.","$100,000 /yr (est.)",51 to 200 Employees,Nonprofit Organization,Nonprofit & NGO,Civic & Social Services,1983,$5 to $25 million (USD)
"World Finance
3.3",3.3,"Greenville, SC",Data Engineer,"**Hybrid role open to local Greenville, SC or Candidates relocating to Greenville, SC**
The Data Engineer implements data-centric solutions to complex business problems. This individual will work alongside a data architect and the data engineering team to expand and optimize data delivery architecture. Providing expertise in data warehousing and data delivery, this individual will work with cross-functional teams including marketing, analytics, operations, and software engineering to build data-aware systems and process flows across the enterprise.
Essential Duties and Responsibilities:
· Protect confidential company information.
· Assist business, technology, and support partners/stakeholders to deliver secure data solutions.
· Design, build, and maintain data delivery solutions in accordance with governing data architecture patterns.
· Model and assemble data sets that meet functional and technical business requirements.
· Implement infrastructure required to optimize ETL and ELT operations across a variety of data sources.
· Process file-based data extracts using data retrieval and management tools to provide timely loading of critical business data.
· Identify, design, and implement process improvements in data flows and data pipelines, focusing on automating data tasks.
· Integrate external systems with internal systems to ensure proper data flow between systems.
· Maintain an accurate and comprehensive inventory of data, data systems, and data storage.
· Communicate with non-technical stakeholders to determine technical solutions to business problems.
· Perform data load, data extraction.
Qualifications:
To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of the knowledge, skill, and/or ability required:
· Expertise with relational databases.
· Advanced SQL proficiency.
· Experience working with data warehousing systems for large enterprises with both internal and customer facing applications, preferably with near real time transactional data.
· Expertise in creating and maintaining data structures in SQL.
· Experience structured and non-structured data paradigms, relational databases, data lake and data warehouse technologies, data vault and dimensional data models.
· Ability to define problems, collect data, establish facts, and draw valid conclusions.
· Ability to interpret an extensive variety of technical instructions in mathematical or diagram form and deal with several abstract and concrete variables.
· Ability to conduct research into systems issues and products to determine integration requirements.
· Ability to communicate ideas in both technical and user-friendly language.
· Ability to work with IT operations to quickly come to resolution of open support requests.
Self-motivated and able to work within a project-based environment.
Possesses strong oral and written communication skills, clearly and accurately communicating complex and/or technical information to both technical and nontechnical audiences.
Python for building and redefining data pipelines is required.
Education and/or Experience:
· Bachelor’s Degree in MIS, Computer Science, or related field, or equivalent professional experience.
· Minimum 6 years of designing and implementing Data engineering solution.
· Hands on experience with building productionized data ingestion and processing pipelines.
· Experience working in business intelligence and data warehousing environments.
· MS SQL Server, Integration Services (SSIS) required.
· Strong TSQL scripting abilities and understanding of complex stored procedures, views, data. aggregation/manipulation through table joins/queries, database design, normalization, and de-normalization techniques.
· Experience with Snowflake.
· Experience using Python.
· Experience with ETL tools (Matillion, Fivetran, Stich, etc) is a plus.
· Familiarity with Dev OPS, Airflow and DBT is a plus.
· Industry experience working with large data sets.
· Experience with Microsoft PowerBI or any BI visualization tool is a plus.
Physical Demands:
· Must be able to constantly remain in a stationary position.
· The person in this position needs to occasionally move about inside the office to access file cabinets, office machinery, etc.
· Constantly operates a computer and other office productivity machinery, such as a calculator, copy machine, and computer printer.
· Occasionally may require light lifting to 25 pounds.
Work Environment:
· Office environment.
· Occasional travel may be required.
This job description reflects management’s assignment of essential functions; and nothing in this herein restricts management’s right to assign or reassign duties and responsibilities to this job at any time.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Employee discount
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Referral program
Vision insurance
Compensation package:
Performance bonus
Yearly pay
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Greenville, SC 29601: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 4 years (Required)
Data warehouse: 4 years (Required)
Work Location: Hybrid remote in Greenville, SC 29601","$95,216 /yr (est.)",1001 to 5000 Employees,Company - Public,Financial Services,Banking & Lending,1962,$500 million to $1 billion (USD)
"Dobbs Defense Solutions, LLC",#N/A,"Fort Meade, MD",Data/NiFi Engineer,"Our Mission
At Dobbs Defense, we deliver mission-centric IT, Cyber, and data analytics solutions for our government and commercial clients through the convergence of automation, innovation, training, and education. Delivering high-quality IT, cybersecurity, and data analytics solutions through proven and innovative methods is our vision.

Job Description
Dobbs Defense Solutions is seeking a Senior Data/NiFi Engineer to design, develop, implement, test, secure, and support data transport and brokering for multiple classified networks and between networks.

Duties
Design, develop, implement, test, secure, and support data transport and brokering.
Data flow engineering for one of DISA’s premier boundary defense systems.
Perform system administration of Niagara Files (NiFi) on Linux VMs.
Support backup/failover/load balancing of data brokering to improve availability.
Monitor availability & amp, usage.
Respond to outages, take corrective action, and issue outage reports.
Familiarity with DISA STIGs and SCAP.
Familiarity with Java programing.

Qualifications
Required education and experience:
5+ years of Data Engineering experience.
Significant experience with Niagara Files (NiFi) and MiNiFi or similar data engineering tools.
Excellent written and verbal communication skills.
IAM or IAT Level II Certification (e.g., GSEC, SEC+, SCNP, SSCP, etc.).
Active Secret clearance.

Required Clearance
Secret

Working Environment
Onsite

Our Equal Employment Opportunity Policy
Dobbs Defense Solutions is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, creed, sex (including pregnancy, childbirth, breastfeeding, or medical conditions related to pregnancy, childbirth or breastfeeding), sexual orientation, gender or gender identity (except where gender is a bona fide occupational qualification), gender expression and transgender status, national origin, ancestry, age, disability, military or veteran status, marital or domestic partner status, genetic information, citizenship, low-income status or any other status or characteristic protected by applicable law. We are committed to equal employment opportunity in all decisions related to employment, promotion, wages, benefits, and all other privileges, terms, and conditions of employment.
4Cmw9yg2oT","$83,799 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"RevolutionParts
4.0",4.0,United States,Data Engineer,"RevolutionParts is dedicated to modernizing the auto industry through our parts e-commerce platform. And we are pretty great at it too! We have enabled thousands of dealerships to sell auto parts online by transforming the way buyers and sellers connect.

And not only are we dedicated to revolutionizing the auto industry; we are also passionate about building a revolutionary team. Our Revolutionaries (as we call ourselves) are talented humans who have a shared goal of delivering an exceptional product and customer experience. Plus, we have fun while doing it!

The Role
RevolutionParts is on a mission to take our data to the next level. In this key role, you will help design and implement the next generation of our ETL pipeline for our parts catalog, pricing, and inventory data, which powers all of our eCommerce solutions. You will also be involved in establishing an enterprise-grade data platform for our largest partners. This is a high-impact role where you will be driving initiatives affecting teams and decisions across the company and setting standards for all our data stakeholders. Does the idea of spearheading a data practice in a high-growth e-commerce business sound exciting? If so, read on.
Responsibilities
We’re pulling in diverse data sources. You’ll need to learn our data and bring a strong grasp of ETL & ELT, workflows, AWS Glue, and data organization via efficient data lake and relational designs.
You will help design and build all stages of data from access to transformation and modeling.
You will build quality into the pipeline from day one using automated tests and data validation.
You will work with stakeholders in Product and data science to run ad hoc analysis of our data to answer questions and help prototype solutions.
You must own business problems through to resolution both individually and as part of a data team.
You will support product engineering teams by performing query analysis and optimization, as well as work with product teams to implement data driven product features.
Requirements
You should have 4+ years experience as a data engineer; or at least 2 years experience as a Data Engineer and 3+ as a software engineer.
You need to show us that you know what good looks like. This means experience implementing automated tests in a multi-stage data pipeline to ensure quality.
You are highly analytical and curious by nature.
You must have the ability to own business problems and the design and solutions that drive business outcomes.
You must be a team player with the ability to work with others and know when to support and when to push.
This role requires strong communication and collaboration skills; comfortable discussing projects with anyone from end users up to executive leadership.
Fluency with the programming language of your trade. Our primary languages for data are Golang and Python, but we use others as well. You must be comfortable learning new skills on the job.
We require fluency with best practices in an object-oriented design and programming; experience as a backend software engineer is necessary. Demonstrable experience with a functional paradigm is also valuable.
The ability to write and optimize complex SQL statements is a base requirement.
Familiarity with ETL/ELT pipelines and modern tools is fundamental to this role. We are building workflows managed in Argo utilizing Docker containers deployed within Kubernetes.
You should have experience working in a cloud-based software development environment, preferably with AWS.
Familiarity with no-SQL databases such as ElasticSearch, DynamoDB or others is helpful.
Bachelor’s Degree in Computer Science or equivalent is required.
RevolutionParts is proud to provide all full-time Revolutionaries with a comprehensive employment package including competitive compensation, career development, benefits, 401K match, parental leave, and many more valuable perks. You can learn more about our core-value driven culture at our career page.

RevolutionParts is an Equal Opportunity Employer; we value diversity. We do not discriminate on the basis of race, religion, color, national origin, gender, gender orientation, gender identity or expression, sexual identity, sexual orientation, age, marital status, family status, genetic information, veteran status, or disability status.",#N/A,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2013,$5 to $25 million (USD)
"Maven LLC
4.3",4.3,Remote,Senior Data Engineer,"Description:
Please apply to job with this link: https://portal.dynamicsats.com/Application/WebForm/3e11b0af-f443-4cfb-ac80-066fe882c27b?portal=17b1dd1f-7af3-4dae-96aa-8723901631fa&job=f5ac005e-44e8-ed11-a7c6-002248081803
All About Us
Our client is a family community of banking teams that deliver an exceptional depth of resources to individuals and businesses, with a hands-on, personalized approach. Our client offers great benefits including health insurance for you and your whole family, 11 paid holidays, PTO, and tuition reimbursement to name a few.
About the Role
You'll be responsible for building, managing, and optimizing data pipelines and effectively converting into production for key data and analytics consumers. The Senior Data Engineer is also collaborating with Data Engineers, Data Architects, and Data Scientists across the enterprise, as well as Enterprise Architecture, to follow the standardization of data engineering integration architectures.
Job Essentials
Design, create and maintain projects involving data collection and storage systems
Recommend and deploy data models and solutions for existing data systems
Collaborate across teams on the design and maintenance of our client's Operations data mart (ETL, data modeling, metric design, metric design, reporting/dashboarding) to assure a stable reporting infrastructure
Work with data architects and data analysts to determine design needs
Mentor and provide guidance to junior engineers and new team members
Ensure data users and consumers utilize data responsibly through data governance and compliance initiatives
Perform data validation testing to ensure accurate data flows
Promote data and analytics capabilities and awareness to business unit leaders in order to leverage opportunities to achieve strategic business initiatives
Define coding standards, including Object Oriented design standards and design patterns
Implement a standardized library of reusable objects across the enterprise, such as the core banking platform
Maintain awareness of and adherence to Bank’s compliance requirements and risk management concepts, expectations, policies and procedures and apply them to daily tasks
Deliver a consistent, high level of service within our client's standards
Other duties as assigned
Requirements:
Bachelor's degree in computer science, information technology, or related field
Technical Expertise with SQL, PL SQL
Previous knowledge and experience integrating Informatica ETL
Experience with Tableau, Cognos, SSRS or equivalent BI tool
Previous experience using SSIS, Informatica or equivalent ETL
Skilled in Oracle, Snowflake, and SQL servers
Experience in AWS and Azure
Interpersonal/Customer Service Skills
Written and Verbal Communication. Ability to work as part of a team
Adaptable to change. Able to Multi-Task or Juggle Priorities
Analytical Thinking and Problem/Situation Analysis
Experience with Master Data Management (MDM) is preferred
An Equal Opportunity Employer
We do not discriminate based on race, color, religion, national origin, sex, age, disability, genetic information, or any other status protected by law or regulation. It is our intention that all qualified applicants are given equal opportunity and that selection decisions be based on job-related factors.",#N/A,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Mastery Logistics Systems, Inc
3.9",3.9,"Omaha, NE",Senior Data Engineer (Kafka),"About the Role
In the world of transportation, data is constantly moving, and Kafka is the roadway that keeps that traffic running smoothly to its destination. As a technical expert, you must be comfortable working across teams on multiple, high impact projects. You will be a valued part of a team that is constantly maturing Kafka use and event-driven architecture. Members of this team are responsible for the overall use and implementation of Kafka components including the Confluent platform, observability, governance, best practices, and solution development. An understanding of Kafka principles and enterprise integration patterns is required.
In order to be successful:
You are a self-directed person who can identify priorities.
You are a detail-oriented person who takes pride in keeping data correct and always having a backup plan.
You are a problem-solver who might write a script or find a tool to get things done when there isn't an established solution.
You want to learn and grow in the event-driven world.
You love Kafka! When you hear terms like ""event-driven"" or ""real-time streaming"" you're ready ready to dive in!
Responsibilities
Lead a team of Kafka engineers in an operational capacity
Develop and implement solutions using Kafka.
Administer and improve use of Kafka across the organization including Kafka Connect, ksqlDB, Streams, and custom implementations.
Work with multiple teams to ensure best use of Kafka and data-safe event streaming.
Understand and apply event-driven architecture patterns and Kafka best practices. Enable development teams to do the same.
Assist developers in choosing correct patterns, event modeling, and ensuring data integrity.
Continuous learning to be a Confluent/Kafka subject matter expert.
Work with Kafka and Confluent API's (e.g. metadata, metrics, admin) to provide pro-active insights and automation.
Work with SRE's to ensure Kafka-related metrics are exported to New Relic.
Perform regular reviews of performance data to ensure efficiency and resiliency.
Contribute regularly to event-driven patterns, best practices, and guidance.
Review feature release and change logs for Kafka, Confluent, and other related components to ensure best use of these systems across the organization.
Work with lead to ensure all teams are aware of technology changes and impact.
Acquire a deep understanding of source and sink connector technical details for a variety of platforms including PostgreSQL, MS SQL Server, Snowflake, and others as required.
Requirements
Be able to describe the primary components of Kafka and their function (brokers, zookeeper, topics).
At least two years of experience supporting applications in a production environment.
You will be expected to read and navigate code in multiple languages. Multi-language fluency and writing is not required.
Experience in a microservice architecture
Experience with event driven architecture
Proficiency in at least one programming language and one scripting language.
Proficiency with Docker containers.
Ability to participate in and contribute to code management in Github including actively collaborating in peer-reviews, feature branches, and resolving conflicts and commits.
Excellent written and verbal communication skills.
Strong sense of responsibility with a bias towards action.
Comfortable self-directing and prioritizing your own work.
Microservices experience is a plus.
Distributed tracing experience a plus.
An understanding of any cloud (Azure preferred) infrastructure and components is a plus, but is not required.
Create reference solutions.","$96,493 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
National Group Corporation,#N/A,"Dallas, TX",Data Engineer,"Who are we?
ReAlign Insurance Holdings is a leading primary underwriter of program administrator managed specialty insurance programs in the U.S. With the extensive knowledge and experience our team has in building and managing carriers and programs we are able to reach decisions on program opportunities quicker than our competition.
Why choose ReAlign?
At ReAlign, we believe that we are only as strong as our people. We strive to create an inclusive and welcoming culture where employees feel comfortable and empowered to be themselves. We support a diverse and inclusive work environment, with a focus on a healthy work-life balance. We understand that comprehensive benefits are important which is why we offer a benefit package that includes: medical, dental, vision, company paid short and long-term disability, life insurance, health spending account, 401(k) match every pay period, PTO, paid holidays, floating holidays, tuition reimbursement, and referral bonuses.
Job Summary
The Data Engineer position will involve data exchange process analysis, design, development, unit testing, and support. This role would work with other data team members on database ETL processes and reporting needs in both the on-premise database environment as well as the Azure data environment. This position requires strong technical and analytical skills to review business processes and data, recommend solutions, conduct functional and code analysis, and lead and perform database development tasks. This position will also involve production support and require experience within a DevOps environment.
Essential Functions
· Design, development, and support data ETL processing for both external and internal customers
· Support the internal and Azure cloud-based data warehouse platforms
· Work with DBA/System Admin to develop and maintain automated data processing
· Design and analyze queries and stored procedures
· Write and support SSRS and PowerBI reports
· Work in a data development life cycle environment
Work with stakeholders and data analysts to understand business requirements
Work with end users and the testing team to perform unit, functional, and integration testing and validation prior to production implementation
Troubleshoot and resolve operational problems with a focus on delivery
· Communicate solutions and architectural design to IT leadership
· Work in both Kanban and Scrum Agile environments
· Continually improve processes and solutions, seeking ways to use new tools, features, and capabilities
Job Requirements
3-10 years of experience as a Database Developer
Highly proficient in writing advanced SQL queries, stored procedures, SSRS reports, and ETL processes
Strong knowledge of relational and transactional database architecture
Strong technical and analytical skills
Experience with the Microsoft Azure and PowerBI platforms and Agile development methodology a plus
Bachelor’s Degree in computer science, database development, or related field preferred
Detail oriented with the objective of delivering a solution to a problem in a timely and efficient manner
Self-driven, willing to take initiative when an opportunity presents itself
Strong collaboration and communication skills and the ability to work in a team environment a must
Ability and willingness to work in a partial on-prem and partial remote office environment
Experience in the Property and Casualty Insurance industry a plus
Proficient with standard desktop software used in a business environment (MS Word, Excel, Visio, etc.)
Must be able to work in a hybrid in-office and remote work environment
Must be authorized to work in the United States
Job Type: Full-time
Pay: $90,000.00 - $125,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Health savings account
Life insurance
Paid time off
Retirement plan
Vision insurance
Experience level:
3 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you currently local to the Dallas area and able to commute to the office 2 to 3 times per week? (Deal breaker)
Experience:
writing advanced SQL queries and ETL processes: 3 years (Required)
cloud database platform (i.e. Azure, AWS): 1 year (Preferred)
Azure Data Factory: 1 year (Preferred)
Work Location: In person","$107,500 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Purpose Financial
4.2",4.2,Remote,Data Engineer,"Address : 322 Rhett St, Greenville, South Carolina, United States - 29601
Purpose Financial, Inc. is an innovative consumer financial services company that offers a diverse suite of credit products, promoting financial inclusion and meeting consumers wherever they are. Through its brands, the company is committed to helping customers achieve their version of financial stability in the moment and in the future. Since 1997, Purpose Financial has been a pioneer in the consumer credit and financial services market offering money solutions in over 1,000 storefronts locations and online lending. Providing services in over 20 states, Purpose Financial employs over 2,700 team members.
At Purpose Financial, our people are our greatest asset. We are inclusive and encourage different points of view. We all belong here. We respect our colleagues. We value our customers, and we support our communities.
What do we do? Our sole purpose is to help each customer achieve their version of financial stability in the moment and in the future.
We have a history! Consumers know that when they choose Purpose, they’re backed by a trusted, reliable financial services company with 25 years in the business, 2,700+ employees, 1,000+ store locations, and operating in 24 states.
Benefits and Perks. We believe our employee benefits should make life better. Purpose Financial offers a comprehensive benefits package to everyone on our team.
Health & Welfare Benefits. When you take care of yourself, you’re a better you – which is better for everyone!
Health Savings & Flexible Spending Accounts. Pre-tax money for your family’s out-of-pocket expenses.
401k with Matching Contributions. Think bigger! You can build a better future.
Wellness & Employee Assistance Programs. Take care of yourself in every way that matters.
Generous Vacation Policy & Paid Time Off. Time off to take care of yourself, your family – and your bucket list!
Volunteer Days. We want you to care. Always. Take 2 days off each year to volunteer and make a difference in your community.
Tuition Reimbursement. Pursue your goals without fretting over all the costs.
Employee Discounts. Save on home and auto insurance, cell phone service, entertainment, shopping, pet insurance and even travel.
Performance Incentives & Awards. We want you to feel valued for all your contributions.
Hybrid Workforce Support. We believe in getting it done – in whatever way works best for you and your team.
Want to meet our executive team? Visit our home page at www.havepurpose.com.
Position Summary
The job function is responsible for building a scalable data infrastructure. The role will be part of the effort to enable Purpose Financial to democratize data by developing self-serving and curated data models for our single source of truth. The job function includes understanding of business requirements, defining data requirements, identifying enterprise data sources, planning, designing, and building data acquisition from sources, defining data model, designing data ingestion pipelines, data normalization, data transformation, and consumption using platforms such as Snowflake and AWS.
Job Responsibility
Take ownership of building/optimizing data pipelines to increase efficiency and performance
Develop and maintain data ingestion and integration processes that focus on data quality and monitoring
Use AWS and Snowflake tools such as Glue, Lambda, Snowpark, and Snowpipe to build data ingestion pipelines from various sources
Incorporate standards and best practices into data pipeline solutions
Participate in troubleshooting and problem-solving design discussions
Provide logical and physical data design, and database modeling
Solve complex data issues around data integration, unusable data elements, unstructured data sets, and other data processing incidents
Continuously design the future state of our data & reporting process by collaborating with IT and business partners.
Partner with enterprise teams, data scientist, architects to define development standards and solutions.
Manage cloud infrastructure as code (IaC), using Terraform.
Work closely with other members of the data team to better design and build metrics that enhance our analytical capabilities
Manage workflows and data build processes that help the BI team build key business metrics and design dashboard visualizations
Assess the effectiveness and accuracy of new data sources, data points, and data gathering techniques
Troubleshoot problems with data (missing, corrupted, invalid) and build data recovery strategy
Apply best practices for AWS & Snowflake architecture, ELT and data models
Be a leader for our data journey, act as SME for data and database related topics, and assist with development of data lakes and warehouses
Education Required
Bachelor’s degree (or foreign equivalent degree) in any Computer, Engineering, or other technical field.
Experience Required
5+ years experience in data warehousing
3+ years of experience in data architecture and design, with a good understanding of various data systems and structures (i.e. SQL, NoSQL, Key-Value, Streams)
3+ years of experience with building data pipelines and using data orchestration tools and implementing end to end Monitoring & Alerting solutions
3+ years of experience working with AWS or public cloud provider.
Strong understanding of SQL, data structures, data integrity, and schema design best practices
Programming experience required, preferably Python.
A strong advocate of data governance and data quality
Experience with database internals, database design, SQL and database programming
Experience in financial services (Banking and consumer lending preferred)
Experience of authoring CI/CD pipelines, automation elements related to infrastructure composition, deployment orchestration, and monitoring.
Familiarity and experience with Agile/SCRUM based development methodology.
Knowledge Required
Excellent written and verbal communications skills as would be needed to communicate in person, by phone, and through email; adaptability and flexibility to changing environment; and comfortable working in a dynamic, high volume, fast-paced environment. Ability to read, write, evaluate, and apply information. Ability to interact professionally and exhibit appropriate social skills. Ability to understand and ensure compliance with policies, procedures, and laws governing our industry/business and products. Ability to develop and maintain business relationships.
Physical Requirements
Sitting for long periods of time; standing occasionally; walking; bending; squatting; kneeling; pushing/pulling; reaching; twisting; frequent lifting of less than 10 lbs., occasional lifting of up to 20 lbs.; driving and having access during the workday to an insured and reliable transportation; typing; data entry; grasping; transferring items between hands and/or to another person or receptacle; use of office equipment to include computers; ability to travel to, be physically present at, and complete the physical requirements of the position at any assigned location.
Competencies
Integrity
Customer Advocacy
Interpersonal Skills
Results/Accountability
Inclusiveness
Technical Proficiency/Leadership
Big Data Trends
Data Management
Adaptability
Travel - None
Attire - Business Casual
Other - Must be eligible to work in the USA and able to pass a background check.
Job Type: Full-time
Compensation package:
Yearly pay
Experience level:
5 years
Schedule:
Day shift
Work Location: Remote",#N/A,Unknown,Company - Private,Financial Services,Financial Transaction Processing,#N/A,Unknown / Non-Applicable
United Digestive,#N/A,"Atlanta, GA",Data Engineer,"GENERAL SUMMARY OF DUTIES: United Digestive’s Data Engineer will be responsible for developing and maintaining data pipelines, data warehouses, and data models to support the company's Business Intelligence reporting initiatives.
REPORTS TO: Director of Business Intelligence & Analytics
RESPONSIBILITIES:
Duties may include but are not limited to:
Implement, maintain, and improve data integration processes to collect, process, and manage large datasets
Collaborate with stakeholders to understand business requirements and design data solutions that meet their needs
Implement and maintain data quality checks and validations to ensure data accuracy and completeness
Optimize data processing and storage to ensure high availability, scalability, and performance
Develop and maintain automated data workflows and scheduling tools
Code ETL process using Azure Data Factory workflows and other technologies as appropriate
Troubleshoot and resolve data-related issues and incidents
Stay up to date with the latest technologies and trends in data engineering, specifically the Azure suite, and recommend improvements and optimizations to the data architecture and infrastructure
Ability to develop solutions and manage existing Azure DevOps pipelines
Maintains a high level of confidentiality with sensitive information
Performs any other duties and/or special tasks as assigned by United Digestive leadership
REQUIRED EDUCATION, SKILLS & EXPERIENCE:
Bachelor’s degree in Computer Science, Software Engineering, Information Technology, or in a related field required; healthcare-related Data Analytics experience preferred; 2+ years of data pipeline management required. Proficiency in SQL and experience with relational databases (e.g., Oracle, SQL Server, PostgreSQL); experience with data modeling, data warehousing, and ETL processes, proficiency in various programming language (e.g., Python, SQL, Java), experience with cloud platforms (e.g., AWS, Azure, GCP) and big data technologies (e.g., Databricks, Hadoop, Spark, Kafka, Cassandra). Must have knowledge of data visualization and BI tools (e.g., Tableau, Power BI) and how to support these tools in an efficient manner.
ADDITIONAL SKILLS AND EXPERIENCE:
Data Engineer must have:
Excellent verbal and written communication skills.
Excellent organizational skills and attention to detail.
Ability to multi-task and prioritize job needs
Ability to deal with patients, visitors, co-workers, and physicians with courtesy and respect
Work under pressure; assess, respond to, and communicate issues in a timely manner.
Self-motivation and be a proactive individual with a professional can-do attitude
PHYSICAL/MENTAL/ENVIRONMENTAL DEMANDS
Requires sitting and standing associated with a normal office environment. Travel as business needs dictate.
DRUG FREE WORKPLACE
United Digestive is a drug free workplace. All offers of employment are contingent upon passing a pre-employment drug screening.
EQUAL OPPORTUNITY EMPLOYER
United Digestive is an Equal Opportunity Employer and does not discriminate on the basis of race, religion, gender, color, or national origin in its employment practices.
Location: 1355 Peachtree Street NE STE 1600, Atlanta, GA 30309
Job Type: Full-time",#N/A,Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"American Power & Gas
2.5",2.5,"Largo, FL",Data Engineer,"Because of expansive growth American Power & Gas is seeking a Data Engineer to add to our technical team. This is a fulltime permanent on-site role.
We have been offering Green Energy solutions to both residential and small commercial customers for over 20 years and have won the award for fastest growing company in the Tampa Bay Business Journal as well as being featured in Forbes and the Huffington Post.
**
**
Key Responsibilities
Support operational executives in solving business problems by designing, developing, troubleshooting, and implementing data driven solutions to complex technical objectives.
Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes.
Deploy sophisticated analytics programs, machine learning and statistical models to predict business outcomes and continually optimize performance through data science.
Gather and summarize technical requirements associated with strategic business initiatives.
Document, maintain, support and enhance company technology platforms and analytics applications.
Look for opportunities to streamline and automate various reporting processes to help support internal teams.
Assist with the research and development of internal business case studies to support onboarding new data tools.
Extract, manipulate and cleanse raw data from various data sources – call center, web, and CRM systems.
Work independently as well as collaboratively with other team members and key stakeholders as required to troubleshoot and resolve data issues.
Provide comprehensive technical and consultative services to support development and maintenance of internal and third-party platforms.
Create functional test cases/criteria to verify all functionality adheres to specifications and create end user manuals.
Routinely represent the Analytics department in cross-functional status & data strategy meetings.
Assist business analysts in provision of regular performance trends reporting, forecasts, and insights for marketing and sales team leaders as well as senior executive management to maintain a tight finger on the pulse of emerging performance trends and opportunities.
Partner with internal cross functional teams to identify business needs and analytics opportunities, developing tools and techniques to analyze and provide performance-improving recommendations.
Partner with the Operations team to optimize data workflows from sourcing to storage to reporting to deliverables to maximize for value-added and time-efficiency.
Develop, enhance, and manage various analytical solutions in support of business objectives.
Determine what data is needed and how to consume and store this data to support reporting needs and ad hoc performance-improving analysis for internal stakeholders.
Ensure deliverables are adapted properly to stakeholder audience; adapting terminology and visuals as needed to “speak the stakeholder’s language”, thus communicating with maximum effectiveness.
Troubleshoot and QA data, reporting, and tracking anomalies as needed, with proactive communication to stakeholders.
Relentlessly challenge the status quo. Always be critical of how we can be more effective or more efficient as an individual, as a team, and as a business.
Provide ongoing and proactive client service to your internal customers as required to continue elevating the performance of the business.
Regularly work with and analyze data across marketing channels and the customer journey through website analytics, call center activity, and CRM systems.
Requirements
University degree or college graduate in Engineering, Computer Science, Mathematics, Statistics, related technical/programming discipline, or the equivalent hands on experience in Data Engineering or Software Development.
Proficiency with coding in SQL is required. Ability to also write in Python, R, Java, or similar programming languages is preferred.
Strong technical prowess, including an understanding of algorithms, systems architecture and end user experience.
Experience with modern source, build, and deploy tools such as Git, Grub, Maven, Yeoman, etc. is a plus.
Ability to think unconventionally to derive innovative and creative solutions.
Competency in accurately estimating development timelines.
Experience with data warehouse design, relational databases, SQL/NoSQL data modeling, RESTful API standards and large scale data processing solutions.
Demonstrated skill in database development with solid understanding of schema design, stored procedure development, query optimization and ETL processes.
Excellent troubleshooting ability. Must be able to resolve issues tied to capturing and processing data in a timely manner.
Excellent English written and verbal communication skills, especially explaining technical concepts to non-technical business leaders.
Exceptional critical thinking and problem-solving skills; able to distill overall objectives into the actionable steps required to achieve those objectives.
Capable of effectively managing projects, priorities, timelines, and working relationships.
Must possess the intellectual curiosity to succeed in a dynamic, entrepreneurial, fast paced, sales driven organizational culture.
Excited by the opportunity to disrupt the status quo and uncover the eureka moment insights that will take the business to the next level – proactively searching for problems to solve, knowing there is so much to learn.
We offer Health, Dental, Optical and Life Insurance, PTO (paid time off) and the opportunity for promotions and room to advance.
For immediate consideration please send a resume to Carl Schumacher the Manager of Recruiting CarlS@goapg.com
Job Type: Full-time","$92,733 /yr (est.)",201 to 500 Employees,Company - Private,"Energy, Mining & Utilities",Energy & Utilities,2010,$100 to $500 million (USD)
"FiberSense
5.0",5.0,"Oakland, CA",Data Engineer,"Why join us: Our benefits
Global team and remote role
Inclusive and flexible work culture
Motivated and vibrant team
Small team, large career growth opportunities
Opportunities and support to learn and receive mentoring
Fast growing start-up with unique, highly technical and exciting product
We are making the world safer and more sustainable
Our technology is one of a kind – there is no one else in the world doing what we're doing
About the opportunity: Data Engineer
Hybrid (from Oakland) or remote | Very flexible hours | $141,000 - $155,000 USD per annum
The software team works in a hybrid work environment from an office in Oakland, so ideally you would be based in Oakland too. We will consider candidates who are 100% remote within the USA. We encourage all types of flexible working arrangements.
As FiberSense's first dedicated Data Engineer, you will have a significant role in enhancing our existing data pipelines, while also taking charge of designing, developing, and maintaining new robust and scalable data infrastructure.
You will report directly to the head of ML/AI, located in Oakland, California. However, for the right candidate, we support remote work and offer flexible working hours.
Your responsibilities will include:
Design, build, and maintain data infrastructure focusing on reliability and scalability that can accommodate the growing volume of data from the sensor fleet, using AWS cloud technologies (AWS S3, Lambda, RDS, EMR, and Glue/EventBridge).
Extract and integrate data streaming from our rapidly growing fleet of sensors via RESTful APIs into storage buckets and databases
Ensure data quality, accuracy, and completeness by implementing data validation protocols and testing
Provide input on which APIs and data formats to expose for easier ingestion of streaming data from the sensors
Evaluate infrastructure options that fit the business's needs and optimize costs at scale
Stay up to date with emerging technologies and industry trends
Automate data security and develop data microservices
Build scalable and performant databases using SQL, such as PostgreSQL and MySQL.
Implement data monitoring and alerting, ensure the data infrastructure is running smoothly and that any issues are detected and resolved quickly
Within 90 days, you will:
Ensured the robustness of, or rebuild, the current event-triggered data extraction pipelines to S3/RDS
Have met with key stakeholders to scope needs and priorities both long and short-term
Explored existing data pipelines and contribute significantly to a roadmap for data infrastructure and pipelines
Begun implementing proposed infrastructure and pipelines
About you: The type of person who will be a good fit
If you are excited about this role but not sure if you meet all of the criteria, please apply. Research shows women and minority groups are less likely to apply for roles when they don't meet 100% of the criteria. We'd like to change this.
Essential:
Minimum of 3 years of experience in data engineering
Strong programming skills in relevant languages such as Python, SQL (required) and Golang (or other low-level language and a willingness to learn Go), Java (preferred)
Bachelor's or Master's degree in Computer Science, Information Systems, or a related field
Demonstrated proficiency in AWS cloud technologies, including AWS S3, EMR, EC2, RDS, Glue, and Lambda
Experience using workflow managers and building ETL processes
Experience with data pipeline development using tools such as Apache Airflow, AWS Glue, and AWS Data Pipeline
Expertise in data warehousing, database architecture, and SQL/NoSQL databases, such as PostgreSQL, MySQL, MongoDB, Cassandra, and DynamoDB.
Strong understanding of API development and implementation, including RESTful APIs using tools such AWS API Gateway
Hands-on experience with data ingestion tools such as Apache Flume and Kafka to collect, store and process large volumes of data in real-time or batch mode
Excellent problem-solving and analytical skills
Excellent communication and collaboration skills
Highly desired:
Experience working with IoT or sensor fleets particularly time-series data
Experience working with data stored in protobufs
Experience working with large and rapidly increasing data throughput (10s GB/day)
Experience serving data to be consumed by AI/ML teams
Experience building AI/ML models
Experience designing and building data infrastructure/pipelines end-to-end from inception
Experience working in a rapidly-growing start-up
About FiberSense:
FiberSense is a deep tech company with a remarkable set of capabilities and new service offerings that will transform the way we perceive and respond in public spaces. Our mission is to build the first centralized ubiquitous sensing fabric for all moving objects and events in public spaces in all cities around the world. Our expertise and technology sits at the intersection of optical fiber sensing networks, integrated photonics and machine learning. More than a sensor company, FiberSense is building out a global scale sensing platform using our patented VID+R™ (Vibration Detection and Ranging) technology. The company is Australian founded and already has employees across the globe, which is also true of our customers. FiberSense is implementing a giant vision and a culture built on two key pillars of massive innovation and effective execution. We offer an environment that is exciting, inclusive, challenging, fun and extremely rewarding.
FiberSense is an equal opportunity employer committed to providing a working environment that embraces and values diversity and inclusion. If you have any support or reasonable adjustment requirements, we encourage you to advise us at time of application. All qualified applicants will receive consideration for employment without regard to age, ancestry, colour, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances.
Apply now. Join us on our growth journey as we transform the way we perceive and respond in public spaces!","$148,000 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"LatentView Analytics
4.0",4.0,"Dallas, TX",Data Engineer,"Role : Data Engineer
Experience : 6 - 8+ Years
Location : Dallas,Tx (Onsite)
Position: FullTime Only
Skills :Python,SQL Server ,Scala, Hadoop, HPCC, Storm, Cloudera, Cassandra,Excel, R,Docker,Kubernetes,Snowflake,Azure,Kafka,Redshift,Hadoop,AWS.
Job Type: Full-time
Pay: $80,000.00 - $1,100,000.00 per year
Compensation package:
Yearly pay
Experience level:
6 years
Schedule:
8 hour shift
Experience:
data engineer: 6 years (Preferred)
Work Location: In person
Speak with the employer
+91 9876543210","$109,012 /yr (est.)",1001 to 5000 Employees,Company - Public,Management & Consulting,Business Consulting,2006,$25 to $100 million (USD)
Alter Tech Solutions,#N/A,"Jersey City, NJ",Senior Data Engineer,"Job Title: Data Engineer
Location: Jersey City, NJ
Job Type: Hybrid
Job Description:
Position is more about scale - application monitoring, automation framework, test automation - data engineering - data pipelines - rest api's
system design - experience with distributed systems, design the right components, push out to the AWS cloud, data structures/algorithms
Experience:
10+ years of experience cross functional experience - data engineering and sw engineering background (working with risk, audit teams) need hands on coding but also design
Qualifications & Requirements:
TOP SKILLS java preferred language but would consider strong python - Apache Flink, Kafka, Cassandra, GraphQL, SPARK, MACHINE LEARNING.
Job Types: Full-time, Permanent
Salary: $60.00 - $75.00 per hour
Experience:
Java: 8 years (Preferred)
GraphQL: 8 years (Preferred)
Python: 5 years (Preferred)
Work Location: On the road",$67.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Pomeroy
3.1",3.1,"Cincinnati, OH",Data Engineer,"General Function:
The Data Engineer will play an important role in our growing Enterprise Data and Analytics
team. The person in this role will build out a new centralized Analytics Data Lakehouse,
help maintain our existing Operational Data Warehouse, and the infrastructure that
underlies both. We are looking for a candidate with experience creatively solving data
complexities of various sizes and levels of cleanliness - with the goal of enabling data
analysts and business users throughout Pomeroy to make decisions backed by data.
Job Type: Full-time
Pay: $70,000.00 - $80,000.00 per year
Benefits:
401(k)
Dental insurance
Employee assistance program
Flexible schedule
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Vision insurance
Compensation package:
Yearly pay
Experience level:
4 years
5 years
6 years
7 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
SQL: 4 years (Required)
Oracle Cloud Integrator: 4 years (Required)
Data warehouse: 4 years (Required)
Work Location: In person","$75,000 /yr (est.)",1001 to 5000 Employees,Contract,Information Technology,Information Technology Support Services,1982,Unknown / Non-Applicable
"World Finance
3.3",3.3,"Greenville, SC",Data Engineer,"**Hybrid role open to local Greenville, SC or Candidates relocating to Greenville, SC**
The Data Engineer implements data-centric solutions to complex business problems. This individual will work alongside a data architect and the data engineering team to expand and optimize data delivery architecture. Providing expertise in data warehousing and data delivery, this individual will work with cross-functional teams including marketing, analytics, operations, and software engineering to build data-aware systems and process flows across the enterprise.
Essential Duties and Responsibilities:
· Protect confidential company information.
· Assist business, technology, and support partners/stakeholders to deliver secure data solutions.
· Design, build, and maintain data delivery solutions in accordance with governing data architecture patterns.
· Model and assemble data sets that meet functional and technical business requirements.
· Implement infrastructure required to optimize ETL and ELT operations across a variety of data sources.
· Process file-based data extracts using data retrieval and management tools to provide timely loading of critical business data.
· Identify, design, and implement process improvements in data flows and data pipelines, focusing on automating data tasks.
· Integrate external systems with internal systems to ensure proper data flow between systems.
· Maintain an accurate and comprehensive inventory of data, data systems, and data storage.
· Communicate with non-technical stakeholders to determine technical solutions to business problems.
· Perform data load, data extraction.
Qualifications:
To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of the knowledge, skill, and/or ability required:
· Expertise with relational databases.
· Advanced SQL proficiency.
· Experience working with data warehousing systems for large enterprises with both internal and customer facing applications, preferably with near real time transactional data.
· Expertise in creating and maintaining data structures in SQL.
· Experience structured and non-structured data paradigms, relational databases, data lake and data warehouse technologies, data vault and dimensional data models.
· Ability to define problems, collect data, establish facts, and draw valid conclusions.
· Ability to interpret an extensive variety of technical instructions in mathematical or diagram form and deal with several abstract and concrete variables.
· Ability to conduct research into systems issues and products to determine integration requirements.
· Ability to communicate ideas in both technical and user-friendly language.
· Ability to work with IT operations to quickly come to resolution of open support requests.
Self-motivated and able to work within a project-based environment.
Possesses strong oral and written communication skills, clearly and accurately communicating complex and/or technical information to both technical and nontechnical audiences.
Python for building and redefining data pipelines is required.
Education and/or Experience:
· Bachelor’s Degree in MIS, Computer Science, or related field, or equivalent professional experience.
· Minimum 6 years of designing and implementing Data engineering solution.
· Hands on experience with building productionized data ingestion and processing pipelines.
· Experience working in business intelligence and data warehousing environments.
· MS SQL Server, Integration Services (SSIS) required.
· Strong TSQL scripting abilities and understanding of complex stored procedures, views, data. aggregation/manipulation through table joins/queries, database design, normalization, and de-normalization techniques.
· Experience with Snowflake.
· Experience using Python.
· Experience with ETL tools (Matillion, Fivetran, Stich, etc) is a plus.
· Familiarity with Dev OPS, Airflow and DBT is a plus.
· Industry experience working with large data sets.
· Experience with Microsoft PowerBI or any BI visualization tool is a plus.
Physical Demands:
· Must be able to constantly remain in a stationary position.
· The person in this position needs to occasionally move about inside the office to access file cabinets, office machinery, etc.
· Constantly operates a computer and other office productivity machinery, such as a calculator, copy machine, and computer printer.
· Occasionally may require light lifting to 25 pounds.
Work Environment:
· Office environment.
· Occasional travel may be required.
This job description reflects management’s assignment of essential functions; and nothing in this herein restricts management’s right to assign or reassign duties and responsibilities to this job at any time.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Employee discount
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Referral program
Vision insurance
Compensation package:
Performance bonus
Yearly pay
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Greenville, SC 29601: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 4 years (Required)
Data warehouse: 4 years (Required)
Work Location: Hybrid remote in Greenville, SC 29601","$95,216 /yr (est.)",1001 to 5000 Employees,Company - Public,Financial Services,Banking & Lending,1962,$500 million to $1 billion (USD)
"Direct Line
3.8",3.8,"Ashburn, VA",Data Center Infrastructure Engineer,"SUMMARY:
Direct Line (“DL”) is a high growth global technology services company with primary focus in providing design, integration, installation, maintenance and managed services to well-known data center operators and technology companies. Direct Line deploys decades of experience and knowledge through key partnerships with hyperscale technology companies and multi-tenant data center operators that give its clients a competitive marketplace advantage. Direct Line is committed to continually improving our industry through certified training of cutting-edge technicians that deliver superior results with a passion for detail. Direct Line is headquartered in Fremont, California with additional locations in Virginia, Tennessee, North Carolina, New Mexico, the Pacific Northwest, Asia-Pacific, and Europe.
POSITION: Data Center Infrastructure Engineer
LOCATION: MUST BE LOCATED WITHIN A COMMUTABLE DISTANCE TO SANTA CLARA, CA OR ASHBURN, VA / MUST BE WILLING TO TRAVEL (75-80% travel)
Job description
We are now looking for a Data Center Infrastructure Engineer!
We are looking to grow our company and grow with the hardest working people in the world. Academic and commercial groups around the world are powering a revolution in artificial intelligence using deep learning techniques running on GPUs, enabling breakthroughs in the most complex problems from autonomous driving to medial image processing to natural language processing. Come work on an innovative company's AI technologies!
What You’ll Be Doing:
You will lead all aspects of and personally implement complex architectures in one of several data centers.
Solutions will include network, storage, and compute resources to meet customer requirements, SLAs and high levels of uptime. As a key member of the engineering team you will develop, implement, and lead rack-level elevation designs to ensure velocity and scale while efficiently utilizing space, power, and cooling.
Review, evaluate and improve the design and implementation of structured cable solutions to support network topologies
Establish continuous improvements in the design, implementation, deployment and operation of large-scale cloud-based solutions in power-dense air- and water-cooled environments
Develop and maintain processes and procedures associated with the management and deployment of data center infrastructure including asset management and RMAs
Support and expand data center monitoring applications, with a strong focus on CI/CD automation
You will ensure standards supporting operating procedures and engineering issues for problem incident management are followed, including all safety requirements
Handle network, electrical and mechanical operations at data centers focusing on availability, service delivery, and internal customer relationship management
Analyze and resolve critical engineering issues, often under tight timeframe pressures; Off hours and on-call hours are to be encouraged
What We Need to See:
You love solving hard problems and can work independently or as part of a team under tight timelines
You are passionate about providing outstanding support to customers
Bachelor’s degree in Math, Computer Science, or Engineering subject area. Equivalent background in Military Technical School also acceptable or equivalent experience in datacenter engineering operations.
6+ years’ experience as datacenter operations engineers with critical systems and telecommunications Infrastructure Standards, network certification is very desirable
Deep knowledge of data center operations including network, power, rack layouts, cabling, Raised Floor Systems, HOT/COLD aisle containment. Operational experience with compute, storage and GPU servers in both air- and water-cooled environments
Install, config, and maintain all NW and 3rd party HW
Experience with ERMA, Break-fix, etc.
Reading and understanding P2P cabling, labeling and cable mgt/dressing etc.
Ways to stand out of the crowd:
An obvious passion for getting things done in a fast-paced technology environment
Deep understanding of data center power and cooling infrastructure, of network and cabling infrastructure.
Experience with NetBox, CMMS, SNOW and Inventory Management tools.
You're a self-starter with an attitude for growth, continuous learning, and constantly looking to improve the team.
Attention to detail with superb interpersonal skills and the ability to effectively manage multiple priorities.
A positive attitude with a strategic outlook.
Constantly look to improve the team and build strong business relationships
Direct Line is a proud equal opportunity employer. We are a drug free, EEO employer committed to a diverse workforce. We will consider all qualified candidates regardless of race, color, national origin, sex, age, marital status, personal appearance, sexual orientation, gender identity, family responsibilities, disability, political affiliation, or veteran status.
Job Type: Full-time
Pay: $45.00 - $50.00 per hour
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible spending account
Health insurance
Life insurance
Paid time off
Referral program
Vision insurance
Schedule:
Monday to Friday
Experience:
CI/CD: 3 years (Required)
Linux: 3 years (Preferred)
Server Support: 3 years (Required)
Break/Fix / ERMA: 3 years (Preferred)
Data center Engineering: 3 years (Required)
Work Location: On the road",$47.50 /hr (est.),501 to 1000 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,1997,Unknown / Non-Applicable
"Seamless.AI
3.4",3.4,"Columbus, OH",Senior Data Engineer - Remote US,"Role: Senior Data Engineer
The Opportunity
We are seeking a Senior Data Engineer with a strong background in data ingestion and processing for large data sets. The ideal candidate is maniacal about data accuracy and has a proven track record of building robust and scalable data pipelines. This role requires an experienced professional who can work collaboratively with cross-functional teams to design and implement solutions that meet business requirements and adhere to data security and privacy policies.
If you are a senior data engineer with a passion for data accuracy and a proven track record of building robust and scalable data pipelines, we encourage you to apply for this role. We offer competitive compensation, a flexible work environment, and opportunities for growth and advancement within the organization.
About Seamless
Seamless delivers the world's best sales leads. Through our product, we help sales teams maximize revenue, increase sales, and easily acquire their total addressable market using artificial intelligence; by development of a robust real-time contact and company search engine as well as a suite of technically-advanced tools to support sales and lead generation. We have been recognized as one of Ohio's fastest growing companies and have been recently ranked No. 7 in LinkedIn's Top 50 Startups of 2022, featured in Forbes as #1 Software company in Ohio in 2022, and on G2's ""Top 100 Highest Satisfaction Products for 2022"" list!
The Seamless Team
We have an amazing culture and work environment that anyone would want to be a part of. We encourage a culture of positivity. We thrive off of continuous feedback and do whatever it takes to help our team and customers be successful. You will grow as an individual, professionally, and be able to see and feel the impact you are making to the growth of Seamless every day.
Role Responsibilities
Design and develop data ingestion and processing pipelines that ensure data accuracy and completeness.
Work closely with cross-functional teams to understand business requirements and design solutions that meet those requirements.
Lead efforts to identify and implement data quality control measures to ensure consistency and reliability of data.
Monitor data pipelines to identify issues and proactively resolve them.
Evaluate and recommend new technologies and techniques to improve data pipeline performance and scalability.
Ensure compliance with data security and privacy policies and procedures.
Mentor junior engineers and provide technical leadership in data engineering best practices.

Candidate Requirements
Bachelor's or Master's degree in Computer Science, Software Engineering, or a related field.
3+ years of experience in data engineering, with a focus on data ingestion and accuracy.
Strong experience with large-scale data processing frameworks such as Hadoop, Spark, or Flink.
Proficiency in programming languages such as Python, Java, or Scala.
Experience with SQL and NoSQL databases.
Familiarity with cloud-based data processing and storage platforms such as AWS, Google Cloud, or Azure.
Strong experience in designing and implementing data quality control measures.
Excellent problem-solving and analytical skills.
Strong communication and collaboration skills.
Experience mentoring and leading teams of data engineers.
Seamless.AI has been delivering the world's best sales leads since 2015. Our product is the first real time, B2B search engine helping sales teams maximize revenue, increase sales, and easily acquire their total addressable market using artificial intelligence. We have been recognized as one of Ohio's fastest growing companies and won 2020 Best Places to Work and LinkedIn's Top 50 Tech Startups in 2020 and 2022. We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. Visa Sponsorship is not included in our hiring package. Applicants will need to be authorized to work in the U.S.","$93,575 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable
"NexTek, LLC",#N/A,"Shoreview, MN",Sr. Data Engineer,"Nextek, LLC is a software development company in the financial services industry that creates web applications and solutions for financial services professionals. We are seeking a motivated Senior Data Engineer to join the team.
What we provide:
A laptop and other equipment/software necessary for work
Health Insurance, Dental Insurance, Vision Insurance
Paid time off (PTO), Volunteer time off (VTO)
Hybrid remote work options
401K
Life Insurance
Flexible schedules
Monthly Social Activities
Casual dress code
Professional development
Here is what we are looking for:
Experience in database administration, security, backup, and recovery
Preferred Knowledge:4+ years of experience in data/software engineering
Experience in the financial services industry
Experience with a variety of relational databases (particularly MySQL & MSSQL)
Experience with collecting and presenting analytics
Here is what you will be doing:
Display proficiency in relevant technical skills.
Maintain a deep understanding of the data structure.
Provide evaluations and recommendations for our data environment based on potential issues, tradeoffs, and risks.
Scope and implement solutions.
Make good estimates and set realistic expectations for delivery.
Ask for guidance when necessary.
Maintain and monitor the health of databases and associated solutions.
Make use of the same environments setup by the Software Engineering team
Thoroughly test code for bugs and security issues before deploying to production environments
Maintain and monitor actively deployed solutions
Follow and help define best practices
Peer reviews
Maintain awareness of industry trends
Share experience and insights to help your team grow
Increase understanding of impact on other teams
Increase understanding of the customer
All other duties as assigned
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Shoreview, MN: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Health Federation Of Philadelphia
3.9",3.9,"Philadelphia, PA",EHR Data Engineer,"Equal Opportunity Employer
The mission of the Health Federation of Philadelphia is to promote health equity for marginalized communities by advancing access to high-quality, integrated, and comprehensive health and human services. Health equity is at the heart of all our work. We believe in and are firmly committed to equal employment opportunity for employees and applicants. We do not discriminate on the basis of race, color, national or ethnic origin, ancestry, age, religion, disability, sex or gender, gender identity and/or expression, sexual orientation, military or veteran status. This commitment applies to all aspects of the Health Federation of Philadelphia’s employment practices, including recruiting, hiring, training, and promotion.
JOB SUMMARY
The Electronic Health Records (EHR) Data Engineer will be responsible for managing all aspects of EHR related data with a focus on database administration, data integration, and data flow design to deliver advanced clinical and operational analytics for the Philadelphia Department of Public Health, Ambulatory Health Services (AHS), a large community health organization. The EHR Data Engineer will utilize a systematic approach and advanced tools to design and support AHS data management services. The position requires skills and knowledge to connect, store, manage, secure, analyze, report, distribute and govern EHR related data. This position works directly with AHS Leadership, EHR Program Management Office (EHR PMO) Program staff, Health IT (HIT), City Office of Innovation and Technology (OIT) and respective partner personnel to support comprehensive planning and tactical delivery of data management services in production operations. In addition, responsibilities include Tier 2 and Tier 3 support for EHR production issues and Helpdesk tickets regarding data, database, and respective services.
Strategic direction, programmatic and operational prioritization will be specified through the authority of AHS Leadership. Accountability is managed through the EHR PMO with AHS oversight.
JOB SPECIFICATIONS
Responsibilities/Duties
Support the design, development, deployment, and ongoing management of AHS data. Work with EHR PMO to integrate data management strategies for projects, programs, and initiatives.
Assist in the design, development, deployment, and support of relational and non-relational database designs in a team-oriented environment using various data visualization tools in accordance with City OIT standards.
Responsible for overseeing patch management and maintenance on reporting infrastructure and data related systems. Develop version and release migration methodologies.
Design, implement and control access to data and databases with/for approved AHS partners and City entities.
Assure systems and data integrity.
Oversee functional configuration components to support monitoring and administering assigned systems and applications.
Serve as the technical intermediary between departments and vendors for various database solutions and datasets within on-premise and cloud hosted systems.
Assist to design, implement, and manage integration points for automated retrieval, sharing and delivery of incoming and outgoing data.
Assist with data conversion, migration, and validation of data to ensure quality and accuracy.
Develop Standard Operating Procedures (SOPs) for EHR data, database, and system administration establishing standards and procedures to maintain consistent practices.
Abide by the EHR PMO change control policies and procedures while ensuring change control methods are congruent with overall Health IT and OIT procedures; Submit and present various changes related to AHS data management.
Support the backup and recovery strategy for database environments. Oversee and administer business continuity and disaster recovery tests and methods.
Perform daily administration and performance monitoring of database related systems.
Monitor and proactively support capacity and performance planning and analysis.
Identify database sizing requirements based on AHS patient population, provider, user and legal retention requirements; monitor space usage and alert appropriate staff members to resolve sizing issues and conditions that may cause application failures.
Perform root cause analyses and troubleshoot internal and external data-related problems.
Aid in the support/writing/configuration of data transformation services (DTS) and extract, transform, load (ETL) packages, Web Services, and SQL database objects to both monitor and maintain the database and support applications.
Actively participate in the creative process to continuously improve architectural designs and implementation to ensure optimal efficiency and cost effectiveness.
Assist AHS in developing and conducting presentations related to data management.
Maintain working relationships with AHS, EHR, HIT, City OIT, and Partner staff including regular status communications using EHR and AHS methods of communication.
Provide off-hours support for upgrades, problem resolution, outages, or disaster recovery events as needed.
Stay current with data management tools as departmental infrastructure evolves.
Assist AHS leadership & staff as needed.
Other data analysis and management duties as required.
Education
Bachelors Degree in Computer Science/Information Systems, Data Analytics, or related experience in the field.
Database and/or Server Administration related Certification
Industry related certificates or certification a plus
Skills/Experience
Demonstrated knowledge of the Systems Development Life Cycle
5-7 years of experience working with SQL Server (2016 or later) and related products; special consideration given to Microsoft Business Intelligence Platform/Development Studio suite (SSRS, SSIS, and SSAS)
5-7 years of experience working with Windows Server (2016 or later) and related products
Five (5) years working in health services related field
HIPAA/HITECH trained
In depth understanding of conceptual, logical, and physical database structures, architectures, and integration
Ability to translate, integrate, and export data from various cross-platform software and databases
Comfortable leading the design of both relational and OLAP databases
Database related expertise working with SQL Server (2012 or later), Oracle SQL developer, and related products; special consideration given to Microsoft Business Intelligence Development Studio suite (SSRS, SSIS, and SSAS)
Experience with working in physical and virtual server environments
Experience with log shipping, SQL back-up restore, and SFTP
Strong analytical skills including experience with tools for: Business Intelligence; Analytics; Dashboards; Reporting
Well-developed computer skills including: Office365, Visio, entity relational diagrams
Effective organizational skills, detail-oriented
Excellent communication and presentation skills, both written and oral
Well-developed interpersonal skills including proper phone etiquette
Ability to work independently, be flexible and handle multiple tasks
Ability to work with a variety of cultures and diverse audiences
Additional proficiencies not required, but preferred
Experience with FQHC, ambulatory, municipal government and/or public health
Knowledge of healthcare, medical, lab and/or public health terminology
Experience with eClinicalWorks, i2i Tracks, Rhapsody and VMware
Experience with Microsoft SQL, Oracle database, Azure Synapse Analytics, and other multi-model database management or data warehouse systems
Experience with Microsoft PowerShell, SharePoint, and Visual Studio
Experience with data visualization software such as Tableau, Power BI, Red Cap, Quickbase, etc.
Web Analytics; batch processing development; HTML; XML; IIS server administration
Work Environment
Standard office setting with extended periods at work station and periodic use of office equipment
Position Type and Work Schedule
Full time position, typical hours are Monday through Friday 8:30 am to 5:00 pm. Flex office schedule options available with supervisor approval.
Travel
Local travel to multiple sites could be required for periodic implementations, upgrades, outages or onsite meetings.
Physical Demands
Ability to manage in an office environment located in a City health center.
Ability to work in a cubicle or shared space environment.
Ability to travel between city locations.
Ability to lift up to 50 pounds.
Ability to work in a fast paced, high pressure setting.
Ability to tolerate extensive use of keyboard, typing, computer.
Salary and Benefits Our employees are our most valuable resource, so we offer a competitive and comprehensive benefits package, which can include:

Medical with vision benefits
Dental insurance
Flexible spending accounts
Life, AD&D and long term care insurance
Short- and long-term disability insurance
403(b) Retirement Plan, with a company contribution
Paid time off including vacation, sick, personal and holiday
Employee Assistance Program
Eligibility and participation is consistent with the plan documents and HFP policy.
DISCLAIMER
The Health Federation reserves the right to modify, interpret, or apply this job description in any way the Company desires. The above statements are intended to describe the general nature and level of work being performed by an employee assigned to this position. This job description in no way implies that these are the only duties, including essential duties, responsibilities and/or skills to be performed by the employee occupying this position. This job description is not an employment contract, implied, or otherwise. The employment relationship remains “at will.” The aforementioned job requirements are subject to change to reasonably accommodate qualified disabled individuals.","$100,000 /yr (est.)",51 to 200 Employees,Nonprofit Organization,Nonprofit & NGO,Civic & Social Services,1983,$5 to $25 million (USD)
"Kroenke Sports Enterprises
3.4",3.4,"Denver, CO",Data Engineer,"Job Title: Data Engineer
Department: Hockey Operations
Business Unit: Colorado Avalanche
Location: Denver, CO or Remote
Reports To: Director of Analytics
Employment Type: Full Time – Salaried - Exempt
Supervisor Position: No
_____________________________________________________________________________________
Kroenke Sports & Entertainment (KSE) is an American Sports and Entertainment holding company based in Denver, Colorado. KSE is committed to providing world class sports and entertainment for both live and broadcast audiences. We are the employer of choice as the owner and operator of Ball Arena, DICK’S Sporting Goods Park, the Paramount Theatre, 1STBANK Center, Denver Nuggets (NBA), the Colorado Avalanche (NHL), Colorado Mammoth (NLL), Colorado Rapids (MLS), KIMN,KXKL, KKSE (FM/AM), Altitude Sports & Entertainment, Major League Fishing/Fishing League Worldwide (MLFLW), Winnercomm, Outdoor Sportsman Group and SkyCam.

Nature of Work:
The Colorado Avalanche are looking to hire a full-time Data Engineer to work within the team’s Hockey Operations Department. This person will be responsible for maintaining and expanding the Avalanche hockey operations database. They will be tasked with importing and integrating data from external providers and interacting with the rest of the hockey operations department to ensure optimal dissemination of information to the appropriate parties. This person will also have the opportunity to analyze data and share insights with members of the Analytics department as well as the broader Hockey Operations department if desired. They will report to the Director of Analytics.

Examples of work performed:
Manage and improve the organization’s data storage, structure, and ETL pipeline while optimizing query performance for large datasets
Design automated processes to oversee data integrity and query performance on a regular basis, as well as being available to spot and resolve data issues that may arise at any time
Ensure that our automated processes run on schedule without issue
Incorporate expansive new datasets from disparate sources into our structure in a seamless fashion

This description is a summary only and is describing the general level of work being performed, it is not intended to be all-inclusive. The duties of this position may change from time to time and/or based on business needs. We reserve the right to add or delete duties and responsibilities at the discretion of the supervisor and/or hiring authority.

Working Conditions & Physical Demands:
Typical Office Conditions
Travel may be required

Qualifications:
Required
Strong knowledge of ETL architecture and development in a cloud-based environment
Academic and/or industry experience in database architecture, back-end software design, and query optimization
Expertise in SQL and Spark
Excellent attention to detail, problem-solving abilities, and work ethic
Preferred
1-3 years of work experience in a database-related position
An advanced degree in software engineering, computer science, information technology, or a related field
Familiarity with R and Python, as well as R Studio / Posit
Experience with databricks and Linux operating system
Knowledge of NHL players and teams as well as a passion for hockey and hockey analytics
Some front-end development and/or data analysis experience is considered a plus

Competencies/Knowledge, Skills & Abilities:
Ability to maintain positive attitude and demonstrate professionalism
Ability to maintain a high level of confidentiality
Ability to complete work accurately and in a timely manner
Ability to work independently & in a group setting and demonstrate good judgment skills
Ability to communicate effectively orally and in writing
Possesses excellent interpersonal skills
Ability to multi-task, prioritize and adapt to changing environments

Compensation:
Salary Range $80,000 - $110,000 per annum

Benefits Include:
12 Paid Company Holidays
Health Insurance (Medical, Dental, Vision)
Paid Time Off (PTO)
Life Insurance
Short and Long-term Disability
Health Savings Account (HSA)
Flexible Spending plans (FSAs)
401K/Employer Match

Equal Employment Opportunity
Kroenke Sports & Entertainment (KSE) provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.","$95,000 /yr (est.)",1001 to 5000 Employees,Company - Private,"Arts, Entertainment & Recreation",Sports & Recreation,#N/A,Unknown / Non-Applicable
"Financial Information Technologies LLC
3.7",3.7,"Tampa, FL",Senior Data Engineer,"Join Fintech as a Senior Data Engineer!
Fintech is the leading business solutions provider for the beverage alcohol industry, empowering alcohol suppliers, distributors, and retailers with smart solutions that simplify beverage alcohol management. Our unique, thriving company culture promotes collaboration and growth at every level, and our comprehensive employee benefits have earned Fintech the title of a Tampa Bay Times Top 100 Workplaces for 2020 and 2021.
Fintech’s Senior Data Engineer brings a depth of relational database modeling and an understanding of transactional processing across a myriad of database types. They can analyze and assess new data sets to understand nuances of content in the context of purpose with an ability to conceptualize cleansing, harmonization, and modeling efforts. Working under the direction of the principal process architect the senior data engineer will lead a small team of experienced data wranglers to tackle a myriad of ad-hoc custom projects as well as service the development needs within our warehouse and app abstraction layers.
Essential Functions:
Collaborates with ELT/process automation, data insights, and data science teams
Builds data models in accordance with prescribed methodologies
Serves as knowledgeable backstop for level III ticket resolution
Guides and instructs junior developers and engineers on how to implement directives in accordance with project needs within adopted framework
Gains a familiarity with and contributes to the core meta-data driven data processing engine
Advising on data model consumption in analytics layers
Contributes to knowledge base
Qualifications:
8 + years of experience with SQL in multiple database flavors (SQL Server, Oracle, Snowflake, Postgres, Greenplum)
5 + years of experience with data ingest transformations and harmonization
5 + years of experience with database object creation and modeling
Analytical thinker that can adapt and problem solve in a fast-paced environment
Team oriented
Must be able to consume, understand, and implement a complicated but flexible processing back-end in a short time frame
Our Benefits:
Employer Matched 401K (Up to 10% of Employee Salary)
Company Paid Medical Insurance Option (Employee and Dependent Children)
Company Paid Dental Insurance Option (Employee only)
Company Paid Vision Insurance Option (Employee only)
Company Paid Long and Short-Term Disability
Company Paid Life and AD&D Insurance
Employee Recognition Program
18 PTO Days a Year
Six Paid Holidays
Business Casual Dress Code
Check out www.fintech.com for more information!
We E-Verify.
Fintech is an Equal Opportunity Employer that does not discriminate on the basis of actual or perceived race, color, creed, religion, national origin, ancestry, citizenship status, age, sex or gender (including pregnancy, childbirth and pregnancy-related conditions), gender identity or expression (including transgender status), sexual orientation, marital status, military service and veteran status, physical or mental disability, genetic information, or any other characteristic protected by applicable federal, state, or local laws and ordinances. Fintech’s management team is dedicated to this policy with respect to recruitment, hiring, placement, promotion, transfer, training, compensation, benefits, employee activities, access to facilities and programs and general treatment during employment.
Fintech is a Drug-Free Workplace.","$109,213 /yr (est.)",51 to 200 Employees,Company - Private,Financial Services,Financial Transaction Processing,1991,$25 to $100 million (USD)
"Wish
3.2",3.2,"San Francisco, CA",Data Engineer,"Company Description

Wish is a mobile e-commerce platform that flips traditional shopping on its head. We connect hundreds of millions of people with the widest selection of delightful, surprising, and—most importantly—affordable products delivered directly to their doors. Each day on Wish, millions of customers in more than 60 countries around the world discover new products. For our over 250,000 merchant partners, anyone with a good idea and a mobile phone can instantly tap into a global market.
We're fueled by creating unique products and experiences that give people access to a new type of commerce, where all are welcome. If you’ve been searching for a supportive environment to chase your curiosity and use data to investigate the questions that matter most to you, this is the place.

Job Description

Our engineers move extremely fast, while solving unique and challenging problems. Our team is small and nimble. We release every day to ensure that engineers are able to iterate quickly, and make an impact immediately. We’re looking for engineers to work on our massive semi-structured datasets.
You'll develop software to process, transform and analyze the data to identify signals from billions of events we collect every day. You'll provide insights that improve the experience of hundreds of millions of users worldwide. You should be results-driven, highly motivated, and have a track record of using data analytics to drive the understanding, growth, and success of a product.
What you'll be doing:
Design and Develop data collecting and processing systems to handle large data sets. You’ll have the opportunity to design innovative data solutions and solve challenging problems.
Design, Develop and Support highly-parallel, and fault-tolerant applications.
Build and integrate scalable backend systems, services, platforms, and tools
Contribute to the design and code of complex data pipelines operating on production data
Optimize current approaches to efficiently handle ever-increasing volumes of data
Build proof of concept using modern technologies and convert them into production-grade implementation.
Create best-practice reports and dashboards based on data mining, analysis, and visualization

Qualifications
5 + years of experience as a Software Engineer or Data Engineer using Python, java or any other programming language
Expertise with SQL and data storage systems
Experience and knowledge of modern data warehouse, pipeline and reporting/analytic techniques and tools such as Airflow, Presto/Hive, Spark, or any other scheduling frameworks, Tableau or other reporting tools
Experience working on Amazon Web Services or other cloud computing platforms
Bachelor's degree in Computer Science or related field.
Preferred Qualifications:
Experience in data visualization a plus.
Here at Wish, you are joining a team and company at a time of growth and transformation. You will love being surrounded by people who are as passionate as you are about e-commerce, technology, and a data-driven culture. Even if you don't meet 100% of the above, we encourage you to still apply!
The estimated base salary range for this position varies by the candidate's location as follows:
Candidates located in California: $178,000 - $241,000 annually
Candidates located in New York and Washington: $201,000 - $241,000 annually
Candidates located in Colorado: $155,000 - $186,000 annually
For states not listed, the estimated base salary range for this position starts around $155,000 annually

Please note that individual total compensation for this position will be determined at the Company's sole discretion and may vary based on several factors, including but not limited to, location, skill level, years and depth of relevant experience, qualifications and other business considerations.

Additional Information

Wish values diversity and is committed to creating an inclusive work environment. We provide equal employment opportunities for all applicants and employees. We do not discriminate based on any legally-protected class or characteristic. Employment decisions are made based on qualifications, merit, and business needs. If you need assistance or accommodation due to a disability, please let your recruiter know. For job positions in San Francisco, CA, and other locations where required, we will consider employment for qualified applicants with arrest and conviction records.
Individuals applying for positions at Wish, including California residents, can see our privacy policy here.","$155,000 /yr (est.)",501 to 1000 Employees,Company - Public,Information Technology,Internet & Web Services,2010,$1 to $5 billion (USD)
"Abile Group, Inc.
5.0",5.0,"Saint Louis, MO",Data Cloud Engineer - Master,"Overview:
Abile Group has an exciting and challenging opportunity for a Data Cloud Engineer-Master on a 10 year contract providing User Facing and Data Center Services supporting an Intelligence Community customer. All the personnel on the team will work together to support innovative design, engineering, procurement, implementation, operations, sustainment and disposal of user facing and data center information technology (IT) services on multiple networks and security domains, at multiple locations worldwide, to support the IC mission.

The right candidate will possess the below skills and qualifications and be ready to handle all responsibilities independently and professionally.
Responsibilities:
Provides technical/management leadership on major tasks or technology assignments.
Establishes goals and plans that meet project objectives. Has domain and expert technical knowledge.
Directs and controls activities for a client, having overall responsibility for financial management, methods, and staffing to ensure that technical requirements are met.
Interactions involve client negotiations and interfacing with senior management.
Decision making and domain knowledge may have a critical impact on overall project implementation.
May supervise others.
Qualifications:
Clearance Required: TS/SCI

Degree and Years of Experience: BS/BA and 10 -15 years of relevant experience

Required Skills:
Experience in the various aspects of hybrid cloud activities.
Supports procurement and deployment of Platform Services to enable application portability across the private and public cloud environments offered by NGA.
Readies NGA's Hybrid Cloud Environment for system migration to IC ITE and oversee the future expansion of NGA Hybrid Cloud to additional public clouds.
In concert with DCS Government, supports standardization of DCS operations in a NGA Hybrid Cloud Management environment.
Transforms Government cloud requirements into appropriate technological alternatives and provides expertise in hybrid virtualization and cloud environments.
Experience developing systems, products, and/or processes based on a total systems perspective.
Consults, plans, analyzes designs, develops tests, assures quality, configures, installs, implements, integrates, maintains, and manages systems.
Has and maintains a diverse set of skills across multiple technical disciplines with recognized expertise in multiple disciplines and possess advanced knowledge of multiple mature and emerging technologies.
Works across organizational boundaries, both internally and externally and helps to drive the relationship between technical solutions and business needs of customers. Analyzes, defines and documents customer needs and required functionality.
Designs, develops and tests theoretical and/or physical models and develops the system design, considering operational impacts, performance, testing, manufacturing, cost and schedule, training, maintenance, and support.
Performs system level design trade analysis, reviews and approves system specifications and description documents, determines how a system is to be built, tested, and implemented, plans the system development execution and ensures adherence to appropriate standards, policies, principles, and practices.
Analyzes system capacity and performance to support problem resolution and system enhancements and monitors systems tests.
Responds to inquiries from a variety of sources for the purpose of providing technical assistance, consultation, advice and support, and regularly provides advice and recommends actions and solutions involving complex issues.

About Abile Group, Inc.:
Abile Group, Inc. was formed in July 2004 to partner with the Intelligence Community and their Contractors in the areas of Enterprise Analytics & Performance Management, IT & Systems Engineering and Program & Project Management. We have significant experience with the Federal Government and are an EDWOSB dedicated to our employees and clients. We are looking for high performing employees who enjoy providing advice and guidance along with solutions development and implementation support, crafted by combining industry best practices with the clients’ subject matter experience and Abile’s breadth of expertise.
EEO Statement:
Abile Group, Inc. is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability. Anyone requiring reasonable accommodations should email careers@abilegroup.com with requested details. A member of the HR team will respond to your request within 2 business days.

Please review our current job openings and apply for the positions you believe may be a fit. If you are not an immediate fit, we will also keep your resume in our database for future opportunities.",#N/A,51 to 200 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Dime Community Bank
3.4",3.4,"Hauppauge, NY",Data Engineer,"Summary: Dime Community Bank is looking for a Data Engineer based in Hauppauge, NY.
Salary commensurate with experience, ranging from $100,000 to $135,000 annually. The exact compensation may vary based on relevant experience, skills, education, training, licensure and certifications, and location.
All applicants need to attach a recent resume.
Responsibilities:
Work with relational databases and data warehousing. Add new and modify existing data models to ensure data warehouse is up to date for reporting needs.
Handle data visualization for extract transform and load (ETL).
Handle job pipelining and orchestration. Automate data loads and systems with Powershell scripting and Python. Orchestrate jobs with Dagster to ensure proper data processing, swift execution of tasks, and accurate monitoring. Build ETL pipelines to bring all data sources to the data lake, data warehouse and data marts. Work with python & SQL, and scripting in Powershell for automation.
Build python packages for use by the team for report building, accessing databases, etc.
Develop and maintain operational data stores (ODS) in our datalake and data warehouse architecture. Utilize ETL concepts and handle transformation with data build tool (DBT). 20468.3.6
**Telecommuting may be permitted.
Mail resume to Robin.Derin@dime.com. Must reference job 20468.3.6.
Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities
The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information. 41 CFR 60-1.35(c)","$117,500 /yr (est.)",501 to 1000 Employees,Company - Public,Financial Services,Banking & Lending,2021,$100 to $500 million (USD)
"YSI
3.7",3.7,Remote,Senior Data Engineer,"Position Title: Senior Data Engineer/ Oracle Apex Developer
Job Id: 202301002
Location: Herndon, VA (Remote)
Yakshna Solutions, Inc., (YSI) is a CMMI Level 3 assessed, ISO 9001, 20000:1, 27001 certified, woman-owned small business enterprises, headquartered in Herndon, Virginia, USA. YSI provides professional IT solutions and services to business corporations and government organizations. YSI is committed to serve its business communities as a leading IT vendor providing innovative, quality, and cost-effective IT business solutions and services.
Our benefits are very competitive that include 401(k), health, dental, and vision insurance, Life insurance, short-term and long-term disability insurance, paid time off, training, and professional development assistance.
YSI is seeking a highly qualified Senior Data Engineer. The selected candidate will be able to communicate effectively (written/verbal), possess strong interpersonal skills, be self-motivated, and be innovative in a fast-paced environment.
Responsibilities:
The Data Engineer will be the senior technical expert on work associated with data management, data quality and data structures, coordinating with the ADA as needed to ensure development and data structures are synchronized.
The Data Engineer will design the approach for data tasks and will contribute to completion of data tasks and oversee execution, providing advice and guidance as necessary to junior staff.
Required Qualifications and Skills:
Bachelors or master’s in relevant filed.
Good Data Engineering/Management experience
· Should be familiar with the the Civil Works missions for Hydropower, Recreation, Environmental Stewardship, and Water Supply.
· Extensive technical knowledge of programming in a software stack that includes an Oracle Relational database, SQL, PL/SQL, Oracle Spatial, JavaScript, Oracle REST Data Services (ORDS), and Oracle APEX to make the necessary revisions to the relevant systems. In addition to these general skills and experience, also possess the following:
· Knowledge and experience in developing and maintaining a relational database operated in Amazon Web Services (AWS cloud).
· Knowledge and experience of data entry and options to increase automation and efficiency.
· Knowledge and experience with documenting data systems and processes and creating reports for increased efficiencies.
· Knowledge and experience in optimizing the performance of existing Oracle based applications, including procedures, functions, etc.
· Knowledge and experience with creating Representational State Transfer (RESTful) services utilizing common data dissemination formats.
· Knowledge and experience with making websites 508 compliant.
· Knowledge and experience with authentication and authorization procedures
· Knowledge and experience with maintaining geospatial data in oracle relational database.
· Knowledge and experience with geospatial web services (OGC & ESRI REST)
· Knowledge and experience with cloud native development methodologies
Job Types: Full-time, Contract
Pay: $110,000.00 - $120,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Professional development assistance
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
11+ years
8 years
9 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Herndon, VA 20170: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Preferred)
Experience:
Oracle Apex: 5 years (Preferred)
AWS CLOUD: 5 years (Preferred)
Erwin: 8 years (Preferred)
Data modeling: 8 years (Preferred)
Metadata: 5 years (Preferred)
Work Location: Remote","$115,000 /yr (est.)",10000+ Employees,Company - Public,Manufacturing,Machinery Manufacturing,2011,$1 to $5 billion (USD)
"Maven LLC
4.3",4.3,Remote,Senior Data Engineer,"Description:
Please apply to job with this link: https://portal.dynamicsats.com/Application/WebForm/3e11b0af-f443-4cfb-ac80-066fe882c27b?portal=17b1dd1f-7af3-4dae-96aa-8723901631fa&job=f5ac005e-44e8-ed11-a7c6-002248081803
All About Us
Our client is a family community of banking teams that deliver an exceptional depth of resources to individuals and businesses, with a hands-on, personalized approach. Our client offers great benefits including health insurance for you and your whole family, 11 paid holidays, PTO, and tuition reimbursement to name a few.
About the Role
You'll be responsible for building, managing, and optimizing data pipelines and effectively converting into production for key data and analytics consumers. The Senior Data Engineer is also collaborating with Data Engineers, Data Architects, and Data Scientists across the enterprise, as well as Enterprise Architecture, to follow the standardization of data engineering integration architectures.
Job Essentials
Design, create and maintain projects involving data collection and storage systems
Recommend and deploy data models and solutions for existing data systems
Collaborate across teams on the design and maintenance of our client's Operations data mart (ETL, data modeling, metric design, metric design, reporting/dashboarding) to assure a stable reporting infrastructure
Work with data architects and data analysts to determine design needs
Mentor and provide guidance to junior engineers and new team members
Ensure data users and consumers utilize data responsibly through data governance and compliance initiatives
Perform data validation testing to ensure accurate data flows
Promote data and analytics capabilities and awareness to business unit leaders in order to leverage opportunities to achieve strategic business initiatives
Define coding standards, including Object Oriented design standards and design patterns
Implement a standardized library of reusable objects across the enterprise, such as the core banking platform
Maintain awareness of and adherence to Bank’s compliance requirements and risk management concepts, expectations, policies and procedures and apply them to daily tasks
Deliver a consistent, high level of service within our client's standards
Other duties as assigned
Requirements:
Bachelor's degree in computer science, information technology, or related field
Technical Expertise with SQL, PL SQL
Previous knowledge and experience integrating Informatica ETL
Experience with Tableau, Cognos, SSRS or equivalent BI tool
Previous experience using SSIS, Informatica or equivalent ETL
Skilled in Oracle, Snowflake, and SQL servers
Experience in AWS and Azure
Interpersonal/Customer Service Skills
Written and Verbal Communication. Ability to work as part of a team
Adaptable to change. Able to Multi-Task or Juggle Priorities
Analytical Thinking and Problem/Situation Analysis
Experience with Master Data Management (MDM) is preferred
An Equal Opportunity Employer
We do not discriminate based on race, color, religion, national origin, sex, age, disability, genetic information, or any other status protected by law or regulation. It is our intention that all qualified applicants are given equal opportunity and that selection decisions be based on job-related factors.",#N/A,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
TEKletics,#N/A,"Scottsdale, AZ",Azure Data Engineer,"Azure Data Engineer
Tekletics is an information technology company focused on providing quality resources to support our customers needs. Successful candidates will work with world class organizations to deliver projects. Each candidate is required to have a strong work ethic and the ability to handle high pressure situations.
A little about this role:
As the Azure Data Engineer, you are primarily responsible for the collection and transformation of data across a multitude of data sources. This individual is also responsible for the optimization of the environment, structure, and processes associated with said data.
A day in the life:
Data Warehouse - As the Azure Data Engineer, you are responsible for the data warehouse design, development, testing, support, and configuration. You will review business requests for data warehouse data and data warehouse usage. You will also research data sources for new and better data feeds ensuring consistency and integration with existing warehouse structure.
Data Collection – You will be responsible for developing automated data pipelines and/or data integrations within the Azure Synapse environment. You will use SQL, Python scripts and Azure Functions to automate data collection from a wide variety of sources.
o API utilization – ability to leverage REST APIs as needed.
Data Transformation – You will create BI (Business Intelligence) and Data Warehousing cube design. You will create and manage ETL/ELT processes to transform and load data into data warehouse for reporting and analytics.
Data Optimization – As the Azure Data Engineer, you will create and maintain standards and policies. You will identify, design, and implement internal process improvements, including automation of manual processes and optimization of data delivery. You will continuously improve data reliability, efficiency, and quality.
Training – Identify and demonstrate techniques to optimize reporting for Data Visualization Analyst, provide and participate in internal and external training sessions, and produce documentation to support understanding and learnings around the Presence data/reporting environment.
This job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Projects and responsibilities may change at any time with or without notice due to our business, industry, and/or market changes.
What we are looking for:
Previous experience using SQL, Python scripts and Azure Functions
Experienced in Azure Synapse environment
Experience designing, building, and maintaining data processing system
Dependable, extroverted, diplomatic person, able to problem-solve successfully with a wide variety of people and issues
Attention to detail and strong organizational skills, self-motivated
Ability to work independently while being a strong team player
Ability to mentor junior level developers
Passion for innovation and “can do” attitude to thrive in a fast-paced environment
Proficient in time management and adhering to deadlines
Knowledge and interest of the natural products/brands and retail landscape is a plus
Proficient computer (MS Office applications) and data-mining skills
Flexibility to successfully multi-task in a fast-paced environment with a positive attitude
Regular and predictable attendance is required
Ability to manage time and deadlines
Job Type: Full-time
Pay: $79,947.00 - $142,509.31 per year
Experience level:
3 years
Schedule:
8 hour shift
Ability to commute/relocate:
Scottsdale, AZ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure: 3 years (Required)
SQL: 3 years (Required)
Data warehouse: 3 years (Required)
Work Location: In person","$111,228 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Colorado Community Managed Care Network
3.2",3.2,"Denver, CO",Data Engineer,"CCMCN focuses on improving the lives of Coloradoans through innovative solutions. Our areas of focus include Population Health, Care Coordination, Social Health Information Exchange, Interoperability, and Clinical Quality Improvement. Through working with health centers and community partners, CCMCN provides best-in-class solutions that create a more comprehensive and collaborative network of care for our most needed Coloradoans.
WHY
Work for CCMCN
At CCMCN, we strive to support our employees and their growth within their career life-cycle. We are committed to a diverse and inclusive work environment where new ideas and innovation are not only invited, but celebrated. We value collaboration, innovation, adaptability, and a great work-life balance.
At CCMCN, you will see the impact of your work directly in the improvements that we make with our partners and their communities.
Our Benefits:
Remote work (depending on position) with access to CCMCN's physical office
Friday flextime and volunteer time off
Health, dental, and vision insurance plans
HSA, FSA, DCA, and employer-sponsored HRA
Life, AD&D, and long-term disability insurance plans
401K retirement plan with employer match
Employee Assistance Program (EAP)
Paid leave including vacation, sick, holidays, & 2 floating holidays

Category: Full-time, Exempt
Salary: $70,000 - $85,000 (DOE)
Reports To: Data Engineering Director
At CCMCN, our mission is to provide services that enable our members and their community partners to succeed as efficient, effective, and accountable systems of care.
CCMCN’s vision is that all Coloradans have access to high-quality, integrated, accountable health care. Areas of focus include population health, accountable care, shared services, health information technology, and clinical quality improvement programming. CCMCN is governed by a Board of Directors comprised of organizational representatives from each of its health center members as well as representation from Colorado Community Health Network (CCHN) and clinician representatives. Through working with health centers and community partners, CCMCN provides technological and analytical tools that help create a more comprehensive and collaborative network of care for Coloradans.
Position Description:
The Data Operations department provides data management, integration, and reporting services for multiple external and internal consumers. The Data Engineer will be responsible for all aspects of data management that support CCMCN’s production services. Candidate must have experience in Microsoft SQL database development, data integration, ETL (extract, transform, and load) tools and methods, analytics, reporting, and documentation.
Essential Functions:
Provide development and maintenance support for data integrations between systems.
Provide development and maintenance support for the EDW and supporting databases.
Provide automated solutions whenever possible and proactively suggest alternative solutions.
Assist in the development of new databases and associated processes as necessary.
Provide data analytics report development for specific projects as needed.
Develop data validation reports and analysis where applicable.
Develop technical documentation of data integrations and processes.
Communicate and collaborate with other team members and clients to develop innovative data solutions.
Utilize up-to-date knowledge of database and data quality best practices to produce effective solutions.
Remain knowledgeable in healthcare data standards, measures, and code sets as well as applicable data privacy practices and legal requirements.
Required Skills and Experience:
Provide development and maintenance support for data integrations between systems.
Provide development and maintenance support for the EDW and supporting databases.
Provide automated solutions whenever possible and proactively suggest alternative solutions.
Assist in the development of new databases and associated processes as necessary.
Provide data analytics report development for specific projects as needed.
Develop data validation reports and analysis where applicable.
Develop technical documentation of data integrations and processes.
Communicate and collaborate with other team members and clients to develop innovative data solutions.
Utilize up-to-date knowledge of database and data quality best practices to produce effective solutions.
Remain knowledgeable in healthcare data standards, measures, and code sets as well as applicable data privacy practices and legal requirements.
Additional Preferred Qualifications:
Provide development and maintenance support for data integrations between systems.
Provide development and maintenance support for the EDW and supporting databases.
Provide automated solutions whenever possible and proactively suggest alternative solutions.
Assist in the development of new databases and associated processes as necessary.
Provide data analytics report development for specific projects as needed.
Develop data validation reports and analysis where applicable.
Develop technical documentation of data integrations and processes.
Communicate and collaborate with other team members and clients to develop innovative data solutions.
Utilize up-to-date knowledge of database and data quality best practices to produce effective solutions.
Remain knowledgeable in healthcare data standards, measures, and code sets as well as applicable data privacy practices and legal requirements.","$77,500 /yr (est.)",1 to 50 Employees,Nonprofit Organization,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Fathom Management LLC
2.0",2.0,Remote,Sr. Data Engineer Remote Opportunity,"Sr. Data Engineer

seeking a Senior Data Engineer who possesses expert level knowledge of appropriate data sources to address the specific requirements of projects for data modeling. Understand business requirements and translating into technical work. Design and implement features in collaboration with team engineers, product owners, data analysts, business partners using Agile/SCRUM Methodology.

This is a full- time position / 100% Remote.
The salary range of $140,000 - $160,000 will be based on technical experience and technical interview.

Responsibilities:

Ability to build programs or systems that can take data and turn it into meaningful information that can be studied.
Build ETL/ELT jobs and workflows to combine data from disparate sources.
Install continuous pipelines of huge pools of filtered information so that data analyst/scientists can pull relevant data sets for their analyses.
Build data workflows using SQL Server Integration Services (SSIS)
Build data workflows using Microsoft Azure (Azure Data Factory, Storage Accounts, Synapse)
Build data workflows using Databricks
Collaborate with business stakeholders, business operations, and product engineering teams in analyzing business problems, in building and testing solutions and data models.
Experience implementing and operating analytic models and services.
Document the current-state and target-state software architecture and create roadmap plans for success on various software components.
Assist in the design, implementation, and maintenance of complex solutions.
Build systems that collect, manage, and convert raw data into usable information for business analysts to interpret.
Make data accessible for evaluation and optimization
Collaborate with business stakeholders, business operations, and product engineering teams.
Coordinate activities with other technical personnel as appropriate.
Works with back-end data and develops tables using SQL scripts, SSIS, and SSMS.
Experience with Azure cloud platforms and Data bricks

Required Experience and Education:
Master's degree in computer science, systems engineering, or related technical discipline is preferred with 7-10 years of experience as a Data Engineer/Administrator or similar role. OR , B.S. in Computer Science with 15 years of relevant experience.

Benefits Overview: Full-time employees are offered comprehensive and competitive benefits package including paid vacation, sick leave, holidays, health insurance, life insurance, military leave, training, tuition reimbursement, a wellness program, short- and long-term disability, 401(k) retirement plan with company matches/immediate vesting, commuter benefits, and more.
COVID Policy: In accordance with the Federal Executive Order on Ensuring Adequate COVID Safety Protocols for Federal Contractors, this position requires that you are fully vaccinated at least 2 weeks before your start date. You will be required to provide proof of vaccination before you begin employment.
EEO Policy: It is our policy to promote equal employment opportunities. All personnel decisions, including, but not limited to, recruiting, hiring, training, promotion, compensation, benefits, and termination, are made without regard to race, creed, color, religion, national origin, sex, age, marital status, sexual orientation, gender identity, citizenship status, veteran status, disability, or any other characteristic protected by applicable federal, state or local law.","$150,000 /yr (est.)",1 to 50 Employees,Self-employed,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"DocuSign
3.7",3.7,"San Francisco, CA",Data Engineer,"Company Overview

DocuSign helps organizations connect and automate how they agree. Our flagship product, eSignature, is the world’s #1 way to sign electronically on practically any device, from virtually anywhere, at any time. Today, more than a million customers and a billion users in over 180 countries use DocuSign to accelerate the process of doing business and simplify people’s lives.


What you'll do

DocuSign is seeking a talented and results-oriented Data Engineer to focus on delivering trusted data to the business. The Data Engineer delivers data for analytics using our Enterprise Data Warehouse, enabling the global DocuSign analytics community via curated, governed and cleansed data. As a member of the Global Data and Analytics team, the Data Engineer leverages a variety of technologies to accomplish this goal, ranging tools like Airflow, Matillion, dbt, Snowflake and Fivetran to languages like SQL and Python. The successful candidate will develop solutions with innovative cloud technologies, work on a variety of fast-paced assignments, and partner with world-class technical and business teams to maximize the value of data.

This position is an individual contributor role reporting to the Manager, Data Analytics.

Responsibility
Build data pipelines using Fivetran, dbt/Matillion, Snowflake and Airflow
Develop and maintain data documentation including ERD, data dictionaries, data lineage, and metadata
Ensure data quality and integrity by implementing appropriate data validation and cleansing techniques
Monitor and troubleshoot data pipelines to identify and resolve issues in a timely manner
Build POCs to validate new concepts and new technologies
Collaborate with business, engineering, and data science teams to understand their data needs and design efficient solutions to support their requirements


Job Designation

Hybrid:
Employee divides their time between in-office and remote work. Access to an office location is required. (Frequency: Minimum 2 days per week; may vary by team but will be weekly in-office expectation)


What you bring

Basic
Bachelor’s Degree in Computer Science, Data Analytics, Information Systems or similar, or equivalent work experience
5+ years of relevant experience
2+ years of dimensional and relational data modeling experience
Experience with modern data integration and transformation tools such as Fivetran, Dbt, and Matillion
Experience with workflow orchestration tools such as Airflow
Experience with MPP databases like Snowflake, Redshift and BigQuery
Experience with cloud platforms like AWS, Azure and GCP
Experience with versioning tools like git
Experience working with tools like Jira and Confluence
Experience with SQL and Python
Experience with document and data debugging

Preferred
Good knowledge of database and data warehouse concepts such as facts and dimensions to design and develop data models that support enterprise reporting and analytics needs
Ability to work independently with minimal supervision, as well as in a team environment
Excellent communication skills
Eye for detail, good data intuition, and a passion for data quality
Comfortable working in a rapidly changing environment with ambiguous requirements
Organizational and time management skills, with the ability to prioritize tasks and meet deadlines


Wage Transparency

Based on applicable legislation, the below details pay ranges in the following locations:

California: $118,300 - $182,775 base salary

Washington: $111,600 - $162,625 base salary

This role is also eligible for bonus, equity and
benefits.


Life at DocuSign

Working here
DocuSign is committed to building trust and making the world more agreeable for our employees, customers and the communities in which we live and work. You can count on us to listen, be honest, and try our best to do what’s right, every day. At DocuSign, everything is equal.

We each have a responsibility to ensure every team member has an equal opportunity to succeed, to be heard, to exchange ideas openly, to build lasting relationships, and to do the work of their life. Best of all, you will be able to feel deep pride in the work you do, because your contribution helps us make the world better than we found it. And for that, you’ll be loved by us, our customers, and the world in which we live.

Accommodation
DocuSign provides reasonable accommodations for qualified individuals with disabilities in job application procedures. If you need such an accommodation, including if you need accommodation to properly utilize our online system, you may contact us at accommodations@docusign.com.

If you experience any technical difficulties or issues during the application process, or with our interview tools, please reach out to us at taops@docusign.com for assistance.

Our global benefits
Paid time off
Take time to unwind with earned days off, plus paid company holidays based on your region.
Paid parental leave
Take up to six months off with your child after birth, adoption or foster care placement.
Full health benefits
Options for 100% employer-paid health plans from day one of employment.
Retirement plans
Select retirement and pension programs with potential for employer contributions.
Learning & development
Grow your career with coaching, online courses and education reimbursements.
Compassionate care leave
Paid time off following the loss of a loved one and other life-changing events.","$137,113 /yr (est.)",5001 to 10000 Employees,Company - Public,Information Technology,Information Technology Support Services,2003,$1 to $5 billion (USD)
"Spencer Gifts & Spirit Halloween
3.6",3.6,"Egg Harbor Township, NJ",Azure Data Engineer,"We are currently looking for a Data Engineer to optimize our data integration at scale. Inside our Data Integration team, you will be designing pipelines and warehouses to model data from multiple sources that will allow us to derive business insight. Using Azure and other open-source technologies, such as Azure Data Factory and PySpark, you will design and build our next-generation ETL pipelines and data models.
Build data pipelines and python-based ETL tools for acquiring, processing, and delivering data
Develop data models and schemas in our data warehouse that enable performant, intuitive analysis
Handle the challenges that come with managing terabytes of data
Collaborate with business leaders and analysts to define key metrics and build reporting to monitor and understand company performance
Develop the server applications and APIs that are used by our Data Team
Responsibilities:
Build data pipelines and python-based ETL tools for acquiring, processing, and delivering data
Develop data models and schemas in our data warehouse that enable performant, intuitive analysis
Handle the challenges that come with managing terabytes of data
Collaborate with business leaders and analysts to define key metrics and build reporting to monitor and understand company performance
Develop the server applications and APIs that are used by our Data Team
Requirements:
Bachelor’s degree or higher in Computer Science, Computer Engineering, Information Technology or related field
Fluent in several programming languages such as Python, R, or Scala
6+ years of work experience in building ETL pipelines in production data processing and analysis
Experience designing SQL tables, choosing indexes, tuning queries, and optimizations across different functional environments.
Hands-on experience writing complex SQL queries and using a BI tool
Experience with data lakes and designing and maintaining data solutions using Spark and Azure serverless services such as ADF
Familiarity with data ingestion APIs, data sharing technologies, and warehouse infrastructure and development
* Proof of vaccination for COVID-19 required for employment, reasonable accommodations considered for medical, pregnancy or sincerely held religious beliefs.
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Experience level:
6 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Egg Harbor Township, NJ: Reliably commute or willing to relocate with an employer-provided relocation package (Required)
Work Location: In person",#N/A,5001 to 10000 Employees,Company - Private,Retail & Wholesale,"Gift, Novelty & Souvenir Stores",1947,$100 to $500 million (USD)
"Opensignal
4.5",4.5,"Boston, MA",Senior Data Engineer,"Department: Product/Technology
Location: Boston / US East Coast or Canada

Purpose of Role

We’re looking for a Senior Data Engineer to join our Marketing Performance Group in
transforming our real-world raw data into valuable and credible industry-leading metrics that
provide insights to our analysts and our customers.

What you will be doing
The creation and implementation of a framework to assist in building complex statistical
models. Working closely with our data scientists and our data engineers to create and
evolve products that measure market dynamics in the Telecommunication space that drive
our customers short-term marketing campaign tactics and their longer-term customer
acquisition and retention strategies. This role reports to our Engineering Manager.

We expect our Lead Data Engineer to do:
Own and improve our data pipeline. Assemble large, complex data sets that
meet business requirements, with engineering best practices in mind.
Champion building scalable and resilient data infrastructure, as well as tools to
extract and transform data used by stakeholders and customers.
Be security conscious and sensitive to privacy concerns and legislation related
to the data within the platform.
Have a continuous improvement mindset when it comes to both the platform
and the process.
Work efficiently, automate manual processes where possible, and take a test-
driven approach to engineering.
Take a keen interest in improving the platform’s scalability while understanding
the cost.
Be a good team player, with an agile approach and a can-do attitude.
Keep yourself current and make sure we follow best practices and engineering
standards.
Be an advocate for the platform and its health. Take ownership of your work
from conception through to support.
Be detail orientated and understand the importance of the credibility of our
metrics. Document and communicate with stakeholders in a language
understood by all.
Can work in a fast-paced environment with an ability to shift priorities and focus
on changing requirements and market demands.
Able to coach and mentor Data Engineers in best practices.
Cross-collaborate with the wider team to drive and maintain high standards in
our data pipeline builds.
Comfortable and effective at working in a remote capacity, collaborating with
team members across different locations through digital channels.

What we need from you:
As a Senior Data Engineer, we would expect you to have previous experience in
manipulating, processing, storing, and extracting value from big data.
Advanced with hands-on experience in architecting, crafting, documenting, and
developing highly scalable distributed data processing systems.
Advanced with big data tools, specifically Apache Spark.
Advanced with relational SQL databases. Prior experience with MSSQL,
Postgres, AWS Athena (Presto).
Advanced with SQL query authoring including DBT.
Experience with data pipeline / workflow tools i.e. Apache Airflow.
Experience with AWS cloud services like EC2, S3, managed Kubernetes, AWS
ECS, and Aurora.
Experience with object-oriented/object function scripting languages: Python and
Scala.
Experience in implementing complex clustering and classification models on
large datasets to support new product development.
Experience in writing tests, especially in BDD style and working with Git.
Strong analytic skills in working with unstructured datasets.
Experience in root cause analysis of data when asked to answer specific
business questions.
Experience building and optimizing & ""big data"" data pipelines.
Experience supporting and working with cross-functional teams.
Strong self-organizational skills
Experience being part of a cross-functional team, using agile methodologies.
Bachelor’s degree

For US applicants only

At this time, the company will not sponsor a new applicant for employment sponsorship for this position.
This employer participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S.

About Us
Opensignal is the leading global provider of independent insight and data into network
experience and market performance. Our user-centric approach allows communication
providers to constantly improve their network and maximize commercial performance.
Leading analysts, investors and financial institutions place a high value on our independent
analysis and we are regular contributors to their reports.
Real network experience is our focus and ultimately that’s what influences customer choice.
Our mission is to advance connectivity for all and here at Opensignal, the team is leading
the industry in enabling operators to link their network experience and market performance
in a way that has never before been possible.
With offices in London, Boston and Victoria, British Columbia, we are truly global, with
employees working across four continents and representing over 25 nationalities. We are
an equal opportunity employer dedicated to building an inclusive and diverse workforce.

Benefits:
We believe we are stronger when we not only celebrate our many differences, values, and
voices but include them in everyday practice. Having a diverse and inclusive culture is
essential, which is why we offer a flexible approach to work-life balance, operating in a
remote-hybrid way. We’ll help you get set up with the essentials you need to work from
home or the office. We also offer an attractive range of additional benefits, including:
Competitive compensation packages including a long-term equity program.
Comprehensive group benefits package and company-sponsored retirement
savings plan (details depend on your country of work).
Professional development opportunities: education reimbursement, learning
allowance, company-sponsored workshops, and more!
Generous holiday allowance, sick leave, parental leave, flexibility including Flex
Fridays, and the opportunity to work from abroad.
Charity matching and time off for community volunteering and DE&I
program/committees.
Regular virtual and in-person events and socials.","$148,405 /yr (est.)",51 to 200 Employees,Company - Private,Management & Consulting,Business Consulting,2010,Unknown / Non-Applicable
"Shutterfly
3.3",3.3,"Eden Prairie, MN",Senior Data Engineer,"At Shutterfly, we make life’s experiences unforgettable. We believe there is extraordinary power in the self-expression. That’s why our family of brands helps customers create products and capture moments that reflect who they uniquely are.

As part of the Data Engineering team you will tackle the scalability, performance and distributed computing challenges needed to collect, process and store data for Shutterfly. You will enhance the Data Warehouse at Shutterfly on AWS, supporting the breadth and depth of the company’s analytic needs.
What You'll Do as a Sr. Data Engineer II
Build design, develop, test, deploy, maintain and enhance full-stack data engineering solutions
Provide technical leadership to the Data Warehouse and Shutterfly teams who use Shutterfly’s Enterprise Data Warehouse
With your technical expertise own and manage project priorities, deadlines and deliverables
Evangelize the benefits of existing solutions and new technologies to drive the use and push the technology of the Data Warehouse forward
Work closely with Data Operations to improve CI/CD pipelines am improve the operation and performance of the Data Warehouse
The Skills You'll Bring
Expert knowledge Python, Spark, SQL scripting and Java
7+ years of hands-on experience with building data ingress frameworks and experience with dealing with a wide variety of source data formats
Hands-on experience in building data applications, including design, implementation, debugging, and support.
Supporting a diverse and inclusive workforce is important to Shutterfly not only because it directly reflects our value of Embracing our Differences, but also because it’s the right thing to do for our business and for our people. Learn more about our commitment to Diversity, Equity and Inclusion at Shutterfly DE&I.","$132,354 /yr (est.)",10000+ Employees,Company - Private,Information Technology,Internet & Web Services,1999,$1 to $5 billion (USD)
"Ikigai Labs, Inc.
4.9",4.9,"Cambridge, MA","Software Engineer, Data Engineering","Ikigai Labs is a fast growing startup founded out of MIT to empower data operators. We are building an easy to use AI augmented data processing and analytics platform on the cloud. Our users depend on us to automate, maintain, and enhance day-to-day mission critical operations. We are a team of talented, hardworking and fun-loving engineers, data scientists, and data analysts working towards the goal of building the next generation of data tools.
Job Description
JOB TITLE: Software Engineer, Data Engineer [Full-time]
LOCATION: Cambridge, MA
SUMMARY:
Ikigai Labs is seeking a dynamic and passionate engineer with strong software fundamentals to join a high-performing data platform development team. We are looking for a team player who is a quick learner, performs in a rapid development cycle, has a drive to surpass expectations, and an eagerness to share their work and knowledge.
We encourage applicants from all backgrounds and communities. We are committed to having a team that is made up of diverse skills, experiences, and abilities.
Technologies
Languages: Python3, SQL
Databases: Postgres, Elasticsearch, DynamoDB, RDS
Cloud: Kubernetes, Helm, EKS, Terraform, AWS
Data Engineering: Apache Arrow, Dremio, Ray
Misc.: Apache Superset, Plotly Dash, Metabase, Jupyterhub, Stripe, Fivetran
The Position
Design and develop scalable data integration (ETL/ELT) processes
Design and develop an on-demand predictive modeling platform with gRPC
Utilize Kubernetes to orchestrate the deployment, scaling and management of Docker containers
Utilize and learn various AWS services to solve cloud-native problems
Implement a testing platform which performs sanity check, load test, scale test, heartbeat test, and performance test
Provide periodic support to our customer success team
Qualifications
0-3 years of experience with a bachelor's degree in Computer Science, Math, or Engineering; or a master's degree
Experience with Python, AWS services, and/or ETL/ELT pipeline experiences
Experience with Kubernetes and/or EKS (optional)
Understanding of the fundamentals of design patterns and testing best practices
The ability to learn quickly in a fast-paced environment
Excellent organizational, time management, and communication skills
The desire to work in an AGILE environment with a focus on pair programming
Willingness to discuss obstacles, find creative solutions, and take initiative
The ability to receive and give both constructive and encouragement feedback","$102,419 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Summit2Sea Consulting
5.0",5.0,"Tampa, FL",Data Engineer,"Summit2Sea Consulting, a cBEYONData company, is a technology consulting firm run by hands on technologists that combines people, process and technology to deliver innovative solutions to our clients. We have been named on The Washington Post's list of top work places for the past 3 years! We invest in our biggest asset - our people! You can be a part of a winning team that will contribute to your career growth.
Have you been looking to shift your career into high gear? This is your opportunity to take your ambitions and convert them into a solid career in a supportive and innovative environment!
Summit2Sea, a cBEYONData company is seeking a Data Engineer to support our federal customer's multi-domain technology platform which offers military and business decision makers, analysts, and users at all levels unprecedented access to authoritative enterprise data and structured analytics in a scalable, reliable, and secure environment.
Responsibilities:
Support data collection, ingestion, validation, and loading of optimized data in the appropriate data stores
Work on a team made up of analyst(s), developer(s), data scientist(s), and a product lead
Working directly with the analyst(s) and the product lead, the data engineer identifies and implements solutions for the data requirements, including building pipelines to collect data from disparate, external sources, implementing rules to validate that expected data is received, cleansed, transformed, massaged and in an optimized output format for the data store
Performs validation and analytics corresponding with client requirements and evolves solutions through automation, optimizing performance with minimal human involvement
As pipelines are executed, the data engineer monitors their status, performance, and troubleshoots issues while working on improvements to ensure the solution is the very best version to address the customer need
This role focuses specifically on the development and maintenance of scalable data stores that supply big data in forms needed for business analysis
Apply advanced consulting skills, extensive technical expertise and has full industry knowledge to develop innovative solutions to complex problems
Requirements:
Must Have:
4+ years of experience with SQL
4 years of experience working in a big data and cloud environment
4+ years of experience with a modern programming language such as Python or Java
4+ years of experience developing data pipelines using modern Big Data ETL technologies like NiFi or StreamSets.
Active Secret Clearance or higher
Nice to Have:
2 years of experience working in an agile development environment
Possession of excellent verbal and written communication skills
Preferred experience at the respective command with an understanding of analytical and data paint points and challenges across the J-Codes.
Ability to display a positive, can-do attitude to solve the challenges of tomorrow
Ability to quickly learn technical concepts and communicate with multiple functional groups
Summit2Sea Consulting, a cBEYONData company, is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.
Equal Employment Opportunity:
All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.",#N/A,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2003,$5 to $25 million (USD)
The Data Sherpas,#N/A,Remote,Data Engineer - LATAM,"Who We Are:
Working at The Data Sherpas is like being part of a dynamic and collaborative team of talented individuals passionate about helping clients navigate the ever-changing information technology landscape. At The Data Sherpas, you can work with cutting-edge tools and technologies, tackle challenging data problems, and continuously develop your skills in a supportive environment. As a Data Sherpa, you'll be empowered to lead projects, take ownership of your work, and make meaningful contributions to our client's success.
What We Are Looking For:
We're seeking an ambitious and driven Data Engineer to join our team. As a Data Engineer, you can work on exciting, challenging projects supporting our client's needs. You will be part of a dynamic team of experts dedicated to improving business performance and driving data-driven results.
What You'll Do:
Design and implement data pipelines, ETL processes, and data warehousing solutions
Develop and maintain attribution and measurement models for ad campaigns
Perform data matching and segmentation techniques to create customer profiles and behavior patterns
Configure and maintain AWS infrastructure and services to support data engineering processes
Collaborate with cross-functional teams to identify data needs and develop data-driven solutions.
Stay up-to-date with the latest technologies and industry trends related to Data Engineering and Ad Tech.
What You Have:
Proficiency in data modeling, ETL development, and data warehousing
Ability to design and maintain data pipelines for large-scale data sets
Proven experience with Python programming language for data engineering solutions
Knowledge of data matching techniques for identifying duplicate data and inconsistencies
Experience with data deduplication techniques
Ability to work with large datasets to segment and group data
Experience with clustering algorithms and methodologies
Understanding of customer segmentation and persona development
Familiarity with data visualization tools to showcase segmentation results
Strong experience with data engineering in cloud-based environments, primarily in AWS
Experience using AWS data analytics services like EMR, Redshift, Kinesis, and Glue",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"RelMap Consulting
4.8",4.8,"Addison, TX",Senior Data Engineer (Hybrid),"The data engineer entails:
Advanced SQL Server Development
Ability to create and modify views
Ability to create and modify custom functions
Ability to write complex queries, including joins, temp table processing, and common table expressions (CTE’s)
Ability to create and modify stored procedures using TSQL
Ability to analyze and optimize query performance using indices
Ability to use Red Gate’s SQL Compare to perform a schema (AKA “do a manual roll”) between one server an another, and between revision control and a database
Ability to stand-up new database server VM’s in Linux and Windows using VMWare
Ability to backup and restore a database in a Linux and Windows environment
Ability to manage and script automated database jobs
Ability to use PowerShell / Bash to interact with databases from the command line
Addition skills that would be desired but not required
· Experience with Redis
· Experience with MongoDB / CosmosDB
· Experience with Azure and Azure SQL
· C#, JavaScript (NodeJS / ES6), or Python experience
Job Type: Contract
Pay: $70.00 - $80.00 per hour
Compensation package:
Hourly pay
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Addison, TX: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person",$75.00 /hr (est.),1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2010,$1 to $5 million (USD)
"Seamless.AI
3.4",3.4,"Columbus, OH",Senior Data Engineer - Remote US,"Role: Senior Data Engineer
The Opportunity
We are seeking a Senior Data Engineer with a strong background in data ingestion and processing for large data sets. The ideal candidate is maniacal about data accuracy and has a proven track record of building robust and scalable data pipelines. This role requires an experienced professional who can work collaboratively with cross-functional teams to design and implement solutions that meet business requirements and adhere to data security and privacy policies.
If you are a senior data engineer with a passion for data accuracy and a proven track record of building robust and scalable data pipelines, we encourage you to apply for this role. We offer competitive compensation, a flexible work environment, and opportunities for growth and advancement within the organization.
About Seamless
Seamless delivers the world's best sales leads. Through our product, we help sales teams maximize revenue, increase sales, and easily acquire their total addressable market using artificial intelligence; by development of a robust real-time contact and company search engine as well as a suite of technically-advanced tools to support sales and lead generation. We have been recognized as one of Ohio's fastest growing companies and have been recently ranked No. 7 in LinkedIn's Top 50 Startups of 2022, featured in Forbes as #1 Software company in Ohio in 2022, and on G2's ""Top 100 Highest Satisfaction Products for 2022"" list!
The Seamless Team
We have an amazing culture and work environment that anyone would want to be a part of. We encourage a culture of positivity. We thrive off of continuous feedback and do whatever it takes to help our team and customers be successful. You will grow as an individual, professionally, and be able to see and feel the impact you are making to the growth of Seamless every day.
Role Responsibilities
Design and develop data ingestion and processing pipelines that ensure data accuracy and completeness.
Work closely with cross-functional teams to understand business requirements and design solutions that meet those requirements.
Lead efforts to identify and implement data quality control measures to ensure consistency and reliability of data.
Monitor data pipelines to identify issues and proactively resolve them.
Evaluate and recommend new technologies and techniques to improve data pipeline performance and scalability.
Ensure compliance with data security and privacy policies and procedures.
Mentor junior engineers and provide technical leadership in data engineering best practices.

Candidate Requirements
Bachelor's or Master's degree in Computer Science, Software Engineering, or a related field.
3+ years of experience in data engineering, with a focus on data ingestion and accuracy.
Strong experience with large-scale data processing frameworks such as Hadoop, Spark, or Flink.
Proficiency in programming languages such as Python, Java, or Scala.
Experience with SQL and NoSQL databases.
Familiarity with cloud-based data processing and storage platforms such as AWS, Google Cloud, or Azure.
Strong experience in designing and implementing data quality control measures.
Excellent problem-solving and analytical skills.
Strong communication and collaboration skills.
Experience mentoring and leading teams of data engineers.
Seamless.AI has been delivering the world's best sales leads since 2015. Our product is the first real time, B2B search engine helping sales teams maximize revenue, increase sales, and easily acquire their total addressable market using artificial intelligence. We have been recognized as one of Ohio's fastest growing companies and won 2020 Best Places to Work and LinkedIn's Top 50 Tech Startups in 2020 and 2022. We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. Visa Sponsorship is not included in our hiring package. Applicants will need to be authorized to work in the U.S.","$93,575 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable
"DSFederal Inc
4.0",4.0,Remote,Data Engineer*,"Description:
We are seeking an experienced Data Engineer to design, build, and maintain our government client’s data infrastructure. The Data Engineer will work closely with cross-functional teams to develop scalable data solutions that support our client’s business needs.
Requirements:
Design, develop, and maintain data pipelines and data storage systems.
Collaborate with cross-functional teams to design and implement data-driven solutions.
Optimize and tune data systems for performance and scalability.
Implement and maintain data quality and validation processes.
Troubleshoot and debug data-related issues.
Participate in code reviews and contribute to best practices for data engineering.
Technical Skills Required:
Proficiency in one or more programming languages (Python, Java, etc.)
Experience with data modeling, database design, data marts, and data warehousing concepts
Knowledge of ETL tools and techniques
Experience with cloud-based data platforms such as AWS or Azure
Strong problem-solving and analytical skills
Desired Skills:
Familiarity with data visualization tools such as Tableau or Power BI.
Knowledge of machine learning concepts and frameworks.
Experience Required:
3+ years of experience in data engineering or related field
Education Required:
Bachelor's in engineering

Who We Are:
DSFederal is a leader in health IT and data analytics that delivers innovative solutions to improve healthcare delivery and address critical challenges, including cancer surveillance, HIV/AIDS, maternal and child health, and COVID-19 emergency response. Our highly experienced professionals serve over 50 different federal clients on over 80 projects. Our people are united by one mission – to improve human life through transformative solutions. www.dsfederal.com.
What We Can Offer You:
DSFederal attracts the best people in the business with our competitive benefits package that includes medical, dental and vision coverage, 401k plan with employer contribution, paid holidays, vacation, and more. If you enjoy being a part of a high performing, professional services, and technology focused organization, we encourage you to apply!
Our Mission: Improve human life through transformative solutions.
Our Core Values: Integrity, People First, Disciplined Business Practices, Customer Centric, Value Creation.
EEO Statement: DS Federal is an Equal Opportunity / Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, or protected veteran status.
E-Verify Statement
DS Federal utilizes the E-Verify program for every potential new hire. This makes it possible for us to confirm that every employee who works for DS Federal is eligible to work in the United States. To learn more about E-Verify you can call 1-800-255-7688 or visit their website by clicking the logo below. E-Verify® is a registered trademark of the United States Department of Homeland Security.
#LI-LP1",#N/A,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$25 to $100 million (USD)
Alter Tech Solutions,#N/A,"Jersey City, NJ",Senior Data Engineer,"Job Title: Data Engineer
Location: Jersey City, NJ
Job Type: Hybrid
Job Description:
Position is more about scale - application monitoring, automation framework, test automation - data engineering - data pipelines - rest api's
system design - experience with distributed systems, design the right components, push out to the AWS cloud, data structures/algorithms
Experience:
10+ years of experience cross functional experience - data engineering and sw engineering background (working with risk, audit teams) need hands on coding but also design
Qualifications & Requirements:
TOP SKILLS java preferred language but would consider strong python - Apache Flink, Kafka, Cassandra, GraphQL, SPARK, MACHINE LEARNING.
Job Types: Full-time, Permanent
Salary: $60.00 - $75.00 per hour
Experience:
Java: 8 years (Preferred)
GraphQL: 8 years (Preferred)
Python: 5 years (Preferred)
Work Location: On the road",$67.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"LatentView Analytics
4.0",4.0,"Dallas, TX",Data Engineer,"Role : Data Engineer
Experience : 6 - 8+ Years
Location : Dallas,Tx (Onsite)
Position: FullTime Only
Skills :Python,SQL Server ,Scala, Hadoop, HPCC, Storm, Cloudera, Cassandra,Excel, R,Docker,Kubernetes,Snowflake,Azure,Kafka,Redshift,Hadoop,AWS.
Job Type: Full-time
Pay: $80,000.00 - $1,100,000.00 per year
Compensation package:
Yearly pay
Experience level:
6 years
Schedule:
8 hour shift
Experience:
data engineer: 6 years (Preferred)
Work Location: In person
Speak with the employer
+91 9876543210","$109,012 /yr (est.)",1001 to 5000 Employees,Company - Public,Management & Consulting,Business Consulting,2006,$25 to $100 million (USD)
"Leadstack Inc
4.3",4.3,Remote,Lead Data Engineer with Sql/Python,"LeadStack Inc. is an award winning, one of the nation's fastest growing, certified minority owned (MBE) staffing services provider of contingent workforce. As a recognized industry leader in contingent workforce solutions and Certified as a Great Place to Work, we're proud to partner with some of the most admired Fortune 500 brands in the world.
Title: Lead Data Engineer with Sql/Python
Location: Remote
Duration: 6+months
Direct Client
Immediate interview
Top Skills: Azure, SQL/Python Developer.
Job Description:
Design and develop data system integrations and data models for reporting and alerting.
Essential Job Functions:
Drive digital innovation by leveraging innovative new technologies and approaches to renovate, extend, and transform the existing core data assets, including SQL-based, NoSQL-based, and Cloud-based data platforms
Define high-level migration plans to address the gaps between the current and future state
Present opportunities with cost/benefit analysis to leadership to shape sound architectural decisions
Lead the analysis of the technology environment to detect critical deficiencies and recommend solutions for improvement
Interpreting data, analyzing results using statistical techniques
Developing and implementing data analyses, data collection systems and other strategies that optimize statistical efficiency and quality
Acquiring data from primary or secondary data sources and maintaining databases
Promote the reuse of data assets, including the management of the data catalog for reference
Key Responsibilities:
Innovate, develop, and drive the development and communication of data strategy and roadmaps across the technology organization to support project portfolio and business strategy
Drive the development and communication of enterprise standards for data domains and data solutions, focusing on simplified integration and streamlined operational and analytical uses
Drive digital innovation by leveraging innovative new technologies and approaches to renovate, extend, and transform the existing core data assets, including SQL-based, NoSQL-based, and Cloud-based data platforms
Define high-level migration plans to address the gaps between the current and future state, typically in sync with the budgeting or other capital planning processes Lead the analysis of the technology environment to detect critical deficiencies and recommend solutions for improvement
Mentor team members in data principles, patterns, processes and practices
Promote the reuse of data assets, including the management of the data catalog for referenceDraft and review architectural diagrams, interface specifications and other design documents
To know more about current opportunities at LeadStack , please visit us on https://leadstackinc.com/careers/
Job Type: Contract
Salary: $70.00 - $80.00 per hour
Experience level:
11+ years
Schedule:
8 hour shift
Experience:
Azure: 5 years (Preferred)
Data warehouse: 1 year (Preferred)
SQL developer: 6 years (Preferred)
Work Location: Remote",$75.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2016,Unknown / Non-Applicable
"Transfr
4.5",4.5,"New York, NY",Sr Data Engineer,"Transfr creates job-training simulations in virtual reality designed by each industry that teaches novices in the same manner that experts master their crafts - through trial and error. Data-driven simulations work like virtual coaches, adapting to every person’s pace and skills while responding to their mistakes. Our immersive Experience Training method helps trainees build confidence in their knowledge, skills, and abilities.
We provide a better way to train young adults for the skills they need to succeed on the job. We focus on developing a pipeline of talent for jobs that are going unfilled, significantly reducing cost and risk.

Job Summary:

Transfr is looking for a highly motivated, self-driven Data Engineer to join the team. This individual will have the opportunity to work with researchers, engineers, and product designers across the company and engage in cutting-edge research to contribute to learning research and analytics.

This position can be 100% remote or work with our team from our headquarters in NYC on a flexible, hybrid schedule.

Responsibilities:
Instrument data solutions from multiple sources that enable fast, data-driven decision making on issues including useability, learning, product engagement, and quality.
Build the infrastructure required for optimal extraction, transformation, and loading of data from multiple data sources using SQL, PostgreSQL, and AWS technologies
Build real-time, scalable data solutions that support A/B product testing and support data and learning science through enabling visualizations and views into complex data sets
Create and maintain data pipeline architecture, configuration and implementation
Work with internal teams to assist with data-related technical issues and support their data infrastructure, access and visualization needs.
Support multiple teams with the creation of data strategies and data guidance documentation and architectures
Adhere to data privacy and security guidelines and regulations analysis
Build data expertise and own data quality for allocated areas
Create efficient processes for acquiring, extracting, integrating, transforming, and modeling data to derive useful information
Qualifications:
Minimum 4+ years of experience in a Data Engineer role
Strong preference for candidates with a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases including Postgres and MySQL.
Strong programming and analytics experience with Python and R
Experience with using Python to build ETL pipelines
Strong knowledge and hands-on experience building data lakes and architecting warehouses
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
Architect and implement CI/CD strategy for Data platform
A successful history of manipulating, processing and extracting value from large datasets
Working knowledge of message queuing, stream processing, and highly scalable big data data stores
Experience supporting and working with cross-functional teams in a dynamic environment

What We Offer:

The base salary range for this position is expected to be between $130K - $150K, with the actual base salary amount dependent on a number of factors, including but not limited to a candidate’s credentials, relevant experience, and primary job location. In addition to salary this role will be eligible for additional company benefits such as stock options, 401(k), paid vacation and sick time, and medical/dental/vision insurance.

In Closing:

We invite you to join us. Be a part of creating pathways to prosperity by helping to develop training simulations to teach skills that lead to well-paying jobs, for all.

At Transfr, we embrace diversity because it breeds innovation. Transfr is an equal opportunity employer that participates in E-Verify committed to providing equal employment opportunities to all applicants, consultants, and employees, and prohibits discrimination and harassment of any type without regard to race, color, religion, age, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.

**Must be authorized to work in the United States without restriction**

#LI-Remote

Learn more at https://transfrinc.com/","$125,505 /yr (est.)",51 to 200 Employees,Company - Private,Education,Education & Training Services,2018,$1 to $5 million (USD)
"NexTek, LLC",#N/A,"Shoreview, MN",Sr. Data Engineer,"Nextek, LLC is a software development company in the financial services industry that creates web applications and solutions for financial services professionals. We are seeking a motivated Senior Data Engineer to join the team.
What we provide:
A laptop and other equipment/software necessary for work
Health Insurance, Dental Insurance, Vision Insurance
Paid time off (PTO), Volunteer time off (VTO)
Hybrid remote work options
401K
Life Insurance
Flexible schedules
Monthly Social Activities
Casual dress code
Professional development
Here is what we are looking for:
Experience in database administration, security, backup, and recovery
Preferred Knowledge:4+ years of experience in data/software engineering
Experience in the financial services industry
Experience with a variety of relational databases (particularly MySQL & MSSQL)
Experience with collecting and presenting analytics
Here is what you will be doing:
Display proficiency in relevant technical skills.
Maintain a deep understanding of the data structure.
Provide evaluations and recommendations for our data environment based on potential issues, tradeoffs, and risks.
Scope and implement solutions.
Make good estimates and set realistic expectations for delivery.
Ask for guidance when necessary.
Maintain and monitor the health of databases and associated solutions.
Make use of the same environments setup by the Software Engineering team
Thoroughly test code for bugs and security issues before deploying to production environments
Maintain and monitor actively deployed solutions
Follow and help define best practices
Peer reviews
Maintain awareness of industry trends
Share experience and insights to help your team grow
Increase understanding of impact on other teams
Increase understanding of the customer
All other duties as assigned
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Shoreview, MN: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"CrossAmerica Partners
3.2",3.2,"Allentown, PA",Microsoft SQL Data Engineer,"We are looking for a talented Microsoft SQL Data Engineer to join our team. The successful candidate will be responsible for designing and implementing scalable, reliable, high-performance data solutions using Microsoft SQL Server. The ideal candidate should have a proven track record of delivering successful projects strong expertise in database design, data modeling, and data integration.
Key Accountabilities:
Design and develop data models, database schemas, and ETL processes using Microsoft SQL Server.
Build and maintain data warehouses and data lakes to support business intelligence and analytics
Collaborate with cross-functional teams to understand data requirements, and design solutions that meet business needs.
Develop efficient SQL queries, stored procedures, and triggers to support data access and analysis.
Perform data profiling and data analysis to identify data quality issues and develop strategies to resolve them.
Optimize database performance by tuning SQL queries, database indexes, and query execution plans.
Implement data security, access controls, and auditing mechanisms to protect sensitive data.
Ensure data quality and consistency by performing regular data validation and testing.
Develop and maintain technical documentation, including data dictionaries, database schemas, and ETL workflows.
Qualifications:
Bachelor's degree in Computer Science, Information Systems, or related field.
At least 5 years of experience in Microsoft SQL Server development, including database design, data modeling, and ETL.
Strong proficiency in T-SQL programming and SQL Server Integration Services (SSIS).
Experience with data warehousing, data integration, and data migration projects.
Working knowledge with Microsoft Azure Cloud Services is a plus.
Strong problem-solving skills and ability to work independently and as part of a team.
Excellent communication and interpersonal skills.
If you are passionate about data and have a strong technical background in Microsoft SQL Server, we encourage you to apply for this exciting opportunity.
We offer:
Competitive compensation
Health insurance (Medical, Dental, Vision)
STD/LTD, Life Insurance
401k with employer match
Vacation/Sick time
Paid holidays
& More!
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Life insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Day shift
Monday to Friday
Work Location: In person","$87,950 /yr (est.)",1001 to 5000 Employees,Company - Public,"Energy, Mining & Utilities",Energy & Utilities,#N/A,$1 to $5 billion (USD)
"Talent Group
2.3",2.3,"Austin, TX",data engineer with ML,"Required Skills :
A background in computer science, engineering, mathematics, or similar quantitative field with a minimum of 2 years professional experience .
Experience in implementing data pipelines using python.
Experience with workflow scheduling / orchestration such as Kubernetes, Airflow or Oozie.
Extract Transform Load (ETL) experience using Spark, Kafka, Hadoop, or similar technologies.
Experience with query APIs using JSON, Protocol Buffers, or XML.
Experience with Unix-based command line interface and Bash scripts
Job Type: Full-time
Salary: Up to $130,000.00 per year
Benefits:
401(k)
Experience level:
11+ years
Ability to commute/relocate:
Austin, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$130,000 /yr (est.)",1 to 50 Employees,Company - Public,Media & Communication,Broadcast Media,#N/A,Less than $1 million (USD)
United Digestive,#N/A,"Atlanta, GA",Data Engineer,"GENERAL SUMMARY OF DUTIES: United Digestive’s Data Engineer will be responsible for developing and maintaining data pipelines, data warehouses, and data models to support the company's Business Intelligence reporting initiatives.
REPORTS TO: Director of Business Intelligence & Analytics
RESPONSIBILITIES:
Duties may include but are not limited to:
Implement, maintain, and improve data integration processes to collect, process, and manage large datasets
Collaborate with stakeholders to understand business requirements and design data solutions that meet their needs
Implement and maintain data quality checks and validations to ensure data accuracy and completeness
Optimize data processing and storage to ensure high availability, scalability, and performance
Develop and maintain automated data workflows and scheduling tools
Code ETL process using Azure Data Factory workflows and other technologies as appropriate
Troubleshoot and resolve data-related issues and incidents
Stay up to date with the latest technologies and trends in data engineering, specifically the Azure suite, and recommend improvements and optimizations to the data architecture and infrastructure
Ability to develop solutions and manage existing Azure DevOps pipelines
Maintains a high level of confidentiality with sensitive information
Performs any other duties and/or special tasks as assigned by United Digestive leadership
REQUIRED EDUCATION, SKILLS & EXPERIENCE:
Bachelor’s degree in Computer Science, Software Engineering, Information Technology, or in a related field required; healthcare-related Data Analytics experience preferred; 2+ years of data pipeline management required. Proficiency in SQL and experience with relational databases (e.g., Oracle, SQL Server, PostgreSQL); experience with data modeling, data warehousing, and ETL processes, proficiency in various programming language (e.g., Python, SQL, Java), experience with cloud platforms (e.g., AWS, Azure, GCP) and big data technologies (e.g., Databricks, Hadoop, Spark, Kafka, Cassandra). Must have knowledge of data visualization and BI tools (e.g., Tableau, Power BI) and how to support these tools in an efficient manner.
ADDITIONAL SKILLS AND EXPERIENCE:
Data Engineer must have:
Excellent verbal and written communication skills.
Excellent organizational skills and attention to detail.
Ability to multi-task and prioritize job needs
Ability to deal with patients, visitors, co-workers, and physicians with courtesy and respect
Work under pressure; assess, respond to, and communicate issues in a timely manner.
Self-motivation and be a proactive individual with a professional can-do attitude
PHYSICAL/MENTAL/ENVIRONMENTAL DEMANDS
Requires sitting and standing associated with a normal office environment. Travel as business needs dictate.
DRUG FREE WORKPLACE
United Digestive is a drug free workplace. All offers of employment are contingent upon passing a pre-employment drug screening.
EQUAL OPPORTUNITY EMPLOYER
United Digestive is an Equal Opportunity Employer and does not discriminate on the basis of race, religion, gender, color, or national origin in its employment practices.
Location: 1355 Peachtree Street NE STE 1600, Atlanta, GA 30309
Job Type: Full-time",#N/A,Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Point Predictive, Inc.",#N/A,"San Diego, CA",Senior Data Engineer,"Senior Data Engineer, San Diego, CA based OR willing to relocate to San Diego in 60 Days.
Company
PointPredictive is a fast-growing technology start-up that leverages a patented combination of artificial and natural intelligence [Ai+Ni] to provide risk assessments in the auto lending, mortgage, and retail space. The platform has been proven to reduce lender loan losses by 40-60% with review rates of 5-10% of their applications, resulting in higher productivity of lender risk management departments, significantly lower losses to their bottom lines, and improved customer experience. The company was founded in 2013 by a seasoned team of technology entrepreneurs with over 20 years of experience in the startup space (including several acquisitions) and has financial backing from top tier investors.
Role:
The company is looking for an outstanding Senior Data Engineer to focus on its Data Asset, scale the Database and Data Asset for high performance and reliability. You will also serve as an engineering resource for data related questions, issues, and bugs. Core skills include Python, Snowflake, database systems and SQL, and Amazon Web Services (AWS).
Responsibilities:
· Developing new extract-transform-load (ETL) processes and pipelines. Must be able to manage large volumes of data flowing in from a variety of formats and into a variety of location.
· Identifying, designing and implementing internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes. Automation experience highly desired.
Write complex SQL; be fluent in relational based systems; have strong analytical and problem-solving skills.
Ability to represent complex algorithms in software; bring strong understanding of database technologies, management systems, data structures, and algorithms; a deep understanding in database architecture testing methodology.
Develop and execute test plan, debugging, and testing scripts and tools.
Building real-time streaming data pipelines; building and deploying data pipelines, data streams, and extract-transform-load (ETL) processes.
Manage Data Governance and Data Cleansing, as well as supporting production issues and customer requests.
Provide engineering support to customer issues and bugs. Research and implement fixes.
About you:
You have 5+ years of relevant software development industry experience building and operating scalable, fault-tolerant, distributed systems.
Database and software development experience with Python, SQL, Redshift, and experience with Amazon Web Services, Snowflake along with pub/sub streaming technologies like Kafka, Kinesis, Spark Streaming, etc.
Experience with container services.
Fluid with Amazon Web Services.
Experience with concurrency, multithreading, and the deployment of distributed system architectures.
Experience leading and shipping large scope technical projects in collaboration with multiple experienced engineers.
You have excellent communication skills and the ability to work well within a team and across engineering teams.
You are a strong problem solver and have solid production debugging skills.
You Thrive in a fast-paced environment and see yourself as a partner with the business with the shared goal of moving the business forward.
You have a high level of responsibility, ownership, and accountability.
Job Type: Full-time
Competitive pay, bonus, equity, and benefits:
Benefits:
401(k)
Health insurance
Dental insurance
Flexible spending account
Life insurance
Paid time off
Vision insurance
Schedule:
Monday to Friday
Supplemental pay types:
Bonus pay
Education:
Bachelor's or Master’s (Preferred)
Work Location: San Diego (Del Mar)
Job Type: Full-time
Pay: $105,000.00 - $160,000.00 per year
Benefits:
401(k)
Dental insurance
Flexible spending account
Health insurance
Life insurance
Paid time off
Vision insurance
Compensation package:
Bonus pay
Stock options
Yearly pay
Experience level:
5 years
Schedule:
Monday to Friday
Application Question(s):
Are you willing to be in the office 5 days a week to influence culture and capabilities ?
Are you willing to move to San Diego within 60 days if not within the commuting region ?
Work Location: Hybrid remote in San Diego, CA 92130","$132,500 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"plaxonic
4.6",4.6,Remote,Senior Data Engineer,"We are seeking an experienced Senior Data Engineer.
This position is a remote opportunity.
Qualification
Bachelor’s degree in Computer Science or a related field
Minimum of 4 years of experience in building data driven solutions.
Applicants must be authorized to work in the US without requiring employer sponsorship currently or in the future. CSS does not offer H-1B sponsorship for this position.
Specialized Knowledge & Skills
Expertise in real-time data solutions, good to have knowledge of streams processing, Message Oriented Platforms and ETL/ELT Tools.
Strong scripting experience using Python
Working knowledge of foundational AWS compute, storage, networking and IAM.
AWS scripting experience using lambda functions and knowledge of CloudFormation is nice to have.
Hands on experience with popular cloud-based data warehouse platforms, viz. Redshift, Snowflake.
Experience with one or more data integration tools viz. Attunity (Qlik), AWS Glue ETL, Talend, Kafka etc.
Strong understanding of data security – authorization, authentication, encryption, and network security.
Experience in building data pipelines with related understanding of data ingestion, transformation of structured, semi-structured and unstructured data across cloud services
Demonstrated ability to be self-directed with excellent organization, analytical and interpersonal skills, and consistently meet or exceed deadline deliverables.
Demonstrated experience in data management with a strong understanding of process re/design.
Strong communication skills to facilitate meetings and workshops to collect data, functional and technology requirements, document processes, data flows, gap analysis, and associated data to support data management/governance related efforts.
Knowledge and understanding of data standards and principles to drive best practices around data management activities and solutions.
Strong understanding of the importance and benefits of good data quality, and the ability to champion results across functions.
Ability to lead collaborative meetings which result in clearly documented outcomes, a concrete understanding of meeting attendee performance/reliability, and ongoing management & follow-up for action items.
Acts with integrity and proactively seeks ways to ensure compliance with regulations, policies, and procedures.
Requirements of this position?
- Python
- AWS (Build data pipeline in AWS environment, Foundational AWS services (s3, VPC, IAM). and Programming in AWS )
- Snowflake or Redshift -Data Integration tool
– PySpark or Glue
- USC (US Citizen) or GC (Green Card holder)
Job Type: Full-time
Schedule:
Monday to Friday
Education:
Bachelor's (Preferred)
Experience:
Informatica: 5 years (Preferred)
SQL: 5 years (Preferred)
Data warehouse: 5 years (Preferred)
AWS: 5 years (Preferred)
Python: 5 years (Preferred)
Redshift: 5 years (Preferred)
Snowflake: 5 years (Preferred)
PySpark: 5 years (Preferred)
Glue: 5 years (Preferred)
Work Location: Remote",#N/A,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD)
"BuzzClan Private Limited
4.3",4.3,"Dallas, TX",Data Platform Engineer,"Only w2 CANDIDATES
Job Overview
The Data Platform Engineer shall be responsible for maintaining current data platforms, troubleshooting issues, designing new data platforms, ensuring data platform availability, ensuring data integrity, implementing data platform changes, managing security, and providing on-call support. Current data platforms include – SQL Server, Azure SQL Database, Azure SQL Datawarehouse, Azure Data Lake, SQL Server Analysis Services (SSAS), SQL Server Integration Services (SSIS), SQL Server Reporting Services (SSRS), and PowerBI. Teams supported include: Application Engineering, Data Engineering, Decision Science, and Business Analytics.
For a successful candidate for this position, you must -
Have extensive experience of the setup, deployment and hardening of data-intensive environments using cloud-based database and storage technologies
Have experience in creating robust and automated pipelines to ingest and process structured and unstructured data sources into analytical platforms using batch and streaming mechanisms leveraging cloud native toolset
Be able to drive security paradigms for databases and related infrastructure resources
Leverage the right tools for the right job to deliver testable, maintainable, and modern data solutions
Be comfortable with researching data questions, identify root causes, and interact closely with business users and technical resources on various data related decisions
Understand how to profile code, queries, programming objects and optimize performance
Be able to advice on performance optimizations and best practices for scalable data models, pipelines and queries
Aspire to be efficient, thorough and proactive
Responsibilities and Duties
Provides and designs tools to assist in the management of the data platforms
Works to provide a working model of our transaction processing environment for capacity assessment and planning
Develops a methodology for the ongoing assessment of data platform performance and the identification of problem areas
Develops a security scheme for the data platform environment, as well as assisting in disaster recovery, if necessary
Provides leadership during the development and enhancement of the enterprise’s production applications including working with applications, technical support and operations during the design, development and implementation of applications
Recognizes and identifies potential areas where existing policies and procedures require change, or where new ones need to be developed, especially regarding future business expansion
Fulfills departmental requirements in terms of providing work coverage and administrative notification during periods of personnel illness, vacation, or education
Performs at or above the enterprise’s Information Technology performance standards
Participates with vendors in the assessment of advanced transaction processing and data platform productions including beta and field test participation
Special projects as requested
Performs other duties as assigned
Job Type: Full-time
Salary: $80,000.00 - $90,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Vision insurance
Experience level:
8 years
9 years
Schedule:
8 hour shift
Monday to Friday
Work Location: On the road","$85,000 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$5 to $25 million (USD)
"Financial Information Technologies LLC
3.7",3.7,"Tampa, FL",Senior Data Engineer,"Join Fintech as a Senior Data Engineer!
Fintech is the leading business solutions provider for the beverage alcohol industry, empowering alcohol suppliers, distributors, and retailers with smart solutions that simplify beverage alcohol management. Our unique, thriving company culture promotes collaboration and growth at every level, and our comprehensive employee benefits have earned Fintech the title of a Tampa Bay Times Top 100 Workplaces for 2020 and 2021.
Fintech’s Senior Data Engineer brings a depth of relational database modeling and an understanding of transactional processing across a myriad of database types. They can analyze and assess new data sets to understand nuances of content in the context of purpose with an ability to conceptualize cleansing, harmonization, and modeling efforts. Working under the direction of the principal process architect the senior data engineer will lead a small team of experienced data wranglers to tackle a myriad of ad-hoc custom projects as well as service the development needs within our warehouse and app abstraction layers.
Essential Functions:
Collaborates with ELT/process automation, data insights, and data science teams
Builds data models in accordance with prescribed methodologies
Serves as knowledgeable backstop for level III ticket resolution
Guides and instructs junior developers and engineers on how to implement directives in accordance with project needs within adopted framework
Gains a familiarity with and contributes to the core meta-data driven data processing engine
Advising on data model consumption in analytics layers
Contributes to knowledge base
Qualifications:
8 + years of experience with SQL in multiple database flavors (SQL Server, Oracle, Snowflake, Postgres, Greenplum)
5 + years of experience with data ingest transformations and harmonization
5 + years of experience with database object creation and modeling
Analytical thinker that can adapt and problem solve in a fast-paced environment
Team oriented
Must be able to consume, understand, and implement a complicated but flexible processing back-end in a short time frame
Our Benefits:
Employer Matched 401K (Up to 10% of Employee Salary)
Company Paid Medical Insurance Option (Employee and Dependent Children)
Company Paid Dental Insurance Option (Employee only)
Company Paid Vision Insurance Option (Employee only)
Company Paid Long and Short-Term Disability
Company Paid Life and AD&D Insurance
Employee Recognition Program
18 PTO Days a Year
Six Paid Holidays
Business Casual Dress Code
Check out www.fintech.com for more information!
We E-Verify.
Fintech is an Equal Opportunity Employer that does not discriminate on the basis of actual or perceived race, color, creed, religion, national origin, ancestry, citizenship status, age, sex or gender (including pregnancy, childbirth and pregnancy-related conditions), gender identity or expression (including transgender status), sexual orientation, marital status, military service and veteran status, physical or mental disability, genetic information, or any other characteristic protected by applicable federal, state, or local laws and ordinances. Fintech’s management team is dedicated to this policy with respect to recruitment, hiring, placement, promotion, transfer, training, compensation, benefits, employee activities, access to facilities and programs and general treatment during employment.
Fintech is a Drug-Free Workplace.","$109,213 /yr (est.)",51 to 200 Employees,Company - Private,Financial Services,Financial Transaction Processing,1991,$25 to $100 million (USD)
"Cushman & Wakefield
3.8",3.8,"Delaware, OH",Data Center Operating Engineer,"Cushman & Wakefield (NYSE: CWK) is a leading global real estate services firm that delivers exceptional value for real estate occupiers and owners. Cushman & Wakefield is among the largest real estate services firms with approximately 53,000 employees in over 400 offices and 60 countries. In 2021, the firm had revenue of $9.4 billion across core services of property, facilities and project management, leasing, capital markets, valuation, and other services.
We believe in a work/life balance and value our employees. We offer:
Weekly Pay
Comprehensive benefits that start on your first day
Competitive time off policy
Advancement opportunities
Training
Company provided safety apparel
Tools provided
Currently, we have an opening for a Data Center Operating Engineer in the Delaware, Ohio area.
JOB DESCRIPTION SUMMARY
Responsible to ensure the efficient operation and maintenance of mechanical, electrical, and plumbing equipment and systems for the assigned property(s). Responsibilities include (but are not limited to) maintaining a clean and safe working environment, performing rounds, conducting routine assessments, performing day-to-day preventive and corrective maintenance, painting, and housekeeping for assigned properties
ESSENTIAL FUNCTIONS AND RESPONSIBILITIES
Perform all plumbing, electrical, or HVAC requirements of the building(s)
Maintain heating equipment, chillers (air and/ or water cooled), DX units, pumps, cooling towers, fan coil units, VAV, air distribution systems, etc.
Monitor and adjust all mechanical/pneumatic equipment, steam stations, control gauges, distributor panels, valves, thermostats, diffusers, and other equipment necessary to provide a comfortable environment for the buildings
Verify field conditions and perform any necessary repairs or adjustments
Monitor Energy Management
Repair doors, ceilings, hand railings, and floors and other general repairs, adjustments, and installations about the property
Perform repairs to plumbing fixtures (water closets, urinals, flush valve assemblies, lavatories, etc.)
Perform preventive maintenance duties in accordance with C&W standards, building protocol, manufacturer recommendations, and industry best practices. including changing filters, cleaning coils, flushing condensers, punching tubes, greasing fan, pump, and motor bearings as required, inspecting and adjusting belts, replacing motor bearings, aligning pulleys and shafts, monitoring condenser, chilled, heating and secondary water chemical treatment and its associated feed equipment, clean and maintain cooling towers, and perform annual inspections and other scheduled routines as directed.
Inspect engine room equipment, fan room equipment, cooling tower, all motors, house pumps, electric rooms, backup generator, fire pump(s), sump pump(s), and ejector pumps. Replace lamps and light fixtures, reinstall or replace signage, verify rooms are clean and clear of obstructions and debris
Check for properly operating emergency exit signs and lights and ensure free and clear access to emergency stairs and exits. Perform additional fire and life safety inspections as per NFPA and local jurisdiction, C&W standards, building protocol, and as directed by superiors and property management
Document and report activities to the supervisor
Respond immediately to emergencies (fire, evacuation, equipment failure, etc.) and customer concerns
Comply with all applicable codes, regulations, governmental agency, and company directives as relates to building operations and practice safe work habits
Complete all required C&W Safety Training as scheduled annually
Comply with C&W Uniform Dress Code while working and maintain a neat and clean appearance while on the property at times other than working hours
KEY COMPETENCIES
Technical Proficiency
Initiative
Flexibility
Multi-Tasking
Sense of Urgency
IMPORTANT EDUCATION
High School Diploma or GED Equivalent
Graduate of apprentice program or trade school preferred
IMPORTANT EXPERIENCE
5+ years of related work experience in operating mechanical, electrical, and plumbing systems in a commercial property setting
ADDITIONAL ELIGIBILITY QUALIFICATIONS
Appropriate license/permit for trade as may be required, i.e. Journeyman or Master Electrician License or City Licenses, such as Refrigeration Certificate of Fitness, High-Pressure Boiler License, High-Pressure Steam Operator, etc.)
May be required to have certification as a Universal Technician for CFCs depending on market licensure requirements
Possess and maintain a valid driver’s license and good driving record with periodic checks (where applicable)
Basic Computing Skills in Outlook, Excel & Word
Experience in operation, maintenance, and basic repair of HVAC, boilers, heaters, pumps, refrigerant systems, compressors, water systems, etc.
Knowledgeable in energy management systems, techniques, and operations.
Thorough knowledge of all building systems operations, maintenance, and repair.
Maybe only maintenance staff member on duty during certain shifts; may be required to work extended periods without relief when responding to priority/emergencies (including overtime type assignments); may require shift work and/or on-call duties
WORK ENVIRONMENT
This job operates in a professional office environment. This role routinely uses standard office equipment such as computers, phones, photocopiers, filing cabinets, and fax machines. Regularly required to travel outside between properties in varying weather conditions.
PHYSICAL DEMANDS
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job.
Must have the ability to stoop, stand, climb, frequently lift a minimum of 50 lbs. of equipment (pumps, tools, ladders) and safely install rigging/lifting devices
Regularly required to crouch and reach to install/move equipment by bending forward at the waist or by bending legs and spine
Involves movement between departments, floors, and properties to facilitate work
Ability to speak clearly so others can understand you
Ability to read and understand the information presented orally and in writing
Regularly required to utilize vision abilities, allowing reading of printed material, drawings, and schematics
AAP/EEO STATEMENT
C&W provides equal employment opportunity to all individuals regardless of their race, color, creed, religion, gender, age, sexual orientation, national origin, disability, veteran status, or any other characteristic protected by state, federal, or local law. Further, the company takes affirmative action to ensure that applicants are employed and employees during employment are treated without regard to any of these characteristics. Discrimination of any type will not be tolerated.
OTHER DUTIES
This job description is not designed to cover or contain a comprehensive list of activities, duties, or responsibilities that are required of the employee. Other duties, responsibilities, and activities may change or be assigned at any time with or without notice.
Job Type: Full-time
Pay: $32.00 per hour
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Referral program
Retirement plan
Vision insurance
Experience level:
5 years
Schedule:
12 hour shift
Evening shift
Extended hours
Night shift
Work setting:
In-person
Office
Outdoor work
Warehouse
Ability to commute/relocate:
Delaware, OH: Reliably commute or planning to relocate before starting work (Required)
Education:
High school or equivalent (Preferred)
Experience:
Mechanical knowledge: 5 years (Required)
Electrical: 5 years (Required)
Plumbing: 5 years (Preferred)
Commercial Property Maintenance: 5 years (Required)
License/Certification:
Driver's License (Required)
Work Location: In person",$32.00 /hr (est.),10000+ Employees,Company - Public,Real Estate,Real Estate,1917,$5 to $10 billion (USD)
"Opensignal
4.5",4.5,"Boston, MA",Senior Data Engineer,"Department: Product/Technology
Location: Boston / US East Coast or Canada

Purpose of Role

We’re looking for a Senior Data Engineer to join our Marketing Performance Group in
transforming our real-world raw data into valuable and credible industry-leading metrics that
provide insights to our analysts and our customers.

What you will be doing
The creation and implementation of a framework to assist in building complex statistical
models. Working closely with our data scientists and our data engineers to create and
evolve products that measure market dynamics in the Telecommunication space that drive
our customers short-term marketing campaign tactics and their longer-term customer
acquisition and retention strategies. This role reports to our Engineering Manager.

We expect our Lead Data Engineer to do:
Own and improve our data pipeline. Assemble large, complex data sets that
meet business requirements, with engineering best practices in mind.
Champion building scalable and resilient data infrastructure, as well as tools to
extract and transform data used by stakeholders and customers.
Be security conscious and sensitive to privacy concerns and legislation related
to the data within the platform.
Have a continuous improvement mindset when it comes to both the platform
and the process.
Work efficiently, automate manual processes where possible, and take a test-
driven approach to engineering.
Take a keen interest in improving the platform’s scalability while understanding
the cost.
Be a good team player, with an agile approach and a can-do attitude.
Keep yourself current and make sure we follow best practices and engineering
standards.
Be an advocate for the platform and its health. Take ownership of your work
from conception through to support.
Be detail orientated and understand the importance of the credibility of our
metrics. Document and communicate with stakeholders in a language
understood by all.
Can work in a fast-paced environment with an ability to shift priorities and focus
on changing requirements and market demands.
Able to coach and mentor Data Engineers in best practices.
Cross-collaborate with the wider team to drive and maintain high standards in
our data pipeline builds.
Comfortable and effective at working in a remote capacity, collaborating with
team members across different locations through digital channels.

What we need from you:
As a Senior Data Engineer, we would expect you to have previous experience in
manipulating, processing, storing, and extracting value from big data.
Advanced with hands-on experience in architecting, crafting, documenting, and
developing highly scalable distributed data processing systems.
Advanced with big data tools, specifically Apache Spark.
Advanced with relational SQL databases. Prior experience with MSSQL,
Postgres, AWS Athena (Presto).
Advanced with SQL query authoring including DBT.
Experience with data pipeline / workflow tools i.e. Apache Airflow.
Experience with AWS cloud services like EC2, S3, managed Kubernetes, AWS
ECS, and Aurora.
Experience with object-oriented/object function scripting languages: Python and
Scala.
Experience in implementing complex clustering and classification models on
large datasets to support new product development.
Experience in writing tests, especially in BDD style and working with Git.
Strong analytic skills in working with unstructured datasets.
Experience in root cause analysis of data when asked to answer specific
business questions.
Experience building and optimizing & ""big data"" data pipelines.
Experience supporting and working with cross-functional teams.
Strong self-organizational skills
Experience being part of a cross-functional team, using agile methodologies.
Bachelor’s degree

For US applicants only

At this time, the company will not sponsor a new applicant for employment sponsorship for this position.
This employer participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S.

About Us
Opensignal is the leading global provider of independent insight and data into network
experience and market performance. Our user-centric approach allows communication
providers to constantly improve their network and maximize commercial performance.
Leading analysts, investors and financial institutions place a high value on our independent
analysis and we are regular contributors to their reports.
Real network experience is our focus and ultimately that’s what influences customer choice.
Our mission is to advance connectivity for all and here at Opensignal, the team is leading
the industry in enabling operators to link their network experience and market performance
in a way that has never before been possible.
With offices in London, Boston and Victoria, British Columbia, we are truly global, with
employees working across four continents and representing over 25 nationalities. We are
an equal opportunity employer dedicated to building an inclusive and diverse workforce.

Benefits:
We believe we are stronger when we not only celebrate our many differences, values, and
voices but include them in everyday practice. Having a diverse and inclusive culture is
essential, which is why we offer a flexible approach to work-life balance, operating in a
remote-hybrid way. We’ll help you get set up with the essentials you need to work from
home or the office. We also offer an attractive range of additional benefits, including:
Competitive compensation packages including a long-term equity program.
Comprehensive group benefits package and company-sponsored retirement
savings plan (details depend on your country of work).
Professional development opportunities: education reimbursement, learning
allowance, company-sponsored workshops, and more!
Generous holiday allowance, sick leave, parental leave, flexibility including Flex
Fridays, and the opportunity to work from abroad.
Charity matching and time off for community volunteering and DE&I
program/committees.
Regular virtual and in-person events and socials.","$148,405 /yr (est.)",51 to 200 Employees,Company - Private,Management & Consulting,Business Consulting,2010,Unknown / Non-Applicable
"Glue
3.4",3.4,United States,Senior Data Engineer,"About Glue
Glue is on a mission to make people as happy as they can possibly be at work. To help us get there, we're adding even more of the brightest minds to our team; could that be you?
Glue builds technology empowering distributed teams to stick together. Through an AI-powered connection algorithm HR and People leaders understand and act on the relationships within their organizations. Glue's platform also utilizes Meetups, Events, and Pulse surveys to build connections that further drive retention. Glue is backed by leading investors including Greylock, and Founders' Co-op.
Culture is at the heart of what we do—come join a dynamic and innovative team that's bringing wonder to work!
Job Overview: We are seeking a highly motivated and experienced Senior Data Engineer to join our team. As a Senior Data Engineer, you will be responsible for building and maintaining our data infrastructure, designing and implementing scalable data pipelines, and providing guidance to junior engineers. You will work closely with our data scientists and business analysts to ensure that our data is accurate, reliable, and easily accessible.
Responsibilities:
Design and implement scalable, reliable, and efficient data pipelines to support data ingestion, processing, and delivery
Build and maintain data infrastructure, including data warehouses, databases, and ETL processes
Collaborate with cross-functional teams to ensure that data solutions are aligned with business requirements
Ensure data accuracy, completeness, and consistency by implementing appropriate data quality checks and data governance processes
Provide technical guidance and mentorship to engineers, data engineers, machine learning engineers, and data scientists
Continuously research and evaluate new technologies, tools, and methodologies to improve our data infrastructure and processes
Develop and maintain documentation for data infrastructure, processes, and solutions
Qualifications:
Minimum of 5 years of experience in data engineering, with a proven track record of designing and implementing large-scale data pipelines and data solutions
Experience building and maintaining offline and online machine learning, analytics, and production infrastructure
Strong expertise in data warehousing, ETL, SQL, and technologies (such as Snowflake, PostgreSQL, Airflow, Fivetran, dbt)
Experience with cloud computing platforms and infrastructure-as-code (such as AWS, Docker, Terraform, Kafka, CI/CD)
Experience with event streaming platforms (such as Kafka, Kinesis)
Strong programming and software engineering skills
Excellent communication, collaboration, and problem-solving skills
Strong attention to detail and ability to work as a tech lead and independently in a fast-paced environment
Compensation:
The salary for this role will be between $165,000 and $200,000. In addition to salary this role will include equity in the form of stock options.
Benefits
Glue is proud to provide the following benefits to our employees:
Health, Vision, Dental coverage included
Unlimited PTO with a minimum of 2 weeks a year
Company sponsored 401k
Learning and development Stipend
Home office stipend
Bi-annual all company offsite
Monthly team events
12 weeks paid parental leave","$182,500 /yr (est.)",501 to 1000 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"YSI
3.7",3.7,Remote,Senior Data Engineer,"Position Title: Senior Data Engineer/ Oracle Apex Developer
Job Id: 202301002
Location: Herndon, VA (Remote)
Yakshna Solutions, Inc., (YSI) is a CMMI Level 3 assessed, ISO 9001, 20000:1, 27001 certified, woman-owned small business enterprises, headquartered in Herndon, Virginia, USA. YSI provides professional IT solutions and services to business corporations and government organizations. YSI is committed to serve its business communities as a leading IT vendor providing innovative, quality, and cost-effective IT business solutions and services.
Our benefits are very competitive that include 401(k), health, dental, and vision insurance, Life insurance, short-term and long-term disability insurance, paid time off, training, and professional development assistance.
YSI is seeking a highly qualified Senior Data Engineer. The selected candidate will be able to communicate effectively (written/verbal), possess strong interpersonal skills, be self-motivated, and be innovative in a fast-paced environment.
Responsibilities:
The Data Engineer will be the senior technical expert on work associated with data management, data quality and data structures, coordinating with the ADA as needed to ensure development and data structures are synchronized.
The Data Engineer will design the approach for data tasks and will contribute to completion of data tasks and oversee execution, providing advice and guidance as necessary to junior staff.
Required Qualifications and Skills:
Bachelors or master’s in relevant filed.
Good Data Engineering/Management experience
· Should be familiar with the the Civil Works missions for Hydropower, Recreation, Environmental Stewardship, and Water Supply.
· Extensive technical knowledge of programming in a software stack that includes an Oracle Relational database, SQL, PL/SQL, Oracle Spatial, JavaScript, Oracle REST Data Services (ORDS), and Oracle APEX to make the necessary revisions to the relevant systems. In addition to these general skills and experience, also possess the following:
· Knowledge and experience in developing and maintaining a relational database operated in Amazon Web Services (AWS cloud).
· Knowledge and experience of data entry and options to increase automation and efficiency.
· Knowledge and experience with documenting data systems and processes and creating reports for increased efficiencies.
· Knowledge and experience in optimizing the performance of existing Oracle based applications, including procedures, functions, etc.
· Knowledge and experience with creating Representational State Transfer (RESTful) services utilizing common data dissemination formats.
· Knowledge and experience with making websites 508 compliant.
· Knowledge and experience with authentication and authorization procedures
· Knowledge and experience with maintaining geospatial data in oracle relational database.
· Knowledge and experience with geospatial web services (OGC & ESRI REST)
· Knowledge and experience with cloud native development methodologies
Job Types: Full-time, Contract
Pay: $110,000.00 - $120,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Professional development assistance
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
11+ years
8 years
9 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Herndon, VA 20170: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Preferred)
Experience:
Oracle Apex: 5 years (Preferred)
AWS CLOUD: 5 years (Preferred)
Erwin: 8 years (Preferred)
Data modeling: 8 years (Preferred)
Metadata: 5 years (Preferred)
Work Location: Remote","$115,000 /yr (est.)",10000+ Employees,Company - Public,Manufacturing,Machinery Manufacturing,2011,$1 to $5 billion (USD)
"Colorado Community Managed Care Network
3.2",3.2,"Denver, CO",Data Engineer,"CCMCN focuses on improving the lives of Coloradoans through innovative solutions. Our areas of focus include Population Health, Care Coordination, Social Health Information Exchange, Interoperability, and Clinical Quality Improvement. Through working with health centers and community partners, CCMCN provides best-in-class solutions that create a more comprehensive and collaborative network of care for our most needed Coloradoans.
WHY
Work for CCMCN
At CCMCN, we strive to support our employees and their growth within their career life-cycle. We are committed to a diverse and inclusive work environment where new ideas and innovation are not only invited, but celebrated. We value collaboration, innovation, adaptability, and a great work-life balance.
At CCMCN, you will see the impact of your work directly in the improvements that we make with our partners and their communities.
Our Benefits:
Remote work (depending on position) with access to CCMCN's physical office
Friday flextime and volunteer time off
Health, dental, and vision insurance plans
HSA, FSA, DCA, and employer-sponsored HRA
Life, AD&D, and long-term disability insurance plans
401K retirement plan with employer match
Employee Assistance Program (EAP)
Paid leave including vacation, sick, holidays, & 2 floating holidays

Category: Full-time, Exempt
Salary: $70,000 - $85,000 (DOE)
Reports To: Data Engineering Director
At CCMCN, our mission is to provide services that enable our members and their community partners to succeed as efficient, effective, and accountable systems of care.
CCMCN’s vision is that all Coloradans have access to high-quality, integrated, accountable health care. Areas of focus include population health, accountable care, shared services, health information technology, and clinical quality improvement programming. CCMCN is governed by a Board of Directors comprised of organizational representatives from each of its health center members as well as representation from Colorado Community Health Network (CCHN) and clinician representatives. Through working with health centers and community partners, CCMCN provides technological and analytical tools that help create a more comprehensive and collaborative network of care for Coloradans.
Position Description:
The Data Operations department provides data management, integration, and reporting services for multiple external and internal consumers. The Data Engineer will be responsible for all aspects of data management that support CCMCN’s production services. Candidate must have experience in Microsoft SQL database development, data integration, ETL (extract, transform, and load) tools and methods, analytics, reporting, and documentation.
Essential Functions:
Provide development and maintenance support for data integrations between systems.
Provide development and maintenance support for the EDW and supporting databases.
Provide automated solutions whenever possible and proactively suggest alternative solutions.
Assist in the development of new databases and associated processes as necessary.
Provide data analytics report development for specific projects as needed.
Develop data validation reports and analysis where applicable.
Develop technical documentation of data integrations and processes.
Communicate and collaborate with other team members and clients to develop innovative data solutions.
Utilize up-to-date knowledge of database and data quality best practices to produce effective solutions.
Remain knowledgeable in healthcare data standards, measures, and code sets as well as applicable data privacy practices and legal requirements.
Required Skills and Experience:
Provide development and maintenance support for data integrations between systems.
Provide development and maintenance support for the EDW and supporting databases.
Provide automated solutions whenever possible and proactively suggest alternative solutions.
Assist in the development of new databases and associated processes as necessary.
Provide data analytics report development for specific projects as needed.
Develop data validation reports and analysis where applicable.
Develop technical documentation of data integrations and processes.
Communicate and collaborate with other team members and clients to develop innovative data solutions.
Utilize up-to-date knowledge of database and data quality best practices to produce effective solutions.
Remain knowledgeable in healthcare data standards, measures, and code sets as well as applicable data privacy practices and legal requirements.
Additional Preferred Qualifications:
Provide development and maintenance support for data integrations between systems.
Provide development and maintenance support for the EDW and supporting databases.
Provide automated solutions whenever possible and proactively suggest alternative solutions.
Assist in the development of new databases and associated processes as necessary.
Provide data analytics report development for specific projects as needed.
Develop data validation reports and analysis where applicable.
Develop technical documentation of data integrations and processes.
Communicate and collaborate with other team members and clients to develop innovative data solutions.
Utilize up-to-date knowledge of database and data quality best practices to produce effective solutions.
Remain knowledgeable in healthcare data standards, measures, and code sets as well as applicable data privacy practices and legal requirements.","$77,500 /yr (est.)",1 to 50 Employees,Nonprofit Organization,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Pomeroy
3.1",3.1,"Cincinnati, OH",Data Engineer,"General Function:
The Data Engineer will play an important role in our growing Enterprise Data and Analytics
team. The person in this role will build out a new centralized Analytics Data Lakehouse,
help maintain our existing Operational Data Warehouse, and the infrastructure that
underlies both. We are looking for a candidate with experience creatively solving data
complexities of various sizes and levels of cleanliness - with the goal of enabling data
analysts and business users throughout Pomeroy to make decisions backed by data.
Job Type: Full-time
Pay: $70,000.00 - $80,000.00 per year
Benefits:
401(k)
Dental insurance
Employee assistance program
Flexible schedule
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Vision insurance
Compensation package:
Yearly pay
Experience level:
4 years
5 years
6 years
7 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
SQL: 4 years (Required)
Oracle Cloud Integrator: 4 years (Required)
Data warehouse: 4 years (Required)
Work Location: In person","$75,000 /yr (est.)",1001 to 5000 Employees,Contract,Information Technology,Information Technology Support Services,1982,Unknown / Non-Applicable
"The Swift Group
5.0",5.0,"Chantilly, VA",Senior Data Engineer,"Job Description
The Swift Group is seeking Data Analytic expert to support and advance a learning data analytic program that collects and analyzes training data, measures learning related organizational drivers of success and provides information to facilitate data driven decisions.
Essential Job Responsibilities:
Extract, Transform, and Load (ETL) unstructured and structured data.
Transform unstructured and structured data into new formats facilitating analysis and reporting.
Write script (SQL Developer, Python) to ingest, process, clean, transform, and organize data from the Data Lake and other sources.
Assess data quality, verify data accuracy, and correct data issues.
Set up data models, infrastructure, pipelines, and frameworks.
Monitor and troubleshoot scripts for ingestion of business data from source systems.
Create data flow and data model documentation including data dictionaries.
Participate in development/maintenance/management of a data lake.
Work with team to create, update, and performance tune feeds of learning data from source systems.
Work with team to administer, performance tune, and configure cloud and data lake environments.
Work with team to receive, track, prioritize, develop, test, implement, and deploy new requirements for data feeds, web applications, and cloud services.
Troubleshoot solutions to data related issues.
Required Skills
5 to I0 years of data engineering experience and solid educational training in data engineering.
Advanced skills in data engineering and ability to assess data quality and verify data accuracy.
Advanced ability to Extract, Transform, and Load (ETL) unstructured and structured data.
Advanced experience using SQL Developer and Python programming languages to script the extraction and transformation of data.
Advanced experience with data infrastructure development and data lake management.
Advanced skills in data collection, cleaning, and maintenance.
Advanced ability to assess data quality and verify data accuracy.
.
Desired Skills
Preferred experience with data science techniques including data visualization.
About iC-1 Solutions, LLC.
iC-1 Solutions LLC. is a wholly owned subsidiary of The Swift Group. The Swift Group is a privately held, mission-driven and employee-focused services and solutions company headquartered in Reston, Virginia. Founded in 2019, The Swift Group supports Civilian, Defense, and Intelligence Community customers, across the country and around the globe.

The Swift Groups is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.","$112,276 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2019,Unknown / Non-Applicable
"Sadup Softech
4.2",4.2,United States,Data Engineer,"USA ,
Contract to Hire
5 M ago
Experience
3+ Years
Positions opened
11
Job Description
· Strong knowledge of Java, Java based frameworks like Spring · Good knowledge in Python programing & SQL concepts · Experience in Big data technologies such as Hadoop, Spark / Hive and scheduling (UC4) is preferred · Experience with RESTful web services and SOA concepts · Strong communication skills in a collaborative environment · Strong critical thinking skills, Ability to devise innovative solutions · Should be a strong advocate of code craftsmanship, good coding standards
Desired Candidate Profile
Work with the Agile team to clarify the new products and features requested by the Product team.
Collaborate with Data science & other dependent teams to design and implement the required solutions.
Explore new industry standard practices and implement for the project.
Participate in pair programming in the delivery of both POC and targeted features.
Understand and apply our technical architecture to ensure consistent, reliable, and secure deployments.
Enhance and maintain existing product capabilities.
Participate in formal and informal code reviews to ensure code quality.
Actively contribute to the automated test suite to enable continuous integration.",#N/A,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Everybody Votes Campaign,#N/A,United States,Data Engineer,"ABOUT EVERYBODY VOTES CAMPAIGN
Everybody Votes Campaign (EVC) is a national non-partisan, not-for-profit hiring staff for a large-scale coordinated civic engagement campaign active through the 2024 election cycle. The campaign aims to create a more representative democracy by registering millions of underrepresented voters across the country. This effort focuses on voter registration in a targeted fashion by conducting at-scale, effective, efficient, metrics-driven registration work. Through this work, we seek to fundamentally change the make-up of the electorate and to increase the political power of traditionally underrepresented communities in our democracy.
We directly fund organizations who execute voter registration and run quality control operations. We are dedicated to being active participants with the organizations to ensure their programs are effective and promote an investment in the future of emerging communities.


ABOUT THE OPPORTUNITY
EVC seeks a Data Engineer to implement, maintain, and optimize voter registration data pipelines. The Data Engineer will be responsible for large scale reporting automation with a particular focus on standardizing complicated voter registration data across multiple databases, working closely on a multidisciplinary team of engineers, analysts, project managers, and field practitioners. This individual will be at the forefront of the campaign’s effort to develop advanced voter registration dashboards and pipeline solutions, working closely with partner organizations. The Data Engineer will report to the Director of Data Products.

WHAT YOU WILL DO IN YOUR ROLE

Centralize data from disparate sources across multiple databases and use innovative hygiene solutions to clean up traditionally complicated voter registration data
Optimize and automate the campaign’s voter registration data pipeline, integrate new external data sources, and help our data team automate and streamline manual reporting processes
Set best practices for data standardization and refinement
Participate in maintaining a well-documented, consistent codebase
Work as a team: pair-program, review code, co-design and plan, develop a shared vision for and an understanding of the work, document progress in JIRA and Confluence
This is a great opportunity for someone who:

Enjoys coming up with creative solutions to big questions through collaboration, and is able to use immediate challenges as windows into future opportunities.
Notices and fixes errors that others might overlook. Acknowledges mistakes and turns them into learning opportunities. Has a track record of leaving things better than they found them – simplified pipelines, strong documentation, code sharing discipline.
Plans ahead and finds alternative paths, when needed, to get to the finish line. Bounces back from setbacks and rejections. Holds a high bar when things are hectic.
Brings civic engagement experience working for or with groups that serve communities of color.
Stays ahead of the curve in an ever-changing technology environment.
Identifies decisions, policies, or practices that have disparate impacts based on identity. Is driven to make changes in systems and practices to operationalize equity.
CORE COMPETENCIES

Growth mindset: demonstrated ability to take and receive feedback with professionalism and grace from peers and staff as well as supervisors
Relentlessly goal-oriented: enjoys working toward and achieving ambitious goals; willing to go over, under, around, or through any obstacle that gets in the way of meeting goals with a proven track record of creating and executing/managing comprehensive strategic goals
Cultural competency: able to build relationships and collaborate with colleagues, partners, and stakeholders across multiple lines of identity difference
Keeps Calm in Stressful Situations: demonstrated capacity and willingness to work long hours during peak season, rolling up their sleeves and getting the work done
REQUIREMENTS FOR THIS ROLE

Proficiency and experience using APIs to push and pull data
Strong knowledge of SQL and management of relational databases (in particular, Redshift and PostgreSQL)
Experience working with messy data, and creating and maintaining data pipelines
Strong proficiency in programming (preference for Python)
Experience writing and editing clear, clean code and a willingness to use version control systems like git and Github
Interest in being a member of a diverse, multidisciplinary team that communicates technical concepts to non-technical audiences
Helpful but not required:

Previous experience working in the field of voter registration or voting rights
Familiar with cloud infrastructure (e.g. AWS, GCP)
Understanding of Terminal and the command line interface
Experience creating reports using data visualization & business intelligence tools (e.g. Tableau, PowerBI, Periscope, Google Data Studio)
BENEFITS AND CULTURE
We offer flexible remote forward work, and a generous benefits package; including 100% cost coverage of employee health benefits, 401K with an automatic employer contribution regardless of employee contribution level, virtual therapy, stipend for ergonomic office set ups and generous vacation and leave policies.


All employees must be eligible to work lawfully within the United States upon the commencement of employment. The organization does not sponsor visa applications for prospective or current staff.


Salary Range: $88,000-$102,500
Our work is centered on creating a deeply inclusive and significantly more representative electorate. In order to be successful in this role, the candidate must have the cultural competence to successfully work with a diverse group of staff, partners and stakeholders. We especially strongly encourage applicants with close ties to Black, Latinx, Indigenous, non-English-speaking, disability, and LGBTQ+ communities to apply. We are proudly an Equal Opportunity Employer.","$95,250 /yr (est.)",1 to 50 Employees,Nonprofit Organization,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Bailey Information Technology Consultants, LLC.
5.0",5.0,"Chantilly, VA",TS/SCI Data/ETL Engineer,"Data/ETL Engineer
Location(s): St. Louis, MO; Chantilly, VA; Gaithersburg, MD; Alexandria, VA
Clearance: TS/SCI required.
Required Certifications: DoD 8570 Compliant Certification (Security+)
Level: T3 or T4

JOB DESCRIPTION:

Bailey Information Technology Consultants is recruiting for an experienced ETL Engineer to assist with the development, integration, deployment, and sustainment of critical systems. In this role, the ETL Engineer will work with a large database (Terabyte scale), mission-critical geospatial data, and best-of-breed cloud (AWS) databases and development tools to enable customer mission.

RESPONSIBILITIES:
The primary duties will be developing and maintaining a custom data integration and validation pipeline used by federal mission clients.
Build and support custom Python scripts to transform, standardize, and load an extensive geospatial data archive.
Creating and maintaining a custom data transformation pipeline using Python and Apache NIFI.
Hands-on work with AWS services such as EC2, RDS, Lambda, IAM, and S3
Working and optimizing the performance of an RDS Postgres database
CORE QUALIFICATIONS:
Proactive self-starter demonstrating a positive, willing attitude and excellent oral and written communication skills.
Deep understanding of AWS permission and restrictions
Extensive experience using RDS
Direct experience with Python scripting
Experience using GitLab
Experience with AWS services/technologies (e.g. EC2, Lambda, IAM, S3)
Proficient using Team Collaboration tools (Jira, Confluence, RocketChat)
Must possess an active Top-Secret Security Clearance with SCI (TS/SCI)
Must be a U.S. Citizen
Typically requires a BS in Information Systems, Information Technology, Computer Science or closely related discipline and 8-12 years of prior relevant experience or Masters with 6-10 years of prior relevant experience.
PREFERRED QUALIFICATIONS:
AWS certification (e.g. Database, Big Data)
Experience with ArcGIS or geospatial technologies","$94,089 /yr (est.)",1 to 50 Employees,Contract,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"ZAGG, Inc.
3.6",3.6,"Midvale, UT",Senior Data Engineer,"Position Summary
ZAGG is looking for an individual who can drive business change through insightful development of systems and applications. This position will work closely with business owners to architect, develop, and implement solutions across ERP, CRM, EDW, and other applications. A successful candidate will have excellent communication and technical skills to help implement business requirements into working solutions.
Salary: $120K to $135K
Responsibilities:
Administration – 30%
Work closely with multiple departments locally and overseas contributing and solving complex issues that will directly affect business operations and outcomes
Gather/evaluate requirements for business processes and technology enhancements while uncovering areas for improvement
Manage and prioritize projects and resources to ensure business goals are met and maximum value is created
Evangelize applications through effective process design and user training
Create and maintain documentation for operational and security audits

Development – 60%
Architect and manage solutions across multiple applications ensuring efficient integrations that provide redundancy, visibility, and extensibility
Utilize technology to improve the quality of life by automating and enhancing the ability of users/departments
Enhance cloud based data silos supporting Microsoft Dynamics CRM technologies
Conduct application testing and provide database management support
Create and maintain integrations between core applications, services, databases, etc.
Consolidate multiple data silos into a single EDW used for companywide Reporting and Analytics
Model and build data structures to support multi-dimensional data discovery

Other duties as required – 10%

Qualifications:
7+ years of similar experience in an engineering role utilizing an EDW system
Bachelor’s degree in Computer Science or related area
Exceptional analytical and conceptual thinking skills with a detail oriented and inquisitive personality
Proven experience gathering and interpreting business requirements and converting them to technical blue prints
Knowledgeable in enterprise technology stacks (Servers, Database, Network, EDI, etc)
Strong experience in cloud architecture and development (Azure, AWS)
Strong experience in supporting data structures supporting Microsoft Dynamics Technologies including MSSQL and Cloud CDS
Strong experience with integration tools and web service protocols such as SSIS, Fivetran, Synapse, Jitterbit, Smart Connect, Scribe, SOAP, REST, etc.
Experience in a development/scripting language such as python, powershell, etc.
Strong SQL Skills along with an understanding of data warehouse methodologies
Strong experience with data warehousing platforms (Azure DW, Redshift, Matrix, Snowflake)
Strong experience with visualization tools (PowerBI, Tableau, SSRS, etc)
Strong knowledge of SDLC and Agile/Scrum Framework
Strong understanding of operations in a consumer goods industry – sales forecasting, inventory, etc. is a plus
Be able to communicate with customers and co-workers in an effective, timely and professional manner
Strong interpersonal and meeting facilitation skills with technical project management experience being a plus
Must have a collaborative style and be able to cultivate and maintain an open environment where ideas are shared, questioned and tested","$127,500 /yr (est.)",501 to 1000 Employees,Company - Public,Manufacturing,Consumer Product Manufacturing,2005,$500 million to $1 billion (USD)
"Farm Credit East
3.9",3.9,"Enfield, CT",Data Engineer,"Be part of a team focused on the success of our customers, the success of our communities, and the success of each other. Farm Credit East is the leading provider of loans and farm advisory services to farm, forest product, fishing, and other agricultural business owners across the northeast. We are One Team Working Together with a focus on our five pillars: Outstanding Customer and Employee Experience, Quality Growth, Operational Excellence, Commitment to our Communities, and Protecting Customer Information.

Position Summary
The Data Engineer is responsible for cleaning, managing, and sharing data that guides business decisions. Using ETL tools you will gather data from a variety of sources, checking for anomalies, automating processes, and generally making it easier for business stakeholders to generate valuable insights. This position will collaborate with internal and external organization to capture requirements, design, create, document, manage, and fulfill requests for on-going and/or ad-hoc reports, dashboards, and scorecards.

Duties and Responsibilities
Work with product stakeholders to implement, maintain, and enhance data models and solutions used to define and measure quality of data domains.
Design data models to meet requirements
Perform ETL (Extract, Transform, and Load) on data to meet stakeholder specifications.
Design and develop data access methods, datasets, views etc.
Develops data modeling and is responsible for data acquisition, access analysis, archive, recovery, load design and implementation.
Coordinates new data developments to ensure consistency with existing warehouse structure.
Collaborates with internal customers to capture requirements, design, create, document, manage and fulfill requests for on-going and/or ad-hoc reports, dashboards, and scorecards.
Assists with the development, implementation, and maintenance of front-end presentation (dashboards), automated report solutions and other BI solutions to support tactical and strategic reporting needs of the organization.
Assists in identification of data integrity problems and recommends solutions.
Work collaboratively with key stakeholders both internally and externally, including but not limited to Senior Management, Business Unit Leaders, Knowledge Exchange, and Farm Credit Financial Partners (FPI).

Job Qualifications/Requirements
Bachelor’s Degree in Computer Science, Business, Finance, or other related field from an accredited University.
Experience with MSFT SQL Server
Microsoft Azure (Data Bricks, Data Factory, Logic Apps, Functions, etc.)
2 plus years of experience in Finance related informatics, performance measurement, or analysis with strong relational database SQL skills.
1 + years of experience using Microsoft Azure product to perform ETL
Familiar with Databricks Unity catalog

Farm Credit East is an Equal Opportunity Employer. As an Equal Opportunity Employer, we do not discriminate on the basis of race, color, religion, national origin, sex, sexual orientation, gender identity or expression, age, marital status, parental status, political affiliation, disability status, protected veteran status, genetic information or any other status protected by federal, state or local law. It is our goal to make employment decisions that further the principle of equal employment opportunity by utilizing objective standards based upon an individual's qualifications for a specific job opening. In compliance with the Americans with Disabilities Act (“ADA”), if you have a disability and would like a reasonable accommodation in order to apply for a position with Farm Credit East, please call 1-800-562-2235 or e-mail FarmCreditCareers@farmcrediteast.com .","$86,685 /yr (est.)",201 to 500 Employees,Company - Private,Financial Services,Banking & Lending,1916,$100 to $500 million (USD)
"Spencer Gifts & Spirit Halloween
3.6",3.6,"Egg Harbor Township, NJ",Azure Data Engineer,"We are currently looking for a Data Engineer to optimize our data integration at scale. Inside our Data Integration team, you will be designing pipelines and warehouses to model data from multiple sources that will allow us to derive business insight. Using Azure and other open-source technologies, such as Azure Data Factory and PySpark, you will design and build our next-generation ETL pipelines and data models.
Build data pipelines and python-based ETL tools for acquiring, processing, and delivering data
Develop data models and schemas in our data warehouse that enable performant, intuitive analysis
Handle the challenges that come with managing terabytes of data
Collaborate with business leaders and analysts to define key metrics and build reporting to monitor and understand company performance
Develop the server applications and APIs that are used by our Data Team
Responsibilities:
Build data pipelines and python-based ETL tools for acquiring, processing, and delivering data
Develop data models and schemas in our data warehouse that enable performant, intuitive analysis
Handle the challenges that come with managing terabytes of data
Collaborate with business leaders and analysts to define key metrics and build reporting to monitor and understand company performance
Develop the server applications and APIs that are used by our Data Team
Requirements:
Bachelor’s degree or higher in Computer Science, Computer Engineering, Information Technology or related field
Fluent in several programming languages such as Python, R, or Scala
6+ years of work experience in building ETL pipelines in production data processing and analysis
Experience designing SQL tables, choosing indexes, tuning queries, and optimizations across different functional environments.
Hands-on experience writing complex SQL queries and using a BI tool
Experience with data lakes and designing and maintaining data solutions using Spark and Azure serverless services such as ADF
Familiarity with data ingestion APIs, data sharing technologies, and warehouse infrastructure and development
* Proof of vaccination for COVID-19 required for employment, reasonable accommodations considered for medical, pregnancy or sincerely held religious beliefs.
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Experience level:
6 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Egg Harbor Township, NJ: Reliably commute or willing to relocate with an employer-provided relocation package (Required)
Work Location: In person",#N/A,5001 to 10000 Employees,Company - Private,Retail & Wholesale,"Gift, Novelty & Souvenir Stores",1947,$100 to $500 million (USD)
The Josef Group Inc.,#N/A,"Herndon, VA",Data Engineer,"Data Engineer
TS/SCI and Poly is Required
Herndon, VA
We are charged with taking commercial and academic innovation high-side, in domains of Artificial Intelligence / Machine Learning (AI / ML) such as Computer Vision (Image and Video Processing), Natural Language Processing (NLP), and Audio Modeling. We also bring the best ideas, tools, and approaches in technology infrastructure (AWS, DevOps, etc.) to the IC. The tech stack used is extremely broad - anything cutting edge in the commercial market, the open source community, or the academic research community is likely involved: and if something isn't being looked at yet, you can make that happen.
This effort supports ALL missions of the Intelligence Community, including cyber-related data science missions. A seamless group of contractor and customer personnel work to create innovations that supply customer groups with the data sets, models, algorithms, software, and infrastructure they need to increase their mission success. Management is hands off, gives the team the freedom to explore new approaches, and markets the best ideas and results to all the other IC customers.
This project regularly needs various types of people - Data Scientists, Data / ETL Engineers, Analytic Software Engineers, Full Stack Developers, UI/UX Developers, and AWS/DevOps experts. We're particularly interested in people with any of the following experience:
developing AI / ML models (neural networks, tree based algorithms, etc.);
conducting data analysis in the fields of Computer Vision, NLP, or audio signal processing;
doing data ingest and ETL into sponsor environments; or
building data-analytic software systems.
Work on this program takes place throughout the Reston/Herndon/Chantilly, VA area (we cannot support remote work) and requires a TS/SCI + Poly clearance (acceptable to this customer).
THE ROLE
Perform extract, transform, load (ETL) development for data pipelines
Integrate and update lab-to-factory services into data pipeline
Integrate and update workflow orchestration
Parsing
Perform API service development
General Skills
ELK Stack (elastic search, logstash, and kibana) - ingestion, management, and access control
Python
Bash Scripting - ad-hoc processing, cron jobs, etc.
Experience working on cloud environments","$190,000 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Shutterfly
3.3",3.3,"Eden Prairie, MN",Senior Data Engineer,"At Shutterfly, we make life’s experiences unforgettable. We believe there is extraordinary power in the self-expression. That’s why our family of brands helps customers create products and capture moments that reflect who they uniquely are.

As part of the Data Engineering team you will tackle the scalability, performance and distributed computing challenges needed to collect, process and store data for Shutterfly. You will enhance the Data Warehouse at Shutterfly on AWS, supporting the breadth and depth of the company’s analytic needs.
What You'll Do as a Sr. Data Engineer II
Build design, develop, test, deploy, maintain and enhance full-stack data engineering solutions
Provide technical leadership to the Data Warehouse and Shutterfly teams who use Shutterfly’s Enterprise Data Warehouse
With your technical expertise own and manage project priorities, deadlines and deliverables
Evangelize the benefits of existing solutions and new technologies to drive the use and push the technology of the Data Warehouse forward
Work closely with Data Operations to improve CI/CD pipelines am improve the operation and performance of the Data Warehouse
The Skills You'll Bring
Expert knowledge Python, Spark, SQL scripting and Java
7+ years of hands-on experience with building data ingress frameworks and experience with dealing with a wide variety of source data formats
Hands-on experience in building data applications, including design, implementation, debugging, and support.
Supporting a diverse and inclusive workforce is important to Shutterfly not only because it directly reflects our value of Embracing our Differences, but also because it’s the right thing to do for our business and for our people. Learn more about our commitment to Diversity, Equity and Inclusion at Shutterfly DE&I.","$132,354 /yr (est.)",10000+ Employees,Company - Private,Information Technology,Internet & Web Services,1999,$1 to $5 billion (USD)
"Titan America
3.8",3.8,"Roanoke, VA",Data Engineer,"Data Engineer
We are seeking a motivated and detail-oriented Data Engineer to join our team based in Roanoke, Virginia. As a Data Engineer, you will play a crucial role in designing, developing and maintaining scalable data pipelines and transformations for business-critical use cases. In addition, you may take on various Software Development and Data Science responsibilities as we frequently share responsibilities and knowledge across our growing team.
Responsibilities:
Design, develop and maintain scalable data pipelines for extracting, transforming, and loading data from various sources
Collaborate with cross-functional teams to identify data needs and determine the best data solutions
Develop and implement data models to support business requirements and ensure data quality
Ensure the security and privacy of sensitive data by implementing appropriate access controls
Monitor and optimize data pipeline performance to ensure timely and accurate data delivery
Document data pipeline processes, data dictionaries, and data storage solutions
Requirements:
Bachelor's degree in Computer Science, Computer Engineering, or a related technical field
Minimum of five years of professional experience working as a Data Engineer or Software Developer
Strong hands-on experience with data warehouse and transformation solutions, i.e. Domo, Snowflake or similar
Proficient in at least one scripting language such as Python, JavaScript, or R
Understanding of data modeling, data integration and data quality processes
Familiarity with cloud platforms such as AWS, Azure, or Google Cloud Platform
Strong analytical and problem solving skills
Full Stack Software Development experience in a professional setting is highly desired, but not required
This is an excellent opportunity for a driven and collaborative individual to make a significant impact in a dynamic and growing team. If you have a passion for data and a desire to work in a fast-paced and dynamic environment, we want to hear from you!
Success begins with hiring the right people to partner with us as we grow and develop our businesses. People are central to everything we do. It is through their efforts and talents that Titan has been successful for over 100 years.
Titan America is an Equal Employment Opportunity (EEO) / Affirmative Action employer and welcomes all qualified applicants. Applicants will receive fair and impartial consideration without regard to race, sex, color, religion, national origin, age, disability, veteran status, genetic data, or other legally protected status.
Job Type: Full-time
Pay: $75,000.00 - $85,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Referral program
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Roanoke, VA 24019: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Do you currently or will you in the future need sponsorship to work in the United States?
This position requires the successful candidate to be in the office in Roanoke, VA at least 3 days a week. Can you meet this requirement?
This position requires the successful candidates to be a full time, W2 employee. Can you meet this requirement?
Education:
Bachelor's (Preferred)
Experience:
Data Engineering: 5 years (Preferred)
Work Location: Hybrid remote in Roanoke, VA 24019","$80,000 /yr (est.)",1001 to 5000 Employees,Company - Public,"Energy, Mining & Utilities",Mining & Metals,1902,Unknown / Non-Applicable
"Appsintegration INC
4.5",4.5,"California, KY",Data Engineer,"Role:- Data Engineer
Position:- Onsite (W2 position)
Experience:- 10+ Years
Responsibilities
Working closely with business teams to understand their requirements and develop reporting or analytic solutions.
Producing and automating the delivery of key metrics and KPIs to the business.
In some cases, this will focus on making data available and others will be developing full reports for end users.
Managing stakeholder demand and a backlog of reporting requirements and needs.
Actively working with users to understand data issues, tracing back data lineage, and helping the business put appropriate data cleansing and quality processes in place.
Managing secure access to data sets for the business.
Requirements
Bachelor's degree in Computer Science or equivalent.
Proficient in creating Data Factory pipelines for on-cloud ETL processing.
Experience implementing data pipelines using the latest technologies.
Experience developing using SQL, relational databases, and data warehousing technologies.
Extensive ETL experience.
Ability to scale systems in Cloud as per requirement.
Experience in handling structured and unstructured datasets.
Good to have experience in data modeling and Advanced SQL techniques.
Must-Have Skills: Azure Data Factory (ADF), SQL, Cloud-based ETL.
Good to Have Skills: Azure File Storage Services, Databricks, Azure Synapse.
you can reach me out on LinkedIn
here is my link :- linkedin.com/in/shravanthi-thakur-705a81231
Job Type: Contract
Salary: $42.00 - $45.00 per hour
Ability to commute/relocate:
California, KY 41007: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",$43.50 /hr (est.),1001 to 5000 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Atika Technologies,#N/A,"Pittsburgh, PA",Data Engineer,"Role: Data Engineer/PySpark Developer, Pittsburgh, PA - Day One Onsite
Fulltime and Contract(W2)
LinkedIn Must
USC/GC/EADs
Three round of interview: First two on Zoom Video Call, End Client Round will be Onsite on the Client's Location (Pittsburgh, PA)
Job Description:
Python(PySpark), Jupiter hub, airflow, Java
Database knowledge - Oracle / SQL / Data Analytics
Regulatory reporting knowledge preferred
Good communication skills
Open / Ability to learn new technologies
Very good Analytical Skills
Experience in ETL is preferable
Job Types: Full-time, Contract
Salary: $120,000.00 - $130,000.00 per year
Benefits:
Dental insurance
Health insurance
Life insurance
Compensation package:
Bonus pay
Performance bonus
Yearly pay
Experience level:
10 years
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Pittsburgh, PA 15259: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Required)
Spark: 4 years (Required)
Regulatory reporting: 3 years (Required)
SQL: 4 years (Required)
IT: 10 years (Required)
Airflow: 4 years (Required)
Jupiter: 4 years (Required)
Work Location: One location","$125,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Apolis
3.9",3.9,"Saint Louis, MO",Data Engineer (Python Focused),"Role: Data Engineer (Python Focused) (W2 ONLY)
Location : St. Louis, MO 63102
Duration: 18 Months
The primary responsibilities of this role will be to work as a part of the data and analytics team of the client. The ideal candidate would have experience using ETL and other data engineering tools in a cloud environment. Additionally, this candidate would have coding and scripting experience creating code to move and manipulate data using tools such in Python such as Spark or Pandas. Microsoft Azure experience is a plus but relevant experience with other cloud-based environments will be considered.
This team uses Microsoft Azure to build and productionize data sets and processes to the product owners and product users and this role will be expected to contribute to those objectives. As a member of this team, you will work in conjunction with other data engineers, platform engineers, solution developers and others to execute and deliver working solutions to the end users.
Technical Skills
High focus on Python
More Programmer/Coder experience, less Cloud experience needed.
Must have experience with data analytics/engineering and cloud technologies and an in-depth working knowledge and understanding of data engineering, design, and integration.
Intermediate or higher level of proficiency with Python Must have the proven ability to learn quickly and apply cloud data engineering principles and skills to new tools and use cases.
Ability to define and take technical ownership of detailed requirements and collaborate effectively with various team members such as Business Analysts and Project Managers and end users.
Excellent communication (verbal and written) and problem-solving skills are essential for this position.
Initiative, Proactive Cooperation, and curiosity are mandatory behavioral skills required for this position.
Experience with several of the following technologies is preferred:
Experience with Microsoft Azure Platform. In particular, the data processing toolsets including Azure Data Factory Pipelines, Azure Databricks, Azure Data Lake Storage.
Experience using Notebooks (Such as Jupyter, DataBricks) also desired.
Experience using code languages such as Python and C#
Experience using query languages such as SQL, MDX and DAX
Experience using scripting languages such as PowerShell, M-Query (Power Query), and Windows batch commands.
Understanding of Agile and DevOps principles for software development and ownership.
Familiar with compliance and controls for software development and deployment.
Education Required
A bachelor's in computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field [or equivalent work experience] is not required but is preferred.
Job Type: Contract
Salary: $60.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Vision insurance
Compensation package:
Overtime pay
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
St. Louis, MO: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Preferred)
ETL: 5 years (Preferred)
Cloud infrastructure: 4 years (Preferred)
Work Location: One location",$65.00 /hr (est.),501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,1996,$25 to $100 million (USD)
"Cureatr
4.1",4.1,Remote,Data Engineer,"About Us
We are a thriving, medication management solutions company, headquartered in Manhattan, but largely virtually distributed, committed to tackling the $528B suboptimal medication management problem. Patient medication information is often incomplete for healthcare providers and systems – it’s our mission to fill in the gaps. We provide real-time, universal access to accurate medication data for over 265 million patients and high-quality interventions by board certified telepharmacists. Through our solutions, healthcare providers can reduce preventable hospitalizations, avoid readmissions, and ultimately, improve patient outcomes. We’re already used by dozens of hospital systems, with tens of thousands of clinicians and supporting staff using us to care for millions of patients.
Here at Cureatr, we are driven by a core set of values and strive to incorporate them into everything we do. We care deeply about our users, patients, and employees, and we aim to live and operate compassionately. We take pride in creating a diverse, inclusive, and equitable company culture, where our open and honest approach to work fosters innovation and encourages employees to achieve professional and personal growth.

The Role
We’re looking for a Data Engineer to join our growing Engineering team. Use the power of healthcare data to find insights that will demonstrate the value Cureatr is delivering to our customers, help improve patient outcomes, and expand the scope of Cureatr’s mission. As a Data Engineer at Cureatr, you will be responsible for designing, implementing and maintaining our data infrastructure, pipelines, automation, and ETL processes. You will work closely with our Data Science and Analytics teams to ensure that our data is clean, accurate, and readily available for analysis. You will also be responsible for ensuring that our data meets industry standards for security, privacy, and compliance.
This is a remote, work from home position, but candidates must be located within the U.S. Ideal candidates must be able to travel to our New York City office, or TBD company-wide offsite locations, up to 4x per year.

You’ll be part of a team working to solve problems like:
Increasing the number of patients who receive the opportunity to do a medication reconciliation, which can help keep them out of the hospital.
Streamlining and automating processes so that our pharmacists can serve as many patients as possible.
Predicting patient events likely to occur so that our clinical team can adequately prevent them before it’s too late.
Helping our pharmacists make the most of data and insights that will help them be as effective as possible in serving patients.
Evaluating the effectiveness of our clinical services impact at the population level.

What You’ll Do:
Design, implement, and maintain our data infrastructure, pipelines, automation, and ETL processes to support our data needs.
Build and maintain scalable data models and databases.
Collaborate with Data Science and Analytics teams to understand their data needs and provide data solutions accordingly.
Ensure data quality and integrity by implementing data validation and cleansing processes.
Ensure data security and compliance with healthcare regulations.
Monitor and optimize data performance and scalability.
Design and implement data governance policies and procedures.
Evaluate and implement new data technologies and tools to improve our data capabilities.

Your Skills:
3+ years of experience as a Data Engineer or similar role.
Strong programming skills in languages such as Python and SQL.
Experience using git to track changes.
Experience with data modeling and database design.
Experience with the “modern data stack” preferred
Experience with ETL and data pipeline development.
Knowledge of healthcare data and related standards (e.g., HL7, FHIR) is a plus.
Familiarity with AWS.
Strong problem-solving and analytical skills.
Excellent communication and collaboration skills.

Technology Stack:
Python
RedShift
dbt
Dagster
MongoDB
Looker
AWS

Educational Requirement: Bachelor’s Degree or relevant work experience

Salary: $110,000 - $140,000 plus full benefits

Why work with us?
Make a measurable impact on patients’ lives
Be part of a company dedicated to living its core values:
Empathy
Honesty
Integrity
Impact
Fun
Equity in Healthcare
Work in the company without micromanagement
Share your vision and make an impact on the product
Opportunities for career growth
Remote work
Competitive salary
Premium benefits (Health, Dental, Vision, 401k, unlimited PTO, more)

Equal Employment Opportunity Policy
Cureatr is an equal opportunity/affirmative action employer. All qualified applicants will receive consideration for employment without regard to sex, pregnancy, race, religion or religious creed, color, gender, gender identity, gender expression, national origin, ancestry, physical or mental disability, medical condition, genetic information, marital status, registered domestic partner status, age, sexual orientation, military or veteran status, protected veteran status, or any other basis protected by federal, state, local law, ordinance, or regulation and will not be discriminated against on these bases
Disclaimer: we are aware of a scam targeting applicants using a cureatrcareers.com email address. Please block and report this sender, as they are not affiliated with Cureatr. If you have any questions, please contact us at jobs@cureatr.com. Thank you!
uBajh3OfeT","$125,000 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2012,Less than $1 million (USD)
WellTrust Medical Group,#N/A,"Charlotte, NC",Data Engineer,"The Data Engineer will assist the senior analyst in complex modeling and projects to ensure their completion within the allotted timeline. Helps track measures of success and develop reports that measure operational and customer metrics. Demonstrates ability to troubleshoot data consumption and data quality problems.

Responsibilities:
Build and maintain reports using My SQL Server
Identify optimal data sources, structure data validation, and cache optimization.
Performs business problem modeling and quantitative analytics including regression analysis, time-series, hierarchical cluster analysis, and data mining.
Helps track measures of success and develop reports that measure operational and customer metrics.
Demonstrates ability to troubleshoot data consumption and data quality problems.
Evaluates and recommend improvements to current models, metrics, and analytic processes to improve operational efficiency and insight.
Design, document, and communicate effective technical solutions meeting end-user requirements
Perform analyses with large amounts of complex data, and uses data visualization tools to communicate results.
Requirements:

Bachelor's degree or equivalent experience in a quantitative field (Mathematics, Statistics, Computer Science, Economics, Finance, Physics, Engineering)
3+ years of professional working experience as a Data Engineer or Analyst
1-3 years of data warehouse experience
Build and maintain reports using My SQL Server
Strong analytical and creative problem-solving skills
Design, document, and communicate effective technical solutions meeting end-user
SQL and Excel Macro development
Strong understanding of various data environments such as SQL, Oracle, Big Query, etc.
Experience with data visualization tools such as Looker Studio, Tableau, D3.js, etc.
Proficient in R, Python, MATLAB, Java, or another programming language.",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Take Command Health
4.9",4.9,"Dallas, TX",Data Engineer,"Company: Take Command
Position: Data Engineer
Salary: $120,000-$130,000 plus bonus
Location: Dallas or Austin, TX
About Take Command
Take Command is a start-up on a mission to improve the healthcare system, starting with health insurance. Pragmatically speaking, we help employers reimburse employees for individual insurance instead of offering a traditional one-size-fits-all group plan. We believe this model can empower employees (when they have the right support) to be savvy healthcare consumers and have a transformative impact on the entire healthcare system.
Let’s be honest—health insurance is usually a confusing, frustrating, and even emotional experience for people. We want to fix that with a new model, great technology, and a superior user experience. We have made a great start, but we need your help to fully realize our long-term vision.
As a part of the dev team, you'll build the framework for both our clients and our team to succeed. You will be a pillar in our company as we grow and make our product the best product on the market!
In this role, you will:
Take ownership for delivering multiple features, their quality and assist team members in delivering their features
Recommend improvements to data quality, processes and code to make it easier to use, manage and maintain
Experience designing and developing data models and data migrations
Experience managing and maintaining ETL or ELT processes (Hevo, BigQuery, Looker, etc.)
Hands-on development experience with SQL, relational database design (ERDs)
Must be able to write SQL & contribute code
Be product-minded; focus not only on what needs to be built but why something needs to be built
You may be a great fit for this role if you have:
3 or more years of professional experience as a data analyst or data engineer
Cloud-Based Data Tools e.g. Hevo, BigQuery, Looker, etc.
Database technologies: SQL, RDBMS / Relational databases (Postgres, Amazon RDS / Aurora, etc.)
Nice to have experience: Python, Java, Typescript or another modern OOP language
Cloud infrastructure: AWS
Source control: Git
Working at Take Command
We’re excited to build a team and culture that reflects our values! We offer competitive pay, health benefits, and equity options to share with this position. We want you to be an owner along with us in revolutionizing healthcare.
A generously funded ICHRA for medical, dental, and vision premiums and medical expenses. You get to use our own product and we think that’s so exciting and rare!
Unlimited personal vacation in addition to regular company holidays.
401(k): 90-day eligibility for 2% match that vests immediately!
Access to LinkedIn Learning.
We have two beautiful offices conveniently located near The Hill (Dallas) and Holly Commons (Austin). The kitchen is well-stocked and we've designed the space to have lots of different areas to work!
Paid parental leave for new parents.
Flexible on where you work – we hope to see you around the office on average 3 days a week especially when white-boarding or building relationships with your colleagues, but you also have the ability to work from home or wherever you'd like when you need to get focus work done.
More About Us
We secured our Series A funding in 2021 and are thrilled to be able to expand our team. Despite being a small startup in a land of health insurance giants, we’re the recognized industry leader for what we do (health insurance reimbursements) and passionate about bringing it to market because we know we can help fix a broken system and improve our clients’ wellbeing and health outcomes.
We’ve been featured in The New York Times, The Wall Street Journal, The Dallas Morning News, and other national healthcare publications and are excited about our growth opportunities.
Take Command knows diversity and inclusion among our teammates is integral to our company’s success and growth. Our vision is to recruit, develop, and retain the best team from a diverse candidate pool.
This has mostly been about us, but we’d love to hear from you-we can’t wait to hear your story!
Take Command is an equal opportunity employer! We celebrate diversity and are committed to creating an inclusive environment for all employees.
Xrygn4NXMr","$125,000 /yr (est.)",1 to 50 Employees,Company - Private,Insurance,Insurance Agencies & Brokerages,2014,Unknown / Non-Applicable
"DSMH LLC
5.0",5.0,Remote,"Data Engineer (AWS, Python)","Title: Data Engineer
Duration: 12+ Months
Technical Skills
(Required)
Four years or more of experience in data engineering and/or software
development.
Three years or more of experience with development, operations, or
architecture in AWS using Python.
Experience with development and delivery of microservices using serverless
AWS services (S3, RDS, Aurora, DynamoDB, Lambda, SNS, SQS, Kinesis,
IAM)
Strong competency in testing, including unit testing to coverage standards and
integration testing.
(Desired)
Software development experience with object-oriented development and design
patterns
Experience with serverless design in AWS
AWS technical certifications (Developer Associate, Solution Architect Associate)
Familiarity with machine learning and data design to support machine learning
Experience with productionizing machine learning workloads
Experience with ElasticSearch/ELK, Kibana, Grafana, and/or Prometheus
Experience with OpenTelemetry
Soft Skills
(Required)
Verbal and written communication skills, problem solving skills, customer
service and interpersonal skills.
Basic ability to work independently and manage one’s time.
(Desired)
Ability to work collaboratively in a complex, rapidly changing, and culturally
diverse environment.
Ability to clearly communicate complex technical ideas.
Comfortable working in a dynamic environment where digital is still evolving as
a core offering.
Job Type: Contract
Pay: $50.00 - $55.00 per hour
Experience level:
6 years
Schedule:
Monday to Friday
Experience:
AWS: 5 years (Required)
Python: 5 years (Required)
Work Location: Remote",$52.50 /hr (est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,$1 to $5 million (USD)
"Iron Bow Technologies
3.5",3.5,"Washington, DC",Sr. Data Center Engineer,"Iron Bow Technologies is seeking a Sr. Data Center Engineer to support our Federal customer, The Department of Justice (DOJ), Criminal Division (CRM), in Washington, DC. The selected candidate will support our flexible and scalable solution with the DOJ CRM to extend cloud-native features to existing legacy and next-generation applications while also future proofing the infrastructure to easily accommodate future requirements, such as transitioning to a public cloud.
This position is hybrid and requires 2 days a week onsite at the customer location in NW Washington, DC and support at the customer data center as needed in Sterling, VA.
Essential Duties and Responsibilities:
Assess and provide VMware solutions to include vCenter, vSphere, SRM, ESXi
Primary storage solutions and data file storage management, NAS file storage solutions
Data Center blade servers and rack servers
Server virtualization
Experience:
10 years of related experience providing system administration support in a virtualized environment, including at least 6 years of VMWare solutioning, including vCenter, vSphere, SRM, and ESXi
Data Center operations experience, with SAN and NAS
Experience supporting primary storage solutions and data file storage management, NAS file storage solutions
Hands on experience with Data Center blade and rack servers
Ability to work staggered shift hours when needed to support critical coverage and/or time-sensitive needs
Be a member of our Team providing coordinated 24x7 IaaS on-call support
U.S. Citizen
Currently hold or obtain/maintain a Public Trust High clearance
Education:
BS in Computer Science, Information Science, or other relevant discipline is preferred – additional years of experience may be considered in lieu of the degree
Preferred Certifications:
VMware Certified Professional
Primary storage certifications such as HPE, Dell-EMC, NetApp, or Pure Storage
VCP-DCV
Travel Requirements:
No travel
PRINCIPALS ONLY. NO AGENCIES PLEASE. MUST BE A US CITIZEN ABLE TO OBTAIN A PUBLIC TRUST SECURITY CLEARANCE.
Iron Bow Technologies is a leading IT solution provider dedicated to successfully transforming technology investments into business capabilities for government, commercial and healthcare clients. Our depth of technical expertise, global reach and strategic partnerships with industry leaders as well as disruptive technology partners uniquely positions Iron Bow to target the right solution to ensure your successful business outcome. We partner with our clients from planning and implementation through ongoing maintenance and management to deliver solutions that are strong, flexible and on target .
Iron Bow Technologies is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.
#LI-TM1",#N/A,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,1983,$500 million to $1 billion (USD)
"shaped.ai Inc.
4.0",4.0,"New York, NY",Data Engineer,"Hello! We’re building the future of content discovery infrastructure with AI — come join us!

Shaped (YC W22) is an API for developers to seamlessly add personalized ranking and recommendation to their products. These frictionless discovery experiences help end-users find what they want faster, and, consequently, grow conversion and engagement business metrics. Behind the scenes, we’re building state-of-the-art ML infrastructure for training and deploying large AI models to power our personalization engine.
We’re a seed stage start-up backed by top-tier investors (e.g. Y-Combinator, Susa Ventures, Tribe Capital etc…) and executives (e.g. Google, Amazon, Uber, Dropbox etc…). Our team comes from Meta, Google, Apple and Uber. We’re a remote team but have a small office in Brooklyn, New York.
We’ve been lucky to have a huge amount of success since our initial launch earlier this year and now looking to expand our real-time data engineering capabilities. We are looking for a data engineer to design, build and optimize Shaped's real-time streaming data ingestion endpoints. You will be a founding engineer working on state-of-the-art infrastructure. As one of Shaped’s early employees you will help shape our product, culture and vision.
This role is for a full-time data engineer at $120k - $200k a year and 0.1 - 0.5% equity.

Responsibilities:
Manage the unification of our batch and real-time data processing pipelines.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Requirements:
Bachelor's in computer science, data science or mathematics related field. Master's degree or PhD will be advantageous.
5+ years of data engineering experience, ideally applied to machine learning use-cases.
Experience with AWS or common cloud technologies
Proficient with Python, SQL.
Extensive knowledge of both offline and online large scale data processing frameworks: Spark, Beam, Flink.
Extensive knowledge with stream technologies like Kakfa or Kinesis.
Excellent written and verbal communication skills.
We’re excited to work with you. Come build the future of AI and discovery with us!","$160,000 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Fulcrum Analytics
4.3",4.3,Remote,Data Engineer,"At Fulcrum, we provide cutting-edge data science consulting and software solutions to help companies solve challenging business problems. Through the utilization of our innovative products and dedicated team, clients are able to operate more efficiently, manage risk, and make informed business decisions every day. Fulcrum Analytics has stood at the forefront of data, analytics, and innovative software for over 25 years, offering sophisticated solutions, groundbreaking technologies, and winning strategies that help companies achieve their targeted results.
Role Description:
We are seeking a highly-skilled data engineer with experience in data analytics and data science. The ideal candidate will be responsible for building and maintaining data pipelines, designing and implementing databases and data models, and developing data-driven solutions to drive business growth for our clients.
Responsibilities:
Design, build, and maintain efficient, scalable, and reliable data pipelines using SQL, or programming languages such as Python or Java.
Develop and maintain databases and data models to meet clients’ analytical needs.
Identify data quality issues and develop processes to ensure data accuracy, completeness, and consistency.
Work with cross-functional teams to define and implement data integration strategies for new and existing systems.
Develop and maintain automated testing and deployment of analytics code and data pipelines.
Collaborate with data scientists to develop and implement machine learning models, statistical models, and other advanced analytics techniques.
Communicate analytical results and recommendations to technical and non-technical stakeholders through presentations and visualizations.
Stay up to date with emerging technologies and industry best practices, and recommend tools and processes that can improve data engineering and analytics processes.
Qualifications:
Bachelor's or master's degree in computer science, statistics, data science, or a related field.
2+ years of experience in data engineering, data analytics, and data science.
Strong programming skills in Python, with experience in building data pipelines using frameworks such as Airflow, Glue, Lambda, Athena, and BigQuery as well as data platforms such as Hadoop, Snowflake, and GCP.
Experience working with SOAP or RESTful web services.
Familiarity with version control, in particular Git.
Experience in designing, building, and optimizing relational and NoSQL databases.
Familiarity with data visualization tools, such as Tableau or Power BI.
Ability to understand and transform complex data into actionable insights.
Strong analytical and problem-solving skills, with a focus on delivering practical solutions to business challenges.
Ability to learn and solve problems independently.
Excellent communication and collaboration skills, with the ability to explain complex concepts to both technical and non-technical audiences.
If you are passionate about data analytics and want to work in a dynamic environment with talented colleagues, we encourage you to apply for this role. We offer competitive salary and benefits packages, as well as opportunities for professional growth and development.


Position Type: Full-time salaried
Reports to: Senior Data Engineer
Base Compensation: $110,000 - $140,000
Benefits: Medical, Dental, Vision, 401k with Match, Bonus, Unlimited PTO
Location: Remote within the United States of America; Eastern time zone preferred","$125,000 /yr (est.)",1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,1993,$5 to $25 million (USD)
DocsInk LLC,#N/A,"Wrightsville Beach, NC",Data Engineer,"DocsInk is on a mission to provide a collaborative and exciting alternative to the traditional healthcare model. Through the complete virtualization of healthcare organizations, we are equipping providers with the ability to deliver better patient care with speed, efficiency, and accuracy. With the widespread focus on providing value-based care, DocsInk is making it possible to provide superior care while also maximizing reimbursements. In today’s environment, shifting the way we approach healthcare is essential and DocsInk is on the cutting edge of where the industry is headed. We are excited to be growing our team and are looking for driven individuals with a passion for making a difference in the healthcare industry.

As our Data Engineer, you will be in the driver’s seat as our team creates elegant and forward-thinking software solutions for our healthcare customers. You will work with fellow developers, designers, training & support, quality assurance, and management in a fast-paced environment to take our products to the next level and exceed expectations. We are looking for someone who is incredibly talented to do what most cannot and yet humble enough to easily work with peers, accept guidance from more experienced staff, and mentor more junior staff. If designing and developing complex integration efforts is what gets you out of bed each morning, then we want you to help us reshape healthcare.

People are our most important asset and the number one reason we all love working at DocsInk. As a team, we value accountability, transparency and collaboration. We have agile teams, with clear, outcomes-focused goals. Delivering innovative solutions and superior service are at the forefront of what we do, and the collective ideas of the team allow us to continue to excel in a fun and energetic environment.

Professional Requirements and Responsibilities:

Role / Responsibilities:

Implement robust ETL solutions that integrate healthcare data feeds using tools such as Apache NiFi, NextGen Connect (JavaScript), PostgreSQL, MySQL, and Linux shell scripting
Configuring and managing deployments to AWS infrastructure consisting of both traditional servers and serverless components. CI/CD tools include Bitbucket and AWS Code Deploy
Building direct interfaces with hospitals and EMR vendors utilizing industry standards such as HL7 2.x, C-CDA, and FHIR as well as implementing interfaces that consume formats such as XML, JSON, and delimited files
Utilizing APIs and web services to permit bi-directional flows of data to and from our partner organizations
Monitoring operational ETL processes, triaging and resolving issues as they arise to meet SLAs
Writing PL/SQL functions and packages to automate workflows in a Postgres environment linked with web technologies
Maintaining data standards (including adherence to HIPAA), documenting work, and championing process improvements
Work as a project team member to jointly deliver high-quality business solutions consistent with project objectives and constraints

Knowledge and Skills Required:

4+ years experience with HL7 2.x
2+ years experience with Apache Nifi
2+ years experience with NextGen Connect
Advanced knowledge of PostgreSQL, MySQL, AWS Code Deploy, HL7 2.x, C-CDA, FHIR, and HIPAA data compliance

Qualifications and Other Attributes Required:

Degree in Computer Science or equivalent field, or equivalent work experience
Highly self-motivated team player who takes initiative
Strong sense of personal accountability; contributes beyond job role and responsibilities
Excellent problem-solving skills; meticulous & methodical","$85,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Kastech Software Solutions Group
4.0",4.0,"Dallas, TX",Data Engineer,"In KastechSSG we are are seeking for a Cloud DB Platform Engineer with Data Enablement Services experience ($50/hour), to work in a Hybrid Schedule (3 days on-site, 2 days home office) for one of our partner companies!
Hybrid Schedule info: onsite 3 days per week from list of locations: Charlotte NC, Dallas TX, Chandler AZ, Newark, NJ - Must be local to one of these locations.
** Contract would be $50 per hour on W2 (18 months). American Citizens or GC Holders Only**
3-5 years experience in:
Terraform, Azure/GCP, DevOps Engineering, ETL, Client/AI, Data lake, Data architecture.
GCP Key Services Experience (ie Big Query, Composer, Vertex AI, DataProc, Data Catalog, Notebooks).
Azure Key Service Experience (ie Synapse, Purview, Data Lake, Logic Apps, Cognitive Services).
Job Type: Full-time
Salary: $50.00 per hour
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you American? If not, please specify your visa type
Would you be willing to work under W2?
Experience:
Terraform: 3 years (Preferred)
Azure Data Lake: 3 years (Preferred)
GCP Key Service Exp: 3 years (Preferred)
DevOps: 3 years (Preferred)
Work Location: In person",$50.00 /hr (est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2008,$25 to $100 million (USD)
"DataArt
4.6",4.6,Remote,Data Engineer,"CLIENT
Our client is a pioneer in US schools’ education since 2000, it is leading the way in next-generation curriculum and formative assessment. They develop a number of solutions and interactive web products for teachers, students and their parents.

The company is technology-driven with a huge number of software engineers involved in product development. They have a very solid approach to technology, employ best practices and processes, with a focus on cutting-edge frameworks, languages and tools.

We invite to the company, not a project
TEAM
There are multiple autonomous and empowered teams who work with their own part of the product and choose their own way of doing things and they expect people to be proactive and good team players.
Responsibilities
Building well-tested and optimized ETL data pipelines for both full and delta extraction
Collaborating with data analysts and learning scientists to store, aggregate, and calculate captured students’ work
Contributing to leading industry data standards, such as Caliper Analytics or xAPI
Improving our deployment and testing automation data pipelines
Requirements
3+ years of professional software development or data engineering experience
Strong CS and data engineering fundamentals
Proven fluency in SQL and a development language such as Python
Experience with the following storages: Snowflake, AWS Storage Services (S3, RDS, Glacier, DynamoDB)
Experience with ETL/BI: Airflow, DBT, Matillion, Looker
Experience with Cloud Infrastructure: AWS Kinesis, Lambda, API Gateway, Terraform
Understanding of ETL/ELT pipelines and Data Warehousing design, tooling, and support
Understanding of different data formatting (JSON, CSV, XML) and data storage techniques (3NF, EAV Model, Star Schema, Data Lake)
BS in Computer Science, Data Science, or equivalent
Experience in education or ed-tech
Proven passion and talent for teaching fellow engineers and non-engineers
Proven passion for building and learning: open source contributions, pet projects, self-education, Stack Overflow
Strong communication skills in writing, conversation",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,1997,$100 to $500 million (USD)
"Virtualan Software LLC
4.5",4.5,"Atlanta, GA","Sr. Data Engineer (Python,Spark & Databricks)","Role : Senior Data Engineer (Python, Spark & Databricks)
Location : Hybrid (Atlanta, GA)
Pay: 60-70$ on C2C / 1099
RESPONSIBILITIES:
Create or modify the conceptual, logical and physical data models.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of sources like Hadoop, Spark, AWS Lambda, etc.
Lead and/or mentor a small team of data engineers.
Design, develop, test, deploy, maintain and improve data integration pipeline.
Develop pipeline objects using Apache Spark , PySpark and/or Python.
Design and develop data pipeline architectures using Spark and related AWS Services.
Communicate effectively with client leadership and business stakeholders.
Participate in proposal and/or SOW development.
REQUIREMENTS:
5+ years of professional work experience designing and implementing data pipelines in a cloud environment is required.
2+ years of experience migrating/developing data solutions in the AWS cloud is required.
1+ years of experience building/implementing data pipelines using DataBricks is required.
Expert level knowledge of using SQL to write complex, highly-optimized queries across large volumes of data.
Hands-on object-oriented programming experience using Python is required.
Professional work experience building real-time data streams using Spark.
Knowledge or experience in architectural best practices in building data lakes.
Bachelor or Master degree in Computer Science, Engineering, Information Systems or relevant degree.
Hybrid work schedule, and must live in the Atlanta, GA metro area. Must be open to up to 25% national travel to client locations when on engagements outside of the Atlanta, GA area.
Job Type: Contract
Pay: $60.00 - $70.00 per hour
Schedule:
Monday to Friday
Application Question(s):
Apply only if you are interested in Hybrid Role in Atlanta, GA. Interested ?
Experience:
Data Engineer: 4 years (Required)
Databricks: 1 year (Required)
Willingness to travel:
25% (Required)
Work Location: On the road",$65.00 /hr (est.),1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,2019,Less than $1 million (USD)
Redzara.us,#N/A,"Atlanta, GA",Azure Data Engineer,"Responsible for developing new and modernizing existing applications to Azure Cloud Platform.
The ideal candidate must demonstrate in-depth knowledge and understanding of Database concepts.
Requirements:
7+ years of experience in Data Warehousing and Business Intelligence
5+ Years in BI development leveraging the Microsoft SQL Server. Needs to be an expert in T-SQL
5+ Years in SSIS and use of Stored Procedures.
1-2 Years in Azure Data Factory, SQL VMs, build and maintain ETL
Ingest and aggregate data from both internal and external data sources to build our data lake
Strong understanding data modeling (i.e., conceptual, logical, and physical model design, experience with Operation Data Stores, Enterprise Data warehouses and Data Marts).
Job Type: Contract
Pay: $43.49 - $57.00 per hour
Compensation package:
1099 contract
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Atlanta, GA 30334: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure Data Factory: 3 years (Required)
Azure: 3 years (Required)
Data warehouse: 1 year (Preferred)
Work Location: Hybrid remote in Atlanta, GA 30334",$50.25 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Integrated Technology Strategies, Inc.
4.0",4.0,"New York, NY",Data Engineer,"Integrated Technology Strategies is a provider of information technology consulting. Digital transformation, improving business performance, developing strategies and enhancing value is at the core of what we are.

Integrated Technology Strategies is looking to hire a Data Engineer to be a part of our Consulting team.

We are looking for someone passionate about data is focused on developing the platform for critical data products, including real time business metrics and analytics capabilities. The role requires supporting and collaborating with groups including Data Analytics/BI/Product, as well as our core backend API team. The individual will not be afraid to think out of the box and will play a key role in technical decision making. We are highly focused on giving ownership and responsibility to autonomous teams, using the right tools for the job, and building flexible architectures.

Responsibility:
Continue to evolve the internal Reporting and Analytics platform on top of Snowflake on AWS infrastructure.
Experience in Architect, design and implementing scalable ETL and data processing systems to handle the big data ecosystem including data collection, processing, ETL and Data warehouse.
Build soft real time capabilities and insight into product metrics to help product managers and BI/Analytics understand and optimize product features and guide product decisions.
Participate and contribute to the capabilities and engineering priorities across the organization.
Contribute to the codebase and participate in code review.
Build analytics tools that utilize the data pipeline to
provide actionable insights into operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product,
Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Reporting to the Senior Director, Software Engineering, you’ll be responsible for overseeing engineering product quality and delivery and setting and overseeing technical standards for teams who are working on everything from customer-facing applications.

Skill Requirements:
Solid understanding of real time data processing with Kafka, Spark and Flink and batch data processing frameworks on EMR and Snowflake.
Passion for building world-class data platforms that support a global customer base
Solid engineering background and understanding of programming languages such as Python, Java or equivalent
5+ years of progressive experience in data infrastructure development, with a track record of successful high-quality deliveries
Experience of working in an agile environment and embracing engineering best practices
Ability to apply both technical competence and interpersonal skills to achieve business outcomes
High emotional intelligence, sound temperament, and professional attitude
Strong understanding of SQL, experience with key databases such as Snowflake, MS-SQL and Postgres
Knowledge of the internals of how database systems work to design models for varied use cases.
Experience with CI and CD in an AWS environment with Terraforms
Experience with key Data technologies, such as Sqoop. Kafka will be a plus
Proven experience in building secure data platforms
Bachelor’s degree in Computer Science or equivalent","$110,128 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$5 to $25 million (USD)
"Fuge Technologies Inc
4.7",4.7,"Jersey City, NJ",Big Data Engineer,"Big Data Engineering Technical Lead
Mandatory Skills
Big Data – Spark, Kafka, Databricks, Hive, Nifi
AWS, Java, Scala
Client Interview Needed for Selection (yes / No)
Yes
Detailed JD (Pl share the Detailed Description, 1 liner JD will not work)
10+ yrs experience as Data Engineer
Extensive experience in spark on data processing
Kafka design experience like monitoring & parallelism.
Comfortable with Python/Java/Scala coding experience
Crisp / clear cut communication
Flexible with learning new technology
Flink experience is a plus
AWS experience is nice to have
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Jersey City, NJ 07302: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",$72.50 /hr (est.),1 to 50 Employees,Contract,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Egrove Systems Corporation
4.4",4.4,"Dallas, TX",Data Engineer Flink,"Java, Bigdata, Azure & Flink
Flink Sr. Developer, who has implemented and dealt with failure scenarios of processing data through Flink.
A senior who can help his Flink development team, guiding and helping them implement custom solutions through Flink.
Seasoned Java developer who knows about all aspects of SDLC.
Worked on integrations of other technologies with Flink, eg: Kafka, MongoDB, etc
Azure experience in the areas of Messaging, Data processing, preferably on Flink or on Databricks.
Job Type: Contract
Salary: From $51.83 per hour
Compensation package:
Monthly bonus
Experience level:
11+ years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
Work Location: On the road
Speak with the employer
+91 732)-860-7733",$51.83 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Internet & Web Services,#N/A,Unknown / Non-Applicable
National Group Corporation,#N/A,"Dallas, TX",Data Engineer,"Who are we?
ReAlign Insurance Holdings is a leading primary underwriter of program administrator managed specialty insurance programs in the U.S. With the extensive knowledge and experience our team has in building and managing carriers and programs we are able to reach decisions on program opportunities quicker than our competition.
Why choose ReAlign?
At ReAlign, we believe that we are only as strong as our people. We strive to create an inclusive and welcoming culture where employees feel comfortable and empowered to be themselves. We support a diverse and inclusive work environment, with a focus on a healthy work-life balance. We understand that comprehensive benefits are important which is why we offer a benefit package that includes: medical, dental, vision, company paid short and long-term disability, life insurance, health spending account, 401(k) match every pay period, PTO, paid holidays, floating holidays, tuition reimbursement, and referral bonuses.
Job Summary
The Data Engineer position will involve data exchange process analysis, design, development, unit testing, and support. This role would work with other data team members on database ETL processes and reporting needs in both the on-premise database environment as well as the Azure data environment. This position requires strong technical and analytical skills to review business processes and data, recommend solutions, conduct functional and code analysis, and lead and perform database development tasks. This position will also involve production support and require experience within a DevOps environment.
Essential Functions
· Design, development, and support data ETL processing for both external and internal customers
· Support the internal and Azure cloud-based data warehouse platforms
· Work with DBA/System Admin to develop and maintain automated data processing
· Design and analyze queries and stored procedures
· Write and support SSRS and PowerBI reports
· Work in a data development life cycle environment
Work with stakeholders and data analysts to understand business requirements
Work with end users and the testing team to perform unit, functional, and integration testing and validation prior to production implementation
Troubleshoot and resolve operational problems with a focus on delivery
· Communicate solutions and architectural design to IT leadership
· Work in both Kanban and Scrum Agile environments
· Continually improve processes and solutions, seeking ways to use new tools, features, and capabilities
Job Requirements
3-10 years of experience as a Database Developer
Highly proficient in writing advanced SQL queries, stored procedures, SSRS reports, and ETL processes
Strong knowledge of relational and transactional database architecture
Strong technical and analytical skills
Experience with the Microsoft Azure and PowerBI platforms and Agile development methodology a plus
Bachelor’s Degree in computer science, database development, or related field preferred
Detail oriented with the objective of delivering a solution to a problem in a timely and efficient manner
Self-driven, willing to take initiative when an opportunity presents itself
Strong collaboration and communication skills and the ability to work in a team environment a must
Ability and willingness to work in a partial on-prem and partial remote office environment
Experience in the Property and Casualty Insurance industry a plus
Proficient with standard desktop software used in a business environment (MS Word, Excel, Visio, etc.)
Must be able to work in a hybrid in-office and remote work environment
Must be authorized to work in the United States
Job Type: Full-time
Pay: $90,000.00 - $125,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Health savings account
Life insurance
Paid time off
Retirement plan
Vision insurance
Experience level:
3 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you currently local to the Dallas area and able to commute to the office 2 to 3 times per week? (Deal breaker)
Experience:
writing advanced SQL queries and ETL processes: 3 years (Required)
cloud database platform (i.e. Azure, AWS): 1 year (Preferred)
Azure Data Factory: 1 year (Preferred)
Work Location: In person","$107,500 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Quadrant Resource
4.1",4.1,Remote,Data Engineer,"Job Description:
4-6 years of experience as a Data Engineer in the Snowflake-Fivetran-dbt stack
• Possess a solid understanding of end to end Data ingestion, ETL, data analysis, Reporting processes
• Expert in ETL development using Fivetran and experienced in implementing data pipelines for Snowflake Cloud DWH
•Experienced in using dbt for data transformation
•Expertise on Tableau/Metabase for building reports/dashboards
•Expert in troubleshooting and resolving ETL issues, data load failures/problems and transformation translation problems
•Excellent communicator with the ability to work effectively with distributed teams
Job Type: Full-time
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Data Engineer: 9 years (Required)
Fivetran: 8 years (Required)
DBT: 7 years (Required)
Snowflake: 6 years (Required)
Work Location: Remote",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,2004,Unknown / Non-Applicable
"Hiscox
4.1",4.1,"Atlanta, GA",Senior Data Engineer,"job description:
You will be a senior member of our US IT Team data engineering practice. You will be an early member of this team and will work closely with the Director of Data Engineering to help establish it. We will have our work cut out for us delivering planned work, building the practice and evolving our technical landscape. This is an exciting opportunity for the right individual to join at the very beginning and grow with the new team.
As a senior you will be expected to lead, mentor others and deputise for the director of data engineering when required. Yoi will also continue to contribute as a hands-on engineer. You will have the knowledge, best earned through 5+ years of experience, to truly inhabit this senior role in an ambitious team.
What you’ll be doing:
Working with business stakeholders and our delivery team to understand high value business problems that can be solved through the application of data processing and analytical systems.
Developing, expanding and evolving our existing databases and ETL pipelines.
Working with architects to design, build and support a transformational cloud-based data platform for the business.
Being a core and professional member of the new data engineering practice
Understand business requirements and help refine into development tasks and estimate their complexity.
Research, evaluate and adopt new technologies with a right tool for the job mentality.
Focus on both speed of delivery and quality, with suitable pragmatism – ensuring your solutions are always appropriate and not too complex or over-engineered.
Quick progression of projects from proof-of-concept to post-production stage.
Communication and presentation of ideas to colleagues in all parts of the wider tech team.
Participating in code reviews for the data engineering practice.
Act as a servant leader in our practice – ensuring team members have
person specification:
Knowledge of, or ability to rapidly adopt our core languages for data engineering – SQL and/or Python.
Experience working using a modern DevOps approach. You should not just have written data code but also the CI/CD pipelines necessary to test and deploy that code in a professional environment.
A robust understanding of core data engineering topics – ETL vs ELT, structured and unstructured data, data quality and data governance.
Ability to contribute to all aspects of a solution – design, infrastructure, development, testing and maintenance.
The ability to design and advocate for technical solutions to business problems.
Effective collaboration with technical and non-technical team members through agile ceremonies – roadmap planning, feature workshops, backlog elaboration, code review.
Understanding of cloud technology and significant hands-on experience working with it. You must understand how cloud-native solutions must be built differently from traditional ones.
Track record of taking initiative and delivering projects end-to-end; clear evidence of being self-driven and motivated
Immense curiosity, high energy and desire to go the extra mile to make a difference.

Desirable Skills
Beyond the required skills we are open to individuals of diverse talents. Experience with additional technologies, data science knowledge or a business background are all valued. We want to know how your unique abilities can contribute to our team.
Our Technology
We operate in a diverse technical landscape and are looking for flexible engineers who can adapt to and use many different tools. We would not expect any engineer to be familiar with the entire tech stack – no engineer can. Instead, we seek people with a good understanding of data structures and algorithms and the ability to apply this knowledge in learning new tools.
We have an existing data landscape involving both MS SQL Server and Oracle databases. We use SSIS for ETL. We will also need to source data from our integration layer – webMethods.
If some of that sounds like it needs bringing up to date, we know! We hope to start building a modern cloud data platform to realise the full value of our data. The technologies we use for that platform are still to be decided. Likely it will be based on Snowflake with Azure Data Factory for ELT. Other parts of our business have had success using Prefect for orchestration and dbt for transformation and we will reuse where it is sensible.
We prepare data for use by analysts working with a variety of tools – Tableau, PowerBI, and even Excel.
We take a DevOps approach and strive for continuous integration / continuous deployment. We use Azure Pipelines to deploy our code. We deploy infrastructure the same way using Terraform and Docker.
Our technology landscape is not fixed. Our engineering and architecture teams drive the technology we adopt.


Salary range $140,000 - $170,000","$155,000 /yr (est.)",1001 to 5000 Employees,Company - Public,Insurance,Insurance Carriers,1901,$1 to $5 billion (USD)
"Intersec
4.5",4.5,Remote,Senior Data Engineer,"Architect and implement highly scalable data pipeline to streamline migration of data from source data repositories to the centralized data platform
Implement agreed upon data models (e.g., data marts) to enable faster and efficient consumption of data by downstream consumers
Conduct performance tuning to optimize data migration techniques and ensure adherence to identified service-level agreements (SLAs)
Define and implement automated validation checks to ensure data integrity between source and target
Assist in onboarding consumers with existing technology to connect to the platform
Setup predictive alert mechanism for pipeline degradation for IMF pipelines.
Enhance data quality framework for data pipelines and derived datasets
Enhance Data Audit Balance and Control mechanism for data pipelines
Enhance pipelines to be certified copies of authoritative source
Populate metadata for the Enterprise with particular focus on the Data Sources being onboarded to EDP (Technical and Business Glossaries) related to IMF Modernization Programs
Validate correct lineage for datasets
Required:
*8+ years of Databricks, Data Modeling, and/or Metadata management experience
* Familiarity with architecting and developing real and near-real-time solutions
* Bachelor's Degree in Engineering, Mathematics, computer Science, Statistics or other quantitative discipline, or 4 years equivalent professional experience
* US Citizenship and ability to gain a public trust
Preferred:
* Familiarity with IRS systems and ETL pipelines
* Ability to work independently and manage multiple tasks
* IRS MBI Clearance with IRS Laptop and Informatica Installed
Job Type: Full-time
Salary: $70.00 - $75.00 per hour
Schedule:
Monday to Friday
Application Question(s):
Are you a US/GC card holder?
How many years of Databricks, Data Modeling, and/or Metadata management experience do you have?
Work Location: Remote",$72.50 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,Unknown / Non-Applicable
"spar information systems
3.5",3.5,Remote,Azure Data Engineer,"Role: Sr Azure Data Engineer
Location: Remote
Duration: 3 Months Contract to hire Full Time (W2 Only)
Must have 11+ IT Experience
Required Skills:
3+ years of experience in data software development, programming languages and developing with big data technologies
2+ years of experience designing and building on existing and new data applications
2+ years of experience in Cloud DevOps concepts, Cloud Services and Architecture, and Azure/AWS/GCP DevOps Operational Framework
1+ years of experience in open-source data tools and frameworks, or one of the following: .net Core, asp.Net, Angular, or Express.
Thanks & Regards,
Arvind Kumar Bind
Cell: 732 716 7403 (Text)
Direct Number:- 469-750-0607
Email : Arvind.B@sparinfosys.com
Job Types: Full-time, Contract, Permanent
Pay: $120,555.79 - $150,000.00 per year
Benefits:
401(k)
Health insurance
Paid time off
Experience level:
11+ years
Schedule:
8 hour shift
Experience:
Big data: 3 years (Required)
SQL: 1 year (Required)
Data lake: 3 years (Preferred)
Work Location: Remote","$135,278 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2012,$25 to $100 million (USD)
"Barracuda Networks Inc.
3.6",3.6,"Alpharetta, GA",Data Engineer,"Job ID: 23-579

Come Join Our Passionate Team! At Barracuda, we make the world a safer place. We believe every business deserves access to cloud-enabled, enterprise-grade security solutions that are easy to buy, deploy, and use. We protect email, networks, data and applications with innovative solutions that grow and adapt with our customers’ journey. More than 200,000 organizations worldwide trust Barracuda to protect them — in ways they may not even know they are at risk — so they can focus on taking their business to the next level.
We know a diverse workforce adds to our collective value and strength as an organization. Barracuda Networks is proud to be an Equal Opportunity Employer, committed to equal employment opportunity and equitable compensation regardless of race, gender, religion, sex, sexual orientation, national origin, or disability.
Envision yourself at Barracuda
We currently have an exciting new position open for a Data Engineer. This person will assist in driving technology innovation for engineering projects and work on architecture of AWS, Elastic and Kubernetes. They will also work closely with the Technology Architect to create and drive the Architecture Review Board.
What you’ll be working on
Determine internal / external customer data needs and identify system specifications
Analyze the needs of large systems and breaking them down into smaller manageable parts
Responsible for designing the journey of data from various sources to the database and everything in between
Plan and design the structure of data, brainstorm with team
Analyze best technology products and systems for internal teams
Help provide build vs buy decisions on data architecture
Communicate data requirements to engineers; explain data architecture to them and provide assistance throughout SDLC
Choose / maintain suitable software, hardware and integration methods
Help resolve technical problems as and when they arise
Review testing procedures to ensure data performant and error free
Ensure that database satisfy quality standards and procedures. Setup such standards.
Work with senior IT personnel to devise plans for future IT requirements of the organization
Produce progress reports on key infrastructures - Elastic and Data Pipelines
What you bring to the role
Bachelor Degree or Post Graduate in Computer Science, or relevant work experience
5+ years of experience in an Engineering / Operations heavy organization
1+ years of experience as a Data Analysis and Data Design responsible for data architecture of a SaaS application
Experience in understanding and designing data sharding in a distributed database
Some Hands-on experience in software development and system administration
Knowledge of strategic IT solutions
Familiarity with programming languages like JavaScript or Java or Python
Familiarity with Linux OS and Windows alike
Experience in cloud computing and cloud Technologies (AWS, GCP, Azure)
Excellent communication skills – you should be adept at listening to, understanding, and explaining key concepts to managerial and technical resources.
Good Anticipation and problem-solving skills
Ability to prioritize and manage time – responsible for multiple streams of work at the same time
Relationship building - Ability to form a good rapport with internal and external stakeholders
In-depth knowledge about quality standards and security best practices

Preferred Skills:
Expert level Understanding of Elastic eco system - Elasticsearch, Logstash, Kibana, Filebeats
Understanding of secure tunnel communication using VPN
Experience designing data at scale
AWS experience is preferred in Cloud Technologies
What you’ll get from us
A team where you can voice your opinion, make an impact, and where you and your experience are valued. Internal mobility – there are opportunities for cross training and the ability to attain your next career step within Barracuda.
Equity, in the form of non-qualified options
High-quality health benefits
Retirement Plan with employer match
Career-growth opportunities
Flexible Time Off and Paid Time Off benefits
Volunteer opportunities
#LI-Remote","$98,161 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,$100 to $500 million (USD)
"RelMap Consulting
4.8",4.8,"Addison, TX",Senior Data Engineer (Hybrid),"The data engineer entails:
Advanced SQL Server Development
Ability to create and modify views
Ability to create and modify custom functions
Ability to write complex queries, including joins, temp table processing, and common table expressions (CTE’s)
Ability to create and modify stored procedures using TSQL
Ability to analyze and optimize query performance using indices
Ability to use Red Gate’s SQL Compare to perform a schema (AKA “do a manual roll”) between one server an another, and between revision control and a database
Ability to stand-up new database server VM’s in Linux and Windows using VMWare
Ability to backup and restore a database in a Linux and Windows environment
Ability to manage and script automated database jobs
Ability to use PowerShell / Bash to interact with databases from the command line
Addition skills that would be desired but not required
· Experience with Redis
· Experience with MongoDB / CosmosDB
· Experience with Azure and Azure SQL
· C#, JavaScript (NodeJS / ES6), or Python experience
Job Type: Contract
Pay: $70.00 - $80.00 per hour
Compensation package:
Hourly pay
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Addison, TX: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person",$75.00 /hr (est.),1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2010,$1 to $5 million (USD)
Cox powered by Atrium,#N/A,"Atlanta, GA",AWS Data Engineer- Hybrid,"Minimum Qualifications:
Bachelor’s degree or equivalent work experience
A minimum of 3+ years’ experience in Microsoft Windows/ SQL Server Technologies, .Net development, AWS Administration.
Experience working on 24x7 environments oriented towards a zero downtime target.
Working knowledge or previous administration of SQL 2016-SQL 2022 and Windows Server 2012+ preferred.
Ability to work with minimal direction, in a team environment.
Performance tuning for AWS/DataLake systems.
Some Experience with SQL in virtual, physical and cloud-based environments.
Experience with Athena and data modeling for cloud technologies.
Proven ability to quickly learn and implement new technologies.
Experience with Administration, Security/Identity Management and Terraforms in AWS.
Preferred Qualifications:
Experience with SentryOne, a plus.
Ability to code Powershell commands and maintain code in GitHub, a plus.
Some Experience with Metabase and Collibra, a plus.
Experience with ETL in AWS, a plus.
Pay Range:
$60-$68/hr
Requisition Disclaimer:
This job posting is for a temporary role as an employee of Atrium on assignment at Cox. The individual selected for this role will be offered the role as an employee of Atrium; compensation, medical benefits, fringe benefits and other terms and conditions of employment shall be presented by Atrium upon offer. The pay rate range provided is a reasonable estimate of the anticipated compensation range for this job at the time of posting. The actual pay rate will be based on a number of factors, including skills, competencies, experience, location and/or being pursued and other job-related factors permitted by law. In addition, this role will be eligible for overtime pay, in accordance with federal and state requirements

By applying for this position you agree to the Atrium Terms and Conditions. Agreeing to these terms, includes permission to use the email address and mobile phone number you provide during the application process or throughout the duration of your prospective or actual employees to notify you of job openings, profiles, articles, news, and other employment-related information, as well as to notify you of special promotions or additional products and services offered by us or our affiliates and partners (collectively, “Atrium Alerts”). Atrium Alerts may be sent by email, phone or text message. Your personal information will be safely stored in our database. Atrium does not sell your personal information to third parties. Text message and data rates may apply. To OPT OUT of text messaging or to modify your communication preferences for Atrium Alerts at any time, please contact us at privacyadministrator@atriumstaff.com.

If you do not agree with the Atrium Terms and Conditions, you can still complete your application for this position by emailing your resume to our team at coxrecruit@atriumworks.com. Please include the job title in the subject of your email.

As a woman-owned firm, Atrium values diversity. We are an equal opportunity employer and will consider all applications without regard to race, sex, age, color, religion, national origin, veteran status, disability, genetic information or any other characteristic protected by law. We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.
Posting: #zip",$64.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Tipico - North America
4.0",4.0,"Denver, CO",Data Engineer,"Company Description

Tipico is an energetic, passionate, bold tech company, and we love sports! Tipico has recently expanded to the US market, establishing offices in Hoboken, New Jersey and Denver, Colorado. We are driven by our core values: passion, progress and trust. Our goal is to build the best possible mobile sports betting product in the industry, in order to amplify the emotions of millions of sports fans every day. Let’s Make This Interesting ™
Please note: this role is located in our Denver, CO office; we work off a hybrid model and come into the office 2 days per week.

Job Description

The Data Engineer will report directly to the Data Engineering Manager and work together with rest of the data engineers, BI analysts, and product teams. The position holder ensures this is done by doing the following:
Create and maintain both batch (ETL/ELT) and real-time data pipelines and architecture
Ensure all data provided by Data Platform is of the highest quality and is delivered in a timely manner and in line with agreed SLAs
Assemble large and complex data sets that meet functional and non-functional business requirements
Identify, design, and implement internal process improvements, including automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies
Collaborate with stakeholders including data architects, Executive, Product, Data and IT team members from the beginning to the delivery of a project
Build a scalable, modern, and trusted data ecosystem, making sure it is responsive, available and in line with business requirements
Take part in sprints, workloads, and deliverables through agile methodologies
Manage quality assurance, verification of accuracy and consistency of data, specifically KPI measures sent to the business
Thrive working in a fast-paced and challenging environment
Open to continuous learning individually and with the team
Comfortable with being challenged / seeking challenge
Willing to work flexible hours if needed

Qualifications
3+ years in Data Platforms / Datawarehouse / BI experience
Must have experience in successfully implementing data-centric applications, such as data warehouses, operational data stores, and data integration projects
Strong analytical and problem-solving skills that balance with creative approaches
Ability to learn new and complex concepts quickly
Strong experience with large-scale production databases and SQL
Solid understanding of cloud data services: AWS services such as S3, Athena, EC2, RedShift, EMR, EKS, RDS and Lambda
Proficient with one or more big data & new data technologies such as DBT, NIFI, Airflow, Kafka
Experience with data modelling
Experience with ETL development (extractions, data load, aggregation, transformation)
Knowledge in working with time-series/analytics databases such as Elasticsearch
Understanding of containerization and orchestration technologies like Docker/Kubernetes
Must be able to take ownership of projects with a high level of attention to detail and priority
Team player with a hands-on approach and attitude
Must be comfortable with on call rotation
Nice to have:
Familiarity with Marketing tooling and/or CRM systems is preferred
Familiarity with using business intelligence tools such as Domo, Tableau or Looker
Knowledge about and / or experience with sports betting
Background in digital companies, online, gambling, gaming, e-commerce, marketing, or mobile apps
Familiarity with one scripting language, preferably Python

Additional Information

What's in it for you:
Work in an environment where you, your work and ideas matter and have an impact.
You will be a be part of the newest and up and coming USA online iGaming and sports betting companies in the market.
Work in a new and young business with high growth potential.
Build your own success story together with us.
Work with self-organized, self-responsible and entrepreneurial employees.
Start-up feeling, backed-up by a leading European sports betting house. We are a high-volume business and are taking off in the US!
Competitive salary, Medical, Vision, Dental Benefits, Unlimited PTO, and more
Salary for this position is $120k+ & annual performance bonus","$120,000 /yr (est.)",201 to 500 Employees,Company - Private,"Arts, Entertainment & Recreation",Gambling,2004,Unknown / Non-Applicable
Radiant System,#N/A,"Pleasanton, CA",Data Engineer,"First Priority: Pleasanton, CA –
Second Priority for a superstar: Plano, TX
Experience in developing and deploying data pipelines, preferably in the Cloud
Experience working with Spark, Databricks, and other relevant technologies in Microsoft Azure cloud
Experience in productionizing and deploying Big Data platforms and applications
Expert in writing SQL statements
Job Type: Contract
Application Question(s):
Are you willing to work on W2?
Work Location: In person",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Fracsys Inc,#N/A,"Washington, DC",Data Engineer,"Fracsys Inc is hiring a Data Engineer position. The ideal candidate must have at least 12+ years of industry experience. He or she must be responsible for successful technical delivery of Data Warehousing and Business Intelligence projects using a formal project management methodology.
Responsibilities
Directs, motivates and develops key technical personnel
Provides technical direction across the full lifecycle system development process
Provides strategic planning and comprehensive senior level technical consulting to IT senior management and senior technical staff
Working independently within guidelines
Provides detailed inputs for plans and schedules, including goals, risks, and resource allocation
Conduct diagnostic, troubleshooting steps and present conclusions to a varied audience
comfortable slinging some code to solve a problem
Build Data Marts and Data warehouses to facilitate analytical capabilities across the varied sources of data
Design and implement data patterns on cloud using cloud Data warehouse and data virtualization tools
Evaluate new tools and technologies for target state on cloud
Learn and implement new ETL, BI tools and data warehousing technologies
Qualifications
Minimum twelve (12) years in a Technical Lead or Consulting role leading Information Management related initiatives (system integration, data warehouse build, data mart build, or similar)
MUST have proven leadership experience for team of at least 3 technical resources.
Experience creating Data Marts, Data warehouses on-premise and on cloud for real time and batch processing frameworks
Experience building and optimizing big-data data pipelines and data sets including Postgres, AWS Relational Data Service
extensive hands-on experience building and optimizing data structures for data analytics, data science and business intelligence
Experience building self-service data consumption patterns and knowledge of cloud-based data Lake platforms
Experience wrangling data (structured and unstructured), in-depth knowledge of database architecture
Experience utilizing or leading implementations leveraging ETL tools (Informatica / Talend), BI Reporting tools such as MicroStrategy, Microsoft Power BI, data modeling tools such as Erwin, Oracle, SQL server, NoSQL, JDBC, UNIX shell scripting, PERL and JAVA, XML/JSON files, SAS , Python, AWS cloud-native technologies, S3, Athena, Redshift
Familiarity with the following technologies: Hadoop, Kafka, Airflow, Hive, Presto, Athena, S3, Aurora, EMR, Spark
Ability to drive, contribute to, and communicate solutions to technical product challenges
Ability to roll-up your sleeves and work in process or resource gaps and fill it in yourself
Excellent written and oral communication skills in English
Education
Bachelor’s degree in computer science, Data Analytics, Information Systems, or a related degree or equivalent experience. Master’s degree is preferred.
Job Type: Full-time
Pay: From $96,336.72 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Experience level:
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Washington, DC: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person","$96,337 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Titan America
3.8",3.8,"Roanoke, VA",Data Engineer,"Data Engineer
We are seeking a motivated and detail-oriented Data Engineer to join our team based in Roanoke, Virginia. As a Data Engineer, you will play a crucial role in designing, developing and maintaining scalable data pipelines and transformations for business-critical use cases. In addition, you may take on various Software Development and Data Science responsibilities as we frequently share responsibilities and knowledge across our growing team.
Responsibilities:
Design, develop and maintain scalable data pipelines for extracting, transforming, and loading data from various sources
Collaborate with cross-functional teams to identify data needs and determine the best data solutions
Develop and implement data models to support business requirements and ensure data quality
Ensure the security and privacy of sensitive data by implementing appropriate access controls
Monitor and optimize data pipeline performance to ensure timely and accurate data delivery
Document data pipeline processes, data dictionaries, and data storage solutions
Requirements:
Bachelor's degree in Computer Science, Computer Engineering, or a related technical field
Minimum of five years of professional experience working as a Data Engineer or Software Developer
Strong hands-on experience with data warehouse and transformation solutions, i.e. Domo, Snowflake or similar
Proficient in at least one scripting language such as Python, JavaScript, or R
Understanding of data modeling, data integration and data quality processes
Familiarity with cloud platforms such as AWS, Azure, or Google Cloud Platform
Strong analytical and problem solving skills
Full Stack Software Development experience in a professional setting is highly desired, but not required
This is an excellent opportunity for a driven and collaborative individual to make a significant impact in a dynamic and growing team. If you have a passion for data and a desire to work in a fast-paced and dynamic environment, we want to hear from you!
Success begins with hiring the right people to partner with us as we grow and develop our businesses. People are central to everything we do. It is through their efforts and talents that Titan has been successful for over 100 years.
Titan America is an Equal Employment Opportunity (EEO) / Affirmative Action employer and welcomes all qualified applicants. Applicants will receive fair and impartial consideration without regard to race, sex, color, religion, national origin, age, disability, veteran status, genetic data, or other legally protected status.
Job Type: Full-time
Pay: $75,000.00 - $85,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Referral program
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Roanoke, VA 24019: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Do you currently or will you in the future need sponsorship to work in the United States?
This position requires the successful candidate to be in the office in Roanoke, VA at least 3 days a week. Can you meet this requirement?
This position requires the successful candidates to be a full time, W2 employee. Can you meet this requirement?
Education:
Bachelor's (Preferred)
Experience:
Data Engineering: 5 years (Preferred)
Work Location: Hybrid remote in Roanoke, VA 24019","$80,000 /yr (est.)",1001 to 5000 Employees,Company - Public,"Energy, Mining & Utilities",Mining & Metals,1902,Unknown / Non-Applicable
"Appsintegration INC
4.5",4.5,"California, KY",Data Engineer,"Role:- Data Engineer
Position:- Onsite (W2 position)
Experience:- 10+ Years
Responsibilities
Working closely with business teams to understand their requirements and develop reporting or analytic solutions.
Producing and automating the delivery of key metrics and KPIs to the business.
In some cases, this will focus on making data available and others will be developing full reports for end users.
Managing stakeholder demand and a backlog of reporting requirements and needs.
Actively working with users to understand data issues, tracing back data lineage, and helping the business put appropriate data cleansing and quality processes in place.
Managing secure access to data sets for the business.
Requirements
Bachelor's degree in Computer Science or equivalent.
Proficient in creating Data Factory pipelines for on-cloud ETL processing.
Experience implementing data pipelines using the latest technologies.
Experience developing using SQL, relational databases, and data warehousing technologies.
Extensive ETL experience.
Ability to scale systems in Cloud as per requirement.
Experience in handling structured and unstructured datasets.
Good to have experience in data modeling and Advanced SQL techniques.
Must-Have Skills: Azure Data Factory (ADF), SQL, Cloud-based ETL.
Good to Have Skills: Azure File Storage Services, Databricks, Azure Synapse.
you can reach me out on LinkedIn
here is my link :- linkedin.com/in/shravanthi-thakur-705a81231
Job Type: Contract
Salary: $42.00 - $45.00 per hour
Ability to commute/relocate:
California, KY 41007: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",$43.50 /hr (est.),1001 to 5000 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Atika Technologies,#N/A,"Pittsburgh, PA",Data Engineer,"Role: Data Engineer/PySpark Developer, Pittsburgh, PA - Day One Onsite
Fulltime and Contract(W2)
LinkedIn Must
USC/GC/EADs
Three round of interview: First two on Zoom Video Call, End Client Round will be Onsite on the Client's Location (Pittsburgh, PA)
Job Description:
Python(PySpark), Jupiter hub, airflow, Java
Database knowledge - Oracle / SQL / Data Analytics
Regulatory reporting knowledge preferred
Good communication skills
Open / Ability to learn new technologies
Very good Analytical Skills
Experience in ETL is preferable
Job Types: Full-time, Contract
Salary: $120,000.00 - $130,000.00 per year
Benefits:
Dental insurance
Health insurance
Life insurance
Compensation package:
Bonus pay
Performance bonus
Yearly pay
Experience level:
10 years
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Pittsburgh, PA 15259: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Required)
Spark: 4 years (Required)
Regulatory reporting: 3 years (Required)
SQL: 4 years (Required)
IT: 10 years (Required)
Airflow: 4 years (Required)
Jupiter: 4 years (Required)
Work Location: One location","$125,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Fuge Technologies Inc
4.7",4.7,"Jersey City, NJ",Big Data Engineer,"Big Data Engineering Technical Lead
Mandatory Skills
Big Data – Spark, Kafka, Databricks, Hive, Nifi
AWS, Java, Scala
Client Interview Needed for Selection (yes / No)
Yes
Detailed JD (Pl share the Detailed Description, 1 liner JD will not work)
10+ yrs experience as Data Engineer
Extensive experience in spark on data processing
Kafka design experience like monitoring & parallelism.
Comfortable with Python/Java/Scala coding experience
Crisp / clear cut communication
Flexible with learning new technology
Flink experience is a plus
AWS experience is nice to have
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Jersey City, NJ 07302: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",$72.50 /hr (est.),1 to 50 Employees,Contract,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"DSMH LLC
5.0",5.0,Remote,"Data Engineer (AWS, Python)","Title: Data Engineer
Duration: 12+ Months
Technical Skills
(Required)
Four years or more of experience in data engineering and/or software
development.
Three years or more of experience with development, operations, or
architecture in AWS using Python.
Experience with development and delivery of microservices using serverless
AWS services (S3, RDS, Aurora, DynamoDB, Lambda, SNS, SQS, Kinesis,
IAM)
Strong competency in testing, including unit testing to coverage standards and
integration testing.
(Desired)
Software development experience with object-oriented development and design
patterns
Experience with serverless design in AWS
AWS technical certifications (Developer Associate, Solution Architect Associate)
Familiarity with machine learning and data design to support machine learning
Experience with productionizing machine learning workloads
Experience with ElasticSearch/ELK, Kibana, Grafana, and/or Prometheus
Experience with OpenTelemetry
Soft Skills
(Required)
Verbal and written communication skills, problem solving skills, customer
service and interpersonal skills.
Basic ability to work independently and manage one’s time.
(Desired)
Ability to work collaboratively in a complex, rapidly changing, and culturally
diverse environment.
Ability to clearly communicate complex technical ideas.
Comfortable working in a dynamic environment where digital is still evolving as
a core offering.
Job Type: Contract
Pay: $50.00 - $55.00 per hour
Experience level:
6 years
Schedule:
Monday to Friday
Experience:
AWS: 5 years (Required)
Python: 5 years (Required)
Work Location: Remote",$52.50 /hr (est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,$1 to $5 million (USD)
WellTrust Medical Group,#N/A,"Charlotte, NC",Data Engineer,"The Data Engineer will assist the senior analyst in complex modeling and projects to ensure their completion within the allotted timeline. Helps track measures of success and develop reports that measure operational and customer metrics. Demonstrates ability to troubleshoot data consumption and data quality problems.

Responsibilities:
Build and maintain reports using My SQL Server
Identify optimal data sources, structure data validation, and cache optimization.
Performs business problem modeling and quantitative analytics including regression analysis, time-series, hierarchical cluster analysis, and data mining.
Helps track measures of success and develop reports that measure operational and customer metrics.
Demonstrates ability to troubleshoot data consumption and data quality problems.
Evaluates and recommend improvements to current models, metrics, and analytic processes to improve operational efficiency and insight.
Design, document, and communicate effective technical solutions meeting end-user requirements
Perform analyses with large amounts of complex data, and uses data visualization tools to communicate results.
Requirements:

Bachelor's degree or equivalent experience in a quantitative field (Mathematics, Statistics, Computer Science, Economics, Finance, Physics, Engineering)
3+ years of professional working experience as a Data Engineer or Analyst
1-3 years of data warehouse experience
Build and maintain reports using My SQL Server
Strong analytical and creative problem-solving skills
Design, document, and communicate effective technical solutions meeting end-user
SQL and Excel Macro development
Strong understanding of various data environments such as SQL, Oracle, Big Query, etc.
Experience with data visualization tools such as Looker Studio, Tableau, D3.js, etc.
Proficient in R, Python, MATLAB, Java, or another programming language.",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Egrove Systems Corporation
4.4",4.4,"Dallas, TX",Data Engineer Flink,"Java, Bigdata, Azure & Flink
Flink Sr. Developer, who has implemented and dealt with failure scenarios of processing data through Flink.
A senior who can help his Flink development team, guiding and helping them implement custom solutions through Flink.
Seasoned Java developer who knows about all aspects of SDLC.
Worked on integrations of other technologies with Flink, eg: Kafka, MongoDB, etc
Azure experience in the areas of Messaging, Data processing, preferably on Flink or on Databricks.
Job Type: Contract
Salary: From $51.83 per hour
Compensation package:
Monthly bonus
Experience level:
11+ years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
Work Location: On the road
Speak with the employer
+91 732)-860-7733",$51.83 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Internet & Web Services,#N/A,Unknown / Non-Applicable
"DataArt
4.6",4.6,Remote,Data Engineer,"CLIENT
Our client is a pioneer in US schools’ education since 2000, it is leading the way in next-generation curriculum and formative assessment. They develop a number of solutions and interactive web products for teachers, students and their parents.

The company is technology-driven with a huge number of software engineers involved in product development. They have a very solid approach to technology, employ best practices and processes, with a focus on cutting-edge frameworks, languages and tools.

We invite to the company, not a project
TEAM
There are multiple autonomous and empowered teams who work with their own part of the product and choose their own way of doing things and they expect people to be proactive and good team players.
Responsibilities
Building well-tested and optimized ETL data pipelines for both full and delta extraction
Collaborating with data analysts and learning scientists to store, aggregate, and calculate captured students’ work
Contributing to leading industry data standards, such as Caliper Analytics or xAPI
Improving our deployment and testing automation data pipelines
Requirements
3+ years of professional software development or data engineering experience
Strong CS and data engineering fundamentals
Proven fluency in SQL and a development language such as Python
Experience with the following storages: Snowflake, AWS Storage Services (S3, RDS, Glacier, DynamoDB)
Experience with ETL/BI: Airflow, DBT, Matillion, Looker
Experience with Cloud Infrastructure: AWS Kinesis, Lambda, API Gateway, Terraform
Understanding of ETL/ELT pipelines and Data Warehousing design, tooling, and support
Understanding of different data formatting (JSON, CSV, XML) and data storage techniques (3NF, EAV Model, Star Schema, Data Lake)
BS in Computer Science, Data Science, or equivalent
Experience in education or ed-tech
Proven passion and talent for teaching fellow engineers and non-engineers
Proven passion for building and learning: open source contributions, pet projects, self-education, Stack Overflow
Strong communication skills in writing, conversation",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,1997,$100 to $500 million (USD)
"Intersec
4.5",4.5,Remote,Senior Data Engineer,"Architect and implement highly scalable data pipeline to streamline migration of data from source data repositories to the centralized data platform
Implement agreed upon data models (e.g., data marts) to enable faster and efficient consumption of data by downstream consumers
Conduct performance tuning to optimize data migration techniques and ensure adherence to identified service-level agreements (SLAs)
Define and implement automated validation checks to ensure data integrity between source and target
Assist in onboarding consumers with existing technology to connect to the platform
Setup predictive alert mechanism for pipeline degradation for IMF pipelines.
Enhance data quality framework for data pipelines and derived datasets
Enhance Data Audit Balance and Control mechanism for data pipelines
Enhance pipelines to be certified copies of authoritative source
Populate metadata for the Enterprise with particular focus on the Data Sources being onboarded to EDP (Technical and Business Glossaries) related to IMF Modernization Programs
Validate correct lineage for datasets
Required:
*8+ years of Databricks, Data Modeling, and/or Metadata management experience
* Familiarity with architecting and developing real and near-real-time solutions
* Bachelor's Degree in Engineering, Mathematics, computer Science, Statistics or other quantitative discipline, or 4 years equivalent professional experience
* US Citizenship and ability to gain a public trust
Preferred:
* Familiarity with IRS systems and ETL pipelines
* Ability to work independently and manage multiple tasks
* IRS MBI Clearance with IRS Laptop and Informatica Installed
Job Type: Full-time
Salary: $70.00 - $75.00 per hour
Schedule:
Monday to Friday
Application Question(s):
Are you a US/GC card holder?
How many years of Databricks, Data Modeling, and/or Metadata management experience do you have?
Work Location: Remote",$72.50 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,Unknown / Non-Applicable
National Group Corporation,#N/A,"Dallas, TX",Data Engineer,"Who are we?
ReAlign Insurance Holdings is a leading primary underwriter of program administrator managed specialty insurance programs in the U.S. With the extensive knowledge and experience our team has in building and managing carriers and programs we are able to reach decisions on program opportunities quicker than our competition.
Why choose ReAlign?
At ReAlign, we believe that we are only as strong as our people. We strive to create an inclusive and welcoming culture where employees feel comfortable and empowered to be themselves. We support a diverse and inclusive work environment, with a focus on a healthy work-life balance. We understand that comprehensive benefits are important which is why we offer a benefit package that includes: medical, dental, vision, company paid short and long-term disability, life insurance, health spending account, 401(k) match every pay period, PTO, paid holidays, floating holidays, tuition reimbursement, and referral bonuses.
Job Summary
The Data Engineer position will involve data exchange process analysis, design, development, unit testing, and support. This role would work with other data team members on database ETL processes and reporting needs in both the on-premise database environment as well as the Azure data environment. This position requires strong technical and analytical skills to review business processes and data, recommend solutions, conduct functional and code analysis, and lead and perform database development tasks. This position will also involve production support and require experience within a DevOps environment.
Essential Functions
· Design, development, and support data ETL processing for both external and internal customers
· Support the internal and Azure cloud-based data warehouse platforms
· Work with DBA/System Admin to develop and maintain automated data processing
· Design and analyze queries and stored procedures
· Write and support SSRS and PowerBI reports
· Work in a data development life cycle environment
Work with stakeholders and data analysts to understand business requirements
Work with end users and the testing team to perform unit, functional, and integration testing and validation prior to production implementation
Troubleshoot and resolve operational problems with a focus on delivery
· Communicate solutions and architectural design to IT leadership
· Work in both Kanban and Scrum Agile environments
· Continually improve processes and solutions, seeking ways to use new tools, features, and capabilities
Job Requirements
3-10 years of experience as a Database Developer
Highly proficient in writing advanced SQL queries, stored procedures, SSRS reports, and ETL processes
Strong knowledge of relational and transactional database architecture
Strong technical and analytical skills
Experience with the Microsoft Azure and PowerBI platforms and Agile development methodology a plus
Bachelor’s Degree in computer science, database development, or related field preferred
Detail oriented with the objective of delivering a solution to a problem in a timely and efficient manner
Self-driven, willing to take initiative when an opportunity presents itself
Strong collaboration and communication skills and the ability to work in a team environment a must
Ability and willingness to work in a partial on-prem and partial remote office environment
Experience in the Property and Casualty Insurance industry a plus
Proficient with standard desktop software used in a business environment (MS Word, Excel, Visio, etc.)
Must be able to work in a hybrid in-office and remote work environment
Must be authorized to work in the United States
Job Type: Full-time
Pay: $90,000.00 - $125,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Health savings account
Life insurance
Paid time off
Retirement plan
Vision insurance
Experience level:
3 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you currently local to the Dallas area and able to commute to the office 2 to 3 times per week? (Deal breaker)
Experience:
writing advanced SQL queries and ETL processes: 3 years (Required)
cloud database platform (i.e. Azure, AWS): 1 year (Preferred)
Azure Data Factory: 1 year (Preferred)
Work Location: In person","$107,500 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Fracsys Inc,#N/A,"Washington, DC",Data Engineer,"Fracsys Inc is hiring a Data Engineer position. The ideal candidate must have at least 12+ years of industry experience. He or she must be responsible for successful technical delivery of Data Warehousing and Business Intelligence projects using a formal project management methodology.
Responsibilities
Directs, motivates and develops key technical personnel
Provides technical direction across the full lifecycle system development process
Provides strategic planning and comprehensive senior level technical consulting to IT senior management and senior technical staff
Working independently within guidelines
Provides detailed inputs for plans and schedules, including goals, risks, and resource allocation
Conduct diagnostic, troubleshooting steps and present conclusions to a varied audience
comfortable slinging some code to solve a problem
Build Data Marts and Data warehouses to facilitate analytical capabilities across the varied sources of data
Design and implement data patterns on cloud using cloud Data warehouse and data virtualization tools
Evaluate new tools and technologies for target state on cloud
Learn and implement new ETL, BI tools and data warehousing technologies
Qualifications
Minimum twelve (12) years in a Technical Lead or Consulting role leading Information Management related initiatives (system integration, data warehouse build, data mart build, or similar)
MUST have proven leadership experience for team of at least 3 technical resources.
Experience creating Data Marts, Data warehouses on-premise and on cloud for real time and batch processing frameworks
Experience building and optimizing big-data data pipelines and data sets including Postgres, AWS Relational Data Service
extensive hands-on experience building and optimizing data structures for data analytics, data science and business intelligence
Experience building self-service data consumption patterns and knowledge of cloud-based data Lake platforms
Experience wrangling data (structured and unstructured), in-depth knowledge of database architecture
Experience utilizing or leading implementations leveraging ETL tools (Informatica / Talend), BI Reporting tools such as MicroStrategy, Microsoft Power BI, data modeling tools such as Erwin, Oracle, SQL server, NoSQL, JDBC, UNIX shell scripting, PERL and JAVA, XML/JSON files, SAS , Python, AWS cloud-native technologies, S3, Athena, Redshift
Familiarity with the following technologies: Hadoop, Kafka, Airflow, Hive, Presto, Athena, S3, Aurora, EMR, Spark
Ability to drive, contribute to, and communicate solutions to technical product challenges
Ability to roll-up your sleeves and work in process or resource gaps and fill it in yourself
Excellent written and oral communication skills in English
Education
Bachelor’s degree in computer science, Data Analytics, Information Systems, or a related degree or equivalent experience. Master’s degree is preferred.
Job Type: Full-time
Pay: From $96,336.72 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Experience level:
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Washington, DC: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person","$96,337 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"RelMap Consulting
4.8",4.8,"Addison, TX",Senior Data Engineer (Hybrid),"The data engineer entails:
Advanced SQL Server Development
Ability to create and modify views
Ability to create and modify custom functions
Ability to write complex queries, including joins, temp table processing, and common table expressions (CTE’s)
Ability to create and modify stored procedures using TSQL
Ability to analyze and optimize query performance using indices
Ability to use Red Gate’s SQL Compare to perform a schema (AKA “do a manual roll”) between one server an another, and between revision control and a database
Ability to stand-up new database server VM’s in Linux and Windows using VMWare
Ability to backup and restore a database in a Linux and Windows environment
Ability to manage and script automated database jobs
Ability to use PowerShell / Bash to interact with databases from the command line
Addition skills that would be desired but not required
· Experience with Redis
· Experience with MongoDB / CosmosDB
· Experience with Azure and Azure SQL
· C#, JavaScript (NodeJS / ES6), or Python experience
Job Type: Contract
Pay: $70.00 - $80.00 per hour
Compensation package:
Hourly pay
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Addison, TX: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person",$75.00 /hr (est.),1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2010,$1 to $5 million (USD)
DocsInk LLC,#N/A,"Wrightsville Beach, NC",Data Engineer,"DocsInk is on a mission to provide a collaborative and exciting alternative to the traditional healthcare model. Through the complete virtualization of healthcare organizations, we are equipping providers with the ability to deliver better patient care with speed, efficiency, and accuracy. With the widespread focus on providing value-based care, DocsInk is making it possible to provide superior care while also maximizing reimbursements. In today’s environment, shifting the way we approach healthcare is essential and DocsInk is on the cutting edge of where the industry is headed. We are excited to be growing our team and are looking for driven individuals with a passion for making a difference in the healthcare industry.

As our Data Engineer, you will be in the driver’s seat as our team creates elegant and forward-thinking software solutions for our healthcare customers. You will work with fellow developers, designers, training & support, quality assurance, and management in a fast-paced environment to take our products to the next level and exceed expectations. We are looking for someone who is incredibly talented to do what most cannot and yet humble enough to easily work with peers, accept guidance from more experienced staff, and mentor more junior staff. If designing and developing complex integration efforts is what gets you out of bed each morning, then we want you to help us reshape healthcare.

People are our most important asset and the number one reason we all love working at DocsInk. As a team, we value accountability, transparency and collaboration. We have agile teams, with clear, outcomes-focused goals. Delivering innovative solutions and superior service are at the forefront of what we do, and the collective ideas of the team allow us to continue to excel in a fun and energetic environment.

Professional Requirements and Responsibilities:

Role / Responsibilities:

Implement robust ETL solutions that integrate healthcare data feeds using tools such as Apache NiFi, NextGen Connect (JavaScript), PostgreSQL, MySQL, and Linux shell scripting
Configuring and managing deployments to AWS infrastructure consisting of both traditional servers and serverless components. CI/CD tools include Bitbucket and AWS Code Deploy
Building direct interfaces with hospitals and EMR vendors utilizing industry standards such as HL7 2.x, C-CDA, and FHIR as well as implementing interfaces that consume formats such as XML, JSON, and delimited files
Utilizing APIs and web services to permit bi-directional flows of data to and from our partner organizations
Monitoring operational ETL processes, triaging and resolving issues as they arise to meet SLAs
Writing PL/SQL functions and packages to automate workflows in a Postgres environment linked with web technologies
Maintaining data standards (including adherence to HIPAA), documenting work, and championing process improvements
Work as a project team member to jointly deliver high-quality business solutions consistent with project objectives and constraints

Knowledge and Skills Required:

4+ years experience with HL7 2.x
2+ years experience with Apache Nifi
2+ years experience with NextGen Connect
Advanced knowledge of PostgreSQL, MySQL, AWS Code Deploy, HL7 2.x, C-CDA, FHIR, and HIPAA data compliance

Qualifications and Other Attributes Required:

Degree in Computer Science or equivalent field, or equivalent work experience
Highly self-motivated team player who takes initiative
Strong sense of personal accountability; contributes beyond job role and responsibilities
Excellent problem-solving skills; meticulous & methodical","$85,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"spar information systems
3.5",3.5,Remote,Azure Data Engineer,"Role: Sr Azure Data Engineer
Location: Remote
Duration: 3 Months Contract to hire Full Time (W2 Only)
Must have 11+ IT Experience
Required Skills:
3+ years of experience in data software development, programming languages and developing with big data technologies
2+ years of experience designing and building on existing and new data applications
2+ years of experience in Cloud DevOps concepts, Cloud Services and Architecture, and Azure/AWS/GCP DevOps Operational Framework
1+ years of experience in open-source data tools and frameworks, or one of the following: .net Core, asp.Net, Angular, or Express.
Thanks & Regards,
Arvind Kumar Bind
Cell: 732 716 7403 (Text)
Direct Number:- 469-750-0607
Email : Arvind.B@sparinfosys.com
Job Types: Full-time, Contract, Permanent
Pay: $120,555.79 - $150,000.00 per year
Benefits:
401(k)
Health insurance
Paid time off
Experience level:
11+ years
Schedule:
8 hour shift
Experience:
Big data: 3 years (Required)
SQL: 1 year (Required)
Data lake: 3 years (Preferred)
Work Location: Remote","$135,278 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2012,$25 to $100 million (USD)
"Cureatr
4.1",4.1,Remote,Data Engineer,"About Us
We are a thriving, medication management solutions company, headquartered in Manhattan, but largely virtually distributed, committed to tackling the $528B suboptimal medication management problem. Patient medication information is often incomplete for healthcare providers and systems – it’s our mission to fill in the gaps. We provide real-time, universal access to accurate medication data for over 265 million patients and high-quality interventions by board certified telepharmacists. Through our solutions, healthcare providers can reduce preventable hospitalizations, avoid readmissions, and ultimately, improve patient outcomes. We’re already used by dozens of hospital systems, with tens of thousands of clinicians and supporting staff using us to care for millions of patients.
Here at Cureatr, we are driven by a core set of values and strive to incorporate them into everything we do. We care deeply about our users, patients, and employees, and we aim to live and operate compassionately. We take pride in creating a diverse, inclusive, and equitable company culture, where our open and honest approach to work fosters innovation and encourages employees to achieve professional and personal growth.

The Role
We’re looking for a Data Engineer to join our growing Engineering team. Use the power of healthcare data to find insights that will demonstrate the value Cureatr is delivering to our customers, help improve patient outcomes, and expand the scope of Cureatr’s mission. As a Data Engineer at Cureatr, you will be responsible for designing, implementing and maintaining our data infrastructure, pipelines, automation, and ETL processes. You will work closely with our Data Science and Analytics teams to ensure that our data is clean, accurate, and readily available for analysis. You will also be responsible for ensuring that our data meets industry standards for security, privacy, and compliance.
This is a remote, work from home position, but candidates must be located within the U.S. Ideal candidates must be able to travel to our New York City office, or TBD company-wide offsite locations, up to 4x per year.

You’ll be part of a team working to solve problems like:
Increasing the number of patients who receive the opportunity to do a medication reconciliation, which can help keep them out of the hospital.
Streamlining and automating processes so that our pharmacists can serve as many patients as possible.
Predicting patient events likely to occur so that our clinical team can adequately prevent them before it’s too late.
Helping our pharmacists make the most of data and insights that will help them be as effective as possible in serving patients.
Evaluating the effectiveness of our clinical services impact at the population level.

What You’ll Do:
Design, implement, and maintain our data infrastructure, pipelines, automation, and ETL processes to support our data needs.
Build and maintain scalable data models and databases.
Collaborate with Data Science and Analytics teams to understand their data needs and provide data solutions accordingly.
Ensure data quality and integrity by implementing data validation and cleansing processes.
Ensure data security and compliance with healthcare regulations.
Monitor and optimize data performance and scalability.
Design and implement data governance policies and procedures.
Evaluate and implement new data technologies and tools to improve our data capabilities.

Your Skills:
3+ years of experience as a Data Engineer or similar role.
Strong programming skills in languages such as Python and SQL.
Experience using git to track changes.
Experience with data modeling and database design.
Experience with the “modern data stack” preferred
Experience with ETL and data pipeline development.
Knowledge of healthcare data and related standards (e.g., HL7, FHIR) is a plus.
Familiarity with AWS.
Strong problem-solving and analytical skills.
Excellent communication and collaboration skills.

Technology Stack:
Python
RedShift
dbt
Dagster
MongoDB
Looker
AWS

Educational Requirement: Bachelor’s Degree or relevant work experience

Salary: $110,000 - $140,000 plus full benefits

Why work with us?
Make a measurable impact on patients’ lives
Be part of a company dedicated to living its core values:
Empathy
Honesty
Integrity
Impact
Fun
Equity in Healthcare
Work in the company without micromanagement
Share your vision and make an impact on the product
Opportunities for career growth
Remote work
Competitive salary
Premium benefits (Health, Dental, Vision, 401k, unlimited PTO, more)

Equal Employment Opportunity Policy
Cureatr is an equal opportunity/affirmative action employer. All qualified applicants will receive consideration for employment without regard to sex, pregnancy, race, religion or religious creed, color, gender, gender identity, gender expression, national origin, ancestry, physical or mental disability, medical condition, genetic information, marital status, registered domestic partner status, age, sexual orientation, military or veteran status, protected veteran status, or any other basis protected by federal, state, local law, ordinance, or regulation and will not be discriminated against on these bases
Disclaimer: we are aware of a scam targeting applicants using a cureatrcareers.com email address. Please block and report this sender, as they are not affiliated with Cureatr. If you have any questions, please contact us at jobs@cureatr.com. Thank you!
uBajh3OfeT","$125,000 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2012,Less than $1 million (USD)
"Kastech Software Solutions Group
4.0",4.0,"Dallas, TX",Data Engineer,"In KastechSSG we are are seeking for a Cloud DB Platform Engineer with Data Enablement Services experience ($50/hour), to work in a Hybrid Schedule (3 days on-site, 2 days home office) for one of our partner companies!
Hybrid Schedule info: onsite 3 days per week from list of locations: Charlotte NC, Dallas TX, Chandler AZ, Newark, NJ - Must be local to one of these locations.
** Contract would be $50 per hour on W2 (18 months). American Citizens or GC Holders Only**
3-5 years experience in:
Terraform, Azure/GCP, DevOps Engineering, ETL, Client/AI, Data lake, Data architecture.
GCP Key Services Experience (ie Big Query, Composer, Vertex AI, DataProc, Data Catalog, Notebooks).
Azure Key Service Experience (ie Synapse, Purview, Data Lake, Logic Apps, Cognitive Services).
Job Type: Full-time
Salary: $50.00 per hour
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you American? If not, please specify your visa type
Would you be willing to work under W2?
Experience:
Terraform: 3 years (Preferred)
Azure Data Lake: 3 years (Preferred)
GCP Key Service Exp: 3 years (Preferred)
DevOps: 3 years (Preferred)
Work Location: In person",$50.00 /hr (est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2008,$25 to $100 million (USD)
"shaped.ai Inc.
4.0",4.0,"New York, NY",Data Engineer,"Hello! We’re building the future of content discovery infrastructure with AI — come join us!

Shaped (YC W22) is an API for developers to seamlessly add personalized ranking and recommendation to their products. These frictionless discovery experiences help end-users find what they want faster, and, consequently, grow conversion and engagement business metrics. Behind the scenes, we’re building state-of-the-art ML infrastructure for training and deploying large AI models to power our personalization engine.
We’re a seed stage start-up backed by top-tier investors (e.g. Y-Combinator, Susa Ventures, Tribe Capital etc…) and executives (e.g. Google, Amazon, Uber, Dropbox etc…). Our team comes from Meta, Google, Apple and Uber. We’re a remote team but have a small office in Brooklyn, New York.
We’ve been lucky to have a huge amount of success since our initial launch earlier this year and now looking to expand our real-time data engineering capabilities. We are looking for a data engineer to design, build and optimize Shaped's real-time streaming data ingestion endpoints. You will be a founding engineer working on state-of-the-art infrastructure. As one of Shaped’s early employees you will help shape our product, culture and vision.
This role is for a full-time data engineer at $120k - $200k a year and 0.1 - 0.5% equity.

Responsibilities:
Manage the unification of our batch and real-time data processing pipelines.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Requirements:
Bachelor's in computer science, data science or mathematics related field. Master's degree or PhD will be advantageous.
5+ years of data engineering experience, ideally applied to machine learning use-cases.
Experience with AWS or common cloud technologies
Proficient with Python, SQL.
Extensive knowledge of both offline and online large scale data processing frameworks: Spark, Beam, Flink.
Extensive knowledge with stream technologies like Kakfa or Kinesis.
Excellent written and verbal communication skills.
We’re excited to work with you. Come build the future of AI and discovery with us!","$160,000 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Barracuda Networks Inc.
3.6",3.6,"Alpharetta, GA",Data Engineer,"Job ID: 23-579

Come Join Our Passionate Team! At Barracuda, we make the world a safer place. We believe every business deserves access to cloud-enabled, enterprise-grade security solutions that are easy to buy, deploy, and use. We protect email, networks, data and applications with innovative solutions that grow and adapt with our customers’ journey. More than 200,000 organizations worldwide trust Barracuda to protect them — in ways they may not even know they are at risk — so they can focus on taking their business to the next level.
We know a diverse workforce adds to our collective value and strength as an organization. Barracuda Networks is proud to be an Equal Opportunity Employer, committed to equal employment opportunity and equitable compensation regardless of race, gender, religion, sex, sexual orientation, national origin, or disability.
Envision yourself at Barracuda
We currently have an exciting new position open for a Data Engineer. This person will assist in driving technology innovation for engineering projects and work on architecture of AWS, Elastic and Kubernetes. They will also work closely with the Technology Architect to create and drive the Architecture Review Board.
What you’ll be working on
Determine internal / external customer data needs and identify system specifications
Analyze the needs of large systems and breaking them down into smaller manageable parts
Responsible for designing the journey of data from various sources to the database and everything in between
Plan and design the structure of data, brainstorm with team
Analyze best technology products and systems for internal teams
Help provide build vs buy decisions on data architecture
Communicate data requirements to engineers; explain data architecture to them and provide assistance throughout SDLC
Choose / maintain suitable software, hardware and integration methods
Help resolve technical problems as and when they arise
Review testing procedures to ensure data performant and error free
Ensure that database satisfy quality standards and procedures. Setup such standards.
Work with senior IT personnel to devise plans for future IT requirements of the organization
Produce progress reports on key infrastructures - Elastic and Data Pipelines
What you bring to the role
Bachelor Degree or Post Graduate in Computer Science, or relevant work experience
5+ years of experience in an Engineering / Operations heavy organization
1+ years of experience as a Data Analysis and Data Design responsible for data architecture of a SaaS application
Experience in understanding and designing data sharding in a distributed database
Some Hands-on experience in software development and system administration
Knowledge of strategic IT solutions
Familiarity with programming languages like JavaScript or Java or Python
Familiarity with Linux OS and Windows alike
Experience in cloud computing and cloud Technologies (AWS, GCP, Azure)
Excellent communication skills – you should be adept at listening to, understanding, and explaining key concepts to managerial and technical resources.
Good Anticipation and problem-solving skills
Ability to prioritize and manage time – responsible for multiple streams of work at the same time
Relationship building - Ability to form a good rapport with internal and external stakeholders
In-depth knowledge about quality standards and security best practices

Preferred Skills:
Expert level Understanding of Elastic eco system - Elasticsearch, Logstash, Kibana, Filebeats
Understanding of secure tunnel communication using VPN
Experience designing data at scale
AWS experience is preferred in Cloud Technologies
What you’ll get from us
A team where you can voice your opinion, make an impact, and where you and your experience are valued. Internal mobility – there are opportunities for cross training and the ability to attain your next career step within Barracuda.
Equity, in the form of non-qualified options
High-quality health benefits
Retirement Plan with employer match
Career-growth opportunities
Flexible Time Off and Paid Time Off benefits
Volunteer opportunities
#LI-Remote","$98,161 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,$100 to $500 million (USD)
"Quadrant Resource
4.1",4.1,Remote,Data Engineer,"Job Description:
4-6 years of experience as a Data Engineer in the Snowflake-Fivetran-dbt stack
• Possess a solid understanding of end to end Data ingestion, ETL, data analysis, Reporting processes
• Expert in ETL development using Fivetran and experienced in implementing data pipelines for Snowflake Cloud DWH
•Experienced in using dbt for data transformation
•Expertise on Tableau/Metabase for building reports/dashboards
•Expert in troubleshooting and resolving ETL issues, data load failures/problems and transformation translation problems
•Excellent communicator with the ability to work effectively with distributed teams
Job Type: Full-time
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Data Engineer: 9 years (Required)
Fivetran: 8 years (Required)
DBT: 7 years (Required)
Snowflake: 6 years (Required)
Work Location: Remote",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,2004,Unknown / Non-Applicable
Radiant System,#N/A,"Pleasanton, CA",Data Engineer,"First Priority: Pleasanton, CA –
Second Priority for a superstar: Plano, TX
Experience in developing and deploying data pipelines, preferably in the Cloud
Experience working with Spark, Databricks, and other relevant technologies in Microsoft Azure cloud
Experience in productionizing and deploying Big Data platforms and applications
Expert in writing SQL statements
Job Type: Contract
Application Question(s):
Are you willing to work on W2?
Work Location: In person",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Tipico - North America
4.0",4.0,"Denver, CO",Data Engineer,"Company Description

Tipico is an energetic, passionate, bold tech company, and we love sports! Tipico has recently expanded to the US market, establishing offices in Hoboken, New Jersey and Denver, Colorado. We are driven by our core values: passion, progress and trust. Our goal is to build the best possible mobile sports betting product in the industry, in order to amplify the emotions of millions of sports fans every day. Let’s Make This Interesting ™
Please note: this role is located in our Denver, CO office; we work off a hybrid model and come into the office 2 days per week.

Job Description

The Data Engineer will report directly to the Data Engineering Manager and work together with rest of the data engineers, BI analysts, and product teams. The position holder ensures this is done by doing the following:
Create and maintain both batch (ETL/ELT) and real-time data pipelines and architecture
Ensure all data provided by Data Platform is of the highest quality and is delivered in a timely manner and in line with agreed SLAs
Assemble large and complex data sets that meet functional and non-functional business requirements
Identify, design, and implement internal process improvements, including automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies
Collaborate with stakeholders including data architects, Executive, Product, Data and IT team members from the beginning to the delivery of a project
Build a scalable, modern, and trusted data ecosystem, making sure it is responsive, available and in line with business requirements
Take part in sprints, workloads, and deliverables through agile methodologies
Manage quality assurance, verification of accuracy and consistency of data, specifically KPI measures sent to the business
Thrive working in a fast-paced and challenging environment
Open to continuous learning individually and with the team
Comfortable with being challenged / seeking challenge
Willing to work flexible hours if needed

Qualifications
3+ years in Data Platforms / Datawarehouse / BI experience
Must have experience in successfully implementing data-centric applications, such as data warehouses, operational data stores, and data integration projects
Strong analytical and problem-solving skills that balance with creative approaches
Ability to learn new and complex concepts quickly
Strong experience with large-scale production databases and SQL
Solid understanding of cloud data services: AWS services such as S3, Athena, EC2, RedShift, EMR, EKS, RDS and Lambda
Proficient with one or more big data & new data technologies such as DBT, NIFI, Airflow, Kafka
Experience with data modelling
Experience with ETL development (extractions, data load, aggregation, transformation)
Knowledge in working with time-series/analytics databases such as Elasticsearch
Understanding of containerization and orchestration technologies like Docker/Kubernetes
Must be able to take ownership of projects with a high level of attention to detail and priority
Team player with a hands-on approach and attitude
Must be comfortable with on call rotation
Nice to have:
Familiarity with Marketing tooling and/or CRM systems is preferred
Familiarity with using business intelligence tools such as Domo, Tableau or Looker
Knowledge about and / or experience with sports betting
Background in digital companies, online, gambling, gaming, e-commerce, marketing, or mobile apps
Familiarity with one scripting language, preferably Python

Additional Information

What's in it for you:
Work in an environment where you, your work and ideas matter and have an impact.
You will be a be part of the newest and up and coming USA online iGaming and sports betting companies in the market.
Work in a new and young business with high growth potential.
Build your own success story together with us.
Work with self-organized, self-responsible and entrepreneurial employees.
Start-up feeling, backed-up by a leading European sports betting house. We are a high-volume business and are taking off in the US!
Competitive salary, Medical, Vision, Dental Benefits, Unlimited PTO, and more
Salary for this position is $120k+ & annual performance bonus","$120,000 /yr (est.)",201 to 500 Employees,Company - Private,"Arts, Entertainment & Recreation",Gambling,2004,Unknown / Non-Applicable
"Take Command Health
4.9",4.9,"Dallas, TX",Data Engineer,"Company: Take Command
Position: Data Engineer
Salary: $120,000-$130,000 plus bonus
Location: Dallas or Austin, TX
About Take Command
Take Command is a start-up on a mission to improve the healthcare system, starting with health insurance. Pragmatically speaking, we help employers reimburse employees for individual insurance instead of offering a traditional one-size-fits-all group plan. We believe this model can empower employees (when they have the right support) to be savvy healthcare consumers and have a transformative impact on the entire healthcare system.
Let’s be honest—health insurance is usually a confusing, frustrating, and even emotional experience for people. We want to fix that with a new model, great technology, and a superior user experience. We have made a great start, but we need your help to fully realize our long-term vision.
As a part of the dev team, you'll build the framework for both our clients and our team to succeed. You will be a pillar in our company as we grow and make our product the best product on the market!
In this role, you will:
Take ownership for delivering multiple features, their quality and assist team members in delivering their features
Recommend improvements to data quality, processes and code to make it easier to use, manage and maintain
Experience designing and developing data models and data migrations
Experience managing and maintaining ETL or ELT processes (Hevo, BigQuery, Looker, etc.)
Hands-on development experience with SQL, relational database design (ERDs)
Must be able to write SQL & contribute code
Be product-minded; focus not only on what needs to be built but why something needs to be built
You may be a great fit for this role if you have:
3 or more years of professional experience as a data analyst or data engineer
Cloud-Based Data Tools e.g. Hevo, BigQuery, Looker, etc.
Database technologies: SQL, RDBMS / Relational databases (Postgres, Amazon RDS / Aurora, etc.)
Nice to have experience: Python, Java, Typescript or another modern OOP language
Cloud infrastructure: AWS
Source control: Git
Working at Take Command
We’re excited to build a team and culture that reflects our values! We offer competitive pay, health benefits, and equity options to share with this position. We want you to be an owner along with us in revolutionizing healthcare.
A generously funded ICHRA for medical, dental, and vision premiums and medical expenses. You get to use our own product and we think that’s so exciting and rare!
Unlimited personal vacation in addition to regular company holidays.
401(k): 90-day eligibility for 2% match that vests immediately!
Access to LinkedIn Learning.
We have two beautiful offices conveniently located near The Hill (Dallas) and Holly Commons (Austin). The kitchen is well-stocked and we've designed the space to have lots of different areas to work!
Paid parental leave for new parents.
Flexible on where you work – we hope to see you around the office on average 3 days a week especially when white-boarding or building relationships with your colleagues, but you also have the ability to work from home or wherever you'd like when you need to get focus work done.
More About Us
We secured our Series A funding in 2021 and are thrilled to be able to expand our team. Despite being a small startup in a land of health insurance giants, we’re the recognized industry leader for what we do (health insurance reimbursements) and passionate about bringing it to market because we know we can help fix a broken system and improve our clients’ wellbeing and health outcomes.
We’ve been featured in The New York Times, The Wall Street Journal, The Dallas Morning News, and other national healthcare publications and are excited about our growth opportunities.
Take Command knows diversity and inclusion among our teammates is integral to our company’s success and growth. Our vision is to recruit, develop, and retain the best team from a diverse candidate pool.
This has mostly been about us, but we’d love to hear from you-we can’t wait to hear your story!
Take Command is an equal opportunity employer! We celebrate diversity and are committed to creating an inclusive environment for all employees.
Xrygn4NXMr","$125,000 /yr (est.)",1 to 50 Employees,Company - Private,Insurance,Insurance Agencies & Brokerages,2014,Unknown / Non-Applicable
Proits Hub LLC,#N/A,Remote,Data Analytics Engineer,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote
Speak with the employer
+91 9417599933",$60.00 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Integrated Technology Strategies, Inc.
4.0",4.0,"New York, NY",Data Engineer,"Integrated Technology Strategies is a provider of information technology consulting. Digital transformation, improving business performance, developing strategies and enhancing value is at the core of what we are.

Integrated Technology Strategies is looking to hire a Data Engineer to be a part of our Consulting team.

We are looking for someone passionate about data is focused on developing the platform for critical data products, including real time business metrics and analytics capabilities. The role requires supporting and collaborating with groups including Data Analytics/BI/Product, as well as our core backend API team. The individual will not be afraid to think out of the box and will play a key role in technical decision making. We are highly focused on giving ownership and responsibility to autonomous teams, using the right tools for the job, and building flexible architectures.","$110,128 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$5 to $25 million (USD)
"Quirch Foods
3.0",3.0,"Coral Gables, FL",Data Engineer,"About Quirch Foods
In business for over 50 years, Quirch Foods is a global distributor and exporter of fresh and frozen food products with a unique focus on ethnic cultures. We service our customers from 5 distribution centers strategically located in the United States. We have consistently been ranked among the top 50 exporters in the US and are one of the largest importers of seafood. Our customers include independent grocers, chain supermarkets, foodservice distributors, cruise lines, restaurants and food processors/manufacturers. We have an extensive product list with over 8,000 SKU's and carry niche brands such as Panamei, Mambo and Chiquita along with national brands like Tyson, IBP, Excel, JBS, Iowa Premium, Smithfield, Dietz & Watson, along with many others. We are headquartered in Coral Gables, FL.
Quirch is a portfolio company of Palladium Equity Partners, a leading mid-market private equity fund based in New York City, which partnered with the Executive Team to capitalize on Quirch Foods’ record of excellence and to grow it decisively beyond its current business.
Position Summary
At Quirch Foods we believe in leveraging technology as a source of competitive advantage. This includes customized ERP Solutions, Business Intelligence, EDI, Web and Mobile Solutions. We are currently looking for a Data Engineer to join our development efforts.
Essential Duties and Responsibilities:
Develop, test and maintain custom business solutions by leveraging state of the art software development practices using an agile methodology
Evaluate business requirement and help define problems and develop solutions
Participate in design and architecture discussions with business leaders, end users and IT team members
Document business requirements and solution, document code and provide support for creation of end user documentation
Update business knowledge, technical skills and soft skills, leverage educational opportunities, participate in professional organizations
Experience:
MS SQL Programming
Visual Studio\C#\WPS Experience
Microsoft SSAS, SSIS, SSRS desired but not required
Biztalk experience desired but not required
Skills and Requirements:
Bachelor’s degree in Computer Science, Engineering, Math or equivalent and\or related experience and training preferred
Strong analytical and problem-solving skills
Strong collaborator and team player
Great organizational skills, attention to detail and follow thru
Effective oral and written communication skills
Extensive familiarity with data management principles
Benefits:
Professional growth and developmental opportunities to grow your skills using state of the art technology
Comprehensive benefits package that includes: Medical, Dental, Prescription Drug Plan, Disability Plan, Life insurance Plan
401K savings Plan
Paid Holidays
Personal Time off
Employee Discounts
Quirch Foods is an Equal Opportunity Employer (EOE). Qualified applicants are considered for employment without regard to age, race, color, religion, sex, national origin, sexual orientation, disability, or veteran status. All applicants must be eligible to work in the United States.
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Vision insurance
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Coral Gables, FL 33134: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
What is your desired compensation?
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Data Engineering: 1 year (Preferred)
Work Location: In person","$89,340 /yr (est.)",201 to 500 Employees,Company - Private,Retail & Wholesale,Wholesale,1967,$500 million to $1 billion (USD)
"Virtualan Software LLC
4.5",4.5,"Atlanta, GA","Sr. Data Engineer (Python,Spark & Databricks)","Role : Senior Data Engineer (Python, Spark & Databricks)
Location : Hybrid (Atlanta, GA)
Pay: 60-70$ on C2C / 1099
RESPONSIBILITIES:
Create or modify the conceptual, logical and physical data models.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of sources like Hadoop, Spark, AWS Lambda, etc.
Lead and/or mentor a small team of data engineers.
Design, develop, test, deploy, maintain and improve data integration pipeline.
Develop pipeline objects using Apache Spark , PySpark and/or Python.
Design and develop data pipeline architectures using Spark and related AWS Services.
Communicate effectively with client leadership and business stakeholders.
Participate in proposal and/or SOW development.
REQUIREMENTS:
5+ years of professional work experience designing and implementing data pipelines in a cloud environment is required.
2+ years of experience migrating/developing data solutions in the AWS cloud is required.
1+ years of experience building/implementing data pipelines using DataBricks is required.
Expert level knowledge of using SQL to write complex, highly-optimized queries across large volumes of data.
Hands-on object-oriented programming experience using Python is required.
Professional work experience building real-time data streams using Spark.
Knowledge or experience in architectural best practices in building data lakes.
Bachelor or Master degree in Computer Science, Engineering, Information Systems or relevant degree.
Hybrid work schedule, and must live in the Atlanta, GA metro area. Must be open to up to 25% national travel to client locations when on engagements outside of the Atlanta, GA area.
Job Type: Contract
Pay: $60.00 - $70.00 per hour
Schedule:
Monday to Friday
Application Question(s):
Apply only if you are interested in Hybrid Role in Atlanta, GA. Interested ?
Experience:
Data Engineer: 4 years (Required)
Databricks: 1 year (Required)
Willingness to travel:
25% (Required)
Work Location: On the road",$65.00 /hr (est.),1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,2019,Less than $1 million (USD)
Infosoft,#N/A,"Peachtree Corners, GA",Data Pipeline Engineer - 2173,"Here are the job position details for your review:
Job Title: Data Pipeline Engineer (GStreamer/ DeepStream)
Pay Rate: $90 to 120/Hr.
Duration: 3 Months
Location: Peachtree Corners, GA
This position is for the Building Automation division.
We are looking for Data Pipeline Engineer (GStreamer) to participate in the development of Streaming data pipelines using NVIDIA Deepstream, GStreamer, Python programming, messaging queues, and related tools.
Preference is for candidates located in Atlanta, GA metro area. However, ok with non-local candidates who are open to 75% travel to Atlanta, GA.
Understanding the basics of our architecture/setup to be able to do setup/support for demos, testing, etc.
Basic computer hardware assembly and set-up
Possible set-up/installation of the application(s)
Possible development tasks depending on what the rest of the development team has for them
Develops business software applications, system software, and/or IT solutions on the basis of functional specifications.
Scope of position embraces both the customization or reuse/modification of system software and/or solutions as well as their development from scratch.
Consults with customers in creating functional specifications and work estimates, and supports Make-or-Buy decisions.
Analyses performance gaps of existing software environments and proposes measures to address efficiency problems.
Is also responsible for the implementation of data-driven components supporting data modeling and engineering in the context of data analytics (Data Stewardship).
Skills:
Python
Linux
Docker and Docker Swarm
Zero MQ / a good understanding of messaging
Desired Skills:
NVIDIA Deepstream SDK
Web Sockets
Flask
Protobuf
Possible:
GStreamer (if different than RTSP / H264 / MJPEG)
Job Type: Contract
Salary: $90.00 - $120.00 per hour
Ability to commute/relocate:
Peachtree Corners, GA: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
""Please share your Email ID""
Work Location: One location",$105.00 /hr (est.),Unknown,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Fulcrum Analytics
4.3",4.3,Remote,Data Engineer,"At Fulcrum, we provide cutting-edge data science consulting and software solutions to help companies solve challenging business problems. Through the utilization of our innovative products and dedicated team, clients are able to operate more efficiently, manage risk, and make informed business decisions every day. Fulcrum Analytics has stood at the forefront of data, analytics, and innovative software for over 25 years, offering sophisticated solutions, groundbreaking technologies, and winning strategies that help companies achieve their targeted results.
Role Description:
We are seeking a highly-skilled data engineer with experience in data analytics and data science. The ideal candidate will be responsible for building and maintaining data pipelines, designing and implementing databases and data models, and developing data-driven solutions to drive business growth for our clients.
Responsibilities:
Design, build, and maintain efficient, scalable, and reliable data pipelines using SQL, or programming languages such as Python or Java.
Develop and maintain databases and data models to meet clients’ analytical needs.
Identify data quality issues and develop processes to ensure data accuracy, completeness, and consistency.
Work with cross-functional teams to define and implement data integration strategies for new and existing systems.
Develop and maintain automated testing and deployment of analytics code and data pipelines.
Collaborate with data scientists to develop and implement machine learning models, statistical models, and other advanced analytics techniques.
Communicate analytical results and recommendations to technical and non-technical stakeholders through presentations and visualizations.
Stay up to date with emerging technologies and industry best practices, and recommend tools and processes that can improve data engineering and analytics processes.
Qualifications:
Bachelor's or master's degree in computer science, statistics, data science, or a related field.
2+ years of experience in data engineering, data analytics, and data science.
Strong programming skills in Python, with experience in building data pipelines using frameworks such as Airflow, Glue, Lambda, Athena, and BigQuery as well as data platforms such as Hadoop, Snowflake, and GCP.
Experience working with SOAP or RESTful web services.
Familiarity with version control, in particular Git.
Experience in designing, building, and optimizing relational and NoSQL databases.
Familiarity with data visualization tools, such as Tableau or Power BI.
Ability to understand and transform complex data into actionable insights.
Strong analytical and problem-solving skills, with a focus on delivering practical solutions to business challenges.
Ability to learn and solve problems independently.
Excellent communication and collaboration skills, with the ability to explain complex concepts to both technical and non-technical audiences.
If you are passionate about data analytics and want to work in a dynamic environment with talented colleagues, we encourage you to apply for this role. We offer competitive salary and benefits packages, as well as opportunities for professional growth and development.


Position Type: Full-time salaried
Reports to: Senior Data Engineer
Base Compensation: $110,000 - $140,000
Benefits: Medical, Dental, Vision, 401k with Match, Bonus, Unlimited PTO
Location: Remote within the United States of America; Eastern time zone preferred","$125,000 /yr (est.)",1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,1993,$5 to $25 million (USD)
Redzara.us,#N/A,"Atlanta, GA",Azure Data Engineer,"Responsible for developing new and modernizing existing applications to Azure Cloud Platform.
The ideal candidate must demonstrate in-depth knowledge and understanding of Database concepts.
Requirements:
7+ years of experience in Data Warehousing and Business Intelligence
5+ Years in BI development leveraging the Microsoft SQL Server. Needs to be an expert in T-SQL
5+ Years in SSIS and use of Stored Procedures.
1-2 Years in Azure Data Factory, SQL VMs, build and maintain ETL
Ingest and aggregate data from both internal and external data sources to build our data lake
Strong understanding data modeling (i.e., conceptual, logical, and physical model design, experience with Operation Data Stores, Enterprise Data warehouses and Data Marts).
Job Type: Contract
Pay: $43.49 - $57.00 per hour
Compensation package:
1099 contract
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Atlanta, GA 30334: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure Data Factory: 3 years (Required)
Azure: 3 years (Required)
Data warehouse: 1 year (Preferred)
Work Location: Hybrid remote in Atlanta, GA 30334",$50.25 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Siri info solutions inc,#N/A,"Beaverton, OR",Sr. Data Engineer,"Must Have Skills
PySpark
Python on AWS
Responsibilities:
Understanding of advanced data warehousing concepts, data modeling and extract, transform and load development
Advanced SQL skills with experience querying large datasets from multiple sources and developing automated reporting
Python skills for scripting, data manipulation, custom extract, transform and loads and statistical/regression analysis particularly, as they apply to sales and marketing operations and performance
Job Type: Full-time
Salary: $47.00 - $50.00 per hour
Experience level:
7 years
Work Location: On the road",$48.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Titan America
3.8",3.8,"Roanoke, VA",Data Engineer,"Data Engineer
We are seeking a motivated and detail-oriented Data Engineer to join our team based in Roanoke, Virginia. As a Data Engineer, you will play a crucial role in designing, developing and maintaining scalable data pipelines and transformations for business-critical use cases. In addition, you may take on various Software Development and Data Science responsibilities as we frequently share responsibilities and knowledge across our growing team.
Responsibilities:
Design, develop and maintain scalable data pipelines for extracting, transforming, and loading data from various sources
Collaborate with cross-functional teams to identify data needs and determine the best data solutions
Develop and implement data models to support business requirements and ensure data quality
Ensure the security and privacy of sensitive data by implementing appropriate access controls
Monitor and optimize data pipeline performance to ensure timely and accurate data delivery
Document data pipeline processes, data dictionaries, and data storage solutions
Requirements:
Bachelor's degree in Computer Science, Computer Engineering, or a related technical field
Minimum of five years of professional experience working as a Data Engineer or Software Developer
Strong hands-on experience with data warehouse and transformation solutions, i.e. Domo, Snowflake or similar
Proficient in at least one scripting language such as Python, JavaScript, or R
Understanding of data modeling, data integration and data quality processes
Familiarity with cloud platforms such as AWS, Azure, or Google Cloud Platform
Strong analytical and problem solving skills
Full Stack Software Development experience in a professional setting is highly desired, but not required
This is an excellent opportunity for a driven and collaborative individual to make a significant impact in a dynamic and growing team. If you have a passion for data and a desire to work in a fast-paced and dynamic environment, we want to hear from you!
Success begins with hiring the right people to partner with us as we grow and develop our businesses. People are central to everything we do. It is through their efforts and talents that Titan has been successful for over 100 years.
Titan America is an Equal Employment Opportunity (EEO) / Affirmative Action employer and welcomes all qualified applicants. Applicants will receive fair and impartial consideration without regard to race, sex, color, religion, national origin, age, disability, veteran status, genetic data, or other legally protected status.
Job Type: Full-time
Pay: $75,000.00 - $85,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Referral program
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Roanoke, VA 24019: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Do you currently or will you in the future need sponsorship to work in the United States?
This position requires the successful candidate to be in the office in Roanoke, VA at least 3 days a week. Can you meet this requirement?
This position requires the successful candidates to be a full time, W2 employee. Can you meet this requirement?
Education:
Bachelor's (Preferred)
Experience:
Data Engineering: 5 years (Preferred)
Work Location: Hybrid remote in Roanoke, VA 24019","$80,000 /yr (est.)",1001 to 5000 Employees,Company - Public,"Energy, Mining & Utilities",Mining & Metals,1902,Unknown / Non-Applicable
"Appsintegration INC
4.5",4.5,"California, KY",Data Engineer,"Role:- Data Engineer
Position:- Onsite (W2 position)
Experience:- 10+ Years
Responsibilities
Working closely with business teams to understand their requirements and develop reporting or analytic solutions.
Producing and automating the delivery of key metrics and KPIs to the business.
In some cases, this will focus on making data available and others will be developing full reports for end users.
Managing stakeholder demand and a backlog of reporting requirements and needs.
Actively working with users to understand data issues, tracing back data lineage, and helping the business put appropriate data cleansing and quality processes in place.
Managing secure access to data sets for the business.
Requirements
Bachelor's degree in Computer Science or equivalent.
Proficient in creating Data Factory pipelines for on-cloud ETL processing.
Experience implementing data pipelines using the latest technologies.
Experience developing using SQL, relational databases, and data warehousing technologies.
Extensive ETL experience.
Ability to scale systems in Cloud as per requirement.
Experience in handling structured and unstructured datasets.
Good to have experience in data modeling and Advanced SQL techniques.
Must-Have Skills: Azure Data Factory (ADF), SQL, Cloud-based ETL.
Good to Have Skills: Azure File Storage Services, Databricks, Azure Synapse.
you can reach me out on LinkedIn
here is my link :- linkedin.com/in/shravanthi-thakur-705a81231
Job Type: Contract
Salary: $42.00 - $45.00 per hour
Ability to commute/relocate:
California, KY 41007: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",$43.50 /hr (est.),1001 to 5000 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Atika Technologies,#N/A,"Pittsburgh, PA",Data Engineer,"Role: Data Engineer/PySpark Developer, Pittsburgh, PA - Day One Onsite
Fulltime and Contract(W2)
LinkedIn Must
USC/GC/EADs
Three round of interview: First two on Zoom Video Call, End Client Round will be Onsite on the Client's Location (Pittsburgh, PA)
Job Description:
Python(PySpark), Jupiter hub, airflow, Java
Database knowledge - Oracle / SQL / Data Analytics
Regulatory reporting knowledge preferred
Good communication skills
Open / Ability to learn new technologies
Very good Analytical Skills
Experience in ETL is preferable
Job Types: Full-time, Contract
Salary: $120,000.00 - $130,000.00 per year
Benefits:
Dental insurance
Health insurance
Life insurance
Compensation package:
Bonus pay
Performance bonus
Yearly pay
Experience level:
10 years
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Pittsburgh, PA 15259: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Required)
Spark: 4 years (Required)
Regulatory reporting: 3 years (Required)
SQL: 4 years (Required)
IT: 10 years (Required)
Airflow: 4 years (Required)
Jupiter: 4 years (Required)
Work Location: One location","$125,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Apolis
3.9",3.9,"Saint Louis, MO",Data Engineer (Python Focused),"Role: Data Engineer (Python Focused) (W2 ONLY)
Location : St. Louis, MO 63102
Duration: 18 Months
The primary responsibilities of this role will be to work as a part of the data and analytics team of the client. The ideal candidate would have experience using ETL and other data engineering tools in a cloud environment. Additionally, this candidate would have coding and scripting experience creating code to move and manipulate data using tools such in Python such as Spark or Pandas. Microsoft Azure experience is a plus but relevant experience with other cloud-based environments will be considered.
This team uses Microsoft Azure to build and productionize data sets and processes to the product owners and product users and this role will be expected to contribute to those objectives. As a member of this team, you will work in conjunction with other data engineers, platform engineers, solution developers and others to execute and deliver working solutions to the end users.
Technical Skills
High focus on Python
More Programmer/Coder experience, less Cloud experience needed.
Must have experience with data analytics/engineering and cloud technologies and an in-depth working knowledge and understanding of data engineering, design, and integration.
Intermediate or higher level of proficiency with Python Must have the proven ability to learn quickly and apply cloud data engineering principles and skills to new tools and use cases.
Ability to define and take technical ownership of detailed requirements and collaborate effectively with various team members such as Business Analysts and Project Managers and end users.
Excellent communication (verbal and written) and problem-solving skills are essential for this position.
Initiative, Proactive Cooperation, and curiosity are mandatory behavioral skills required for this position.
Experience with several of the following technologies is preferred:
Experience with Microsoft Azure Platform. In particular, the data processing toolsets including Azure Data Factory Pipelines, Azure Databricks, Azure Data Lake Storage.
Experience using Notebooks (Such as Jupyter, DataBricks) also desired.
Experience using code languages such as Python and C#
Experience using query languages such as SQL, MDX and DAX
Experience using scripting languages such as PowerShell, M-Query (Power Query), and Windows batch commands.
Understanding of Agile and DevOps principles for software development and ownership.
Familiar with compliance and controls for software development and deployment.
Education Required
A bachelor's in computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field [or equivalent work experience] is not required but is preferred.
Job Type: Contract
Salary: $60.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Vision insurance
Compensation package:
Overtime pay
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
St. Louis, MO: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Preferred)
ETL: 5 years (Preferred)
Cloud infrastructure: 4 years (Preferred)
Work Location: One location",$65.00 /hr (est.),501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,1996,$25 to $100 million (USD)
WellTrust Medical Group,#N/A,"Charlotte, NC",Data Engineer,"The Data Engineer will assist the senior analyst in complex modeling and projects to ensure their completion within the allotted timeline. Helps track measures of success and develop reports that measure operational and customer metrics. Demonstrates ability to troubleshoot data consumption and data quality problems.

Responsibilities:
Build and maintain reports using My SQL Server
Identify optimal data sources, structure data validation, and cache optimization.
Performs business problem modeling and quantitative analytics including regression analysis, time-series, hierarchical cluster analysis, and data mining.
Helps track measures of success and develop reports that measure operational and customer metrics.
Demonstrates ability to troubleshoot data consumption and data quality problems.
Evaluates and recommend improvements to current models, metrics, and analytic processes to improve operational efficiency and insight.
Design, document, and communicate effective technical solutions meeting end-user requirements
Perform analyses with large amounts of complex data, and uses data visualization tools to communicate results.
Requirements:

Bachelor's degree or equivalent experience in a quantitative field (Mathematics, Statistics, Computer Science, Economics, Finance, Physics, Engineering)
3+ years of professional working experience as a Data Engineer or Analyst
1-3 years of data warehouse experience
Build and maintain reports using My SQL Server
Strong analytical and creative problem-solving skills
Design, document, and communicate effective technical solutions meeting end-user
SQL and Excel Macro development
Strong understanding of various data environments such as SQL, Oracle, Big Query, etc.
Experience with data visualization tools such as Looker Studio, Tableau, D3.js, etc.
Proficient in R, Python, MATLAB, Java, or another programming language.",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Kastech Software Solutions Group
4.0",4.0,"Dallas, TX",Data Engineer,"In KastechSSG we are are seeking for a Cloud DB Platform Engineer with Data Enablement Services experience ($50/hour), to work in a Hybrid Schedule (3 days on-site, 2 days home office) for one of our partner companies!
Hybrid Schedule info: onsite 3 days per week from list of locations: Charlotte NC, Dallas TX, Chandler AZ, Newark, NJ - Must be local to one of these locations.
** Contract would be $50 per hour on W2 (18 months). American Citizens or GC Holders Only**
3-5 years experience in:
Terraform, Azure/GCP, DevOps Engineering, ETL, Client/AI, Data lake, Data architecture.
GCP Key Services Experience (ie Big Query, Composer, Vertex AI, DataProc, Data Catalog, Notebooks).
Azure Key Service Experience (ie Synapse, Purview, Data Lake, Logic Apps, Cognitive Services).
Job Type: Full-time
Salary: $50.00 per hour
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you American? If not, please specify your visa type
Would you be willing to work under W2?
Experience:
Terraform: 3 years (Preferred)
Azure Data Lake: 3 years (Preferred)
GCP Key Service Exp: 3 years (Preferred)
DevOps: 3 years (Preferred)
Work Location: In person",$50.00 /hr (est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2008,$25 to $100 million (USD)
"Fuge Technologies Inc
4.7",4.7,"Jersey City, NJ",Big Data Engineer,"Big Data Engineering Technical Lead
Mandatory Skills
Big Data – Spark, Kafka, Databricks, Hive, Nifi
AWS, Java, Scala
Client Interview Needed for Selection (yes / No)
Yes
Detailed JD (Pl share the Detailed Description, 1 liner JD will not work)
10+ yrs experience as Data Engineer
Extensive experience in spark on data processing
Kafka design experience like monitoring & parallelism.
Comfortable with Python/Java/Scala coding experience
Crisp / clear cut communication
Flexible with learning new technology
Flink experience is a plus
AWS experience is nice to have
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Jersey City, NJ 07302: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",$72.50 /hr (est.),1 to 50 Employees,Contract,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Egrove Systems Corporation
4.4",4.4,"Dallas, TX",Data Engineer Flink,"Java, Bigdata, Azure & Flink
Flink Sr. Developer, who has implemented and dealt with failure scenarios of processing data through Flink.
A senior who can help his Flink development team, guiding and helping them implement custom solutions through Flink.
Seasoned Java developer who knows about all aspects of SDLC.
Worked on integrations of other technologies with Flink, eg: Kafka, MongoDB, etc
Azure experience in the areas of Messaging, Data processing, preferably on Flink or on Databricks.
Job Type: Contract
Salary: From $51.83 per hour
Compensation package:
Monthly bonus
Experience level:
11+ years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
Work Location: On the road
Speak with the employer
+91 732)-860-7733",$51.83 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Internet & Web Services,#N/A,Unknown / Non-Applicable
"shaped.ai Inc.
4.0",4.0,"New York, NY",Data Engineer,"Hello! We’re building the future of content discovery infrastructure with AI — come join us!

Shaped (YC W22) is an API for developers to seamlessly add personalized ranking and recommendation to their products. These frictionless discovery experiences help end-users find what they want faster, and, consequently, grow conversion and engagement business metrics. Behind the scenes, we’re building state-of-the-art ML infrastructure for training and deploying large AI models to power our personalization engine.
We’re a seed stage start-up backed by top-tier investors (e.g. Y-Combinator, Susa Ventures, Tribe Capital etc…) and executives (e.g. Google, Amazon, Uber, Dropbox etc…). Our team comes from Meta, Google, Apple and Uber. We’re a remote team but have a small office in Brooklyn, New York.
We’ve been lucky to have a huge amount of success since our initial launch earlier this year and now looking to expand our real-time data engineering capabilities. We are looking for a data engineer to design, build and optimize Shaped's real-time streaming data ingestion endpoints. You will be a founding engineer working on state-of-the-art infrastructure. As one of Shaped’s early employees you will help shape our product, culture and vision.
This role is for a full-time data engineer at $120k - $200k a year and 0.1 - 0.5% equity.

Responsibilities:
Manage the unification of our batch and real-time data processing pipelines.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Requirements:
Bachelor's in computer science, data science or mathematics related field. Master's degree or PhD will be advantageous.
5+ years of data engineering experience, ideally applied to machine learning use-cases.
Experience with AWS or common cloud technologies
Proficient with Python, SQL.
Extensive knowledge of both offline and online large scale data processing frameworks: Spark, Beam, Flink.
Extensive knowledge with stream technologies like Kakfa or Kinesis.
Excellent written and verbal communication skills.
We’re excited to work with you. Come build the future of AI and discovery with us!","$160,000 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"RelMap Consulting
4.8",4.8,"Addison, TX",Senior Data Engineer (Hybrid),"The data engineer entails:
Advanced SQL Server Development
Ability to create and modify views
Ability to create and modify custom functions
Ability to write complex queries, including joins, temp table processing, and common table expressions (CTE’s)
Ability to create and modify stored procedures using TSQL
Ability to analyze and optimize query performance using indices
Ability to use Red Gate’s SQL Compare to perform a schema (AKA “do a manual roll”) between one server an another, and between revision control and a database
Ability to stand-up new database server VM’s in Linux and Windows using VMWare
Ability to backup and restore a database in a Linux and Windows environment
Ability to manage and script automated database jobs
Ability to use PowerShell / Bash to interact with databases from the command line
Addition skills that would be desired but not required
· Experience with Redis
· Experience with MongoDB / CosmosDB
· Experience with Azure and Azure SQL
· C#, JavaScript (NodeJS / ES6), or Python experience
Job Type: Contract
Pay: $70.00 - $80.00 per hour
Compensation package:
Hourly pay
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Addison, TX: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person",$75.00 /hr (est.),1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2010,$1 to $5 million (USD)
DocsInk LLC,#N/A,"Wrightsville Beach, NC",Data Engineer,"DocsInk is on a mission to provide a collaborative and exciting alternative to the traditional healthcare model. Through the complete virtualization of healthcare organizations, we are equipping providers with the ability to deliver better patient care with speed, efficiency, and accuracy. With the widespread focus on providing value-based care, DocsInk is making it possible to provide superior care while also maximizing reimbursements. In today’s environment, shifting the way we approach healthcare is essential and DocsInk is on the cutting edge of where the industry is headed. We are excited to be growing our team and are looking for driven individuals with a passion for making a difference in the healthcare industry.

As our Data Engineer, you will be in the driver’s seat as our team creates elegant and forward-thinking software solutions for our healthcare customers. You will work with fellow developers, designers, training & support, quality assurance, and management in a fast-paced environment to take our products to the next level and exceed expectations. We are looking for someone who is incredibly talented to do what most cannot and yet humble enough to easily work with peers, accept guidance from more experienced staff, and mentor more junior staff. If designing and developing complex integration efforts is what gets you out of bed each morning, then we want you to help us reshape healthcare.

People are our most important asset and the number one reason we all love working at DocsInk. As a team, we value accountability, transparency and collaboration. We have agile teams, with clear, outcomes-focused goals. Delivering innovative solutions and superior service are at the forefront of what we do, and the collective ideas of the team allow us to continue to excel in a fun and energetic environment.

Professional Requirements and Responsibilities:

Role / Responsibilities:

Implement robust ETL solutions that integrate healthcare data feeds using tools such as Apache NiFi, NextGen Connect (JavaScript), PostgreSQL, MySQL, and Linux shell scripting
Configuring and managing deployments to AWS infrastructure consisting of both traditional servers and serverless components. CI/CD tools include Bitbucket and AWS Code Deploy
Building direct interfaces with hospitals and EMR vendors utilizing industry standards such as HL7 2.x, C-CDA, and FHIR as well as implementing interfaces that consume formats such as XML, JSON, and delimited files
Utilizing APIs and web services to permit bi-directional flows of data to and from our partner organizations
Monitoring operational ETL processes, triaging and resolving issues as they arise to meet SLAs
Writing PL/SQL functions and packages to automate workflows in a Postgres environment linked with web technologies
Maintaining data standards (including adherence to HIPAA), documenting work, and championing process improvements
Work as a project team member to jointly deliver high-quality business solutions consistent with project objectives and constraints

Knowledge and Skills Required:

4+ years experience with HL7 2.x
2+ years experience with Apache Nifi
2+ years experience with NextGen Connect
Advanced knowledge of PostgreSQL, MySQL, AWS Code Deploy, HL7 2.x, C-CDA, FHIR, and HIPAA data compliance

Qualifications and Other Attributes Required:

Degree in Computer Science or equivalent field, or equivalent work experience
Highly self-motivated team player who takes initiative
Strong sense of personal accountability; contributes beyond job role and responsibilities
Excellent problem-solving skills; meticulous & methodical","$85,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Cureatr
4.1",4.1,Remote,Data Engineer,"About Us
We are a thriving, medication management solutions company, headquartered in Manhattan, but largely virtually distributed, committed to tackling the $528B suboptimal medication management problem. Patient medication information is often incomplete for healthcare providers and systems – it’s our mission to fill in the gaps. We provide real-time, universal access to accurate medication data for over 265 million patients and high-quality interventions by board certified telepharmacists. Through our solutions, healthcare providers can reduce preventable hospitalizations, avoid readmissions, and ultimately, improve patient outcomes. We’re already used by dozens of hospital systems, with tens of thousands of clinicians and supporting staff using us to care for millions of patients.
Here at Cureatr, we are driven by a core set of values and strive to incorporate them into everything we do. We care deeply about our users, patients, and employees, and we aim to live and operate compassionately. We take pride in creating a diverse, inclusive, and equitable company culture, where our open and honest approach to work fosters innovation and encourages employees to achieve professional and personal growth.

The Role
We’re looking for a Data Engineer to join our growing Engineering team. Use the power of healthcare data to find insights that will demonstrate the value Cureatr is delivering to our customers, help improve patient outcomes, and expand the scope of Cureatr’s mission. As a Data Engineer at Cureatr, you will be responsible for designing, implementing and maintaining our data infrastructure, pipelines, automation, and ETL processes. You will work closely with our Data Science and Analytics teams to ensure that our data is clean, accurate, and readily available for analysis. You will also be responsible for ensuring that our data meets industry standards for security, privacy, and compliance.
This is a remote, work from home position, but candidates must be located within the U.S. Ideal candidates must be able to travel to our New York City office, or TBD company-wide offsite locations, up to 4x per year.

You’ll be part of a team working to solve problems like:
Increasing the number of patients who receive the opportunity to do a medication reconciliation, which can help keep them out of the hospital.
Streamlining and automating processes so that our pharmacists can serve as many patients as possible.
Predicting patient events likely to occur so that our clinical team can adequately prevent them before it’s too late.
Helping our pharmacists make the most of data and insights that will help them be as effective as possible in serving patients.
Evaluating the effectiveness of our clinical services impact at the population level.

What You’ll Do:
Design, implement, and maintain our data infrastructure, pipelines, automation, and ETL processes to support our data needs.
Build and maintain scalable data models and databases.
Collaborate with Data Science and Analytics teams to understand their data needs and provide data solutions accordingly.
Ensure data quality and integrity by implementing data validation and cleansing processes.
Ensure data security and compliance with healthcare regulations.
Monitor and optimize data performance and scalability.
Design and implement data governance policies and procedures.
Evaluate and implement new data technologies and tools to improve our data capabilities.

Your Skills:
3+ years of experience as a Data Engineer or similar role.
Strong programming skills in languages such as Python and SQL.
Experience using git to track changes.
Experience with data modeling and database design.
Experience with the “modern data stack” preferred
Experience with ETL and data pipeline development.
Knowledge of healthcare data and related standards (e.g., HL7, FHIR) is a plus.
Familiarity with AWS.
Strong problem-solving and analytical skills.
Excellent communication and collaboration skills.

Technology Stack:
Python
RedShift
dbt
Dagster
MongoDB
Looker
AWS

Educational Requirement: Bachelor’s Degree or relevant work experience

Salary: $110,000 - $140,000 plus full benefits

Why work with us?
Make a measurable impact on patients’ lives
Be part of a company dedicated to living its core values:
Empathy
Honesty
Integrity
Impact
Fun
Equity in Healthcare
Work in the company without micromanagement
Share your vision and make an impact on the product
Opportunities for career growth
Remote work
Competitive salary
Premium benefits (Health, Dental, Vision, 401k, unlimited PTO, more)

Equal Employment Opportunity Policy
Cureatr is an equal opportunity/affirmative action employer. All qualified applicants will receive consideration for employment without regard to sex, pregnancy, race, religion or religious creed, color, gender, gender identity, gender expression, national origin, ancestry, physical or mental disability, medical condition, genetic information, marital status, registered domestic partner status, age, sexual orientation, military or veteran status, protected veteran status, or any other basis protected by federal, state, local law, ordinance, or regulation and will not be discriminated against on these bases
Disclaimer: we are aware of a scam targeting applicants using a cureatrcareers.com email address. Please block and report this sender, as they are not affiliated with Cureatr. If you have any questions, please contact us at jobs@cureatr.com. Thank you!
uBajh3OfeT","$125,000 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2012,Less than $1 million (USD)
Redzara.us,#N/A,"Atlanta, GA",Azure Data Engineer,"Responsible for developing new and modernizing existing applications to Azure Cloud Platform.
The ideal candidate must demonstrate in-depth knowledge and understanding of Database concepts.
Requirements:
7+ years of experience in Data Warehousing and Business Intelligence
5+ Years in BI development leveraging the Microsoft SQL Server. Needs to be an expert in T-SQL
5+ Years in SSIS and use of Stored Procedures.
1-2 Years in Azure Data Factory, SQL VMs, build and maintain ETL
Ingest and aggregate data from both internal and external data sources to build our data lake
Strong understanding data modeling (i.e., conceptual, logical, and physical model design, experience with Operation Data Stores, Enterprise Data warehouses and Data Marts).
Job Type: Contract
Pay: $43.49 - $57.00 per hour
Compensation package:
1099 contract
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Atlanta, GA 30334: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure Data Factory: 3 years (Required)
Azure: 3 years (Required)
Data warehouse: 1 year (Preferred)
Work Location: Hybrid remote in Atlanta, GA 30334",$50.25 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Infosoft,#N/A,"Peachtree Corners, GA",Data Pipeline Engineer - 2173,"Here are the job position details for your review:
Job Title: Data Pipeline Engineer (GStreamer/ DeepStream)
Pay Rate: $90 to 120/Hr.
Duration: 3 Months
Location: Peachtree Corners, GA
This position is for the Building Automation division.
We are looking for Data Pipeline Engineer (GStreamer) to participate in the development of Streaming data pipelines using NVIDIA Deepstream, GStreamer, Python programming, messaging queues, and related tools.
Preference is for candidates located in Atlanta, GA metro area. However, ok with non-local candidates who are open to 75% travel to Atlanta, GA.
Understanding the basics of our architecture/setup to be able to do setup/support for demos, testing, etc.
Basic computer hardware assembly and set-up
Possible set-up/installation of the application(s)
Possible development tasks depending on what the rest of the development team has for them
Develops business software applications, system software, and/or IT solutions on the basis of functional specifications.
Scope of position embraces both the customization or reuse/modification of system software and/or solutions as well as their development from scratch.
Consults with customers in creating functional specifications and work estimates, and supports Make-or-Buy decisions.
Analyses performance gaps of existing software environments and proposes measures to address efficiency problems.
Is also responsible for the implementation of data-driven components supporting data modeling and engineering in the context of data analytics (Data Stewardship).
Skills:
Python
Linux
Docker and Docker Swarm
Zero MQ / a good understanding of messaging
Desired Skills:
NVIDIA Deepstream SDK
Web Sockets
Flask
Protobuf
Possible:
GStreamer (if different than RTSP / H264 / MJPEG)
Job Type: Contract
Salary: $90.00 - $120.00 per hour
Ability to commute/relocate:
Peachtree Corners, GA: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
""Please share your Email ID""
Work Location: One location",$105.00 /hr (est.),Unknown,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Integrated Technology Strategies, Inc.
4.0",4.0,"New York, NY",Data Engineer,"Integrated Technology Strategies is a provider of information technology consulting. Digital transformation, improving business performance, developing strategies and enhancing value is at the core of what we are.

Integrated Technology Strategies is looking to hire a Data Engineer to be a part of our Consulting team.

We are looking for someone passionate about data is focused on developing the platform for critical data products, including real time business metrics and analytics capabilities. The role requires supporting and collaborating with groups including Data Analytics/BI/Product, as well as our core backend API team. The individual will not be afraid to think out of the box and will play a key role in technical decision making. We are highly focused on giving ownership and responsibility to autonomous teams, using the right tools for the job, and building flexible architectures.

Responsibility:
Continue to evolve the internal Reporting and Analytics platform on top of Snowflake on AWS infrastructure.
Experience in Architect, design and implementing scalable ETL and data processing systems to handle the big data ecosystem including data collection, processing, ETL and Data warehouse.
Build soft real time capabilities and insight into product metrics to help product managers and BI/Analytics understand and optimize product features and guide product decisions.
Participate and contribute to the capabilities and engineering priorities across the organization.
Contribute to the codebase and participate in code review.
Build analytics tools that utilize the data pipeline to
provide actionable insights into operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product,
Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Reporting to the Senior Director, Software Engineering, you’ll be responsible for overseeing engineering product quality and delivery and setting and overseeing technical standards for teams who are working on everything from customer-facing applications.

Skill Requirements:
Solid understanding of real time data processing with Kafka, Spark and Flink and batch data processing frameworks on EMR and Snowflake.
Passion for building world-class data platforms that support a global customer base
Solid engineering background and understanding of programming languages such as Python, Java or equivalent
5+ years of progressive experience in data infrastructure development, with a track record of successful high-quality deliveries
Experience of working in an agile environment and embracing engineering best practices
Ability to apply both technical competence and interpersonal skills to achieve business outcomes
High emotional intelligence, sound temperament, and professional attitude
Strong understanding of SQL, experience with key databases such as Snowflake, MS-SQL and Postgres
Knowledge of the internals of how database systems work to design models for varied use cases.
Experience with CI and CD in an AWS environment with Terraforms
Experience with key Data technologies, such as Sqoop. Kafka will be a plus
Proven experience in building secure data platforms
Bachelor’s degree in Computer Science or equivalent","$110,128 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$5 to $25 million (USD)
"Take Command Health
4.9",4.9,"Dallas, TX",Data Engineer,"Company: Take Command
Position: Data Engineer
Salary: $120,000-$130,000 plus bonus
Location: Dallas or Austin, TX
About Take Command
Take Command is a start-up on a mission to improve the healthcare system, starting with health insurance. Pragmatically speaking, we help employers reimburse employees for individual insurance instead of offering a traditional one-size-fits-all group plan. We believe this model can empower employees (when they have the right support) to be savvy healthcare consumers and have a transformative impact on the entire healthcare system.
Let’s be honest—health insurance is usually a confusing, frustrating, and even emotional experience for people. We want to fix that with a new model, great technology, and a superior user experience. We have made a great start, but we need your help to fully realize our long-term vision.
As a part of the dev team, you'll build the framework for both our clients and our team to succeed. You will be a pillar in our company as we grow and make our product the best product on the market!
In this role, you will:
Take ownership for delivering multiple features, their quality and assist team members in delivering their features
Recommend improvements to data quality, processes and code to make it easier to use, manage and maintain
Experience designing and developing data models and data migrations
Experience managing and maintaining ETL or ELT processes (Hevo, BigQuery, Looker, etc.)
Hands-on development experience with SQL, relational database design (ERDs)
Must be able to write SQL & contribute code
Be product-minded; focus not only on what needs to be built but why something needs to be built
You may be a great fit for this role if you have:
3 or more years of professional experience as a data analyst or data engineer
Cloud-Based Data Tools e.g. Hevo, BigQuery, Looker, etc.
Database technologies: SQL, RDBMS / Relational databases (Postgres, Amazon RDS / Aurora, etc.)
Nice to have experience: Python, Java, Typescript or another modern OOP language
Cloud infrastructure: AWS
Source control: Git
Working at Take Command
We’re excited to build a team and culture that reflects our values! We offer competitive pay, health benefits, and equity options to share with this position. We want you to be an owner along with us in revolutionizing healthcare.
A generously funded ICHRA for medical, dental, and vision premiums and medical expenses. You get to use our own product and we think that’s so exciting and rare!
Unlimited personal vacation in addition to regular company holidays.
401(k): 90-day eligibility for 2% match that vests immediately!
Access to LinkedIn Learning.
We have two beautiful offices conveniently located near The Hill (Dallas) and Holly Commons (Austin). The kitchen is well-stocked and we've designed the space to have lots of different areas to work!
Paid parental leave for new parents.
Flexible on where you work – we hope to see you around the office on average 3 days a week especially when white-boarding or building relationships with your colleagues, but you also have the ability to work from home or wherever you'd like when you need to get focus work done.
More About Us
We secured our Series A funding in 2021 and are thrilled to be able to expand our team. Despite being a small startup in a land of health insurance giants, we’re the recognized industry leader for what we do (health insurance reimbursements) and passionate about bringing it to market because we know we can help fix a broken system and improve our clients’ wellbeing and health outcomes.
We’ve been featured in The New York Times, The Wall Street Journal, The Dallas Morning News, and other national healthcare publications and are excited about our growth opportunities.
Take Command knows diversity and inclusion among our teammates is integral to our company’s success and growth. Our vision is to recruit, develop, and retain the best team from a diverse candidate pool.
This has mostly been about us, but we’d love to hear from you-we can’t wait to hear your story!
Take Command is an equal opportunity employer! We celebrate diversity and are committed to creating an inclusive environment for all employees.
Xrygn4NXMr","$125,000 /yr (est.)",1 to 50 Employees,Company - Private,Insurance,Insurance Agencies & Brokerages,2014,Unknown / Non-Applicable
Fracsys Inc,#N/A,"Washington, DC",Data Engineer,"Fracsys Inc is hiring a Data Engineer position. The ideal candidate must have at least 12+ years of industry experience. He or she must be responsible for successful technical delivery of Data Warehousing and Business Intelligence projects using a formal project management methodology.
Responsibilities
Directs, motivates and develops key technical personnel
Provides technical direction across the full lifecycle system development process
Provides strategic planning and comprehensive senior level technical consulting to IT senior management and senior technical staff
Working independently within guidelines
Provides detailed inputs for plans and schedules, including goals, risks, and resource allocation
Conduct diagnostic, troubleshooting steps and present conclusions to a varied audience
comfortable slinging some code to solve a problem
Build Data Marts and Data warehouses to facilitate analytical capabilities across the varied sources of data
Design and implement data patterns on cloud using cloud Data warehouse and data virtualization tools
Evaluate new tools and technologies for target state on cloud
Learn and implement new ETL, BI tools and data warehousing technologies
Qualifications
Minimum twelve (12) years in a Technical Lead or Consulting role leading Information Management related initiatives (system integration, data warehouse build, data mart build, or similar)
MUST have proven leadership experience for team of at least 3 technical resources.
Experience creating Data Marts, Data warehouses on-premise and on cloud for real time and batch processing frameworks
Experience building and optimizing big-data data pipelines and data sets including Postgres, AWS Relational Data Service
extensive hands-on experience building and optimizing data structures for data analytics, data science and business intelligence
Experience building self-service data consumption patterns and knowledge of cloud-based data Lake platforms
Experience wrangling data (structured and unstructured), in-depth knowledge of database architecture
Experience utilizing or leading implementations leveraging ETL tools (Informatica / Talend), BI Reporting tools such as MicroStrategy, Microsoft Power BI, data modeling tools such as Erwin, Oracle, SQL server, NoSQL, JDBC, UNIX shell scripting, PERL and JAVA, XML/JSON files, SAS , Python, AWS cloud-native technologies, S3, Athena, Redshift
Familiarity with the following technologies: Hadoop, Kafka, Airflow, Hive, Presto, Athena, S3, Aurora, EMR, Spark
Ability to drive, contribute to, and communicate solutions to technical product challenges
Ability to roll-up your sleeves and work in process or resource gaps and fill it in yourself
Excellent written and oral communication skills in English
Education
Bachelor’s degree in computer science, Data Analytics, Information Systems, or a related degree or equivalent experience. Master’s degree is preferred.
Job Type: Full-time
Pay: From $96,336.72 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Experience level:
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Washington, DC: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person","$96,337 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Intersec
4.5",4.5,Remote,Senior Data Engineer,"Architect and implement highly scalable data pipeline to streamline migration of data from source data repositories to the centralized data platform
Implement agreed upon data models (e.g., data marts) to enable faster and efficient consumption of data by downstream consumers
Conduct performance tuning to optimize data migration techniques and ensure adherence to identified service-level agreements (SLAs)
Define and implement automated validation checks to ensure data integrity between source and target
Assist in onboarding consumers with existing technology to connect to the platform
Setup predictive alert mechanism for pipeline degradation for IMF pipelines.
Enhance data quality framework for data pipelines and derived datasets
Enhance Data Audit Balance and Control mechanism for data pipelines
Enhance pipelines to be certified copies of authoritative source
Populate metadata for the Enterprise with particular focus on the Data Sources being onboarded to EDP (Technical and Business Glossaries) related to IMF Modernization Programs
Validate correct lineage for datasets
Required:
*8+ years of Databricks, Data Modeling, and/or Metadata management experience
* Familiarity with architecting and developing real and near-real-time solutions
* Bachelor's Degree in Engineering, Mathematics, computer Science, Statistics or other quantitative discipline, or 4 years equivalent professional experience
* US Citizenship and ability to gain a public trust
Preferred:
* Familiarity with IRS systems and ETL pipelines
* Ability to work independently and manage multiple tasks
* IRS MBI Clearance with IRS Laptop and Informatica Installed
Job Type: Full-time
Salary: $70.00 - $75.00 per hour
Schedule:
Monday to Friday
Application Question(s):
Are you a US/GC card holder?
How many years of Databricks, Data Modeling, and/or Metadata management experience do you have?
Work Location: Remote",$72.50 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,Unknown / Non-Applicable
"Barracuda Networks Inc.
3.6",3.6,"Alpharetta, GA",Data Engineer,"Job ID: 23-579

Come Join Our Passionate Team! At Barracuda, we make the world a safer place. We believe every business deserves access to cloud-enabled, enterprise-grade security solutions that are easy to buy, deploy, and use. We protect email, networks, data and applications with innovative solutions that grow and adapt with our customers’ journey. More than 200,000 organizations worldwide trust Barracuda to protect them — in ways they may not even know they are at risk — so they can focus on taking their business to the next level.
We know a diverse workforce adds to our collective value and strength as an organization. Barracuda Networks is proud to be an Equal Opportunity Employer, committed to equal employment opportunity and equitable compensation regardless of race, gender, religion, sex, sexual orientation, national origin, or disability.
Envision yourself at Barracuda
We currently have an exciting new position open for a Data Engineer. This person will assist in driving technology innovation for engineering projects and work on architecture of AWS, Elastic and Kubernetes. They will also work closely with the Technology Architect to create and drive the Architecture Review Board.
What you’ll be working on
Determine internal / external customer data needs and identify system specifications
Analyze the needs of large systems and breaking them down into smaller manageable parts
Responsible for designing the journey of data from various sources to the database and everything in between
Plan and design the structure of data, brainstorm with team
Analyze best technology products and systems for internal teams
Help provide build vs buy decisions on data architecture
Communicate data requirements to engineers; explain data architecture to them and provide assistance throughout SDLC
Choose / maintain suitable software, hardware and integration methods
Help resolve technical problems as and when they arise
Review testing procedures to ensure data performant and error free
Ensure that database satisfy quality standards and procedures. Setup such standards.
Work with senior IT personnel to devise plans for future IT requirements of the organization
Produce progress reports on key infrastructures - Elastic and Data Pipelines
What you bring to the role
Bachelor Degree or Post Graduate in Computer Science, or relevant work experience
5+ years of experience in an Engineering / Operations heavy organization
1+ years of experience as a Data Analysis and Data Design responsible for data architecture of a SaaS application
Experience in understanding and designing data sharding in a distributed database
Some Hands-on experience in software development and system administration
Knowledge of strategic IT solutions
Familiarity with programming languages like JavaScript or Java or Python
Familiarity with Linux OS and Windows alike
Experience in cloud computing and cloud Technologies (AWS, GCP, Azure)
Excellent communication skills – you should be adept at listening to, understanding, and explaining key concepts to managerial and technical resources.
Good Anticipation and problem-solving skills
Ability to prioritize and manage time – responsible for multiple streams of work at the same time
Relationship building - Ability to form a good rapport with internal and external stakeholders
In-depth knowledge about quality standards and security best practices

Preferred Skills:
Expert level Understanding of Elastic eco system - Elasticsearch, Logstash, Kibana, Filebeats
Understanding of secure tunnel communication using VPN
Experience designing data at scale
AWS experience is preferred in Cloud Technologies
What you’ll get from us
A team where you can voice your opinion, make an impact, and where you and your experience are valued. Internal mobility – there are opportunities for cross training and the ability to attain your next career step within Barracuda.
Equity, in the form of non-qualified options
High-quality health benefits
Retirement Plan with employer match
Career-growth opportunities
Flexible Time Off and Paid Time Off benefits
Volunteer opportunities
#LI-Remote","$98,161 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,$100 to $500 million (USD)
"Hiscox
4.1",4.1,"Atlanta, GA",Senior Data Engineer,"job description:
You will be a senior member of our US IT Team data engineering practice. You will be an early member of this team and will work closely with the Director of Data Engineering to help establish it. We will have our work cut out for us delivering planned work, building the practice and evolving our technical landscape. This is an exciting opportunity for the right individual to join at the very beginning and grow with the new team.
As a senior you will be expected to lead, mentor others and deputise for the director of data engineering when required. Yoi will also continue to contribute as a hands-on engineer. You will have the knowledge, best earned through 5+ years of experience, to truly inhabit this senior role in an ambitious team.
What you’ll be doing:
Working with business stakeholders and our delivery team to understand high value business problems that can be solved through the application of data processing and analytical systems.
Developing, expanding and evolving our existing databases and ETL pipelines.
Working with architects to design, build and support a transformational cloud-based data platform for the business.
Being a core and professional member of the new data engineering practice
Understand business requirements and help refine into development tasks and estimate their complexity.
Research, evaluate and adopt new technologies with a right tool for the job mentality.
Focus on both speed of delivery and quality, with suitable pragmatism – ensuring your solutions are always appropriate and not too complex or over-engineered.
Quick progression of projects from proof-of-concept to post-production stage.
Communication and presentation of ideas to colleagues in all parts of the wider tech team.
Participating in code reviews for the data engineering practice.
Act as a servant leader in our practice – ensuring team members have
person specification:
Knowledge of, or ability to rapidly adopt our core languages for data engineering – SQL and/or Python.
Experience working using a modern DevOps approach. You should not just have written data code but also the CI/CD pipelines necessary to test and deploy that code in a professional environment.
A robust understanding of core data engineering topics – ETL vs ELT, structured and unstructured data, data quality and data governance.
Ability to contribute to all aspects of a solution – design, infrastructure, development, testing and maintenance.
The ability to design and advocate for technical solutions to business problems.
Effective collaboration with technical and non-technical team members through agile ceremonies – roadmap planning, feature workshops, backlog elaboration, code review.
Understanding of cloud technology and significant hands-on experience working with it. You must understand how cloud-native solutions must be built differently from traditional ones.
Track record of taking initiative and delivering projects end-to-end; clear evidence of being self-driven and motivated
Immense curiosity, high energy and desire to go the extra mile to make a difference.

Desirable Skills
Beyond the required skills we are open to individuals of diverse talents. Experience with additional technologies, data science knowledge or a business background are all valued. We want to know how your unique abilities can contribute to our team.
Our Technology
We operate in a diverse technical landscape and are looking for flexible engineers who can adapt to and use many different tools. We would not expect any engineer to be familiar with the entire tech stack – no engineer can. Instead, we seek people with a good understanding of data structures and algorithms and the ability to apply this knowledge in learning new tools.
We have an existing data landscape involving both MS SQL Server and Oracle databases. We use SSIS for ETL. We will also need to source data from our integration layer – webMethods.
If some of that sounds like it needs bringing up to date, we know! We hope to start building a modern cloud data platform to realise the full value of our data. The technologies we use for that platform are still to be decided. Likely it will be based on Snowflake with Azure Data Factory for ELT. Other parts of our business have had success using Prefect for orchestration and dbt for transformation and we will reuse where it is sensible.
We prepare data for use by analysts working with a variety of tools – Tableau, PowerBI, and even Excel.
We take a DevOps approach and strive for continuous integration / continuous deployment. We use Azure Pipelines to deploy our code. We deploy infrastructure the same way using Terraform and Docker.
Our technology landscape is not fixed. Our engineering and architecture teams drive the technology we adopt.


Salary range $140,000 - $170,000","$155,000 /yr (est.)",1001 to 5000 Employees,Company - Public,Insurance,Insurance Carriers,1901,$1 to $5 billion (USD)
"Tipico - North America
4.0",4.0,"Denver, CO",Data Engineer,"Company Description

Tipico is an energetic, passionate, bold tech company, and we love sports! Tipico has recently expanded to the US market, establishing offices in Hoboken, New Jersey and Denver, Colorado. We are driven by our core values: passion, progress and trust. Our goal is to build the best possible mobile sports betting product in the industry, in order to amplify the emotions of millions of sports fans every day. Let’s Make This Interesting ™
Please note: this role is located in our Denver, CO office; we work off a hybrid model and come into the office 2 days per week.

Job Description

The Data Engineer will report directly to the Data Engineering Manager and work together with rest of the data engineers, BI analysts, and product teams. The position holder ensures this is done by doing the following:
Create and maintain both batch (ETL/ELT) and real-time data pipelines and architecture
Ensure all data provided by Data Platform is of the highest quality and is delivered in a timely manner and in line with agreed SLAs
Assemble large and complex data sets that meet functional and non-functional business requirements
Identify, design, and implement internal process improvements, including automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies
Collaborate with stakeholders including data architects, Executive, Product, Data and IT team members from the beginning to the delivery of a project
Build a scalable, modern, and trusted data ecosystem, making sure it is responsive, available and in line with business requirements
Take part in sprints, workloads, and deliverables through agile methodologies
Manage quality assurance, verification of accuracy and consistency of data, specifically KPI measures sent to the business
Thrive working in a fast-paced and challenging environment
Open to continuous learning individually and with the team
Comfortable with being challenged / seeking challenge
Willing to work flexible hours if needed

Qualifications
3+ years in Data Platforms / Datawarehouse / BI experience
Must have experience in successfully implementing data-centric applications, such as data warehouses, operational data stores, and data integration projects
Strong analytical and problem-solving skills that balance with creative approaches
Ability to learn new and complex concepts quickly
Strong experience with large-scale production databases and SQL
Solid understanding of cloud data services: AWS services such as S3, Athena, EC2, RedShift, EMR, EKS, RDS and Lambda
Proficient with one or more big data & new data technologies such as DBT, NIFI, Airflow, Kafka
Experience with data modelling
Experience with ETL development (extractions, data load, aggregation, transformation)
Knowledge in working with time-series/analytics databases such as Elasticsearch
Understanding of containerization and orchestration technologies like Docker/Kubernetes
Must be able to take ownership of projects with a high level of attention to detail and priority
Team player with a hands-on approach and attitude
Must be comfortable with on call rotation
Nice to have:
Familiarity with Marketing tooling and/or CRM systems is preferred
Familiarity with using business intelligence tools such as Domo, Tableau or Looker
Knowledge about and / or experience with sports betting
Background in digital companies, online, gambling, gaming, e-commerce, marketing, or mobile apps
Familiarity with one scripting language, preferably Python

Additional Information

What's in it for you:
Work in an environment where you, your work and ideas matter and have an impact.
You will be a be part of the newest and up and coming USA online iGaming and sports betting companies in the market.
Work in a new and young business with high growth potential.
Build your own success story together with us.
Work with self-organized, self-responsible and entrepreneurial employees.
Start-up feeling, backed-up by a leading European sports betting house. We are a high-volume business and are taking off in the US!
Competitive salary, Medical, Vision, Dental Benefits, Unlimited PTO, and more
Salary for this position is $120k+ & annual performance bonus","$120,000 /yr (est.)",201 to 500 Employees,Company - Private,"Arts, Entertainment & Recreation",Gambling,2004,Unknown / Non-Applicable
"Merkle
3.7",3.7,"Norristown, PA",Data Engineer,"Company Description

We are Dentsu International. We innovate the way brands are built. That means we do things differently so they’re better than before. In this way, we make our clients’ most important marketing assets—their brands—win in a changing world.
Dentsu International is a modern marketing solutions company. Our mission is to help clients navigate, progress, and thrive in a world of change. Businesses rely on our integrated network of agencies and specialized practices to champion meaningful progress through creative, media, commerce, data, and technology. Part of Dentsu Group, our global network comprises 66,000 diverse people in 143 countries, who are dedicated to teaming for growth and good. Follow us on Twitter @DentsuUSA and visit dentsu.com/us.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.

Job Description
Develop onboarding process to receive and process information related to onboarding a new subscriber
Convert Snowflake Stored Procedures to Python Scripts directly in Airflow
Develop metadata driven solutions for managing data processing and cadence
End to end automation development and testing
Building and maintaining stored procedures and scripts in Snowflake Data Cloud
Building and maintaining complex SQL queries for ETL/ELT processing
Develop Dashboard for data processed for audit and governance needs
Follow Agile methodologies to deliver iterative feature sets rapidly
Maintain codebase using source control methodologies, Git
Collaborate with business analysts, data analysts, and other developers to deliver powerful new features
Coordinate with teams as part of a larger data-sharing system
Contribute to a collaborative, positive, and enjoyable environment for your operations team
Performing quality assurance and testing
Writing user and technical documentation such as runbooks

Qualifications
Strong communication skills both verbal and written
Strong understanding of conditional programming logic
Knowledge of software engineering and development methodologies, techniques, and tools; includes Issue Tracking (JIRA) Software Development Lifecycle SDLC), and Continuous Integration / Continuous Development (CI/CD)
Experience working with large databases and data warehouses utilizing both relational and dimensional design concepts
Eagerness to learn new technologies, programming languages and platforms to solve complex data challenges
Experience with Python required
Experience with SQL required
Experience with Snowflake Data Cloud is highly desirable
Experience with Amazon Web Services (AWS) a plus
Experience with Airflow (or other orchestration) a plus
Experience in database marketing is a plus

Additional Information

Perks:
Great compensation package
Comprehensive healthcare plans
401(k) with employer match
Flexible time-off
16 weeks paid parental leave
Additional Information
The anticipated salary range for this position is $94,000-$146,000. Salary is based on a range of factors that include relevant experience, knowledge, skills, other job-related qualifications, and geography. A range of medical, dental, vision, 401(k) matching, paid time off, and/or other benefits also are available. For more information regarding dentsu benefits, please visit dentsubenefitsplus.com
Employees from diverse or underrepresented backgrounds encouraged to apply.
Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact recruiting@dentsuaegis.com if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.
About dentsu
Dentsu is the network designed for what’s next, helping clients predict and plan for disruptive future opportunities in the sustainable economy. Taking a people-centered approach to business transformation, dentsu combines Japanese innovation with a diverse, global perspective to drive client growth and to shape society www.dentsu.com.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.
Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact your recruiter if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.","$120,000 /yr (est.)",5001 to 10000 Employees,Company - Private,Media & Communication,Advertising & Public Relations,1971,$500 million to $1 billion (USD)
"CDNetworks Inc.
2.9",2.9,"Monrovia, CA",Jr. Data Center Engineer,"CDNetworks is one of the top leading CDN & Edge Service Providers with global offices in Korea, Japan, Singapore, Malaysia, China, Russia, London, and Canada. We focus on delivering integrated cloud and edge computing solutions with unparalleled speed, ultra-low latency, rigorous security, and reliability so that our clients can focus on what’s most important – growing their business.
The Jr. Data Center Engineer is a Full-Time position based in Monrovia, CA. Job is performed indoors in a Data Center or Warehouse environment. 75% Travel required for this position.
Job Responsibilities
Ensure all incidents are logged and resolved, gather all relevant data, and ensure all incidents and tasks follow the appropriate procedures.
Support data center activities and work closely with our system and network team to complete tasks/projects.
First responder to all alerts and problem reports while managing communications between departments and handling crisis documentation and dissemination after the fact.
Utilize internal systems such as JIRA/Wiki to manage project plans and progress.
Performing general system administration duties including OS patching and upgrades, batch job monitoring, system and hardware diagnostics, and other activities to ensure optimal health and performance of all systems as required.
Resolve complex problems related to Server and H/W areas.
Assisting/working closely with Network, System Engineers to configure customer requirements.
Physical deployment of network devices, servers, cables, etc.
Assembling/dissembling server hardware for deployment and OS installation and network equipment testing.
Maintain existing department and system documentation (update workflow, process, training documentation).
Other duties as assigned.
Abilities Required
Good verbal and written communication skills, and ability to work independently with minimum instruction.
Basic degree of mentorship, training, and direction team members skills.
Knowledge of IDC industry.
Intermediate degree of analytical and project management skills.
Other Features of Job
Job is performed indoors in a Data Center or Warehouse environment.
Language Skills
Excellent communication skills (English) – written and verbal. Bilingual Chinese or Korean is a plus!!
Job Type: Full-time
Salary: $20.00 - $25.00 per hour
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Health insurance
Life insurance
Paid time off
Professional development assistance
Referral program
Vision insurance
Schedule:
10 hour shift
8 hour shift
Evening shift
Monday to Friday
Night shift
On call
Overtime
Supplemental pay types:
Bonus pay
Ability to commute/relocate:
Monrovia, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Computer networking: 1 year (Preferred)
Shift availability:
Overnight Shift (Required)
Night Shift (Required)
Day Shift (Required)
Willingness to travel:
75% (Preferred)
Work Location: In person",$22.50 /hr (est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2000,$10+ billion (USD)
"Iron Bow Technologies
3.5",3.5,"Washington, DC",Sr. Data Center Engineer,"Iron Bow Technologies is seeking a Sr. Data Center Engineer to support our Federal customer, The Department of Justice (DOJ), Criminal Division (CRM), in Washington, DC. The selected candidate will support our flexible and scalable solution with the DOJ CRM to extend cloud-native features to existing legacy and next-generation applications while also future proofing the infrastructure to easily accommodate future requirements, such as transitioning to a public cloud.
This position is hybrid and requires 2 days a week onsite at the customer location in NW Washington, DC and support at the customer data center as needed in Sterling, VA.
Essential Duties and Responsibilities:
Assess and provide VMware solutions to include vCenter, vSphere, SRM, ESXi
Primary storage solutions and data file storage management, NAS file storage solutions
Data Center blade servers and rack servers
Server virtualization
Experience:
10 years of related experience providing system administration support in a virtualized environment, including at least 6 years of VMWare solutioning, including vCenter, vSphere, SRM, and ESXi
Data Center operations experience, with SAN and NAS
Experience supporting primary storage solutions and data file storage management, NAS file storage solutions
Hands on experience with Data Center blade and rack servers
Ability to work staggered shift hours when needed to support critical coverage and/or time-sensitive needs
Be a member of our Team providing coordinated 24x7 IaaS on-call support
U.S. Citizen
Currently hold or obtain/maintain a Public Trust High clearance
Education:
BS in Computer Science, Information Science, or other relevant discipline is preferred – additional years of experience may be considered in lieu of the degree
Preferred Certifications:
VMware Certified Professional
Primary storage certifications such as HPE, Dell-EMC, NetApp, or Pure Storage
VCP-DCV
Travel Requirements:
No travel
PRINCIPALS ONLY. NO AGENCIES PLEASE. MUST BE A US CITIZEN ABLE TO OBTAIN A PUBLIC TRUST SECURITY CLEARANCE.
Iron Bow Technologies is a leading IT solution provider dedicated to successfully transforming technology investments into business capabilities for government, commercial and healthcare clients. Our depth of technical expertise, global reach and strategic partnerships with industry leaders as well as disruptive technology partners uniquely positions Iron Bow to target the right solution to ensure your successful business outcome. We partner with our clients from planning and implementation through ongoing maintenance and management to deliver solutions that are strong, flexible and on target .
Iron Bow Technologies is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.
#LI-TM1",#N/A,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,1983,$500 million to $1 billion (USD)
"Fulcrum Analytics
4.3",4.3,Remote,Data Engineer,"At Fulcrum, we provide cutting-edge data science consulting and software solutions to help companies solve challenging business problems. Through the utilization of our innovative products and dedicated team, clients are able to operate more efficiently, manage risk, and make informed business decisions every day. Fulcrum Analytics has stood at the forefront of data, analytics, and innovative software for over 25 years, offering sophisticated solutions, groundbreaking technologies, and winning strategies that help companies achieve their targeted results.
Role Description:
We are seeking a highly-skilled data engineer with experience in data analytics and data science. The ideal candidate will be responsible for building and maintaining data pipelines, designing and implementing databases and data models, and developing data-driven solutions to drive business growth for our clients.
Responsibilities:
Design, build, and maintain efficient, scalable, and reliable data pipelines using SQL, or programming languages such as Python or Java.
Develop and maintain databases and data models to meet clients’ analytical needs.
Identify data quality issues and develop processes to ensure data accuracy, completeness, and consistency.
Work with cross-functional teams to define and implement data integration strategies for new and existing systems.
Develop and maintain automated testing and deployment of analytics code and data pipelines.
Collaborate with data scientists to develop and implement machine learning models, statistical models, and other advanced analytics techniques.
Communicate analytical results and recommendations to technical and non-technical stakeholders through presentations and visualizations.
Stay up to date with emerging technologies and industry best practices, and recommend tools and processes that can improve data engineering and analytics processes.
Qualifications:
Bachelor's or master's degree in computer science, statistics, data science, or a related field.
2+ years of experience in data engineering, data analytics, and data science.
Strong programming skills in Python, with experience in building data pipelines using frameworks such as Airflow, Glue, Lambda, Athena, and BigQuery as well as data platforms such as Hadoop, Snowflake, and GCP.
Experience working with SOAP or RESTful web services.
Familiarity with version control, in particular Git.
Experience in designing, building, and optimizing relational and NoSQL databases.
Familiarity with data visualization tools, such as Tableau or Power BI.
Ability to understand and transform complex data into actionable insights.
Strong analytical and problem-solving skills, with a focus on delivering practical solutions to business challenges.
Ability to learn and solve problems independently.
Excellent communication and collaboration skills, with the ability to explain complex concepts to both technical and non-technical audiences.
If you are passionate about data analytics and want to work in a dynamic environment with talented colleagues, we encourage you to apply for this role. We offer competitive salary and benefits packages, as well as opportunities for professional growth and development.


Position Type: Full-time salaried
Reports to: Senior Data Engineer
Base Compensation: $110,000 - $140,000
Benefits: Medical, Dental, Vision, 401k with Match, Bonus, Unlimited PTO
Location: Remote within the United States of America; Eastern time zone preferred","$125,000 /yr (est.)",1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,1993,$5 to $25 million (USD)
"DataArt
4.6",4.6,Remote,Data Engineer,"CLIENT
Our client is a pioneer in US schools’ education since 2000, it is leading the way in next-generation curriculum and formative assessment. They develop a number of solutions and interactive web products for teachers, students and their parents.

The company is technology-driven with a huge number of software engineers involved in product development. They have a very solid approach to technology, employ best practices and processes, with a focus on cutting-edge frameworks, languages and tools.

We invite to the company, not a project
TEAM
There are multiple autonomous and empowered teams who work with their own part of the product and choose their own way of doing things and they expect people to be proactive and good team players.
Responsibilities
Building well-tested and optimized ETL data pipelines for both full and delta extraction
Collaborating with data analysts and learning scientists to store, aggregate, and calculate captured students’ work
Contributing to leading industry data standards, such as Caliper Analytics or xAPI
Improving our deployment and testing automation data pipelines
Requirements
3+ years of professional software development or data engineering experience
Strong CS and data engineering fundamentals
Proven fluency in SQL and a development language such as Python
Experience with the following storages: Snowflake, AWS Storage Services (S3, RDS, Glacier, DynamoDB)
Experience with ETL/BI: Airflow, DBT, Matillion, Looker
Experience with Cloud Infrastructure: AWS Kinesis, Lambda, API Gateway, Terraform
Understanding of ETL/ELT pipelines and Data Warehousing design, tooling, and support
Understanding of different data formatting (JSON, CSV, XML) and data storage techniques (3NF, EAV Model, Star Schema, Data Lake)
BS in Computer Science, Data Science, or equivalent
Experience in education or ed-tech
Proven passion and talent for teaching fellow engineers and non-engineers
Proven passion for building and learning: open source contributions, pet projects, self-education, Stack Overflow
Strong communication skills in writing, conversation",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,1997,$100 to $500 million (USD)
"SECURE RPO
4.3",4.3,Manhattan,Senior data engineer,"Must have skills:
8+ years of experience building high performance scalable enterpris`e analytics or data centric solutions
8+ or 5+ years of experience in the design and build of data extraction, transformation, and loading processes by writing custom data pipelines
At least 5 years of experience implementing complex ETL pipelines preferably in connection with Glue and/or Spark
Exceptional coding and design skills in Python or Java/Scala
Hands-on experience with AWS (i.e. Glue, Aurora Postgres, Lambda, EMR, EKS, Redshift, etc.)
Experience with visualization tools like QuickSight, PowerBI, Looker or Tableau
Experience with Talend (ETL) is a big plus
Roles and Responsibility:
Drive a high impact and high visibility project that enables data availability, encompasses data analytics, machine learning, and petabyte scale datasets, and provides reliable and timely access to thousands of data sources
Design, architect and support systems for collecting, storing, and analyzing data at scale
Recommend improvements and modifications on new and existing data and ETL pipelines. Create optimal data pipeline architecture and systems using Apache Airflow
Create data analytics for d ata scientists to innovate, build and optimize our ecosystem
Assemble large, complex data sets that meet functional and non-functional business requirements
Be responsible for ingesting data into our data lake and providing frameworks and services for operating on that data including the use of Spark
Analyze, debug and correct issues with data pipelines
Operate on or build solution required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, AWS and Spark technologies
Job Type: Full-time
Salary: $56.80 - $80.16 per hour
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Informatica: 7 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: On the road",$68.48 /hr (est.),1 to 50 Employees,Unknown,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Proits Hub LLC,#N/A,Remote,Data Analytics Engineer,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote
Speak with the employer
+91 9417599933",$60.00 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Radiant System,#N/A,"Pleasanton, CA",Data Engineer,"First Priority: Pleasanton, CA –
Second Priority for a superstar: Plano, TX
Experience in developing and deploying data pipelines, preferably in the Cloud
Experience working with Spark, Databricks, and other relevant technologies in Microsoft Azure cloud
Experience in productionizing and deploying Big Data platforms and applications
Expert in writing SQL statements
Job Type: Contract
Application Question(s):
Are you willing to work on W2?
Work Location: In person",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Quadrant Resource
4.1",4.1,Remote,Data Engineer,"Job Description:
4-6 years of experience as a Data Engineer in the Snowflake-Fivetran-dbt stack
• Possess a solid understanding of end to end Data ingestion, ETL, data analysis, Reporting processes
• Expert in ETL development using Fivetran and experienced in implementing data pipelines for Snowflake Cloud DWH
•Experienced in using dbt for data transformation
•Expertise on Tableau/Metabase for building reports/dashboards
•Expert in troubleshooting and resolving ETL issues, data load failures/problems and transformation translation problems
•Excellent communicator with the ability to work effectively with distributed teams
Job Type: Full-time
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Data Engineer: 9 years (Required)
Fivetran: 8 years (Required)
DBT: 7 years (Required)
Snowflake: 6 years (Required)
Work Location: Remote",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,2004,Unknown / Non-Applicable
"Titan America
3.8",3.8,"Roanoke, VA",Data Engineer,"Data Engineer
We are seeking a motivated and detail-oriented Data Engineer to join our team based in Roanoke, Virginia. As a Data Engineer, you will play a crucial role in designing, developing and maintaining scalable data pipelines and transformations for business-critical use cases. In addition, you may take on various Software Development and Data Science responsibilities as we frequently share responsibilities and knowledge across our growing team.
Responsibilities:
Design, develop and maintain scalable data pipelines for extracting, transforming, and loading data from various sources
Collaborate with cross-functional teams to identify data needs and determine the best data solutions
Develop and implement data models to support business requirements and ensure data quality
Ensure the security and privacy of sensitive data by implementing appropriate access controls
Monitor and optimize data pipeline performance to ensure timely and accurate data delivery
Document data pipeline processes, data dictionaries, and data storage solutions
Requirements:
Bachelor's degree in Computer Science, Computer Engineering, or a related technical field
Minimum of five years of professional experience working as a Data Engineer or Software Developer
Strong hands-on experience with data warehouse and transformation solutions, i.e. Domo, Snowflake or similar
Proficient in at least one scripting language such as Python, JavaScript, or R
Understanding of data modeling, data integration and data quality processes
Familiarity with cloud platforms such as AWS, Azure, or Google Cloud Platform
Strong analytical and problem solving skills
Full Stack Software Development experience in a professional setting is highly desired, but not required
This is an excellent opportunity for a driven and collaborative individual to make a significant impact in a dynamic and growing team. If you have a passion for data and a desire to work in a fast-paced and dynamic environment, we want to hear from you!
Success begins with hiring the right people to partner with us as we grow and develop our businesses. People are central to everything we do. It is through their efforts and talents that Titan has been successful for over 100 years.
Titan America is an Equal Employment Opportunity (EEO) / Affirmative Action employer and welcomes all qualified applicants. Applicants will receive fair and impartial consideration without regard to race, sex, color, religion, national origin, age, disability, veteran status, genetic data, or other legally protected status.
Job Type: Full-time
Pay: $75,000.00 - $85,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Referral program
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Roanoke, VA 24019: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Do you currently or will you in the future need sponsorship to work in the United States?
This position requires the successful candidate to be in the office in Roanoke, VA at least 3 days a week. Can you meet this requirement?
This position requires the successful candidates to be a full time, W2 employee. Can you meet this requirement?
Education:
Bachelor's (Preferred)
Experience:
Data Engineering: 5 years (Preferred)
Work Location: Hybrid remote in Roanoke, VA 24019","$80,000 /yr (est.)",1001 to 5000 Employees,Company - Public,"Energy, Mining & Utilities",Mining & Metals,1902,Unknown / Non-Applicable
"Appsintegration INC
4.5",4.5,"California, KY",Data Engineer,"Role:- Data Engineer
Position:- Onsite (W2 position)
Experience:- 10+ Years
Responsibilities
Working closely with business teams to understand their requirements and develop reporting or analytic solutions.
Producing and automating the delivery of key metrics and KPIs to the business.
In some cases, this will focus on making data available and others will be developing full reports for end users.
Managing stakeholder demand and a backlog of reporting requirements and needs.
Actively working with users to understand data issues, tracing back data lineage, and helping the business put appropriate data cleansing and quality processes in place.
Managing secure access to data sets for the business.
Requirements
Bachelor's degree in Computer Science or equivalent.
Proficient in creating Data Factory pipelines for on-cloud ETL processing.
Experience implementing data pipelines using the latest technologies.
Experience developing using SQL, relational databases, and data warehousing technologies.
Extensive ETL experience.
Ability to scale systems in Cloud as per requirement.
Experience in handling structured and unstructured datasets.
Good to have experience in data modeling and Advanced SQL techniques.
Must-Have Skills: Azure Data Factory (ADF), SQL, Cloud-based ETL.
Good to Have Skills: Azure File Storage Services, Databricks, Azure Synapse.
you can reach me out on LinkedIn
here is my link :- linkedin.com/in/shravanthi-thakur-705a81231
Job Type: Contract
Salary: $42.00 - $45.00 per hour
Ability to commute/relocate:
California, KY 41007: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",$43.50 /hr (est.),1001 to 5000 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Atika Technologies,#N/A,"Pittsburgh, PA",Data Engineer,"Role: Data Engineer/PySpark Developer, Pittsburgh, PA - Day One Onsite
Fulltime and Contract(W2)
LinkedIn Must
USC/GC/EADs
Three round of interview: First two on Zoom Video Call, End Client Round will be Onsite on the Client's Location (Pittsburgh, PA)
Job Description:
Python(PySpark), Jupiter hub, airflow, Java
Database knowledge - Oracle / SQL / Data Analytics
Regulatory reporting knowledge preferred
Good communication skills
Open / Ability to learn new technologies
Very good Analytical Skills
Experience in ETL is preferable
Job Types: Full-time, Contract
Salary: $120,000.00 - $130,000.00 per year
Benefits:
Dental insurance
Health insurance
Life insurance
Compensation package:
Bonus pay
Performance bonus
Yearly pay
Experience level:
10 years
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Pittsburgh, PA 15259: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Required)
Spark: 4 years (Required)
Regulatory reporting: 3 years (Required)
SQL: 4 years (Required)
IT: 10 years (Required)
Airflow: 4 years (Required)
Jupiter: 4 years (Required)
Work Location: One location","$125,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Fuge Technologies Inc
4.7",4.7,"Jersey City, NJ",Big Data Engineer,"Big Data Engineering Technical Lead
Mandatory Skills
Big Data – Spark, Kafka, Databricks, Hive, Nifi
AWS, Java, Scala
Client Interview Needed for Selection (yes / No)
Yes
Detailed JD (Pl share the Detailed Description, 1 liner JD will not work)
10+ yrs experience as Data Engineer
Extensive experience in spark on data processing
Kafka design experience like monitoring & parallelism.
Comfortable with Python/Java/Scala coding experience
Crisp / clear cut communication
Flexible with learning new technology
Flink experience is a plus
AWS experience is nice to have
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Jersey City, NJ 07302: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",$72.50 /hr (est.),1 to 50 Employees,Contract,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Apolis
3.9",3.9,"Saint Louis, MO",Data Engineer (Python Focused),"Role: Data Engineer (Python Focused) (W2 ONLY)
Location : St. Louis, MO 63102
Duration: 18 Months
The primary responsibilities of this role will be to work as a part of the data and analytics team of the client. The ideal candidate would have experience using ETL and other data engineering tools in a cloud environment. Additionally, this candidate would have coding and scripting experience creating code to move and manipulate data using tools such in Python such as Spark or Pandas. Microsoft Azure experience is a plus but relevant experience with other cloud-based environments will be considered.
This team uses Microsoft Azure to build and productionize data sets and processes to the product owners and product users and this role will be expected to contribute to those objectives. As a member of this team, you will work in conjunction with other data engineers, platform engineers, solution developers and others to execute and deliver working solutions to the end users.
Technical Skills
High focus on Python
More Programmer/Coder experience, less Cloud experience needed.
Must have experience with data analytics/engineering and cloud technologies and an in-depth working knowledge and understanding of data engineering, design, and integration.
Intermediate or higher level of proficiency with Python Must have the proven ability to learn quickly and apply cloud data engineering principles and skills to new tools and use cases.
Ability to define and take technical ownership of detailed requirements and collaborate effectively with various team members such as Business Analysts and Project Managers and end users.
Excellent communication (verbal and written) and problem-solving skills are essential for this position.
Initiative, Proactive Cooperation, and curiosity are mandatory behavioral skills required for this position.
Experience with several of the following technologies is preferred:
Experience with Microsoft Azure Platform. In particular, the data processing toolsets including Azure Data Factory Pipelines, Azure Databricks, Azure Data Lake Storage.
Experience using Notebooks (Such as Jupyter, DataBricks) also desired.
Experience using code languages such as Python and C#
Experience using query languages such as SQL, MDX and DAX
Experience using scripting languages such as PowerShell, M-Query (Power Query), and Windows batch commands.
Understanding of Agile and DevOps principles for software development and ownership.
Familiar with compliance and controls for software development and deployment.
Education Required
A bachelor's in computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field [or equivalent work experience] is not required but is preferred.
Job Type: Contract
Salary: $60.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Vision insurance
Compensation package:
Overtime pay
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
St. Louis, MO: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Preferred)
ETL: 5 years (Preferred)
Cloud infrastructure: 4 years (Preferred)
Work Location: One location",$65.00 /hr (est.),501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,1996,$25 to $100 million (USD)
WellTrust Medical Group,#N/A,"Charlotte, NC",Data Engineer,"The Data Engineer will assist the senior analyst in complex modeling and projects to ensure their completion within the allotted timeline. Helps track measures of success and develop reports that measure operational and customer metrics. Demonstrates ability to troubleshoot data consumption and data quality problems.

Responsibilities:
Build and maintain reports using My SQL Server
Identify optimal data sources, structure data validation, and cache optimization.
Performs business problem modeling and quantitative analytics including regression analysis, time-series, hierarchical cluster analysis, and data mining.
Helps track measures of success and develop reports that measure operational and customer metrics.
Demonstrates ability to troubleshoot data consumption and data quality problems.
Evaluates and recommend improvements to current models, metrics, and analytic processes to improve operational efficiency and insight.
Design, document, and communicate effective technical solutions meeting end-user requirements
Perform analyses with large amounts of complex data, and uses data visualization tools to communicate results.
Requirements:

Bachelor's degree or equivalent experience in a quantitative field (Mathematics, Statistics, Computer Science, Economics, Finance, Physics, Engineering)
3+ years of professional working experience as a Data Engineer or Analyst
1-3 years of data warehouse experience
Build and maintain reports using My SQL Server
Strong analytical and creative problem-solving skills
Design, document, and communicate effective technical solutions meeting end-user
SQL and Excel Macro development
Strong understanding of various data environments such as SQL, Oracle, Big Query, etc.
Experience with data visualization tools such as Looker Studio, Tableau, D3.js, etc.
Proficient in R, Python, MATLAB, Java, or another programming language.",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Egrove Systems Corporation
4.4",4.4,"Dallas, TX",Data Engineer Flink,"Java, Bigdata, Azure & Flink
Flink Sr. Developer, who has implemented and dealt with failure scenarios of processing data through Flink.
A senior who can help his Flink development team, guiding and helping them implement custom solutions through Flink.
Seasoned Java developer who knows about all aspects of SDLC.
Worked on integrations of other technologies with Flink, eg: Kafka, MongoDB, etc
Azure experience in the areas of Messaging, Data processing, preferably on Flink or on Databricks.
Job Type: Contract
Salary: From $51.83 per hour
Compensation package:
Monthly bonus
Experience level:
11+ years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
Work Location: On the road
Speak with the employer
+91 732)-860-7733",$51.83 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Internet & Web Services,#N/A,Unknown / Non-Applicable
"Intersec
4.5",4.5,Remote,Senior Data Engineer,"Architect and implement highly scalable data pipeline to streamline migration of data from source data repositories to the centralized data platform
Implement agreed upon data models (e.g., data marts) to enable faster and efficient consumption of data by downstream consumers
Conduct performance tuning to optimize data migration techniques and ensure adherence to identified service-level agreements (SLAs)
Define and implement automated validation checks to ensure data integrity between source and target
Assist in onboarding consumers with existing technology to connect to the platform
Setup predictive alert mechanism for pipeline degradation for IMF pipelines.
Enhance data quality framework for data pipelines and derived datasets
Enhance Data Audit Balance and Control mechanism for data pipelines
Enhance pipelines to be certified copies of authoritative source
Populate metadata for the Enterprise with particular focus on the Data Sources being onboarded to EDP (Technical and Business Glossaries) related to IMF Modernization Programs
Validate correct lineage for datasets
Required:
*8+ years of Databricks, Data Modeling, and/or Metadata management experience
* Familiarity with architecting and developing real and near-real-time solutions
* Bachelor's Degree in Engineering, Mathematics, computer Science, Statistics or other quantitative discipline, or 4 years equivalent professional experience
* US Citizenship and ability to gain a public trust
Preferred:
* Familiarity with IRS systems and ETL pipelines
* Ability to work independently and manage multiple tasks
* IRS MBI Clearance with IRS Laptop and Informatica Installed
Job Type: Full-time
Salary: $70.00 - $75.00 per hour
Schedule:
Monday to Friday
Application Question(s):
Are you a US/GC card holder?
How many years of Databricks, Data Modeling, and/or Metadata management experience do you have?
Work Location: Remote",$72.50 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,Unknown / Non-Applicable
"shaped.ai Inc.
4.0",4.0,"New York, NY",Data Engineer,"Hello! We’re building the future of content discovery infrastructure with AI — come join us!

Shaped (YC W22) is an API for developers to seamlessly add personalized ranking and recommendation to their products. These frictionless discovery experiences help end-users find what they want faster, and, consequently, grow conversion and engagement business metrics. Behind the scenes, we’re building state-of-the-art ML infrastructure for training and deploying large AI models to power our personalization engine.
We’re a seed stage start-up backed by top-tier investors (e.g. Y-Combinator, Susa Ventures, Tribe Capital etc…) and executives (e.g. Google, Amazon, Uber, Dropbox etc…). Our team comes from Meta, Google, Apple and Uber. We’re a remote team but have a small office in Brooklyn, New York.
We’ve been lucky to have a huge amount of success since our initial launch earlier this year and now looking to expand our real-time data engineering capabilities. We are looking for a data engineer to design, build and optimize Shaped's real-time streaming data ingestion endpoints. You will be a founding engineer working on state-of-the-art infrastructure. As one of Shaped’s early employees you will help shape our product, culture and vision.
This role is for a full-time data engineer at $120k - $200k a year and 0.1 - 0.5% equity.

Responsibilities:
Manage the unification of our batch and real-time data processing pipelines.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Requirements:
Bachelor's in computer science, data science or mathematics related field. Master's degree or PhD will be advantageous.
5+ years of data engineering experience, ideally applied to machine learning use-cases.
Experience with AWS or common cloud technologies
Proficient with Python, SQL.
Extensive knowledge of both offline and online large scale data processing frameworks: Spark, Beam, Flink.
Extensive knowledge with stream technologies like Kakfa or Kinesis.
Excellent written and verbal communication skills.
We’re excited to work with you. Come build the future of AI and discovery with us!","$160,000 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Kastech Software Solutions Group
4.0",4.0,"Dallas, TX",Data Engineer,"In KastechSSG we are are seeking for a Cloud DB Platform Engineer with Data Enablement Services experience ($50/hour), to work in a Hybrid Schedule (3 days on-site, 2 days home office) for one of our partner companies!
Hybrid Schedule info: onsite 3 days per week from list of locations: Charlotte NC, Dallas TX, Chandler AZ, Newark, NJ - Must be local to one of these locations.
** Contract would be $50 per hour on W2 (18 months). American Citizens or GC Holders Only**
3-5 years experience in:
Terraform, Azure/GCP, DevOps Engineering, ETL, Client/AI, Data lake, Data architecture.
GCP Key Services Experience (ie Big Query, Composer, Vertex AI, DataProc, Data Catalog, Notebooks).
Azure Key Service Experience (ie Synapse, Purview, Data Lake, Logic Apps, Cognitive Services).
Job Type: Full-time
Salary: $50.00 per hour
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you American? If not, please specify your visa type
Would you be willing to work under W2?
Experience:
Terraform: 3 years (Preferred)
Azure Data Lake: 3 years (Preferred)
GCP Key Service Exp: 3 years (Preferred)
DevOps: 3 years (Preferred)
Work Location: In person",$50.00 /hr (est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2008,$25 to $100 million (USD)
"Take Command Health
4.9",4.9,"Dallas, TX",Data Engineer,"Company: Take Command
Position: Data Engineer
Salary: $120,000-$130,000 plus bonus
Location: Dallas or Austin, TX
About Take Command
Take Command is a start-up on a mission to improve the healthcare system, starting with health insurance. Pragmatically speaking, we help employers reimburse employees for individual insurance instead of offering a traditional one-size-fits-all group plan. We believe this model can empower employees (when they have the right support) to be savvy healthcare consumers and have a transformative impact on the entire healthcare system.
Let’s be honest—health insurance is usually a confusing, frustrating, and even emotional experience for people. We want to fix that with a new model, great technology, and a superior user experience. We have made a great start, but we need your help to fully realize our long-term vision.
As a part of the dev team, you'll build the framework for both our clients and our team to succeed. You will be a pillar in our company as we grow and make our product the best product on the market!
In this role, you will:
Take ownership for delivering multiple features, their quality and assist team members in delivering their features
Recommend improvements to data quality, processes and code to make it easier to use, manage and maintain
Experience designing and developing data models and data migrations
Experience managing and maintaining ETL or ELT processes (Hevo, BigQuery, Looker, etc.)
Hands-on development experience with SQL, relational database design (ERDs)
Must be able to write SQL & contribute code
Be product-minded; focus not only on what needs to be built but why something needs to be built
You may be a great fit for this role if you have:
3 or more years of professional experience as a data analyst or data engineer
Cloud-Based Data Tools e.g. Hevo, BigQuery, Looker, etc.
Database technologies: SQL, RDBMS / Relational databases (Postgres, Amazon RDS / Aurora, etc.)
Nice to have experience: Python, Java, Typescript or another modern OOP language
Cloud infrastructure: AWS
Source control: Git
Working at Take Command
We’re excited to build a team and culture that reflects our values! We offer competitive pay, health benefits, and equity options to share with this position. We want you to be an owner along with us in revolutionizing healthcare.
A generously funded ICHRA for medical, dental, and vision premiums and medical expenses. You get to use our own product and we think that’s so exciting and rare!
Unlimited personal vacation in addition to regular company holidays.
401(k): 90-day eligibility for 2% match that vests immediately!
Access to LinkedIn Learning.
We have two beautiful offices conveniently located near The Hill (Dallas) and Holly Commons (Austin). The kitchen is well-stocked and we've designed the space to have lots of different areas to work!
Paid parental leave for new parents.
Flexible on where you work – we hope to see you around the office on average 3 days a week especially when white-boarding or building relationships with your colleagues, but you also have the ability to work from home or wherever you'd like when you need to get focus work done.
More About Us
We secured our Series A funding in 2021 and are thrilled to be able to expand our team. Despite being a small startup in a land of health insurance giants, we’re the recognized industry leader for what we do (health insurance reimbursements) and passionate about bringing it to market because we know we can help fix a broken system and improve our clients’ wellbeing and health outcomes.
We’ve been featured in The New York Times, The Wall Street Journal, The Dallas Morning News, and other national healthcare publications and are excited about our growth opportunities.
Take Command knows diversity and inclusion among our teammates is integral to our company’s success and growth. Our vision is to recruit, develop, and retain the best team from a diverse candidate pool.
This has mostly been about us, but we’d love to hear from you-we can’t wait to hear your story!
Take Command is an equal opportunity employer! We celebrate diversity and are committed to creating an inclusive environment for all employees.
Xrygn4NXMr","$125,000 /yr (est.)",1 to 50 Employees,Company - Private,Insurance,Insurance Agencies & Brokerages,2014,Unknown / Non-Applicable
Fracsys Inc,#N/A,"Washington, DC",Data Engineer,"Fracsys Inc is hiring a Data Engineer position. The ideal candidate must have at least 12+ years of industry experience. He or she must be responsible for successful technical delivery of Data Warehousing and Business Intelligence projects using a formal project management methodology.
Responsibilities
Directs, motivates and develops key technical personnel
Provides technical direction across the full lifecycle system development process
Provides strategic planning and comprehensive senior level technical consulting to IT senior management and senior technical staff
Working independently within guidelines
Provides detailed inputs for plans and schedules, including goals, risks, and resource allocation
Conduct diagnostic, troubleshooting steps and present conclusions to a varied audience
comfortable slinging some code to solve a problem
Build Data Marts and Data warehouses to facilitate analytical capabilities across the varied sources of data
Design and implement data patterns on cloud using cloud Data warehouse and data virtualization tools
Evaluate new tools and technologies for target state on cloud
Learn and implement new ETL, BI tools and data warehousing technologies
Qualifications
Minimum twelve (12) years in a Technical Lead or Consulting role leading Information Management related initiatives (system integration, data warehouse build, data mart build, or similar)
MUST have proven leadership experience for team of at least 3 technical resources.
Experience creating Data Marts, Data warehouses on-premise and on cloud for real time and batch processing frameworks
Experience building and optimizing big-data data pipelines and data sets including Postgres, AWS Relational Data Service
extensive hands-on experience building and optimizing data structures for data analytics, data science and business intelligence
Experience building self-service data consumption patterns and knowledge of cloud-based data Lake platforms
Experience wrangling data (structured and unstructured), in-depth knowledge of database architecture
Experience utilizing or leading implementations leveraging ETL tools (Informatica / Talend), BI Reporting tools such as MicroStrategy, Microsoft Power BI, data modeling tools such as Erwin, Oracle, SQL server, NoSQL, JDBC, UNIX shell scripting, PERL and JAVA, XML/JSON files, SAS , Python, AWS cloud-native technologies, S3, Athena, Redshift
Familiarity with the following technologies: Hadoop, Kafka, Airflow, Hive, Presto, Athena, S3, Aurora, EMR, Spark
Ability to drive, contribute to, and communicate solutions to technical product challenges
Ability to roll-up your sleeves and work in process or resource gaps and fill it in yourself
Excellent written and oral communication skills in English
Education
Bachelor’s degree in computer science, Data Analytics, Information Systems, or a related degree or equivalent experience. Master’s degree is preferred.
Job Type: Full-time
Pay: From $96,336.72 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Experience level:
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Washington, DC: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person","$96,337 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Tipico - North America
4.0",4.0,"Denver, CO",Data Engineer,"Company Description

Tipico is an energetic, passionate, bold tech company, and we love sports! Tipico has recently expanded to the US market, establishing offices in Hoboken, New Jersey and Denver, Colorado. We are driven by our core values: passion, progress and trust. Our goal is to build the best possible mobile sports betting product in the industry, in order to amplify the emotions of millions of sports fans every day. Let’s Make This Interesting ™
Please note: this role is located in our Denver, CO office; we work off a hybrid model and come into the office 2 days per week.

Job Description

The Data Engineer will report directly to the Data Engineering Manager and work together with rest of the data engineers, BI analysts, and product teams. The position holder ensures this is done by doing the following:
Create and maintain both batch (ETL/ELT) and real-time data pipelines and architecture
Ensure all data provided by Data Platform is of the highest quality and is delivered in a timely manner and in line with agreed SLAs
Assemble large and complex data sets that meet functional and non-functional business requirements
Identify, design, and implement internal process improvements, including automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies
Collaborate with stakeholders including data architects, Executive, Product, Data and IT team members from the beginning to the delivery of a project
Build a scalable, modern, and trusted data ecosystem, making sure it is responsive, available and in line with business requirements
Take part in sprints, workloads, and deliverables through agile methodologies
Manage quality assurance, verification of accuracy and consistency of data, specifically KPI measures sent to the business
Thrive working in a fast-paced and challenging environment
Open to continuous learning individually and with the team
Comfortable with being challenged / seeking challenge
Willing to work flexible hours if needed

Qualifications
3+ years in Data Platforms / Datawarehouse / BI experience
Must have experience in successfully implementing data-centric applications, such as data warehouses, operational data stores, and data integration projects
Strong analytical and problem-solving skills that balance with creative approaches
Ability to learn new and complex concepts quickly
Strong experience with large-scale production databases and SQL
Solid understanding of cloud data services: AWS services such as S3, Athena, EC2, RedShift, EMR, EKS, RDS and Lambda
Proficient with one or more big data & new data technologies such as DBT, NIFI, Airflow, Kafka
Experience with data modelling
Experience with ETL development (extractions, data load, aggregation, transformation)
Knowledge in working with time-series/analytics databases such as Elasticsearch
Understanding of containerization and orchestration technologies like Docker/Kubernetes
Must be able to take ownership of projects with a high level of attention to detail and priority
Team player with a hands-on approach and attitude
Must be comfortable with on call rotation
Nice to have:
Familiarity with Marketing tooling and/or CRM systems is preferred
Familiarity with using business intelligence tools such as Domo, Tableau or Looker
Knowledge about and / or experience with sports betting
Background in digital companies, online, gambling, gaming, e-commerce, marketing, or mobile apps
Familiarity with one scripting language, preferably Python

Additional Information

What's in it for you:
Work in an environment where you, your work and ideas matter and have an impact.
You will be a be part of the newest and up and coming USA online iGaming and sports betting companies in the market.
Work in a new and young business with high growth potential.
Build your own success story together with us.
Work with self-organized, self-responsible and entrepreneurial employees.
Start-up feeling, backed-up by a leading European sports betting house. We are a high-volume business and are taking off in the US!
Competitive salary, Medical, Vision, Dental Benefits, Unlimited PTO, and more
Salary for this position is $120k+ & annual performance bonus","$120,000 /yr (est.)",201 to 500 Employees,Company - Private,"Arts, Entertainment & Recreation",Gambling,2004,Unknown / Non-Applicable
National Group Corporation,#N/A,"Dallas, TX",Data Engineer,"Who are we?
ReAlign Insurance Holdings is a leading primary underwriter of program administrator managed specialty insurance programs in the U.S. With the extensive knowledge and experience our team has in building and managing carriers and programs we are able to reach decisions on program opportunities quicker than our competition.
Why choose ReAlign?
At ReAlign, we believe that we are only as strong as our people. We strive to create an inclusive and welcoming culture where employees feel comfortable and empowered to be themselves. We support a diverse and inclusive work environment, with a focus on a healthy work-life balance. We understand that comprehensive benefits are important which is why we offer a benefit package that includes: medical, dental, vision, company paid short and long-term disability, life insurance, health spending account, 401(k) match every pay period, PTO, paid holidays, floating holidays, tuition reimbursement, and referral bonuses.
Job Summary
The Data Engineer position will involve data exchange process analysis, design, development, unit testing, and support. This role would work with other data team members on database ETL processes and reporting needs in both the on-premise database environment as well as the Azure data environment. This position requires strong technical and analytical skills to review business processes and data, recommend solutions, conduct functional and code analysis, and lead and perform database development tasks. This position will also involve production support and require experience within a DevOps environment.
Essential Functions
· Design, development, and support data ETL processing for both external and internal customers
· Support the internal and Azure cloud-based data warehouse platforms
· Work with DBA/System Admin to develop and maintain automated data processing
· Design and analyze queries and stored procedures
· Write and support SSRS and PowerBI reports
· Work in a data development life cycle environment
Work with stakeholders and data analysts to understand business requirements
Work with end users and the testing team to perform unit, functional, and integration testing and validation prior to production implementation
Troubleshoot and resolve operational problems with a focus on delivery
· Communicate solutions and architectural design to IT leadership
· Work in both Kanban and Scrum Agile environments
· Continually improve processes and solutions, seeking ways to use new tools, features, and capabilities
Job Requirements
3-10 years of experience as a Database Developer
Highly proficient in writing advanced SQL queries, stored procedures, SSRS reports, and ETL processes
Strong knowledge of relational and transactional database architecture
Strong technical and analytical skills
Experience with the Microsoft Azure and PowerBI platforms and Agile development methodology a plus
Bachelor’s Degree in computer science, database development, or related field preferred
Detail oriented with the objective of delivering a solution to a problem in a timely and efficient manner
Self-driven, willing to take initiative when an opportunity presents itself
Strong collaboration and communication skills and the ability to work in a team environment a must
Ability and willingness to work in a partial on-prem and partial remote office environment
Experience in the Property and Casualty Insurance industry a plus
Proficient with standard desktop software used in a business environment (MS Word, Excel, Visio, etc.)
Must be able to work in a hybrid in-office and remote work environment
Must be authorized to work in the United States
Job Type: Full-time
Pay: $90,000.00 - $125,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Health savings account
Life insurance
Paid time off
Retirement plan
Vision insurance
Experience level:
3 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you currently local to the Dallas area and able to commute to the office 2 to 3 times per week? (Deal breaker)
Experience:
writing advanced SQL queries and ETL processes: 3 years (Required)
cloud database platform (i.e. Azure, AWS): 1 year (Preferred)
Azure Data Factory: 1 year (Preferred)
Work Location: In person","$107,500 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"DSMH LLC
5.0",5.0,"Peoria, IL","Sr. Data Engineer (API, Cloud)","Education
- Bachelor degree in Computer Science, Information Systems, or a
related field; a Master degree is preferred
- At least 15 years of professional experience, with a focus on API
design and development, and extensive knowledge of the
Snowflake Data Cloud platform
Technical Skills
(Required)
- Proven experience in leading large-scale API and Snowflake
implementation projects
- Strong knowledge of API technologies, such as REST, GraphQL, or
gRPC
- Experience with ETL and data integration tools, such as Talend,
Informatica, or Matillion
- Deep understanding of data modeling, data warehousing, and data
quality concepts
- Familiarity with cloud platforms, such as AWS, Azure, or GCP, and their
respective services
- Excellent communication, leadership, and problem-solving skills
- Ability to work effectively in a fast-paced, collaborative environment.
- Strong attention to detail and commitment to delivering high-quality work
- Experience with agile methodologies and project management tools,
such as JIRA or Trello
Job Type: Contract
Pay: $55.00 - $60.00 per hour
Experience level:
11+ years
Schedule:
Monday to Friday
Experience:
APIs: 10 years (Required)
AWS: 5 years (Required)
Snowflake: 10 years (Required)
Work Location: In person",$57.50 /hr (est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,$1 to $5 million (USD)
"Cureatr
4.1",4.1,Remote,Data Engineer,"About Us
We are a thriving, medication management solutions company, headquartered in Manhattan, but largely virtually distributed, committed to tackling the $528B suboptimal medication management problem. Patient medication information is often incomplete for healthcare providers and systems – it’s our mission to fill in the gaps. We provide real-time, universal access to accurate medication data for over 265 million patients and high-quality interventions by board certified telepharmacists. Through our solutions, healthcare providers can reduce preventable hospitalizations, avoid readmissions, and ultimately, improve patient outcomes. We’re already used by dozens of hospital systems, with tens of thousands of clinicians and supporting staff using us to care for millions of patients.
Here at Cureatr, we are driven by a core set of values and strive to incorporate them into everything we do. We care deeply about our users, patients, and employees, and we aim to live and operate compassionately. We take pride in creating a diverse, inclusive, and equitable company culture, where our open and honest approach to work fosters innovation and encourages employees to achieve professional and personal growth.

The Role
We’re looking for a Data Engineer to join our growing Engineering team. Use the power of healthcare data to find insights that will demonstrate the value Cureatr is delivering to our customers, help improve patient outcomes, and expand the scope of Cureatr’s mission. As a Data Engineer at Cureatr, you will be responsible for designing, implementing and maintaining our data infrastructure, pipelines, automation, and ETL processes. You will work closely with our Data Science and Analytics teams to ensure that our data is clean, accurate, and readily available for analysis. You will also be responsible for ensuring that our data meets industry standards for security, privacy, and compliance.
This is a remote, work from home position, but candidates must be located within the U.S. Ideal candidates must be able to travel to our New York City office, or TBD company-wide offsite locations, up to 4x per year.

You’ll be part of a team working to solve problems like:
Increasing the number of patients who receive the opportunity to do a medication reconciliation, which can help keep them out of the hospital.
Streamlining and automating processes so that our pharmacists can serve as many patients as possible.
Predicting patient events likely to occur so that our clinical team can adequately prevent them before it’s too late.
Helping our pharmacists make the most of data and insights that will help them be as effective as possible in serving patients.
Evaluating the effectiveness of our clinical services impact at the population level.

What You’ll Do:
Design, implement, and maintain our data infrastructure, pipelines, automation, and ETL processes to support our data needs.
Build and maintain scalable data models and databases.
Collaborate with Data Science and Analytics teams to understand their data needs and provide data solutions accordingly.
Ensure data quality and integrity by implementing data validation and cleansing processes.
Ensure data security and compliance with healthcare regulations.
Monitor and optimize data performance and scalability.
Design and implement data governance policies and procedures.
Evaluate and implement new data technologies and tools to improve our data capabilities.

Your Skills:
3+ years of experience as a Data Engineer or similar role.
Strong programming skills in languages such as Python and SQL.
Experience using git to track changes.
Experience with data modeling and database design.
Experience with the “modern data stack” preferred
Experience with ETL and data pipeline development.
Knowledge of healthcare data and related standards (e.g., HL7, FHIR) is a plus.
Familiarity with AWS.
Strong problem-solving and analytical skills.
Excellent communication and collaboration skills.

Technology Stack:
Python
RedShift
dbt
Dagster
MongoDB
Looker
AWS

Educational Requirement: Bachelor’s Degree or relevant work experience

Salary: $110,000 - $140,000 plus full benefits

Why work with us?
Make a measurable impact on patients’ lives
Be part of a company dedicated to living its core values:
Empathy
Honesty
Integrity
Impact
Fun
Equity in Healthcare
Work in the company without micromanagement
Share your vision and make an impact on the product
Opportunities for career growth
Remote work
Competitive salary
Premium benefits (Health, Dental, Vision, 401k, unlimited PTO, more)

Equal Employment Opportunity Policy
Cureatr is an equal opportunity/affirmative action employer. All qualified applicants will receive consideration for employment without regard to sex, pregnancy, race, religion or religious creed, color, gender, gender identity, gender expression, national origin, ancestry, physical or mental disability, medical condition, genetic information, marital status, registered domestic partner status, age, sexual orientation, military or veteran status, protected veteran status, or any other basis protected by federal, state, local law, ordinance, or regulation and will not be discriminated against on these bases
Disclaimer: we are aware of a scam targeting applicants using a cureatrcareers.com email address. Please block and report this sender, as they are not affiliated with Cureatr. If you have any questions, please contact us at jobs@cureatr.com. Thank you!
uBajh3OfeT","$125,000 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2012,Less than $1 million (USD)
"Hiscox
4.1",4.1,"Atlanta, GA",Senior Data Engineer,"job description:
You will be a senior member of our US IT Team data engineering practice. You will be an early member of this team and will work closely with the Director of Data Engineering to help establish it. We will have our work cut out for us delivering planned work, building the practice and evolving our technical landscape. This is an exciting opportunity for the right individual to join at the very beginning and grow with the new team.
As a senior you will be expected to lead, mentor others and deputise for the director of data engineering when required. Yoi will also continue to contribute as a hands-on engineer. You will have the knowledge, best earned through 5+ years of experience, to truly inhabit this senior role in an ambitious team.
What you’ll be doing:
Working with business stakeholders and our delivery team to understand high value business problems that can be solved through the application of data processing and analytical systems.
Developing, expanding and evolving our existing databases and ETL pipelines.
Working with architects to design, build and support a transformational cloud-based data platform for the business.
Being a core and professional member of the new data engineering practice
Understand business requirements and help refine into development tasks and estimate their complexity.
Research, evaluate and adopt new technologies with a right tool for the job mentality.
Focus on both speed of delivery and quality, with suitable pragmatism – ensuring your solutions are always appropriate and not too complex or over-engineered.
Quick progression of projects from proof-of-concept to post-production stage.
Communication and presentation of ideas to colleagues in all parts of the wider tech team.
Participating in code reviews for the data engineering practice.
Act as a servant leader in our practice – ensuring team members have
person specification:
Knowledge of, or ability to rapidly adopt our core languages for data engineering – SQL and/or Python.
Experience working using a modern DevOps approach. You should not just have written data code but also the CI/CD pipelines necessary to test and deploy that code in a professional environment.
A robust understanding of core data engineering topics – ETL vs ELT, structured and unstructured data, data quality and data governance.
Ability to contribute to all aspects of a solution – design, infrastructure, development, testing and maintenance.
The ability to design and advocate for technical solutions to business problems.
Effective collaboration with technical and non-technical team members through agile ceremonies – roadmap planning, feature workshops, backlog elaboration, code review.
Understanding of cloud technology and significant hands-on experience working with it. You must understand how cloud-native solutions must be built differently from traditional ones.
Track record of taking initiative and delivering projects end-to-end; clear evidence of being self-driven and motivated
Immense curiosity, high energy and desire to go the extra mile to make a difference.

Desirable Skills
Beyond the required skills we are open to individuals of diverse talents. Experience with additional technologies, data science knowledge or a business background are all valued. We want to know how your unique abilities can contribute to our team.
Our Technology
We operate in a diverse technical landscape and are looking for flexible engineers who can adapt to and use many different tools. We would not expect any engineer to be familiar with the entire tech stack – no engineer can. Instead, we seek people with a good understanding of data structures and algorithms and the ability to apply this knowledge in learning new tools.
We have an existing data landscape involving both MS SQL Server and Oracle databases. We use SSIS for ETL. We will also need to source data from our integration layer – webMethods.
If some of that sounds like it needs bringing up to date, we know! We hope to start building a modern cloud data platform to realise the full value of our data. The technologies we use for that platform are still to be decided. Likely it will be based on Snowflake with Azure Data Factory for ELT. Other parts of our business have had success using Prefect for orchestration and dbt for transformation and we will reuse where it is sensible.
We prepare data for use by analysts working with a variety of tools – Tableau, PowerBI, and even Excel.
We take a DevOps approach and strive for continuous integration / continuous deployment. We use Azure Pipelines to deploy our code. We deploy infrastructure the same way using Terraform and Docker.
Our technology landscape is not fixed. Our engineering and architecture teams drive the technology we adopt.


Salary range $140,000 - $170,000","$155,000 /yr (est.)",1001 to 5000 Employees,Company - Public,Insurance,Insurance Carriers,1901,$1 to $5 billion (USD)
Radiant System,#N/A,"Pleasanton, CA",Data Engineer,"First Priority: Pleasanton, CA –
Second Priority for a superstar: Plano, TX
Experience in developing and deploying data pipelines, preferably in the Cloud
Experience working with Spark, Databricks, and other relevant technologies in Microsoft Azure cloud
Experience in productionizing and deploying Big Data platforms and applications
Expert in writing SQL statements
Job Type: Contract
Application Question(s):
Are you willing to work on W2?
Work Location: In person",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Virtualan Software LLC
4.5",4.5,"Atlanta, GA","Sr. Data Engineer (Python,Spark & Databricks)","Role : Senior Data Engineer (Python, Spark & Databricks)
Location : Hybrid (Atlanta, GA)
Pay: 60-70$ on C2C / 1099
RESPONSIBILITIES:
Create or modify the conceptual, logical and physical data models.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of sources like Hadoop, Spark, AWS Lambda, etc.
Lead and/or mentor a small team of data engineers.
Design, develop, test, deploy, maintain and improve data integration pipeline.
Develop pipeline objects using Apache Spark , PySpark and/or Python.
Design and develop data pipeline architectures using Spark and related AWS Services.
Communicate effectively with client leadership and business stakeholders.
Participate in proposal and/or SOW development.
REQUIREMENTS:
5+ years of professional work experience designing and implementing data pipelines in a cloud environment is required.
2+ years of experience migrating/developing data solutions in the AWS cloud is required.
1+ years of experience building/implementing data pipelines using DataBricks is required.
Expert level knowledge of using SQL to write complex, highly-optimized queries across large volumes of data.
Hands-on object-oriented programming experience using Python is required.
Professional work experience building real-time data streams using Spark.
Knowledge or experience in architectural best practices in building data lakes.
Bachelor or Master degree in Computer Science, Engineering, Information Systems or relevant degree.
Hybrid work schedule, and must live in the Atlanta, GA metro area. Must be open to up to 25% national travel to client locations when on engagements outside of the Atlanta, GA area.
Job Type: Contract
Pay: $60.00 - $70.00 per hour
Schedule:
Monday to Friday
Application Question(s):
Apply only if you are interested in Hybrid Role in Atlanta, GA. Interested ?
Experience:
Data Engineer: 4 years (Required)
Databricks: 1 year (Required)
Willingness to travel:
25% (Required)
Work Location: On the road",$65.00 /hr (est.),1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,2019,Less than $1 million (USD)
"Fulcrum Analytics
4.3",4.3,Remote,Data Engineer,"At Fulcrum, we provide cutting-edge data science consulting and software solutions to help companies solve challenging business problems. Through the utilization of our innovative products and dedicated team, clients are able to operate more efficiently, manage risk, and make informed business decisions every day. Fulcrum Analytics has stood at the forefront of data, analytics, and innovative software for over 25 years, offering sophisticated solutions, groundbreaking technologies, and winning strategies that help companies achieve their targeted results.
Role Description:
We are seeking a highly-skilled data engineer with experience in data analytics and data science. The ideal candidate will be responsible for building and maintaining data pipelines, designing and implementing databases and data models, and developing data-driven solutions to drive business growth for our clients.
Responsibilities:
Design, build, and maintain efficient, scalable, and reliable data pipelines using SQL, or programming languages such as Python or Java.
Develop and maintain databases and data models to meet clients’ analytical needs.
Identify data quality issues and develop processes to ensure data accuracy, completeness, and consistency.
Work with cross-functional teams to define and implement data integration strategies for new and existing systems.
Develop and maintain automated testing and deployment of analytics code and data pipelines.
Collaborate with data scientists to develop and implement machine learning models, statistical models, and other advanced analytics techniques.
Communicate analytical results and recommendations to technical and non-technical stakeholders through presentations and visualizations.
Stay up to date with emerging technologies and industry best practices, and recommend tools and processes that can improve data engineering and analytics processes.
Qualifications:
Bachelor's or master's degree in computer science, statistics, data science, or a related field.
2+ years of experience in data engineering, data analytics, and data science.
Strong programming skills in Python, with experience in building data pipelines using frameworks such as Airflow, Glue, Lambda, Athena, and BigQuery as well as data platforms such as Hadoop, Snowflake, and GCP.
Experience working with SOAP or RESTful web services.
Familiarity with version control, in particular Git.
Experience in designing, building, and optimizing relational and NoSQL databases.
Familiarity with data visualization tools, such as Tableau or Power BI.
Ability to understand and transform complex data into actionable insights.
Strong analytical and problem-solving skills, with a focus on delivering practical solutions to business challenges.
Ability to learn and solve problems independently.
Excellent communication and collaboration skills, with the ability to explain complex concepts to both technical and non-technical audiences.
If you are passionate about data analytics and want to work in a dynamic environment with talented colleagues, we encourage you to apply for this role. We offer competitive salary and benefits packages, as well as opportunities for professional growth and development.


Position Type: Full-time salaried
Reports to: Senior Data Engineer
Base Compensation: $110,000 - $140,000
Benefits: Medical, Dental, Vision, 401k with Match, Bonus, Unlimited PTO
Location: Remote within the United States of America; Eastern time zone preferred","$125,000 /yr (est.)",1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,1993,$5 to $25 million (USD)
Proits Hub LLC,#N/A,Remote,Data Analytics Engineer,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote
Speak with the employer
+91 9417599933",$60.00 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Infosoft,#N/A,"Peachtree Corners, GA",Data Pipeline Engineer - 2173,"Here are the job position details for your review:
Job Title: Data Pipeline Engineer (GStreamer/ DeepStream)
Pay Rate: $90 to 120/Hr.
Duration: 3 Months
Location: Peachtree Corners, GA
This position is for the Building Automation division.
We are looking for Data Pipeline Engineer (GStreamer) to participate in the development of Streaming data pipelines using NVIDIA Deepstream, GStreamer, Python programming, messaging queues, and related tools.
Preference is for candidates located in Atlanta, GA metro area. However, ok with non-local candidates who are open to 75% travel to Atlanta, GA.
Understanding the basics of our architecture/setup to be able to do setup/support for demos, testing, etc.
Basic computer hardware assembly and set-up
Possible set-up/installation of the application(s)
Possible development tasks depending on what the rest of the development team has for them
Develops business software applications, system software, and/or IT solutions on the basis of functional specifications.
Scope of position embraces both the customization or reuse/modification of system software and/or solutions as well as their development from scratch.
Consults with customers in creating functional specifications and work estimates, and supports Make-or-Buy decisions.
Analyses performance gaps of existing software environments and proposes measures to address efficiency problems.
Is also responsible for the implementation of data-driven components supporting data modeling and engineering in the context of data analytics (Data Stewardship).
Skills:
Python
Linux
Docker and Docker Swarm
Zero MQ / a good understanding of messaging
Desired Skills:
NVIDIA Deepstream SDK
Web Sockets
Flask
Protobuf
Possible:
GStreamer (if different than RTSP / H264 / MJPEG)
Job Type: Contract
Salary: $90.00 - $120.00 per hour
Ability to commute/relocate:
Peachtree Corners, GA: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
""Please share your Email ID""
Work Location: One location",$105.00 /hr (est.),Unknown,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Merkle
3.7",3.7,"Norristown, PA",Data Engineer,"Company Description

We are Dentsu International. We innovate the way brands are built. That means we do things differently so they’re better than before. In this way, we make our clients’ most important marketing assets—their brands—win in a changing world.
Dentsu International is a modern marketing solutions company. Our mission is to help clients navigate, progress, and thrive in a world of change. Businesses rely on our integrated network of agencies and specialized practices to champion meaningful progress through creative, media, commerce, data, and technology. Part of Dentsu Group, our global network comprises 66,000 diverse people in 143 countries, who are dedicated to teaming for growth and good. Follow us on Twitter @DentsuUSA and visit dentsu.com/us.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.

Job Description
Develop onboarding process to receive and process information related to onboarding a new subscriber
Convert Snowflake Stored Procedures to Python Scripts directly in Airflow
Develop metadata driven solutions for managing data processing and cadence
End to end automation development and testing
Building and maintaining stored procedures and scripts in Snowflake Data Cloud
Building and maintaining complex SQL queries for ETL/ELT processing
Develop Dashboard for data processed for audit and governance needs
Follow Agile methodologies to deliver iterative feature sets rapidly
Maintain codebase using source control methodologies, Git
Collaborate with business analysts, data analysts, and other developers to deliver powerful new features
Coordinate with teams as part of a larger data-sharing system
Contribute to a collaborative, positive, and enjoyable environment for your operations team
Performing quality assurance and testing
Writing user and technical documentation such as runbooks

Qualifications
Strong communication skills both verbal and written
Strong understanding of conditional programming logic
Knowledge of software engineering and development methodologies, techniques, and tools; includes Issue Tracking (JIRA) Software Development Lifecycle SDLC), and Continuous Integration / Continuous Development (CI/CD)
Experience working with large databases and data warehouses utilizing both relational and dimensional design concepts
Eagerness to learn new technologies, programming languages and platforms to solve complex data challenges
Experience with Python required
Experience with SQL required
Experience with Snowflake Data Cloud is highly desirable
Experience with Amazon Web Services (AWS) a plus
Experience with Airflow (or other orchestration) a plus
Experience in database marketing is a plus

Additional Information

Perks:
Great compensation package
Comprehensive healthcare plans
401(k) with employer match
Flexible time-off
16 weeks paid parental leave
Additional Information
The anticipated salary range for this position is $94,000-$146,000. Salary is based on a range of factors that include relevant experience, knowledge, skills, other job-related qualifications, and geography. A range of medical, dental, vision, 401(k) matching, paid time off, and/or other benefits also are available. For more information regarding dentsu benefits, please visit dentsubenefitsplus.com
Employees from diverse or underrepresented backgrounds encouraged to apply.
Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact recruiting@dentsuaegis.com if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.
About dentsu
Dentsu is the network designed for what’s next, helping clients predict and plan for disruptive future opportunities in the sustainable economy. Taking a people-centered approach to business transformation, dentsu combines Japanese innovation with a diverse, global perspective to drive client growth and to shape society www.dentsu.com.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.
Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact your recruiter if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.","$120,000 /yr (est.)",5001 to 10000 Employees,Company - Private,Media & Communication,Advertising & Public Relations,1971,$500 million to $1 billion (USD)
DocsInk LLC,#N/A,"Wrightsville Beach, NC",Data Engineer,"DocsInk is on a mission to provide a collaborative and exciting alternative to the traditional healthcare model. Through the complete virtualization of healthcare organizations, we are equipping providers with the ability to deliver better patient care with speed, efficiency, and accuracy. With the widespread focus on providing value-based care, DocsInk is making it possible to provide superior care while also maximizing reimbursements. In today’s environment, shifting the way we approach healthcare is essential and DocsInk is on the cutting edge of where the industry is headed. We are excited to be growing our team and are looking for driven individuals with a passion for making a difference in the healthcare industry.

As our Data Engineer, you will be in the driver’s seat as our team creates elegant and forward-thinking software solutions for our healthcare customers. You will work with fellow developers, designers, training & support, quality assurance, and management in a fast-paced environment to take our products to the next level and exceed expectations. We are looking for someone who is incredibly talented to do what most cannot and yet humble enough to easily work with peers, accept guidance from more experienced staff, and mentor more junior staff. If designing and developing complex integration efforts is what gets you out of bed each morning, then we want you to help us reshape healthcare.

People are our most important asset and the number one reason we all love working at DocsInk. As a team, we value accountability, transparency and collaboration. We have agile teams, with clear, outcomes-focused goals. Delivering innovative solutions and superior service are at the forefront of what we do, and the collective ideas of the team allow us to continue to excel in a fun and energetic environment.

Professional Requirements and Responsibilities:

Role / Responsibilities:

Implement robust ETL solutions that integrate healthcare data feeds using tools such as Apache NiFi, NextGen Connect (JavaScript), PostgreSQL, MySQL, and Linux shell scripting
Configuring and managing deployments to AWS infrastructure consisting of both traditional servers and serverless components. CI/CD tools include Bitbucket and AWS Code Deploy
Building direct interfaces with hospitals and EMR vendors utilizing industry standards such as HL7 2.x, C-CDA, and FHIR as well as implementing interfaces that consume formats such as XML, JSON, and delimited files
Utilizing APIs and web services to permit bi-directional flows of data to and from our partner organizations
Monitoring operational ETL processes, triaging and resolving issues as they arise to meet SLAs
Writing PL/SQL functions and packages to automate workflows in a Postgres environment linked with web technologies
Maintaining data standards (including adherence to HIPAA), documenting work, and championing process improvements
Work as a project team member to jointly deliver high-quality business solutions consistent with project objectives and constraints

Knowledge and Skills Required:

4+ years experience with HL7 2.x
2+ years experience with Apache Nifi
2+ years experience with NextGen Connect
Advanced knowledge of PostgreSQL, MySQL, AWS Code Deploy, HL7 2.x, C-CDA, FHIR, and HIPAA data compliance

Qualifications and Other Attributes Required:

Degree in Computer Science or equivalent field, or equivalent work experience
Highly self-motivated team player who takes initiative
Strong sense of personal accountability; contributes beyond job role and responsibilities
Excellent problem-solving skills; meticulous & methodical","$85,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Redzara.us,#N/A,"Atlanta, GA",Azure Data Engineer,"Responsible for developing new and modernizing existing applications to Azure Cloud Platform.
The ideal candidate must demonstrate in-depth knowledge and understanding of Database concepts.
Requirements:
7+ years of experience in Data Warehousing and Business Intelligence
5+ Years in BI development leveraging the Microsoft SQL Server. Needs to be an expert in T-SQL
5+ Years in SSIS and use of Stored Procedures.
1-2 Years in Azure Data Factory, SQL VMs, build and maintain ETL
Ingest and aggregate data from both internal and external data sources to build our data lake
Strong understanding data modeling (i.e., conceptual, logical, and physical model design, experience with Operation Data Stores, Enterprise Data warehouses and Data Marts).
Job Type: Contract
Pay: $43.49 - $57.00 per hour
Compensation package:
1099 contract
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Atlanta, GA 30334: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure Data Factory: 3 years (Required)
Azure: 3 years (Required)
Data warehouse: 1 year (Preferred)
Work Location: Hybrid remote in Atlanta, GA 30334",$50.25 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Barracuda Networks Inc.
3.6",3.6,"Alpharetta, GA",Data Engineer,"Job ID: 23-579

Come Join Our Passionate Team! At Barracuda, we make the world a safer place. We believe every business deserves access to cloud-enabled, enterprise-grade security solutions that are easy to buy, deploy, and use. We protect email, networks, data and applications with innovative solutions that grow and adapt with our customers’ journey. More than 200,000 organizations worldwide trust Barracuda to protect them — in ways they may not even know they are at risk — so they can focus on taking their business to the next level.
We know a diverse workforce adds to our collective value and strength as an organization. Barracuda Networks is proud to be an Equal Opportunity Employer, committed to equal employment opportunity and equitable compensation regardless of race, gender, religion, sex, sexual orientation, national origin, or disability.
Envision yourself at Barracuda
We currently have an exciting new position open for a Data Engineer. This person will assist in driving technology innovation for engineering projects and work on architecture of AWS, Elastic and Kubernetes. They will also work closely with the Technology Architect to create and drive the Architecture Review Board.
What you’ll be working on
Determine internal / external customer data needs and identify system specifications
Analyze the needs of large systems and breaking them down into smaller manageable parts
Responsible for designing the journey of data from various sources to the database and everything in between
Plan and design the structure of data, brainstorm with team
Analyze best technology products and systems for internal teams
Help provide build vs buy decisions on data architecture
Communicate data requirements to engineers; explain data architecture to them and provide assistance throughout SDLC
Choose / maintain suitable software, hardware and integration methods
Help resolve technical problems as and when they arise
Review testing procedures to ensure data performant and error free
Ensure that database satisfy quality standards and procedures. Setup such standards.
Work with senior IT personnel to devise plans for future IT requirements of the organization
Produce progress reports on key infrastructures - Elastic and Data Pipelines
What you bring to the role
Bachelor Degree or Post Graduate in Computer Science, or relevant work experience
5+ years of experience in an Engineering / Operations heavy organization
1+ years of experience as a Data Analysis and Data Design responsible for data architecture of a SaaS application
Experience in understanding and designing data sharding in a distributed database
Some Hands-on experience in software development and system administration
Knowledge of strategic IT solutions
Familiarity with programming languages like JavaScript or Java or Python
Familiarity with Linux OS and Windows alike
Experience in cloud computing and cloud Technologies (AWS, GCP, Azure)
Excellent communication skills – you should be adept at listening to, understanding, and explaining key concepts to managerial and technical resources.
Good Anticipation and problem-solving skills
Ability to prioritize and manage time – responsible for multiple streams of work at the same time
Relationship building - Ability to form a good rapport with internal and external stakeholders
In-depth knowledge about quality standards and security best practices

Preferred Skills:
Expert level Understanding of Elastic eco system - Elasticsearch, Logstash, Kibana, Filebeats
Understanding of secure tunnel communication using VPN
Experience designing data at scale
AWS experience is preferred in Cloud Technologies
What you’ll get from us
A team where you can voice your opinion, make an impact, and where you and your experience are valued. Internal mobility – there are opportunities for cross training and the ability to attain your next career step within Barracuda.
Equity, in the form of non-qualified options
High-quality health benefits
Retirement Plan with employer match
Career-growth opportunities
Flexible Time Off and Paid Time Off benefits
Volunteer opportunities
#LI-Remote","$98,161 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,$100 to $500 million (USD)
"Quirch Foods
3.0",3.0,"Coral Gables, FL",Data Engineer,"About Quirch Foods
In business for over 50 years, Quirch Foods is a global distributor and exporter of fresh and frozen food products with a unique focus on ethnic cultures. We service our customers from 5 distribution centers strategically located in the United States. We have consistently been ranked among the top 50 exporters in the US and are one of the largest importers of seafood. Our customers include independent grocers, chain supermarkets, foodservice distributors, cruise lines, restaurants and food processors/manufacturers. We have an extensive product list with over 8,000 SKU's and carry niche brands such as Panamei, Mambo and Chiquita along with national brands like Tyson, IBP, Excel, JBS, Iowa Premium, Smithfield, Dietz & Watson, along with many others. We are headquartered in Coral Gables, FL.
Quirch is a portfolio company of Palladium Equity Partners, a leading mid-market private equity fund based in New York City, which partnered with the Executive Team to capitalize on Quirch Foods’ record of excellence and to grow it decisively beyond its current business.
Position Summary
At Quirch Foods we believe in leveraging technology as a source of competitive advantage. This includes customized ERP Solutions, Business Intelligence, EDI, Web and Mobile Solutions. We are currently looking for a Data Engineer to join our development efforts.
Essential Duties and Responsibilities:
Develop, test and maintain custom business solutions by leveraging state of the art software development practices using an agile methodology
Evaluate business requirement and help define problems and develop solutions
Participate in design and architecture discussions with business leaders, end users and IT team members
Document business requirements and solution, document code and provide support for creation of end user documentation
Update business knowledge, technical skills and soft skills, leverage educational opportunities, participate in professional organizations
Experience:
MS SQL Programming
Visual Studio\C#\WPS Experience
Microsoft SSAS, SSIS, SSRS desired but not required
Biztalk experience desired but not required
Skills and Requirements:
Bachelor’s degree in Computer Science, Engineering, Math or equivalent and\or related experience and training preferred
Strong analytical and problem-solving skills
Strong collaborator and team player
Great organizational skills, attention to detail and follow thru
Effective oral and written communication skills
Extensive familiarity with data management principles
Benefits:
Professional growth and developmental opportunities to grow your skills using state of the art technology
Comprehensive benefits package that includes: Medical, Dental, Prescription Drug Plan, Disability Plan, Life insurance Plan
401K savings Plan
Paid Holidays
Personal Time off
Employee Discounts
Quirch Foods is an Equal Opportunity Employer (EOE). Qualified applicants are considered for employment without regard to age, race, color, religion, sex, national origin, sexual orientation, disability, or veteran status. All applicants must be eligible to work in the United States.
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Vision insurance
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Coral Gables, FL 33134: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
What is your desired compensation?
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Data Engineering: 1 year (Preferred)
Work Location: In person","$89,340 /yr (est.)",201 to 500 Employees,Company - Private,Retail & Wholesale,Wholesale,1967,$500 million to $1 billion (USD)
Atalan Tech,#N/A,Remote,Data Engineer,"Atalan Tech is a start-up with a mission to mitigate the impact of clinician burnout and turnover in our nation’s health system. We are expanding our team and looking to hire the critical role of Data Engineer. The Data Engineer will work closely with the Chief Technology Officer, Chief Scientific Officer (UCLA Ph.D.), Chief Product Officer (Oxford Ph.D.), and Chief Executive Officer to deliver actionable insights from data in a variety of situations.
We are a small, close-knit team. This is an excellent opportunity for someone to join a mission-oriented, early-stage company at the ground floor and grow with us. There will be plenty of room for growth and creativity, and opportunities to tackle challenges that present one of the toughest problems in our society.
Requirements
Strong hands-on experience of designing, developing and managing data pipelines
Strong coding skills in Python (required) and SQL (required) and R (strongly preferred)
Experience building and maintaining data pipelines on AWS using EC2, EBS, Redshift, Postgres, S3, Data Pipeline, Kinesis, Lambda, etc.
Thorough understanding and experience with task management tools (Airflow, NiFi, etc.)
Knowledge of data management best practices for ETL, data governance, data warehousing, data modeling and data science
Basic knowledge of Linux (Redhat preferred) administration
Minimum B.S in Computer Science, software engineering or related field (or equivalent in related professional experience)
Strong communication skills
Strong problem-solving/critical thinking skills
Ability to work in a fast-paced, dynamic start-up environment
Some knowledge of healthcare, health tech, or healthcare data analytics (preferred)
Typical Tasks and Responsibilities include:
Working closely with CTO and CSO to build data pipelines for machine learning
Assisting the CPO in managing and supporting analysis of enterprise data to drive better products and solutions
Managing company’s data infrastructure on AWS (Postgres/RDS, Redshift, S3, etc.)
Synthesizing multiple data sources, such as internal medical records and HR data
Integrating with client systems including EHR (Epic, Cerner), HRIS, flat files, etc.
Working closely with other key members on other projects
Developing and designing data security standards and policies in line with industry best practices
About Us:
Atalan Tech’s mission is to improve the wellness of clinicians and their patients. We blend labor economics, machine learning, and clinical psychology with a team from Harvard, Yale, and UCLA. Atalan Tech is looking to contribute innovative and sustainable solutions to one of the most pressing problems in the healthcare industry. Building upon our proof-of-concept success, we have recently secured multiple engagements with nationwide healthcare organizations and have successfully raised institutional funding to expand our reach.
Contact Us:
If you’re interested in this position, please send us your CV/resume and cover letter.
Job Type: Full-time
Pay: $108,448.20 - $130,604.27 per year
Experience level:
2 years
Schedule:
Monday to Friday
Application Question(s):
Do you know the role is full time?
Can you start within the next month?
What are your salary expectations?
Have you held the title of a data engineer? If not, what experience do you have as a data engineer? Please be very specific.
Have you released Python code to a production environment in the last year?
Experience:
Data analytics: 2 years (Required)
Work Location: Remote","$119,526 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Diamondpick,#N/A,"Alpharetta, GA",Java Software Engineer with Big-Data,"About us
There is someone out there who fits your requirements like a glove! We are an innovative Talent Solutions company with a vision to build talent supply chain models without compromise for the technology industry.
We are a new-age talent solutions company with a vision to build an enterprise recruitment model without compromise. We love to solve complex talent challenges and strive to be an enabler of business success for our clients using data, technology and our team’s vast experience in the technology market.
Senior Java Developer with Big data
Alpharetta, GA (Hybrid Work from Day1)
Job Description:
The ideal candidate must possess strong background on frontend and backend development technologies.
The candidate must possess excellent written and verbal communication skills with the ability to collaborate effectively with domain experts and technical experts in the team.
Responsibilities:
As a Java Senior Developer, you will
Maintain active relationships with Product Owner to understand business requirements, lead requirement gathering meetings and review designs with the product owner
Own his backlog items and coordinate with other team members to develop the features planned for each sprint
Perform technical design reviews and code reviews
Be Responsible for prototyping, developing, and troubleshooting software in the user interface or service layers
Perform peer reviews on source code to ensure reuse, scalability and the use of best practices
Participate in collaborative technical discussions that focus on software user experience, design, architecture, and development
Perform demonstrations for client stakeholders on project features and sub features, which utilizes the latest Front end and Backend development technologies
Requirements:
6+ years of experience in Java/JEE development
Skills in developing applications using multi-tier architecture
Knowledge of google/AWS cloud
Java/JEE, Spring, Spring boot, REST/SOAP web services, Hibernate, SQL, Tomcat, Application servers (WebSphere), SONAR, Agile, AJAX, Jenkins..etc
Skills in UML, application designing/architecture, Design Patterns..etc
Skills in Unit testing application using Junit or similar technologies
Good communication skills
Leadership skills
Provide overlap coverage with onsite/customer teams
Capability to support QA teams with test plans, root cause analysis and defect fixing
Strong experience in Responsive design, cross browser web applications
Strong knowledge of web service models
Strong knowledge in creating and working with APIs
Experience with Cloud services, specifically on Google cloud
Strong exposure in Agile, Scaled Agile based development models
Familiar with Interfaces such as REST web services, swagger profiles, JSON payloads.
Familiar with tools/utilities such as Bitbucket / Jira / Confluence
Job Types: Full-time, Contract, Temporary
Pay: From $60.00 per hour
Benefits:
Flexible schedule
Compensation package:
Hourly pay
Experience level:
8 years
Schedule:
8 hour shift
Day shift
Work Location: Hybrid remote in Alpharetta, GA 30005",$60.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Gridiron IT
4.5",4.5,Remote,Data Engineer,"GridironIT is seeking a Data Engineer.
Responsibilities:
We are looking for seasoned Data Engineer to work with our team and our clients to develop enterprise grade data platforms, services, and pipelines. We are looking for a more than just a ""Data Engineer"", but a technologist with excellent communication and customer service skills and a passion for data and problem solving.
Lead and architect migration of data environments with performance and reliability.
Assess and understand the ETL jobs, workflows, BI tools, and reports
Address technical inquiries concerning customization, integration, enterprise architecture and general feature / functionality of data products
Experience in crafting database / data warehouse solutions in cloud (Preferably AWS. Alternatively Azure, GCP).
Key must have skill sets – Python, AWS
Support an Agile software development lifecycle
You will contribute to the growth of our Data Exploitation Practice!
Qualifications:
US Citizen Only
Ability to hold a position of public trust with the US government.
8+ years industry experience coding commercial software and a passion for solving complex problems.
8+ years direct experience in Data Engineering with experience in tools such as:Big data tools: Hadoop, Spark, Kafka, etc.
Relational SQL and NoSQL databases, including Postgres and Cassandra
Data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
AWS cloud services: EC2, EMR, RDS, Redshift
Data streaming systems: Storm, Spark-Streaming, etc.
Search tools: Solr, Lucene, Elasticsearch
Object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Amazon S3, Athena, Redshift Spectrum, AWS Glue, AWS Glue Catalog, AWS Functions, and Amazon EC2 with SQL Server Developer
Advanced working SQL knowledge and experience working with relational databases, query authoring and optimization (SQL) as well as working familiarity with a variety of databases.
Experience with message queuing, stream processing, and highly scalable ‘big data’ data stores.
Experience manipulating, processing, and extracting value from large, disconnected datasets.
Experience manipulating structured and unstructured data for analysis
Experience constructing complex queries to analyze results using databases or in a data processing development environment
Experience with data modeling tools and process
Experience architecting data systems (transactional and warehouses)
Experience aggregating results and/or compiling information for reporting from multiple datasets
Experience working in an Agile environment
Experience supporting project teams of developers and data scientists who build web-based interfaces, dashboards, reports, and analytics/machine learning models
Experience with Informix and Data Stage
Job Type: Full-time
Pay: $140,000.00 - $150,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Hourly pay
Yearly pay
Experience level:
10 years
11+ years
7 years
8 years
9 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Due to the nature of the role, US Citizenship is required. Do you possess US Citizenship?
Education:
Bachelor's (Preferred)
Experience:
Data Engineering: 7 years (Required)
SQL: 7 years (Required)
AWS: 4 years (Required)
Big data: 5 years (Preferred)
NoSQL: 5 years (Preferred)
Work Location: Remote","$145,000 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable
"Appsintegration INC
4.5",4.5,"California, KY",Data Engineer,"Role:- Data Engineer
Position:- Onsite (W2 position)
Experience:- 10+ Years
Responsibilities
Working closely with business teams to understand their requirements and develop reporting or analytic solutions.
Producing and automating the delivery of key metrics and KPIs to the business.
In some cases, this will focus on making data available and others will be developing full reports for end users.
Managing stakeholder demand and a backlog of reporting requirements and needs.
Actively working with users to understand data issues, tracing back data lineage, and helping the business put appropriate data cleansing and quality processes in place.
Managing secure access to data sets for the business.
Requirements
Bachelor's degree in Computer Science or equivalent.
Proficient in creating Data Factory pipelines for on-cloud ETL processing.
Experience implementing data pipelines using the latest technologies.
Experience developing using SQL, relational databases, and data warehousing technologies.
Extensive ETL experience.
Ability to scale systems in Cloud as per requirement.
Experience in handling structured and unstructured datasets.
Good to have experience in data modeling and Advanced SQL techniques.
Must-Have Skills: Azure Data Factory (ADF), SQL, Cloud-based ETL.
Good to Have Skills: Azure File Storage Services, Databricks, Azure Synapse.
you can reach me out on LinkedIn
here is my link :- linkedin.com/in/shravanthi-thakur-705a81231
Job Type: Contract
Salary: $42.00 - $45.00 per hour
Ability to commute/relocate:
California, KY 41007: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",$43.50 /hr (est.),1001 to 5000 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Titan America
3.8",3.8,"Roanoke, VA",Data Engineer,"Data Engineer
We are seeking a motivated and detail-oriented Data Engineer to join our team based in Roanoke, Virginia. As a Data Engineer, you will play a crucial role in designing, developing and maintaining scalable data pipelines and transformations for business-critical use cases. In addition, you may take on various Software Development and Data Science responsibilities as we frequently share responsibilities and knowledge across our growing team.
Responsibilities:
Design, develop and maintain scalable data pipelines for extracting, transforming, and loading data from various sources
Collaborate with cross-functional teams to identify data needs and determine the best data solutions
Develop and implement data models to support business requirements and ensure data quality
Ensure the security and privacy of sensitive data by implementing appropriate access controls
Monitor and optimize data pipeline performance to ensure timely and accurate data delivery
Document data pipeline processes, data dictionaries, and data storage solutions
Requirements:
Bachelor's degree in Computer Science, Computer Engineering, or a related technical field
Minimum of five years of professional experience working as a Data Engineer or Software Developer
Strong hands-on experience with data warehouse and transformation solutions, i.e. Domo, Snowflake or similar
Proficient in at least one scripting language such as Python, JavaScript, or R
Understanding of data modeling, data integration and data quality processes
Familiarity with cloud platforms such as AWS, Azure, or Google Cloud Platform
Strong analytical and problem solving skills
Full Stack Software Development experience in a professional setting is highly desired, but not required
This is an excellent opportunity for a driven and collaborative individual to make a significant impact in a dynamic and growing team. If you have a passion for data and a desire to work in a fast-paced and dynamic environment, we want to hear from you!
Success begins with hiring the right people to partner with us as we grow and develop our businesses. People are central to everything we do. It is through their efforts and talents that Titan has been successful for over 100 years.
Titan America is an Equal Employment Opportunity (EEO) / Affirmative Action employer and welcomes all qualified applicants. Applicants will receive fair and impartial consideration without regard to race, sex, color, religion, national origin, age, disability, veteran status, genetic data, or other legally protected status.
Job Type: Full-time
Pay: $75,000.00 - $85,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Referral program
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Roanoke, VA 24019: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Do you currently or will you in the future need sponsorship to work in the United States?
This position requires the successful candidate to be in the office in Roanoke, VA at least 3 days a week. Can you meet this requirement?
This position requires the successful candidates to be a full time, W2 employee. Can you meet this requirement?
Education:
Bachelor's (Preferred)
Experience:
Data Engineering: 5 years (Preferred)
Work Location: Hybrid remote in Roanoke, VA 24019","$80,000 /yr (est.)",1001 to 5000 Employees,Company - Public,"Energy, Mining & Utilities",Mining & Metals,1902,Unknown / Non-Applicable
Redzara.us,#N/A,"Atlanta, GA",Azure Data Engineer,"Responsible for developing new and modernizing existing applications to Azure Cloud Platform.
The ideal candidate must demonstrate in-depth knowledge and understanding of Database concepts.
Requirements:
7+ years of experience in Data Warehousing and Business Intelligence
5+ Years in BI development leveraging the Microsoft SQL Server. Needs to be an expert in T-SQL
5+ Years in SSIS and use of Stored Procedures.
1-2 Years in Azure Data Factory, SQL VMs, build and maintain ETL
Ingest and aggregate data from both internal and external data sources to build our data lake
Strong understanding data modeling (i.e., conceptual, logical, and physical model design, experience with Operation Data Stores, Enterprise Data warehouses and Data Marts).
Job Type: Contract
Pay: $43.49 - $57.00 per hour
Compensation package:
1099 contract
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Atlanta, GA 30334: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure Data Factory: 3 years (Required)
Azure: 3 years (Required)
Data warehouse: 1 year (Preferred)
Work Location: Hybrid remote in Atlanta, GA 30334",$50.25 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Hiscox
4.1",4.1,"Atlanta, GA",Senior Data Engineer,"job description:
You will be a senior member of our US IT Team data engineering practice. You will be an early member of this team and will work closely with the Director of Data Engineering to help establish it. We will have our work cut out for us delivering planned work, building the practice and evolving our technical landscape. This is an exciting opportunity for the right individual to join at the very beginning and grow with the new team.
As a senior you will be expected to lead, mentor others and deputise for the director of data engineering when required. Yoi will also continue to contribute as a hands-on engineer. You will have the knowledge, best earned through 5+ years of experience, to truly inhabit this senior role in an ambitious team.
What you’ll be doing:
Working with business stakeholders and our delivery team to understand high value business problems that can be solved through the application of data processing and analytical systems.
Developing, expanding and evolving our existing databases and ETL pipelines.
Working with architects to design, build and support a transformational cloud-based data platform for the business.
Being a core and professional member of the new data engineering practice
Understand business requirements and help refine into development tasks and estimate their complexity.
Research, evaluate and adopt new technologies with a right tool for the job mentality.
Focus on both speed of delivery and quality, with suitable pragmatism – ensuring your solutions are always appropriate and not too complex or over-engineered.
Quick progression of projects from proof-of-concept to post-production stage.
Communication and presentation of ideas to colleagues in all parts of the wider tech team.
Participating in code reviews for the data engineering practice.
Act as a servant leader in our practice – ensuring team members have
person specification:
Knowledge of, or ability to rapidly adopt our core languages for data engineering – SQL and/or Python.
Experience working using a modern DevOps approach. You should not just have written data code but also the CI/CD pipelines necessary to test and deploy that code in a professional environment.
A robust understanding of core data engineering topics – ETL vs ELT, structured and unstructured data, data quality and data governance.
Ability to contribute to all aspects of a solution – design, infrastructure, development, testing and maintenance.
The ability to design and advocate for technical solutions to business problems.
Effective collaboration with technical and non-technical team members through agile ceremonies – roadmap planning, feature workshops, backlog elaboration, code review.
Understanding of cloud technology and significant hands-on experience working with it. You must understand how cloud-native solutions must be built differently from traditional ones.
Track record of taking initiative and delivering projects end-to-end; clear evidence of being self-driven and motivated
Immense curiosity, high energy and desire to go the extra mile to make a difference.

Desirable Skills
Beyond the required skills we are open to individuals of diverse talents. Experience with additional technologies, data science knowledge or a business background are all valued. We want to know how your unique abilities can contribute to our team.
Our Technology
We operate in a diverse technical landscape and are looking for flexible engineers who can adapt to and use many different tools. We would not expect any engineer to be familiar with the entire tech stack – no engineer can. Instead, we seek people with a good understanding of data structures and algorithms and the ability to apply this knowledge in learning new tools.
We have an existing data landscape involving both MS SQL Server and Oracle databases. We use SSIS for ETL. We will also need to source data from our integration layer – webMethods.
If some of that sounds like it needs bringing up to date, we know! We hope to start building a modern cloud data platform to realise the full value of our data. The technologies we use for that platform are still to be decided. Likely it will be based on Snowflake with Azure Data Factory for ELT. Other parts of our business have had success using Prefect for orchestration and dbt for transformation and we will reuse where it is sensible.
We prepare data for use by analysts working with a variety of tools – Tableau, PowerBI, and even Excel.
We take a DevOps approach and strive for continuous integration / continuous deployment. We use Azure Pipelines to deploy our code. We deploy infrastructure the same way using Terraform and Docker.
Our technology landscape is not fixed. Our engineering and architecture teams drive the technology we adopt.


Salary range $140,000 - $170,000","$155,000 /yr (est.)",1001 to 5000 Employees,Company - Public,Insurance,Insurance Carriers,1901,$1 to $5 billion (USD)
"Apolis
3.9",3.9,"Saint Louis, MO",Data Engineer (Python Focused),"Role: Data Engineer (Python Focused) (W2 ONLY)
Location : St. Louis, MO 63102
Duration: 18 Months
The primary responsibilities of this role will be to work as a part of the data and analytics team of the client. The ideal candidate would have experience using ETL and other data engineering tools in a cloud environment. Additionally, this candidate would have coding and scripting experience creating code to move and manipulate data using tools such in Python such as Spark or Pandas. Microsoft Azure experience is a plus but relevant experience with other cloud-based environments will be considered.
This team uses Microsoft Azure to build and productionize data sets and processes to the product owners and product users and this role will be expected to contribute to those objectives. As a member of this team, you will work in conjunction with other data engineers, platform engineers, solution developers and others to execute and deliver working solutions to the end users.
Technical Skills
High focus on Python
More Programmer/Coder experience, less Cloud experience needed.
Must have experience with data analytics/engineering and cloud technologies and an in-depth working knowledge and understanding of data engineering, design, and integration.
Intermediate or higher level of proficiency with Python Must have the proven ability to learn quickly and apply cloud data engineering principles and skills to new tools and use cases.
Ability to define and take technical ownership of detailed requirements and collaborate effectively with various team members such as Business Analysts and Project Managers and end users.
Excellent communication (verbal and written) and problem-solving skills are essential for this position.
Initiative, Proactive Cooperation, and curiosity are mandatory behavioral skills required for this position.
Experience with several of the following technologies is preferred:
Experience with Microsoft Azure Platform. In particular, the data processing toolsets including Azure Data Factory Pipelines, Azure Databricks, Azure Data Lake Storage.
Experience using Notebooks (Such as Jupyter, DataBricks) also desired.
Experience using code languages such as Python and C#
Experience using query languages such as SQL, MDX and DAX
Experience using scripting languages such as PowerShell, M-Query (Power Query), and Windows batch commands.
Understanding of Agile and DevOps principles for software development and ownership.
Familiar with compliance and controls for software development and deployment.
Education Required
A bachelor's in computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field [or equivalent work experience] is not required but is preferred.
Job Type: Contract
Salary: $60.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Vision insurance
Compensation package:
Overtime pay
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
St. Louis, MO: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Preferred)
ETL: 5 years (Preferred)
Cloud infrastructure: 4 years (Preferred)
Work Location: One location",$65.00 /hr (est.),501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,1996,$25 to $100 million (USD)
Proits Hub LLC,#N/A,Remote,Data Analytics Engineer,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote
Speak with the employer
+91 9417599933",$60.00 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Atika Technologies,#N/A,"Pittsburgh, PA",Data Engineer,"Role: Data Engineer/PySpark Developer, Pittsburgh, PA - Day One Onsite
Fulltime and Contract(W2)
LinkedIn Must
USC/GC/EADs
Three round of interview: First two on Zoom Video Call, End Client Round will be Onsite on the Client's Location (Pittsburgh, PA)
Job Description:
Python(PySpark), Jupiter hub, airflow, Java
Database knowledge - Oracle / SQL / Data Analytics
Regulatory reporting knowledge preferred
Good communication skills
Open / Ability to learn new technologies
Very good Analytical Skills
Experience in ETL is preferable
Job Types: Full-time, Contract
Salary: $120,000.00 - $130,000.00 per year
Benefits:
Dental insurance
Health insurance
Life insurance
Compensation package:
Bonus pay
Performance bonus
Yearly pay
Experience level:
10 years
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Pittsburgh, PA 15259: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Required)
Spark: 4 years (Required)
Regulatory reporting: 3 years (Required)
SQL: 4 years (Required)
IT: 10 years (Required)
Airflow: 4 years (Required)
Jupiter: 4 years (Required)
Work Location: One location","$125,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Fuge Technologies Inc
4.7",4.7,"Jersey City, NJ",Big Data Engineer,"Big Data Engineering Technical Lead
Mandatory Skills
Big Data – Spark, Kafka, Databricks, Hive, Nifi
AWS, Java, Scala
Client Interview Needed for Selection (yes / No)
Yes
Detailed JD (Pl share the Detailed Description, 1 liner JD will not work)
10+ yrs experience as Data Engineer
Extensive experience in spark on data processing
Kafka design experience like monitoring & parallelism.
Comfortable with Python/Java/Scala coding experience
Crisp / clear cut communication
Flexible with learning new technology
Flink experience is a plus
AWS experience is nice to have
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Jersey City, NJ 07302: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",$72.50 /hr (est.),1 to 50 Employees,Contract,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"SECURE RPO
4.3",4.3,Manhattan,Senior data engineer,"Must have skills:
8+ years of experience building high performance scalable enterpris`e analytics or data centric solutions
8+ or 5+ years of experience in the design and build of data extraction, transformation, and loading processes by writing custom data pipelines
At least 5 years of experience implementing complex ETL pipelines preferably in connection with Glue and/or Spark
Exceptional coding and design skills in Python or Java/Scala
Hands-on experience with AWS (i.e. Glue, Aurora Postgres, Lambda, EMR, EKS, Redshift, etc.)
Experience with visualization tools like QuickSight, PowerBI, Looker or Tableau
Experience with Talend (ETL) is a big plus
Roles and Responsibility:
Drive a high impact and high visibility project that enables data availability, encompasses data analytics, machine learning, and petabyte scale datasets, and provides reliable and timely access to thousands of data sources
Design, architect and support systems for collecting, storing, and analyzing data at scale
Recommend improvements and modifications on new and existing data and ETL pipelines. Create optimal data pipeline architecture and systems using Apache Airflow
Create data analytics for d ata scientists to innovate, build and optimize our ecosystem
Assemble large, complex data sets that meet functional and non-functional business requirements
Be responsible for ingesting data into our data lake and providing frameworks and services for operating on that data including the use of Spark
Analyze, debug and correct issues with data pipelines
Operate on or build solution required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, AWS and Spark technologies
Job Type: Full-time
Salary: $56.80 - $80.16 per hour
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Informatica: 7 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: On the road",$68.48 /hr (est.),1 to 50 Employees,Unknown,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Egrove Systems Corporation
4.4",4.4,"Dallas, TX",Data Engineer Flink,"Java, Bigdata, Azure & Flink
Flink Sr. Developer, who has implemented and dealt with failure scenarios of processing data through Flink.
A senior who can help his Flink development team, guiding and helping them implement custom solutions through Flink.
Seasoned Java developer who knows about all aspects of SDLC.
Worked on integrations of other technologies with Flink, eg: Kafka, MongoDB, etc
Azure experience in the areas of Messaging, Data processing, preferably on Flink or on Databricks.
Job Type: Contract
Salary: From $51.83 per hour
Compensation package:
Monthly bonus
Experience level:
11+ years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
Work Location: On the road
Speak with the employer
+91 732)-860-7733",$51.83 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Internet & Web Services,#N/A,Unknown / Non-Applicable
"Intersec
4.5",4.5,Remote,Senior Data Engineer,"Architect and implement highly scalable data pipeline to streamline migration of data from source data repositories to the centralized data platform
Implement agreed upon data models (e.g., data marts) to enable faster and efficient consumption of data by downstream consumers
Conduct performance tuning to optimize data migration techniques and ensure adherence to identified service-level agreements (SLAs)
Define and implement automated validation checks to ensure data integrity between source and target
Assist in onboarding consumers with existing technology to connect to the platform
Setup predictive alert mechanism for pipeline degradation for IMF pipelines.
Enhance data quality framework for data pipelines and derived datasets
Enhance Data Audit Balance and Control mechanism for data pipelines
Enhance pipelines to be certified copies of authoritative source
Populate metadata for the Enterprise with particular focus on the Data Sources being onboarded to EDP (Technical and Business Glossaries) related to IMF Modernization Programs
Validate correct lineage for datasets
Required:
*8+ years of Databricks, Data Modeling, and/or Metadata management experience
* Familiarity with architecting and developing real and near-real-time solutions
* Bachelor's Degree in Engineering, Mathematics, computer Science, Statistics or other quantitative discipline, or 4 years equivalent professional experience
* US Citizenship and ability to gain a public trust
Preferred:
* Familiarity with IRS systems and ETL pipelines
* Ability to work independently and manage multiple tasks
* IRS MBI Clearance with IRS Laptop and Informatica Installed
Job Type: Full-time
Salary: $70.00 - $75.00 per hour
Schedule:
Monday to Friday
Application Question(s):
Are you a US/GC card holder?
How many years of Databricks, Data Modeling, and/or Metadata management experience do you have?
Work Location: Remote",$72.50 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,Unknown / Non-Applicable
OpenSecrets.org,#N/A,Remote,Snowflake Data Engineer,"About us
Our goal is to is to track the flow of money in American politics and provide the data and analysis to strengthen democracy. Our vision is for Americans to use this knowledge to create a more vibrant, representative and accountable democracy..
Our work environment includes:
Modern office setting
Work-from-home days
Wellness programs
Data Engineer
OpenSecrets is seeking to add a new member to our data team. The Data Engineer will be responsible for helping build and maintain our data systems.
Responsibilities include:
Implementing methods to improve data reliability and quality
Analyzing raw data and implementing that data into our system using data pipelines
Troubleshooting data errors and pipeline issues
REQUIRED QUALIFICATIONS:
Experience working with and analyzing large datasets, equivalent to 2-3 years in a similar position
Proven ability to build complex data pipelines using Snowpipe and SSIS
The ability to work with diverse populations
Experience with MSSQL Server and Snowflake
Advanced knowledge of ANSI SQL
PREFERRED QUALIFICATIONS:
Excellent written and verbal communication skills.
The ability to manage multiple priorities and projects both independently and as a member of a team
Snowflake SnowPro Data Engineer certification
Experience with Apache Spark
Experience with machine learning matching models
Knowledge of and interest in American government and money in politics.
SALARY AND BENEFITS:
We are a fully remote organization and this person can be based anywhere in the U.S.
OpenSecrets offers an excellent benefits package, including full medical, dental, 403(b) match, generous vacation and sick leave, and all federal holidays paid.
TO APPLY:
Please visit this link: https://airtable.com/shrGe2bugeapKVr8a
People of diverse backgrounds are strongly encouraged to apply. OpenSecrets is an equal opportunity employer committed to creating a diverse and inclusive organizational culture. We do not discriminate against candidates and employees because of their disability, sex, race, gender identity, sexual orientation, religion, national origin, age, veteran status, or any other protected status under the law.
ABOUT US: OpenSecrets is the nation's premier research group tracking money in U.S. politics and its effect on elections and public policy. Our mission is to track the flow of money in American politics and provide the data and analysis to strengthen democracy. Our vision is for Americans to use this knowledge to create a more vibrant, representative and accountable democracy.
Job Type: Full-time
Pay: $83,874.82 - $100,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Retirement plan
Vision insurance
Compensation package:
Yearly pay
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Experience:
SQL: 3 years (Required)
Snowflake: 1 year (Preferred)
ETL: 2 years (Required)
Work Location: Remote","$91,937 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Cureatr
4.1",4.1,Remote,Data Engineer,"About Us
We are a thriving, medication management solutions company, headquartered in Manhattan, but largely virtually distributed, committed to tackling the $528B suboptimal medication management problem. Patient medication information is often incomplete for healthcare providers and systems – it’s our mission to fill in the gaps. We provide real-time, universal access to accurate medication data for over 265 million patients and high-quality interventions by board certified telepharmacists. Through our solutions, healthcare providers can reduce preventable hospitalizations, avoid readmissions, and ultimately, improve patient outcomes. We’re already used by dozens of hospital systems, with tens of thousands of clinicians and supporting staff using us to care for millions of patients.
Here at Cureatr, we are driven by a core set of values and strive to incorporate them into everything we do. We care deeply about our users, patients, and employees, and we aim to live and operate compassionately. We take pride in creating a diverse, inclusive, and equitable company culture, where our open and honest approach to work fosters innovation and encourages employees to achieve professional and personal growth.

The Role
We’re looking for a Data Engineer to join our growing Engineering team. Use the power of healthcare data to find insights that will demonstrate the value Cureatr is delivering to our customers, help improve patient outcomes, and expand the scope of Cureatr’s mission. As a Data Engineer at Cureatr, you will be responsible for designing, implementing and maintaining our data infrastructure, pipelines, automation, and ETL processes. You will work closely with our Data Science and Analytics teams to ensure that our data is clean, accurate, and readily available for analysis. You will also be responsible for ensuring that our data meets industry standards for security, privacy, and compliance.
This is a remote, work from home position, but candidates must be located within the U.S. Ideal candidates must be able to travel to our New York City office, or TBD company-wide offsite locations, up to 4x per year.

You’ll be part of a team working to solve problems like:
Increasing the number of patients who receive the opportunity to do a medication reconciliation, which can help keep them out of the hospital.
Streamlining and automating processes so that our pharmacists can serve as many patients as possible.
Predicting patient events likely to occur so that our clinical team can adequately prevent them before it’s too late.
Helping our pharmacists make the most of data and insights that will help them be as effective as possible in serving patients.
Evaluating the effectiveness of our clinical services impact at the population level.

What You’ll Do:
Design, implement, and maintain our data infrastructure, pipelines, automation, and ETL processes to support our data needs.
Build and maintain scalable data models and databases.
Collaborate with Data Science and Analytics teams to understand their data needs and provide data solutions accordingly.
Ensure data quality and integrity by implementing data validation and cleansing processes.
Ensure data security and compliance with healthcare regulations.
Monitor and optimize data performance and scalability.
Design and implement data governance policies and procedures.
Evaluate and implement new data technologies and tools to improve our data capabilities.

Your Skills:
3+ years of experience as a Data Engineer or similar role.
Strong programming skills in languages such as Python and SQL.
Experience using git to track changes.
Experience with data modeling and database design.
Experience with the “modern data stack” preferred
Experience with ETL and data pipeline development.
Knowledge of healthcare data and related standards (e.g., HL7, FHIR) is a plus.
Familiarity with AWS.
Strong problem-solving and analytical skills.
Excellent communication and collaboration skills.

Technology Stack:
Python
RedShift
dbt
Dagster
MongoDB
Looker
AWS

Educational Requirement: Bachelor’s Degree or relevant work experience

Salary: $110,000 - $140,000 plus full benefits

Why work with us?
Make a measurable impact on patients’ lives
Be part of a company dedicated to living its core values:
Empathy
Honesty
Integrity
Impact
Fun
Equity in Healthcare
Work in the company without micromanagement
Share your vision and make an impact on the product
Opportunities for career growth
Remote work
Competitive salary
Premium benefits (Health, Dental, Vision, 401k, unlimited PTO, more)

Equal Employment Opportunity Policy
Cureatr is an equal opportunity/affirmative action employer. All qualified applicants will receive consideration for employment without regard to sex, pregnancy, race, religion or religious creed, color, gender, gender identity, gender expression, national origin, ancestry, physical or mental disability, medical condition, genetic information, marital status, registered domestic partner status, age, sexual orientation, military or veteran status, protected veteran status, or any other basis protected by federal, state, local law, ordinance, or regulation and will not be discriminated against on these bases
Disclaimer: we are aware of a scam targeting applicants using a cureatrcareers.com email address. Please block and report this sender, as they are not affiliated with Cureatr. If you have any questions, please contact us at jobs@cureatr.com. Thank you!
uBajh3OfeT","$125,000 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2012,Less than $1 million (USD)
"Kastech Software Solutions Group
4.0",4.0,"Dallas, TX",Data Engineer,"In KastechSSG we are are seeking for a Cloud DB Platform Engineer with Data Enablement Services experience ($50/hour), to work in a Hybrid Schedule (3 days on-site, 2 days home office) for one of our partner companies!
Hybrid Schedule info: onsite 3 days per week from list of locations: Charlotte NC, Dallas TX, Chandler AZ, Newark, NJ - Must be local to one of these locations.
** Contract would be $50 per hour on W2 (18 months). American Citizens or GC Holders Only**
3-5 years experience in:
Terraform, Azure/GCP, DevOps Engineering, ETL, Client/AI, Data lake, Data architecture.
GCP Key Services Experience (ie Big Query, Composer, Vertex AI, DataProc, Data Catalog, Notebooks).
Azure Key Service Experience (ie Synapse, Purview, Data Lake, Logic Apps, Cognitive Services).
Job Type: Full-time
Salary: $50.00 per hour
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you American? If not, please specify your visa type
Would you be willing to work under W2?
Experience:
Terraform: 3 years (Preferred)
Azure Data Lake: 3 years (Preferred)
GCP Key Service Exp: 3 years (Preferred)
DevOps: 3 years (Preferred)
Work Location: In person",$50.00 /hr (est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2008,$25 to $100 million (USD)
"Virtualan Software LLC
4.5",4.5,"Atlanta, GA","Sr. Data Engineer (Python,Spark & Databricks)","Role : Senior Data Engineer (Python, Spark & Databricks)
Location : Hybrid (Atlanta, GA)
Pay: 60-70$ on C2C / 1099
RESPONSIBILITIES:
Create or modify the conceptual, logical and physical data models.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of sources like Hadoop, Spark, AWS Lambda, etc.
Lead and/or mentor a small team of data engineers.
Design, develop, test, deploy, maintain and improve data integration pipeline.
Develop pipeline objects using Apache Spark , PySpark and/or Python.
Design and develop data pipeline architectures using Spark and related AWS Services.
Communicate effectively with client leadership and business stakeholders.
Participate in proposal and/or SOW development.
REQUIREMENTS:
5+ years of professional work experience designing and implementing data pipelines in a cloud environment is required.
2+ years of experience migrating/developing data solutions in the AWS cloud is required.
1+ years of experience building/implementing data pipelines using DataBricks is required.
Expert level knowledge of using SQL to write complex, highly-optimized queries across large volumes of data.
Hands-on object-oriented programming experience using Python is required.
Professional work experience building real-time data streams using Spark.
Knowledge or experience in architectural best practices in building data lakes.
Bachelor or Master degree in Computer Science, Engineering, Information Systems or relevant degree.
Hybrid work schedule, and must live in the Atlanta, GA metro area. Must be open to up to 25% national travel to client locations when on engagements outside of the Atlanta, GA area.
Job Type: Contract
Pay: $60.00 - $70.00 per hour
Schedule:
Monday to Friday
Application Question(s):
Apply only if you are interested in Hybrid Role in Atlanta, GA. Interested ?
Experience:
Data Engineer: 4 years (Required)
Databricks: 1 year (Required)
Willingness to travel:
25% (Required)
Work Location: On the road",$65.00 /hr (est.),1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,2019,Less than $1 million (USD)
"GOBankingRates
3.3",3.3,"North, SC",Staff Data Engineer,"GOBankingRates™ is unique in the digital marketing and media industry - we combine marketing, digital, content and fintech. Our performance based approach increases brand awareness and generates targeted audience engagement on our internal web properties and partner sites.
Learn More About What We Do

What's interesting about this role?
GOBankingRates has big growth plans ahead and is looking for a strong Staff Data Engineer experienced in Data warehousing and Python to join our Data Engineering team. The GOBankingRates Data Engineering team is responsible for designing and developing the Data lake, enterprise database, data warehouse, reporting solutions, and pipelines for data processing. If you are a critical thinker with a solid track record of developing data solutions and solving complex problems with SQL and Python, we want you to join our team! You will play a vital role in designing and developing our next generation data pipelines and data platform. Join our team and prototype new data product ideas and concepts!
How will you make an impact?
Build and maintain multiple data pipelines to ingest new data sources (API and file-based) and support products used by external users and internal teams.
Optimize by building tools to evaluate and automatically monitor data quality and develop automated scheduling, testing, and distribution of feeds.
Work with data engineers, data scientists, and product managers to design, rapid prototype, and productize new data product ideas and capabilities.
Design and build cloud-based data lakes and data warehouses.
Conquer complex problems by finding new ways to solve them with simple, efficient approaches focusing on our platforms' reliability, scalability, quality, and cost.
Collaborate with the team to perform root cause analysis and audit internal and external data and processes to help answer specific business questions.
What will you bring to us?
Master's Degree (or a B.S. degree with relevant industry experience) in math, statistics, computer science, or equivalent technical field
Experience with dimensional data modeling and schema design in a database or data warehouse
Expertise with scripting languages such as Python and writing efficient and optimized SQL.
Working experience in building data warehouses and data lakes.
Experience working directly with data analytics to bridge business requirements with data engineering.
Experience with AWS infrastructure
Ability to operate in an agile, entrepreneurial start-up environment and prioritize
Excellent communication and teamwork, and a passion for learning
Curiosity and passion for data, visualization, and solving problems
Willingness to question the validity, accuracy of data, and assumptions
Preferred Qualifications:
Experience building data warehouse, data lake, and data pipeline using Snowflake/Redshift and other AWS Technologies.
Experience with large-scale distributed systems with large datasets.
Experience with event streams and stream processing (e.g., Kafka, Spark, Kinesis)
Hands-on experience with event streaming with modern event streaming tools like Pulsar, Kafka, and Kinesis. Understanding when streaming vs. batch processing is appropriate, and tradeoffs in a given context
Knowledge of advertising platforms.

Benefits
Competitive salary with excellent growth opportunity; we pride ourselves in having a team that exudes leadership, high initiative, creativity and passion.
Awesome medical, dental and vision plans with heavy employer contribution
Paid maternity leave and paternity leave programs
Paid vacation, sick days and holidays
Company funding for outside classes and conferences to help you improve your skills
Contribution to student loan debt payments after the first year of employment
401(k) - employees can start contributing immediately. After the first year, GOBankingRates matches your contribution up to 4% of your salary
A note about our response to COVID -19 and our new norm: The world has changed and we know it's important to adapt and to do our part to take care of our teams in this global pandemic. Our number one priority is to have our team feel safe, balanced and connected. We're committed to providing our teams with the best resources and tools to navigate this new virtual world that we're living in. We've also reinvented the ways in which we recognize, celebrate, and engage with each other to keep our culture strong!
Here's a peek into our world at GOBankingRates -
Our teams are working remotely 100% for the foreseeable future and have flex time. We're in the digital media space so we're mobile and flexible!
Option to work from an office (if you need to get away!)
Tools & resources are available to keep our team connected across North America. (JIRA, Trello, Airtable, Slack, Zoom and so much more!)
To keep our community engaged and connected, virtual team building events are held weekly and monthly.
For wellness and balance, weekly virtual fitness classes such as yoga are available.
To care for the local communities that we're a part of across the U.S our team members host socially distanced philanthropic events every quarter.
And most importantly, we've committed to consistent and transparent communication to help us all stay informed, engaged and to keep us on our path to success and #greatness.
We are an equal-opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.","$134,519 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Internet & Web Services,#N/A,$25 to $100 million (USD)
"spar information systems
3.5",3.5,Remote,Azure Data Engineer,"Role: Sr Azure Data Engineer
Location: Remote
Duration: 3 Months Contract to hire Full Time (W2 Only)
Must have 11+ IT Experience
Required Skills:
3+ years of experience in data software development, programming languages and developing with big data technologies
2+ years of experience designing and building on existing and new data applications
2+ years of experience in Cloud DevOps concepts, Cloud Services and Architecture, and Azure/AWS/GCP DevOps Operational Framework
1+ years of experience in open-source data tools and frameworks, or one of the following: .net Core, asp.Net, Angular, or Express.
Thanks & Regards,
Arvind Kumar Bind
Cell: 732 716 7403 (Text)
Direct Number:- 469-750-0607
Email : Arvind.B@sparinfosys.com
Job Types: Full-time, Contract, Permanent
Pay: $120,555.79 - $150,000.00 per year
Benefits:
401(k)
Health insurance
Paid time off
Experience level:
11+ years
Schedule:
8 hour shift
Experience:
Big data: 3 years (Required)
SQL: 1 year (Required)
Data lake: 3 years (Preferred)
Work Location: Remote","$135,278 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2012,$25 to $100 million (USD)
"Take Command Health
4.9",4.9,"Dallas, TX",Data Engineer,"Company: Take Command
Position: Data Engineer
Salary: $120,000-$130,000 plus bonus
Location: Dallas or Austin, TX
About Take Command
Take Command is a start-up on a mission to improve the healthcare system, starting with health insurance. Pragmatically speaking, we help employers reimburse employees for individual insurance instead of offering a traditional one-size-fits-all group plan. We believe this model can empower employees (when they have the right support) to be savvy healthcare consumers and have a transformative impact on the entire healthcare system.
Let’s be honest—health insurance is usually a confusing, frustrating, and even emotional experience for people. We want to fix that with a new model, great technology, and a superior user experience. We have made a great start, but we need your help to fully realize our long-term vision.
As a part of the dev team, you'll build the framework for both our clients and our team to succeed. You will be a pillar in our company as we grow and make our product the best product on the market!
In this role, you will:
Take ownership for delivering multiple features, their quality and assist team members in delivering their features
Recommend improvements to data quality, processes and code to make it easier to use, manage and maintain
Experience designing and developing data models and data migrations
Experience managing and maintaining ETL or ELT processes (Hevo, BigQuery, Looker, etc.)
Hands-on development experience with SQL, relational database design (ERDs)
Must be able to write SQL & contribute code
Be product-minded; focus not only on what needs to be built but why something needs to be built
You may be a great fit for this role if you have:
3 or more years of professional experience as a data analyst or data engineer
Cloud-Based Data Tools e.g. Hevo, BigQuery, Looker, etc.
Database technologies: SQL, RDBMS / Relational databases (Postgres, Amazon RDS / Aurora, etc.)
Nice to have experience: Python, Java, Typescript or another modern OOP language
Cloud infrastructure: AWS
Source control: Git
Working at Take Command
We’re excited to build a team and culture that reflects our values! We offer competitive pay, health benefits, and equity options to share with this position. We want you to be an owner along with us in revolutionizing healthcare.
A generously funded ICHRA for medical, dental, and vision premiums and medical expenses. You get to use our own product and we think that’s so exciting and rare!
Unlimited personal vacation in addition to regular company holidays.
401(k): 90-day eligibility for 2% match that vests immediately!
Access to LinkedIn Learning.
We have two beautiful offices conveniently located near The Hill (Dallas) and Holly Commons (Austin). The kitchen is well-stocked and we've designed the space to have lots of different areas to work!
Paid parental leave for new parents.
Flexible on where you work – we hope to see you around the office on average 3 days a week especially when white-boarding or building relationships with your colleagues, but you also have the ability to work from home or wherever you'd like when you need to get focus work done.
More About Us
We secured our Series A funding in 2021 and are thrilled to be able to expand our team. Despite being a small startup in a land of health insurance giants, we’re the recognized industry leader for what we do (health insurance reimbursements) and passionate about bringing it to market because we know we can help fix a broken system and improve our clients’ wellbeing and health outcomes.
We’ve been featured in The New York Times, The Wall Street Journal, The Dallas Morning News, and other national healthcare publications and are excited about our growth opportunities.
Take Command knows diversity and inclusion among our teammates is integral to our company’s success and growth. Our vision is to recruit, develop, and retain the best team from a diverse candidate pool.
This has mostly been about us, but we’d love to hear from you-we can’t wait to hear your story!
Take Command is an equal opportunity employer! We celebrate diversity and are committed to creating an inclusive environment for all employees.
Xrygn4NXMr","$125,000 /yr (est.)",1 to 50 Employees,Company - Private,Insurance,Insurance Agencies & Brokerages,2014,Unknown / Non-Applicable
"DiNi Communications, Inc.
3.0",3.0,"New York, NY",Data Center Engineer,"DiNi Communications, Inc. is seeking a Data Center Engineer. This person needs to be a team player with exceptional communication skills. Maintaining the confidentiality of the client's information is of utmost importance.
Required Education and Experience
· At least 8 years of experience as a large data center technician
· Knowledge of Networking,
· Ability to lift equipment at least (50lbs) Rack and Stack (Network Switches, Servers, Storage,
Appliances)
· Knowledge of structured Cabling in a Data Center.
· General knowledge of Operating Systems, (Windows, Unix).
· General knowledge of networking in N Tier Environment (App, Web, DMZ,).
· Level 1 and Level 2 troubleshooting in a N Tier Environment.
· Ability to follow and execute commands to start up and shutdown systems.
· Physically travel to and be available at site B to perform technical changes that require a
person at Site B.
· Performing backup and restore activities.
· Asset Management.
· Maintaining document.
· Work with vendor on circuit issues, (WAN and Internet).
· Cable Patch the equipment in the environment.
· Participate in Site B fall-over testing
Preferred Network Engineering Experience
· Knowledge of N Tier networking.
· Certification in Networking (CISCO).
· Certification in various Operating systems (Linux and Windows), Storage, Messaging
(Microsoft).
· Certification in IT Security.
· Knowledge of Databases.
· Experience working with CISCO, EMC, Symantec, HP, Oracle, Bluecoat, MacAfee, and other vendors' equipment and software in a networked environment.
Responsibilities include but are not limited to:
Adding equipment to support agency applications in site B Environment,
Coordinating Site B testing with Site A technical resources.
Monitoring the B environment.
Rack and Stack new and replacement equipment
Troubleshooting and resolving incidents and problems in the
Physically travel to and be available at Site B to perform technical changes that require a person at Site B.
Upgrading and expanding the environment
Performing backup and restore activities
Manage Site B assets
Maintaining document
Cable Patch the equipment in the environment
Participate in Site B fall-over testing.
Job Type: Full-time
Pay: $60.00 - $70.00 per hour
Schedule:
Monday to Friday
Experience:
large data center technician: 8 years (Required)
Networking in N Tier Environment (App, Web, DMZ,): 3 years (Required)
Ability to lift equipment at least (50lbs): 1 year (Required)
structured Cabling in a Data Center: 3 years (Required)
Level 1 and 2 troubleshooting in a N Tier Environment: 3 years (Required)
Performing backup and restore activities: 3 years (Required)
Cable Patch the equipment: 3 years (Required)
Certification in Networking (CISCO): 1 year (Preferred)
Certification in IT Security: 3 years (Preferred)
CISCO, EMC, Symantec, HP, Oracle, Bluecoat, MacAfee: 3 years (Preferred)
Work Location: In person",$65.00 /hr (est.),Unknown,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"Trella Health
4.8",4.8,"Philadelphia, PA",Senior Data Engineer,"At Trella Health, we are passionate and committed to our mission – empowering meaningful change in healthcare. Since our founding in 2015, we continue to grow our team, enhance our solution and services offerings, accelerate into new markets, and expand our customer base. We are rapidly growing and are looking for new Trellavators to join our team!
""What is a Trellavator?"" you ask. Innovate and elevate is the name of our game! We go above and beyond to collaborate with and support each other – we believe that when a colleague or customers succeeds, we succeed. By learning from others, building on our successes, and taking risks, we constantly raise the bar – continuous improvement is in our DNA. Our word is paramount, we keep our commitments, and we always follow through. We have a strong, reliable support system that fuels growth, collaboration, and passion – and together, we create a positive environment where everyone at Trella Health, including the customers we support, can thrive. Are you ready to learn more about the opportunities with our team? Trell-yeah you are!
Position Overview:
The Senior Data Engineer will be responsible for designing, implementing, and maintaining an effective data pipeline for Trella Health's applications. This position is responsible for turning data concepts into effective data models that are efficient and scalable. The Senior Data Engineer will ensure a smooth data release and deployment process throughout the data development lifecycle.
Successful candidate will be a highly organized, self-starter that can roll up their sleeves and solve problems efficiently. Ability to document procedures, manage time, and have a knack for optimizing data flows.
Location: Atlanta HQ; Raleigh, NC; Nashville, TN; Philadelphia, PA; Remote considered
Reports to: Manager, Data Engineering
As a member of the Data team at Trella, you will:
Champion a comprehensive data vision and operating principles that improve data quality, consistency, accessibility, decision making, use, and stewardship.
Manage data pipeline by developing processes for data input, transformation, and storage
Act as an interface between the Data Science team and Application Developers
Architect and implement front-end (application layer) and back-end (ETL layer) database technologies in a highly customized data environment
Develop automated processes for moving data between systems
Implement ETL processes that transform source data into functional data stores consumed by front-end applications
Scale data pipeline by utilizing the most efficient tools and technologies
This job might be a fit for you if you have:
Bachelor's degree in Computer Science, Engineering, Mathematics, or other technical discipline (proven professional experience will be considered in lieu of a technical degree)
2+ full years of recent advanced Snowflake development experience
Experience manipulating healthcare data
In-depth knowledge of Amazon Web Services (AWS) and Platform as a Service (PAAS)
Demonstrable experience with Snowflake utilities including SnowSQL & Snowflake Javascript
Analyze all aspects of the Snowflake Query Engine to improve bottlenecks that may exist
Ability to automate data ingestion from Amazon S3, FTP, and other technologies
Experience with scripting languages, extensive Python experience, Node.js experience a plus
Working knowledge of Bitbucket and Git
Experience managing and loading data in AWS RDS (MySQL/MariaDb)
Ability to optimize MySQL query execution plans for robust performance
Technical Job Requirements (Nice to Have)
Experience manipulating healthcare data including claims, ADT data
Foundational knowledge of CI/CD platforms, preferably TeamCity
Development and consumption of API services
Experience with data science tools (Jupyter, SageMaker, R)
Understanding of Atlassian suite of tools, including Jira and Confluence
Soft Skill Job Requirements (Nice to Have)
Strong analytical skills and ability to rapidly switch contexts
Excellent communication and problem-solving skills
Sole contributor that is willing to independently uncover and improve processes that are inefficient or sub-optimal
Track record of implementing creative solutions to complex problems
Experience working in a nimble, fast-paced, start-up environment
Knowledge of HIPPA, PHI, PII policies
About Trella Health:
Trella Health provides unmatched, actionable market intelligence to post-acute care and value-based care providers of all sizes. Our industry leading analytics paired with CRM and EHR integration workflows positions us as the most advanced sales enablement platform for the post-acute care market. One of only a few companies to be deemed both a Qualified Entity by CMS and an Innovator under its Virtual Research Data Center Program, Trella has elite access to billions of performance metrics and referral data to enable competitive positioning across the market. Its standardized insights, representing 90% of all lives 65+ U.S. population, help customers identify, engage, and manage critical relationships and advance their organizations with certainty.
Since launching our first product in 2016, we have experienced tremendous growth and have highlighted a few key data points to get an at-a-glance view of Trella:
Over 700 customers including 10 of the top 10 post-acute providers and 4 of the top 10 health systems
Over 120 team members and growing
Recognized as one of Modern Healthcare's ""Best Places to Work"" in consecutive years and were ranked in the Top 50 startups in the US by LinkedIn
Headquartered in Atlanta, GA with remote hubs in Nashville, Philadelphia, Raleigh, and the Philippines
The Trellavator Experience:
At Trella Health, you can expect to join a welcoming team that truly embodies our core values. Our collaborative culture is anchored by trust, transparency, and inclusion — and we also have a lot of fun. No matter where you work, at Trella Health you can expect an awesome team, frequent virtual gatherings, engaging events, casual attire, and more.
We offer competitive salaries with a comprehensive benefits package to all employees and provide an environment that fosters work-life harmonization with unlimited PTO, company-observed holidays, and Summer Fridays. As we continue to see exponential growth, our goal is to continue to put team members first and strive to offer our team members the best culture and benefits possible. Some of the benefits we provide are:
Health, Dental, Vision & Voluntary Benefits
Competitive Salary & Bonus Plans
401k Retirement Savings
Unlimited PTO & 10 Paid Holidays
Flexible Work Hours
Equity Shares
Paid Leave Programs
Marketplace for discounted retail and entertainment
Equal Opportunity Employer
Trella Health is an equal opportunity employer. All persons will receive consideration for employment without regard to race, color, gender, sexual orientation, gender identity or expression, religion, national origin, marital status, age, disability, handicap, veteran status, genetic information, or any other protected status as recognized by federal, state, or local laws.","$113,846 /yr (est.)",51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2015,Unknown / Non-Applicable
"MoneyDolly
5.0",5.0,"Salt Lake City, UT",General Data Engineer,"Position Title: General Data Engineer
Location: Remote (US Based only)
Commitment: Full-Time

Job Brief

About Us
MoneyDolly is a fast growing Fintech Saas tech company, headquartered in Sandy, Utah. We are leading innovation in supporter relationship management, wherein teams meet all their fundraising goals. Teams can create their own page for your supporters to visit, offer products and incentives for their contributions, then simply invite their team to spread the word and watch the money roll in. MoneyDolly has already helped thousands of organizations, groups and teams nationwide raise over $20 million. We are still private, and our best work is still ahead of us. This is a massive industry with antiquated methods and no clear market leader. This is the spot for a qualified team player looking to build something new, make a real impact, and actually change the world.

Job Description:
We are looking for a highly motivated and talented Generalist Data Engineer to join our development team. The ideal candidate will be someone who loves to wear multiple hats and thrives in an environment of high autonomy to accomplish our business goals. You will be responsible for collaborating with our business team, creating flexible data models, and maintaining data pipelines to ensure the smooth flow of data in our organization.
Responsibilities:
Interact with our business team to gather requirements and understand data needs
Design and create flexible data models that allow for easy report generation and ad-hoc analysis
Write and maintain DBT transformations to generate flexible data models from our production database
Develop and maintain required data pipelines in Python to collect and process data from various sources
Develop and maintain Tableau and Hex.Tech dashboards to analyze the collected data
Ensure data quality, integrity, and security in all data processes
Collaborate with other team members to implement data solutions and integrate them into the existing infrastructure
Continuously monitor and optimize data models and pipelines to meet changing business requirements
Requirements:
Bachelor's degree in Computer Science, Engineering, or a related field
Strong experience in data modeling, ETL processes, and data pipeline development
Proficiency in Python and SQL
Familiarity with DBT transformations and best practices
Experience working with relational databases and big data technologies
Excellent problem-solving skills and attention to detail
Strong communication and collaboration skills
Ability to work independently and adapt to a fast-paced, dynamic environment
Nice to have:
Experience with data visualization tools (e.g., Tableau, Power BI)
Experience with AWS and GCP (RDS Postgres and BigQuery are our current stack)
Knowledge of the fundraising industry or a strong interest in learning more about it
What We Offer:
A competitive salary ($100k-$140k range) and benefits package
A supportive and collaborative work environment
Opportunities for professional growth and development
The chance to make a significant impact in a growing startup
If you are passionate about data engineering and excited about the prospect of revolutionizing the fundraising industry, we'd love to hear from you. Apply now to join the MoneyDolly team!","$120,000 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"shaped.ai Inc.
4.0",4.0,"New York, NY",Data Engineer,"Hello! We’re building the future of content discovery infrastructure with AI — come join us!

Shaped (YC W22) is an API for developers to seamlessly add personalized ranking and recommendation to their products. These frictionless discovery experiences help end-users find what they want faster, and, consequently, grow conversion and engagement business metrics. Behind the scenes, we’re building state-of-the-art ML infrastructure for training and deploying large AI models to power our personalization engine.
We’re a seed stage start-up backed by top-tier investors (e.g. Y-Combinator, Susa Ventures, Tribe Capital etc…) and executives (e.g. Google, Amazon, Uber, Dropbox etc…). Our team comes from Meta, Google, Apple and Uber. We’re a remote team but have a small office in Brooklyn, New York.
We’ve been lucky to have a huge amount of success since our initial launch earlier this year and now looking to expand our real-time data engineering capabilities. We are looking for a data engineer to design, build and optimize Shaped's real-time streaming data ingestion endpoints. You will be a founding engineer working on state-of-the-art infrastructure. As one of Shaped’s early employees you will help shape our product, culture and vision.
This role is for a full-time data engineer at $120k - $200k a year and 0.1 - 0.5% equity.

Responsibilities:
Manage the unification of our batch and real-time data processing pipelines.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Requirements:
Bachelor's in computer science, data science or mathematics related field. Master's degree or PhD will be advantageous.
5+ years of data engineering experience, ideally applied to machine learning use-cases.
Experience with AWS or common cloud technologies
Proficient with Python, SQL.
Extensive knowledge of both offline and online large scale data processing frameworks: Spark, Beam, Flink.
Extensive knowledge with stream technologies like Kakfa or Kinesis.
Excellent written and verbal communication skills.
We’re excited to work with you. Come build the future of AI and discovery with us!","$160,000 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Siri info solutions inc,#N/A,"Beaverton, OR",Sr. Data Engineer,"Must Have Skills
PySpark
Python on AWS
Responsibilities:
Understanding of advanced data warehousing concepts, data modeling and extract, transform and load development
Advanced SQL skills with experience querying large datasets from multiple sources and developing automated reporting
Python skills for scripting, data manipulation, custom extract, transform and loads and statistical/regression analysis particularly, as they apply to sales and marketing operations and performance
Job Type: Full-time
Salary: $47.00 - $50.00 per hour
Experience level:
7 years
Work Location: On the road",$48.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Radiant System,#N/A,"Pleasanton, CA",Data Engineer,"First Priority: Pleasanton, CA –
Second Priority for a superstar: Plano, TX
Experience in developing and deploying data pipelines, preferably in the Cloud
Experience working with Spark, Databricks, and other relevant technologies in Microsoft Azure cloud
Experience in productionizing and deploying Big Data platforms and applications
Expert in writing SQL statements
Job Type: Contract
Application Question(s):
Are you willing to work on W2?
Work Location: In person",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Quadrant Resource
4.1",4.1,Remote,Data Engineer,"Job Description:
4-6 years of experience as a Data Engineer in the Snowflake-Fivetran-dbt stack
• Possess a solid understanding of end to end Data ingestion, ETL, data analysis, Reporting processes
• Expert in ETL development using Fivetran and experienced in implementing data pipelines for Snowflake Cloud DWH
•Experienced in using dbt for data transformation
•Expertise on Tableau/Metabase for building reports/dashboards
•Expert in troubleshooting and resolving ETL issues, data load failures/problems and transformation translation problems
•Excellent communicator with the ability to work effectively with distributed teams
Job Type: Full-time
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Data Engineer: 9 years (Required)
Fivetran: 8 years (Required)
DBT: 7 years (Required)
Snowflake: 6 years (Required)
Work Location: Remote",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,2004,Unknown / Non-Applicable
"Tipico - North America
4.0",4.0,"Denver, CO",Data Engineer,"Company Description

Tipico is an energetic, passionate, bold tech company, and we love sports! Tipico has recently expanded to the US market, establishing offices in Hoboken, New Jersey and Denver, Colorado. We are driven by our core values: passion, progress and trust. Our goal is to build the best possible mobile sports betting product in the industry, in order to amplify the emotions of millions of sports fans every day. Let’s Make This Interesting ™
Please note: this role is located in our Denver, CO office; we work off a hybrid model and come into the office 2 days per week.

Job Description

The Data Engineer will report directly to the Data Engineering Manager and work together with rest of the data engineers, BI analysts, and product teams. The position holder ensures this is done by doing the following:
Create and maintain both batch (ETL/ELT) and real-time data pipelines and architecture
Ensure all data provided by Data Platform is of the highest quality and is delivered in a timely manner and in line with agreed SLAs
Assemble large and complex data sets that meet functional and non-functional business requirements
Identify, design, and implement internal process improvements, including automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies
Collaborate with stakeholders including data architects, Executive, Product, Data and IT team members from the beginning to the delivery of a project
Build a scalable, modern, and trusted data ecosystem, making sure it is responsive, available and in line with business requirements
Take part in sprints, workloads, and deliverables through agile methodologies
Manage quality assurance, verification of accuracy and consistency of data, specifically KPI measures sent to the business
Thrive working in a fast-paced and challenging environment
Open to continuous learning individually and with the team
Comfortable with being challenged / seeking challenge
Willing to work flexible hours if needed

Qualifications
3+ years in Data Platforms / Datawarehouse / BI experience
Must have experience in successfully implementing data-centric applications, such as data warehouses, operational data stores, and data integration projects
Strong analytical and problem-solving skills that balance with creative approaches
Ability to learn new and complex concepts quickly
Strong experience with large-scale production databases and SQL
Solid understanding of cloud data services: AWS services such as S3, Athena, EC2, RedShift, EMR, EKS, RDS and Lambda
Proficient with one or more big data & new data technologies such as DBT, NIFI, Airflow, Kafka
Experience with data modelling
Experience with ETL development (extractions, data load, aggregation, transformation)
Knowledge in working with time-series/analytics databases such as Elasticsearch
Understanding of containerization and orchestration technologies like Docker/Kubernetes
Must be able to take ownership of projects with a high level of attention to detail and priority
Team player with a hands-on approach and attitude
Must be comfortable with on call rotation
Nice to have:
Familiarity with Marketing tooling and/or CRM systems is preferred
Familiarity with using business intelligence tools such as Domo, Tableau or Looker
Knowledge about and / or experience with sports betting
Background in digital companies, online, gambling, gaming, e-commerce, marketing, or mobile apps
Familiarity with one scripting language, preferably Python

Additional Information

What's in it for you:
Work in an environment where you, your work and ideas matter and have an impact.
You will be a be part of the newest and up and coming USA online iGaming and sports betting companies in the market.
Work in a new and young business with high growth potential.
Build your own success story together with us.
Work with self-organized, self-responsible and entrepreneurial employees.
Start-up feeling, backed-up by a leading European sports betting house. We are a high-volume business and are taking off in the US!
Competitive salary, Medical, Vision, Dental Benefits, Unlimited PTO, and more
Salary for this position is $120k+ & annual performance bonus","$120,000 /yr (est.)",201 to 500 Employees,Company - Private,"Arts, Entertainment & Recreation",Gambling,2004,Unknown / Non-Applicable
National Group Corporation,#N/A,"Dallas, TX",Data Engineer,"Who are we?
ReAlign Insurance Holdings is a leading primary underwriter of program administrator managed specialty insurance programs in the U.S. With the extensive knowledge and experience our team has in building and managing carriers and programs we are able to reach decisions on program opportunities quicker than our competition.
Why choose ReAlign?
At ReAlign, we believe that we are only as strong as our people. We strive to create an inclusive and welcoming culture where employees feel comfortable and empowered to be themselves. We support a diverse and inclusive work environment, with a focus on a healthy work-life balance. We understand that comprehensive benefits are important which is why we offer a benefit package that includes: medical, dental, vision, company paid short and long-term disability, life insurance, health spending account, 401(k) match every pay period, PTO, paid holidays, floating holidays, tuition reimbursement, and referral bonuses.
Job Summary
The Data Engineer position will involve data exchange process analysis, design, development, unit testing, and support. This role would work with other data team members on database ETL processes and reporting needs in both the on-premise database environment as well as the Azure data environment. This position requires strong technical and analytical skills to review business processes and data, recommend solutions, conduct functional and code analysis, and lead and perform database development tasks. This position will also involve production support and require experience within a DevOps environment.
Essential Functions
· Design, development, and support data ETL processing for both external and internal customers
· Support the internal and Azure cloud-based data warehouse platforms
· Work with DBA/System Admin to develop and maintain automated data processing
· Design and analyze queries and stored procedures
· Write and support SSRS and PowerBI reports
· Work in a data development life cycle environment
Work with stakeholders and data analysts to understand business requirements
Work with end users and the testing team to perform unit, functional, and integration testing and validation prior to production implementation
Troubleshoot and resolve operational problems with a focus on delivery
· Communicate solutions and architectural design to IT leadership
· Work in both Kanban and Scrum Agile environments
· Continually improve processes and solutions, seeking ways to use new tools, features, and capabilities
Job Requirements
3-10 years of experience as a Database Developer
Highly proficient in writing advanced SQL queries, stored procedures, SSRS reports, and ETL processes
Strong knowledge of relational and transactional database architecture
Strong technical and analytical skills
Experience with the Microsoft Azure and PowerBI platforms and Agile development methodology a plus
Bachelor’s Degree in computer science, database development, or related field preferred
Detail oriented with the objective of delivering a solution to a problem in a timely and efficient manner
Self-driven, willing to take initiative when an opportunity presents itself
Strong collaboration and communication skills and the ability to work in a team environment a must
Ability and willingness to work in a partial on-prem and partial remote office environment
Experience in the Property and Casualty Insurance industry a plus
Proficient with standard desktop software used in a business environment (MS Word, Excel, Visio, etc.)
Must be able to work in a hybrid in-office and remote work environment
Must be authorized to work in the United States
Job Type: Full-time
Pay: $90,000.00 - $125,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Health savings account
Life insurance
Paid time off
Retirement plan
Vision insurance
Experience level:
3 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you currently local to the Dallas area and able to commute to the office 2 to 3 times per week? (Deal breaker)
Experience:
writing advanced SQL queries and ETL processes: 3 years (Required)
cloud database platform (i.e. Azure, AWS): 1 year (Preferred)
Azure Data Factory: 1 year (Preferred)
Work Location: In person","$107,500 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
WellTrust Medical Group,#N/A,"Charlotte, NC",Data Engineer,"The Data Engineer will assist the senior analyst in complex modeling and projects to ensure their completion within the allotted timeline. Helps track measures of success and develop reports that measure operational and customer metrics. Demonstrates ability to troubleshoot data consumption and data quality problems.

Responsibilities:
Build and maintain reports using My SQL Server
Identify optimal data sources, structure data validation, and cache optimization.
Performs business problem modeling and quantitative analytics including regression analysis, time-series, hierarchical cluster analysis, and data mining.
Helps track measures of success and develop reports that measure operational and customer metrics.
Demonstrates ability to troubleshoot data consumption and data quality problems.
Evaluates and recommend improvements to current models, metrics, and analytic processes to improve operational efficiency and insight.
Design, document, and communicate effective technical solutions meeting end-user requirements
Perform analyses with large amounts of complex data, and uses data visualization tools to communicate results.
Requirements:

Bachelor's degree or equivalent experience in a quantitative field (Mathematics, Statistics, Computer Science, Economics, Finance, Physics, Engineering)
3+ years of professional working experience as a Data Engineer or Analyst
1-3 years of data warehouse experience
Build and maintain reports using My SQL Server
Strong analytical and creative problem-solving skills
Design, document, and communicate effective technical solutions meeting end-user
SQL and Excel Macro development
Strong understanding of various data environments such as SQL, Oracle, Big Query, etc.
Experience with data visualization tools such as Looker Studio, Tableau, D3.js, etc.
Proficient in R, Python, MATLAB, Java, or another programming language.",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Merkle
3.7",3.7,"Norristown, PA",Data Engineer,"Company Description

We are Dentsu International. We innovate the way brands are built. That means we do things differently so they’re better than before. In this way, we make our clients’ most important marketing assets—their brands—win in a changing world.
Dentsu International is a modern marketing solutions company. Our mission is to help clients navigate, progress, and thrive in a world of change. Businesses rely on our integrated network of agencies and specialized practices to champion meaningful progress through creative, media, commerce, data, and technology. Part of Dentsu Group, our global network comprises 66,000 diverse people in 143 countries, who are dedicated to teaming for growth and good. Follow us on Twitter @DentsuUSA and visit dentsu.com/us.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.

Job Description
Develop onboarding process to receive and process information related to onboarding a new subscriber
Convert Snowflake Stored Procedures to Python Scripts directly in Airflow
Develop metadata driven solutions for managing data processing and cadence
End to end automation development and testing
Building and maintaining stored procedures and scripts in Snowflake Data Cloud
Building and maintaining complex SQL queries for ETL/ELT processing
Develop Dashboard for data processed for audit and governance needs
Follow Agile methodologies to deliver iterative feature sets rapidly
Maintain codebase using source control methodologies, Git
Collaborate with business analysts, data analysts, and other developers to deliver powerful new features
Coordinate with teams as part of a larger data-sharing system
Contribute to a collaborative, positive, and enjoyable environment for your operations team
Performing quality assurance and testing
Writing user and technical documentation such as runbooks

Qualifications
Strong communication skills both verbal and written
Strong understanding of conditional programming logic
Knowledge of software engineering and development methodologies, techniques, and tools; includes Issue Tracking (JIRA) Software Development Lifecycle SDLC), and Continuous Integration / Continuous Development (CI/CD)
Experience working with large databases and data warehouses utilizing both relational and dimensional design concepts
Eagerness to learn new technologies, programming languages and platforms to solve complex data challenges
Experience with Python required
Experience with SQL required
Experience with Snowflake Data Cloud is highly desirable
Experience with Amazon Web Services (AWS) a plus
Experience with Airflow (or other orchestration) a plus
Experience in database marketing is a plus

Additional Information

Perks:
Great compensation package
Comprehensive healthcare plans
401(k) with employer match
Flexible time-off
16 weeks paid parental leave
Additional Information
The anticipated salary range for this position is $94,000-$146,000. Salary is based on a range of factors that include relevant experience, knowledge, skills, other job-related qualifications, and geography. A range of medical, dental, vision, 401(k) matching, paid time off, and/or other benefits also are available. For more information regarding dentsu benefits, please visit dentsubenefitsplus.com
Employees from diverse or underrepresented backgrounds encouraged to apply.
Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact recruiting@dentsuaegis.com if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.
About dentsu
Dentsu is the network designed for what’s next, helping clients predict and plan for disruptive future opportunities in the sustainable economy. Taking a people-centered approach to business transformation, dentsu combines Japanese innovation with a diverse, global perspective to drive client growth and to shape society www.dentsu.com.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.
Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact your recruiter if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.","$120,000 /yr (est.)",5001 to 10000 Employees,Company - Private,Media & Communication,Advertising & Public Relations,1971,$500 million to $1 billion (USD)
Kanini,#N/A,"North Druid Hills, GA",Senior Data Engineer,"About Kanini
Kanini provides Agile Software Development, Cloud Computing, Data Science, and Location Intelligence services to public and private organizations. We have successfully served our clients in government, finance, transportation, utility, and software industries since 2003.
Why you should join
Working at Kanini is flexible and personal. We are a highly motivated, collaborative team experimenting with the latest technologies. We are committed to everyone having a healthy work/life balance, and we provide extensive mentorship and training resources to help you succeed.
Kanini is looking for Data Engineer who has a deep experience in Data Engineering, Agile planning, Snowflake, SQL, MySQL, PostgreSQL, Git, AWS Data Tools, Python, Jira
Job Description
The Senior Data Engineer is responsible for processing structured and unstructured data, validating data quality, and developing and supporting data products. The Senior Data Engineer also plays a role in Agile planning, providing advice and guidance, and monitoring emerging technologies.
Responsibilities
Gathers and processes raw, structured, semi-structured, and unstructured data using batch and real-time data processing frameworks.
Understands and enforces appropriate data master management techniques.
Ensures data quality and implements tools and frameworks for automating the identification of data quality issues.
Work with internal and external data providers on data validation providing feedback and making customized changes to data feeds and data mappings.
Understands the challenges that the analytics organization faces in their day-to-day work and partner with them to design viable data solutions.
Recommends improvements to processes, technology, and interfaces that improve the effectiveness of the team and reduce technical debt.
Implements and optimizes data solutions in enterprise data warehouses and big data repositories.
Installs, maintains, monitors, and supports business intelligence, distributed computation, and big data analytics tools.
Provides ongoing support, monitoring, and maintenance of deployed products.
Actively works with less experienced data engineers providing technical guidance and oversight.
Actively participates in the engineering community, staying up to date on new data technologies and best practices and shares insights with others in the organization.
Qualifications:
Bachelor’s degree in computer science or related field and 4 years work experience
Working experience with batch and real-time data processing frameworks
Working experience with data modelling, data access, schemas, and data storage techniques within Snowflake
Working experience in design, development, and implementation of highly scalable, high-volume software systems and components, client-facing web applications, and major Internet-oriented applications and systems
Working experience working with relational databases such as SQL, MySQL, Postgres/PostgreSQL
Working experience with business intelligence tools and platforms
Working experience with data quality tools
Working experience with application lifecycle methodologies (e.g. waterfall, agile, iterative)
Experience with ETL processes and tools
Experience working with Git.
Desirable Qualifications
Scrum Developer Certification or equivalent
Working experience with Sisense platform
Working experience with SQL Server Integration Services
Working experience with AWS data tools (Database Migration Service)
Working experience with Python
Experience working with Jira, Rally or similar tools
Kanini Software Solutions, Inc. does not discriminate in employment matters based on race, gender, religion, age, national origin, citizenship, veteran status, family status, disability status, or any other protected class. We support workplace diversity. If you have a disability, please let us know if there is anything we can do to improve the interview process for you; we’re happy to accommodate.
Kanini Software Solutions, Inc., 25 Century Blvd., Ste. 602, Nashville, TN 37214
Job Types: Full-time, Contract
Pay: $60.00 - $65.00 per hour
Application Question(s):
Current Location?
Work Location: Remote",$62.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Titan America
3.8",3.8,"Roanoke, VA",Data Engineer,"Data Engineer
We are seeking a motivated and detail-oriented Data Engineer to join our team based in Roanoke, Virginia. As a Data Engineer, you will play a crucial role in designing, developing and maintaining scalable data pipelines and transformations for business-critical use cases. In addition, you may take on various Software Development and Data Science responsibilities as we frequently share responsibilities and knowledge across our growing team.
Responsibilities:
Design, develop and maintain scalable data pipelines for extracting, transforming, and loading data from various sources
Collaborate with cross-functional teams to identify data needs and determine the best data solutions
Develop and implement data models to support business requirements and ensure data quality
Ensure the security and privacy of sensitive data by implementing appropriate access controls
Monitor and optimize data pipeline performance to ensure timely and accurate data delivery
Document data pipeline processes, data dictionaries, and data storage solutions
Requirements:
Bachelor's degree in Computer Science, Computer Engineering, or a related technical field
Minimum of five years of professional experience working as a Data Engineer or Software Developer
Strong hands-on experience with data warehouse and transformation solutions, i.e. Domo, Snowflake or similar
Proficient in at least one scripting language such as Python, JavaScript, or R
Understanding of data modeling, data integration and data quality processes
Familiarity with cloud platforms such as AWS, Azure, or Google Cloud Platform
Strong analytical and problem solving skills
Full Stack Software Development experience in a professional setting is highly desired, but not required
This is an excellent opportunity for a driven and collaborative individual to make a significant impact in a dynamic and growing team. If you have a passion for data and a desire to work in a fast-paced and dynamic environment, we want to hear from you!
Success begins with hiring the right people to partner with us as we grow and develop our businesses. People are central to everything we do. It is through their efforts and talents that Titan has been successful for over 100 years.
Titan America is an Equal Employment Opportunity (EEO) / Affirmative Action employer and welcomes all qualified applicants. Applicants will receive fair and impartial consideration without regard to race, sex, color, religion, national origin, age, disability, veteran status, genetic data, or other legally protected status.
Job Type: Full-time
Pay: $75,000.00 - $85,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Referral program
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Roanoke, VA 24019: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Do you currently or will you in the future need sponsorship to work in the United States?
This position requires the successful candidate to be in the office in Roanoke, VA at least 3 days a week. Can you meet this requirement?
This position requires the successful candidates to be a full time, W2 employee. Can you meet this requirement?
Education:
Bachelor's (Preferred)
Experience:
Data Engineering: 5 years (Preferred)
Work Location: Hybrid remote in Roanoke, VA 24019","$80,000 /yr (est.)",1001 to 5000 Employees,Company - Public,"Energy, Mining & Utilities",Mining & Metals,1902,Unknown / Non-Applicable
"Appsintegration INC
4.5",4.5,"California, KY",Data Engineer,"Role:- Data Engineer
Position:- Onsite (W2 position)
Experience:- 10+ Years
Responsibilities
Working closely with business teams to understand their requirements and develop reporting or analytic solutions.
Producing and automating the delivery of key metrics and KPIs to the business.
In some cases, this will focus on making data available and others will be developing full reports for end users.
Managing stakeholder demand and a backlog of reporting requirements and needs.
Actively working with users to understand data issues, tracing back data lineage, and helping the business put appropriate data cleansing and quality processes in place.
Managing secure access to data sets for the business.
Requirements
Bachelor's degree in Computer Science or equivalent.
Proficient in creating Data Factory pipelines for on-cloud ETL processing.
Experience implementing data pipelines using the latest technologies.
Experience developing using SQL, relational databases, and data warehousing technologies.
Extensive ETL experience.
Ability to scale systems in Cloud as per requirement.
Experience in handling structured and unstructured datasets.
Good to have experience in data modeling and Advanced SQL techniques.
Must-Have Skills: Azure Data Factory (ADF), SQL, Cloud-based ETL.
Good to Have Skills: Azure File Storage Services, Databricks, Azure Synapse.
you can reach me out on LinkedIn
here is my link :- linkedin.com/in/shravanthi-thakur-705a81231
Job Type: Contract
Salary: $42.00 - $45.00 per hour
Ability to commute/relocate:
California, KY 41007: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",$43.50 /hr (est.),1001 to 5000 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Atika Technologies,#N/A,"Pittsburgh, PA",Data Engineer,"Role: Data Engineer/PySpark Developer, Pittsburgh, PA - Day One Onsite
Fulltime and Contract(W2)
LinkedIn Must
USC/GC/EADs
Three round of interview: First two on Zoom Video Call, End Client Round will be Onsite on the Client's Location (Pittsburgh, PA)
Job Description:
Python(PySpark), Jupiter hub, airflow, Java
Database knowledge - Oracle / SQL / Data Analytics
Regulatory reporting knowledge preferred
Good communication skills
Open / Ability to learn new technologies
Very good Analytical Skills
Experience in ETL is preferable
Job Types: Full-time, Contract
Salary: $120,000.00 - $130,000.00 per year
Benefits:
Dental insurance
Health insurance
Life insurance
Compensation package:
Bonus pay
Performance bonus
Yearly pay
Experience level:
10 years
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Pittsburgh, PA 15259: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Required)
Spark: 4 years (Required)
Regulatory reporting: 3 years (Required)
SQL: 4 years (Required)
IT: 10 years (Required)
Airflow: 4 years (Required)
Jupiter: 4 years (Required)
Work Location: One location","$125,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"shaped.ai Inc.
4.0",4.0,"New York, NY",Data Engineer,"Hello! We’re building the future of content discovery infrastructure with AI — come join us!

Shaped (YC W22) is an API for developers to seamlessly add personalized ranking and recommendation to their products. These frictionless discovery experiences help end-users find what they want faster, and, consequently, grow conversion and engagement business metrics. Behind the scenes, we’re building state-of-the-art ML infrastructure for training and deploying large AI models to power our personalization engine.
We’re a seed stage start-up backed by top-tier investors (e.g. Y-Combinator, Susa Ventures, Tribe Capital etc…) and executives (e.g. Google, Amazon, Uber, Dropbox etc…). Our team comes from Meta, Google, Apple and Uber. We’re a remote team but have a small office in Brooklyn, New York.
We’ve been lucky to have a huge amount of success since our initial launch earlier this year and now looking to expand our real-time data engineering capabilities. We are looking for a data engineer to design, build and optimize Shaped's real-time streaming data ingestion endpoints. You will be a founding engineer working on state-of-the-art infrastructure. As one of Shaped’s early employees you will help shape our product, culture and vision.
This role is for a full-time data engineer at $120k - $200k a year and 0.1 - 0.5% equity.

Responsibilities:
Manage the unification of our batch and real-time data processing pipelines.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Requirements:
Bachelor's in computer science, data science or mathematics related field. Master's degree or PhD will be advantageous.
5+ years of data engineering experience, ideally applied to machine learning use-cases.
Experience with AWS or common cloud technologies
Proficient with Python, SQL.
Extensive knowledge of both offline and online large scale data processing frameworks: Spark, Beam, Flink.
Extensive knowledge with stream technologies like Kakfa or Kinesis.
Excellent written and verbal communication skills.
We’re excited to work with you. Come build the future of AI and discovery with us!","$160,000 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Tipico - North America
4.0",4.0,"Denver, CO",Data Engineer,"Company Description

Tipico is an energetic, passionate, bold tech company, and we love sports! Tipico has recently expanded to the US market, establishing offices in Hoboken, New Jersey and Denver, Colorado. We are driven by our core values: passion, progress and trust. Our goal is to build the best possible mobile sports betting product in the industry, in order to amplify the emotions of millions of sports fans every day. Let’s Make This Interesting ™
Please note: this role is located in our Denver, CO office; we work off a hybrid model and come into the office 2 days per week.

Job Description

The Data Engineer will report directly to the Data Engineering Manager and work together with rest of the data engineers, BI analysts, and product teams. The position holder ensures this is done by doing the following:
Create and maintain both batch (ETL/ELT) and real-time data pipelines and architecture
Ensure all data provided by Data Platform is of the highest quality and is delivered in a timely manner and in line with agreed SLAs
Assemble large and complex data sets that meet functional and non-functional business requirements
Identify, design, and implement internal process improvements, including automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies
Collaborate with stakeholders including data architects, Executive, Product, Data and IT team members from the beginning to the delivery of a project
Build a scalable, modern, and trusted data ecosystem, making sure it is responsive, available and in line with business requirements
Take part in sprints, workloads, and deliverables through agile methodologies
Manage quality assurance, verification of accuracy and consistency of data, specifically KPI measures sent to the business
Thrive working in a fast-paced and challenging environment
Open to continuous learning individually and with the team
Comfortable with being challenged / seeking challenge
Willing to work flexible hours if needed

Qualifications
3+ years in Data Platforms / Datawarehouse / BI experience
Must have experience in successfully implementing data-centric applications, such as data warehouses, operational data stores, and data integration projects
Strong analytical and problem-solving skills that balance with creative approaches
Ability to learn new and complex concepts quickly
Strong experience with large-scale production databases and SQL
Solid understanding of cloud data services: AWS services such as S3, Athena, EC2, RedShift, EMR, EKS, RDS and Lambda
Proficient with one or more big data & new data technologies such as DBT, NIFI, Airflow, Kafka
Experience with data modelling
Experience with ETL development (extractions, data load, aggregation, transformation)
Knowledge in working with time-series/analytics databases such as Elasticsearch
Understanding of containerization and orchestration technologies like Docker/Kubernetes
Must be able to take ownership of projects with a high level of attention to detail and priority
Team player with a hands-on approach and attitude
Must be comfortable with on call rotation
Nice to have:
Familiarity with Marketing tooling and/or CRM systems is preferred
Familiarity with using business intelligence tools such as Domo, Tableau or Looker
Knowledge about and / or experience with sports betting
Background in digital companies, online, gambling, gaming, e-commerce, marketing, or mobile apps
Familiarity with one scripting language, preferably Python

Additional Information

What's in it for you:
Work in an environment where you, your work and ideas matter and have an impact.
You will be a be part of the newest and up and coming USA online iGaming and sports betting companies in the market.
Work in a new and young business with high growth potential.
Build your own success story together with us.
Work with self-organized, self-responsible and entrepreneurial employees.
Start-up feeling, backed-up by a leading European sports betting house. We are a high-volume business and are taking off in the US!
Competitive salary, Medical, Vision, Dental Benefits, Unlimited PTO, and more
Salary for this position is $120k+ & annual performance bonus","$120,000 /yr (est.)",201 to 500 Employees,Company - Private,"Arts, Entertainment & Recreation",Gambling,2004,Unknown / Non-Applicable
"Barracuda Networks Inc.
3.6",3.6,"Alpharetta, GA",Data Engineer,"Job ID: 23-579

Come Join Our Passionate Team! At Barracuda, we make the world a safer place. We believe every business deserves access to cloud-enabled, enterprise-grade security solutions that are easy to buy, deploy, and use. We protect email, networks, data and applications with innovative solutions that grow and adapt with our customers’ journey. More than 200,000 organizations worldwide trust Barracuda to protect them — in ways they may not even know they are at risk — so they can focus on taking their business to the next level.
We know a diverse workforce adds to our collective value and strength as an organization. Barracuda Networks is proud to be an Equal Opportunity Employer, committed to equal employment opportunity and equitable compensation regardless of race, gender, religion, sex, sexual orientation, national origin, or disability.
Envision yourself at Barracuda
We currently have an exciting new position open for a Data Engineer. This person will assist in driving technology innovation for engineering projects and work on architecture of AWS, Elastic and Kubernetes. They will also work closely with the Technology Architect to create and drive the Architecture Review Board.
What you’ll be working on
Determine internal / external customer data needs and identify system specifications
Analyze the needs of large systems and breaking them down into smaller manageable parts
Responsible for designing the journey of data from various sources to the database and everything in between
Plan and design the structure of data, brainstorm with team
Analyze best technology products and systems for internal teams
Help provide build vs buy decisions on data architecture
Communicate data requirements to engineers; explain data architecture to them and provide assistance throughout SDLC
Choose / maintain suitable software, hardware and integration methods
Help resolve technical problems as and when they arise
Review testing procedures to ensure data performant and error free
Ensure that database satisfy quality standards and procedures. Setup such standards.
Work with senior IT personnel to devise plans for future IT requirements of the organization
Produce progress reports on key infrastructures - Elastic and Data Pipelines
What you bring to the role
Bachelor Degree or Post Graduate in Computer Science, or relevant work experience
5+ years of experience in an Engineering / Operations heavy organization
1+ years of experience as a Data Analysis and Data Design responsible for data architecture of a SaaS application
Experience in understanding and designing data sharding in a distributed database
Some Hands-on experience in software development and system administration
Knowledge of strategic IT solutions
Familiarity with programming languages like JavaScript or Java or Python
Familiarity with Linux OS and Windows alike
Experience in cloud computing and cloud Technologies (AWS, GCP, Azure)
Excellent communication skills – you should be adept at listening to, understanding, and explaining key concepts to managerial and technical resources.
Good Anticipation and problem-solving skills
Ability to prioritize and manage time – responsible for multiple streams of work at the same time
Relationship building - Ability to form a good rapport with internal and external stakeholders
In-depth knowledge about quality standards and security best practices

Preferred Skills:
Expert level Understanding of Elastic eco system - Elasticsearch, Logstash, Kibana, Filebeats
Understanding of secure tunnel communication using VPN
Experience designing data at scale
AWS experience is preferred in Cloud Technologies
What you’ll get from us
A team where you can voice your opinion, make an impact, and where you and your experience are valued. Internal mobility – there are opportunities for cross training and the ability to attain your next career step within Barracuda.
Equity, in the form of non-qualified options
High-quality health benefits
Retirement Plan with employer match
Career-growth opportunities
Flexible Time Off and Paid Time Off benefits
Volunteer opportunities
#LI-Remote","$98,161 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,$100 to $500 million (USD)
"Take Command Health
4.9",4.9,"Dallas, TX",Data Engineer,"Company: Take Command
Position: Data Engineer
Salary: $120,000-$130,000 plus bonus
Location: Dallas or Austin, TX
About Take Command
Take Command is a start-up on a mission to improve the healthcare system, starting with health insurance. Pragmatically speaking, we help employers reimburse employees for individual insurance instead of offering a traditional one-size-fits-all group plan. We believe this model can empower employees (when they have the right support) to be savvy healthcare consumers and have a transformative impact on the entire healthcare system.
Let’s be honest—health insurance is usually a confusing, frustrating, and even emotional experience for people. We want to fix that with a new model, great technology, and a superior user experience. We have made a great start, but we need your help to fully realize our long-term vision.
As a part of the dev team, you'll build the framework for both our clients and our team to succeed. You will be a pillar in our company as we grow and make our product the best product on the market!
In this role, you will:
Take ownership for delivering multiple features, their quality and assist team members in delivering their features
Recommend improvements to data quality, processes and code to make it easier to use, manage and maintain
Experience designing and developing data models and data migrations
Experience managing and maintaining ETL or ELT processes (Hevo, BigQuery, Looker, etc.)
Hands-on development experience with SQL, relational database design (ERDs)
Must be able to write SQL & contribute code
Be product-minded; focus not only on what needs to be built but why something needs to be built
You may be a great fit for this role if you have:
3 or more years of professional experience as a data analyst or data engineer
Cloud-Based Data Tools e.g. Hevo, BigQuery, Looker, etc.
Database technologies: SQL, RDBMS / Relational databases (Postgres, Amazon RDS / Aurora, etc.)
Nice to have experience: Python, Java, Typescript or another modern OOP language
Cloud infrastructure: AWS
Source control: Git
Working at Take Command
We’re excited to build a team and culture that reflects our values! We offer competitive pay, health benefits, and equity options to share with this position. We want you to be an owner along with us in revolutionizing healthcare.
A generously funded ICHRA for medical, dental, and vision premiums and medical expenses. You get to use our own product and we think that’s so exciting and rare!
Unlimited personal vacation in addition to regular company holidays.
401(k): 90-day eligibility for 2% match that vests immediately!
Access to LinkedIn Learning.
We have two beautiful offices conveniently located near The Hill (Dallas) and Holly Commons (Austin). The kitchen is well-stocked and we've designed the space to have lots of different areas to work!
Paid parental leave for new parents.
Flexible on where you work – we hope to see you around the office on average 3 days a week especially when white-boarding or building relationships with your colleagues, but you also have the ability to work from home or wherever you'd like when you need to get focus work done.
More About Us
We secured our Series A funding in 2021 and are thrilled to be able to expand our team. Despite being a small startup in a land of health insurance giants, we’re the recognized industry leader for what we do (health insurance reimbursements) and passionate about bringing it to market because we know we can help fix a broken system and improve our clients’ wellbeing and health outcomes.
We’ve been featured in The New York Times, The Wall Street Journal, The Dallas Morning News, and other national healthcare publications and are excited about our growth opportunities.
Take Command knows diversity and inclusion among our teammates is integral to our company’s success and growth. Our vision is to recruit, develop, and retain the best team from a diverse candidate pool.
This has mostly been about us, but we’d love to hear from you-we can’t wait to hear your story!
Take Command is an equal opportunity employer! We celebrate diversity and are committed to creating an inclusive environment for all employees.
Xrygn4NXMr","$125,000 /yr (est.)",1 to 50 Employees,Company - Private,Insurance,Insurance Agencies & Brokerages,2014,Unknown / Non-Applicable
WellTrust Medical Group,#N/A,"Charlotte, NC",Data Engineer,"The Data Engineer will assist the senior analyst in complex modeling and projects to ensure their completion within the allotted timeline. Helps track measures of success and develop reports that measure operational and customer metrics. Demonstrates ability to troubleshoot data consumption and data quality problems.

Responsibilities:
Build and maintain reports using My SQL Server
Identify optimal data sources, structure data validation, and cache optimization.
Performs business problem modeling and quantitative analytics including regression analysis, time-series, hierarchical cluster analysis, and data mining.
Helps track measures of success and develop reports that measure operational and customer metrics.
Demonstrates ability to troubleshoot data consumption and data quality problems.
Evaluates and recommend improvements to current models, metrics, and analytic processes to improve operational efficiency and insight.
Design, document, and communicate effective technical solutions meeting end-user requirements
Perform analyses with large amounts of complex data, and uses data visualization tools to communicate results.
Requirements:

Bachelor's degree or equivalent experience in a quantitative field (Mathematics, Statistics, Computer Science, Economics, Finance, Physics, Engineering)
3+ years of professional working experience as a Data Engineer or Analyst
1-3 years of data warehouse experience
Build and maintain reports using My SQL Server
Strong analytical and creative problem-solving skills
Design, document, and communicate effective technical solutions meeting end-user
SQL and Excel Macro development
Strong understanding of various data environments such as SQL, Oracle, Big Query, etc.
Experience with data visualization tools such as Looker Studio, Tableau, D3.js, etc.
Proficient in R, Python, MATLAB, Java, or another programming language.",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
DocsInk LLC,#N/A,"Wrightsville Beach, NC",Data Engineer,"DocsInk is on a mission to provide a collaborative and exciting alternative to the traditional healthcare model. Through the complete virtualization of healthcare organizations, we are equipping providers with the ability to deliver better patient care with speed, efficiency, and accuracy. With the widespread focus on providing value-based care, DocsInk is making it possible to provide superior care while also maximizing reimbursements. In today’s environment, shifting the way we approach healthcare is essential and DocsInk is on the cutting edge of where the industry is headed. We are excited to be growing our team and are looking for driven individuals with a passion for making a difference in the healthcare industry.

As our Data Engineer, you will be in the driver’s seat as our team creates elegant and forward-thinking software solutions for our healthcare customers. You will work with fellow developers, designers, training & support, quality assurance, and management in a fast-paced environment to take our products to the next level and exceed expectations. We are looking for someone who is incredibly talented to do what most cannot and yet humble enough to easily work with peers, accept guidance from more experienced staff, and mentor more junior staff. If designing and developing complex integration efforts is what gets you out of bed each morning, then we want you to help us reshape healthcare.

People are our most important asset and the number one reason we all love working at DocsInk. As a team, we value accountability, transparency and collaboration. We have agile teams, with clear, outcomes-focused goals. Delivering innovative solutions and superior service are at the forefront of what we do, and the collective ideas of the team allow us to continue to excel in a fun and energetic environment.

Professional Requirements and Responsibilities:

Role / Responsibilities:

Implement robust ETL solutions that integrate healthcare data feeds using tools such as Apache NiFi, NextGen Connect (JavaScript), PostgreSQL, MySQL, and Linux shell scripting
Configuring and managing deployments to AWS infrastructure consisting of both traditional servers and serverless components. CI/CD tools include Bitbucket and AWS Code Deploy
Building direct interfaces with hospitals and EMR vendors utilizing industry standards such as HL7 2.x, C-CDA, and FHIR as well as implementing interfaces that consume formats such as XML, JSON, and delimited files
Utilizing APIs and web services to permit bi-directional flows of data to and from our partner organizations
Monitoring operational ETL processes, triaging and resolving issues as they arise to meet SLAs
Writing PL/SQL functions and packages to automate workflows in a Postgres environment linked with web technologies
Maintaining data standards (including adherence to HIPAA), documenting work, and championing process improvements
Work as a project team member to jointly deliver high-quality business solutions consistent with project objectives and constraints

Knowledge and Skills Required:

4+ years experience with HL7 2.x
2+ years experience with Apache Nifi
2+ years experience with NextGen Connect
Advanced knowledge of PostgreSQL, MySQL, AWS Code Deploy, HL7 2.x, C-CDA, FHIR, and HIPAA data compliance

Qualifications and Other Attributes Required:

Degree in Computer Science or equivalent field, or equivalent work experience
Highly self-motivated team player who takes initiative
Strong sense of personal accountability; contributes beyond job role and responsibilities
Excellent problem-solving skills; meticulous & methodical","$85,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Cureatr
4.1",4.1,Remote,Data Engineer,"About Us
We are a thriving, medication management solutions company, headquartered in Manhattan, but largely virtually distributed, committed to tackling the $528B suboptimal medication management problem. Patient medication information is often incomplete for healthcare providers and systems – it’s our mission to fill in the gaps. We provide real-time, universal access to accurate medication data for over 265 million patients and high-quality interventions by board certified telepharmacists. Through our solutions, healthcare providers can reduce preventable hospitalizations, avoid readmissions, and ultimately, improve patient outcomes. We’re already used by dozens of hospital systems, with tens of thousands of clinicians and supporting staff using us to care for millions of patients.
Here at Cureatr, we are driven by a core set of values and strive to incorporate them into everything we do. We care deeply about our users, patients, and employees, and we aim to live and operate compassionately. We take pride in creating a diverse, inclusive, and equitable company culture, where our open and honest approach to work fosters innovation and encourages employees to achieve professional and personal growth.

The Role
We’re looking for a Data Engineer to join our growing Engineering team. Use the power of healthcare data to find insights that will demonstrate the value Cureatr is delivering to our customers, help improve patient outcomes, and expand the scope of Cureatr’s mission. As a Data Engineer at Cureatr, you will be responsible for designing, implementing and maintaining our data infrastructure, pipelines, automation, and ETL processes. You will work closely with our Data Science and Analytics teams to ensure that our data is clean, accurate, and readily available for analysis. You will also be responsible for ensuring that our data meets industry standards for security, privacy, and compliance.
This is a remote, work from home position, but candidates must be located within the U.S. Ideal candidates must be able to travel to our New York City office, or TBD company-wide offsite locations, up to 4x per year.

You’ll be part of a team working to solve problems like:
Increasing the number of patients who receive the opportunity to do a medication reconciliation, which can help keep them out of the hospital.
Streamlining and automating processes so that our pharmacists can serve as many patients as possible.
Predicting patient events likely to occur so that our clinical team can adequately prevent them before it’s too late.
Helping our pharmacists make the most of data and insights that will help them be as effective as possible in serving patients.
Evaluating the effectiveness of our clinical services impact at the population level.

What You’ll Do:
Design, implement, and maintain our data infrastructure, pipelines, automation, and ETL processes to support our data needs.
Build and maintain scalable data models and databases.
Collaborate with Data Science and Analytics teams to understand their data needs and provide data solutions accordingly.
Ensure data quality and integrity by implementing data validation and cleansing processes.
Ensure data security and compliance with healthcare regulations.
Monitor and optimize data performance and scalability.
Design and implement data governance policies and procedures.
Evaluate and implement new data technologies and tools to improve our data capabilities.

Your Skills:
3+ years of experience as a Data Engineer or similar role.
Strong programming skills in languages such as Python and SQL.
Experience using git to track changes.
Experience with data modeling and database design.
Experience with the “modern data stack” preferred
Experience with ETL and data pipeline development.
Knowledge of healthcare data and related standards (e.g., HL7, FHIR) is a plus.
Familiarity with AWS.
Strong problem-solving and analytical skills.
Excellent communication and collaboration skills.

Technology Stack:
Python
RedShift
dbt
Dagster
MongoDB
Looker
AWS

Educational Requirement: Bachelor’s Degree or relevant work experience

Salary: $110,000 - $140,000 plus full benefits

Why work with us?
Make a measurable impact on patients’ lives
Be part of a company dedicated to living its core values:
Empathy
Honesty
Integrity
Impact
Fun
Equity in Healthcare
Work in the company without micromanagement
Share your vision and make an impact on the product
Opportunities for career growth
Remote work
Competitive salary
Premium benefits (Health, Dental, Vision, 401k, unlimited PTO, more)

Equal Employment Opportunity Policy
Cureatr is an equal opportunity/affirmative action employer. All qualified applicants will receive consideration for employment without regard to sex, pregnancy, race, religion or religious creed, color, gender, gender identity, gender expression, national origin, ancestry, physical or mental disability, medical condition, genetic information, marital status, registered domestic partner status, age, sexual orientation, military or veteran status, protected veteran status, or any other basis protected by federal, state, local law, ordinance, or regulation and will not be discriminated against on these bases
Disclaimer: we are aware of a scam targeting applicants using a cureatrcareers.com email address. Please block and report this sender, as they are not affiliated with Cureatr. If you have any questions, please contact us at jobs@cureatr.com. Thank you!
uBajh3OfeT","$125,000 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2012,Less than $1 million (USD)
"Apolis
3.9",3.9,"Saint Louis, MO",Data Engineer (Python Focused),"Role: Data Engineer (Python Focused) (W2 ONLY)
Location : St. Louis, MO 63102
Duration: 18 Months
The primary responsibilities of this role will be to work as a part of the data and analytics team of the client. The ideal candidate would have experience using ETL and other data engineering tools in a cloud environment. Additionally, this candidate would have coding and scripting experience creating code to move and manipulate data using tools such in Python such as Spark or Pandas. Microsoft Azure experience is a plus but relevant experience with other cloud-based environments will be considered.
This team uses Microsoft Azure to build and productionize data sets and processes to the product owners and product users and this role will be expected to contribute to those objectives. As a member of this team, you will work in conjunction with other data engineers, platform engineers, solution developers and others to execute and deliver working solutions to the end users.
Technical Skills
High focus on Python
More Programmer/Coder experience, less Cloud experience needed.
Must have experience with data analytics/engineering and cloud technologies and an in-depth working knowledge and understanding of data engineering, design, and integration.
Intermediate or higher level of proficiency with Python Must have the proven ability to learn quickly and apply cloud data engineering principles and skills to new tools and use cases.
Ability to define and take technical ownership of detailed requirements and collaborate effectively with various team members such as Business Analysts and Project Managers and end users.
Excellent communication (verbal and written) and problem-solving skills are essential for this position.
Initiative, Proactive Cooperation, and curiosity are mandatory behavioral skills required for this position.
Experience with several of the following technologies is preferred:
Experience with Microsoft Azure Platform. In particular, the data processing toolsets including Azure Data Factory Pipelines, Azure Databricks, Azure Data Lake Storage.
Experience using Notebooks (Such as Jupyter, DataBricks) also desired.
Experience using code languages such as Python and C#
Experience using query languages such as SQL, MDX and DAX
Experience using scripting languages such as PowerShell, M-Query (Power Query), and Windows batch commands.
Understanding of Agile and DevOps principles for software development and ownership.
Familiar with compliance and controls for software development and deployment.
Education Required
A bachelor's in computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field [or equivalent work experience] is not required but is preferred.
Job Type: Contract
Salary: $60.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Vision insurance
Compensation package:
Overtime pay
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
St. Louis, MO: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Preferred)
ETL: 5 years (Preferred)
Cloud infrastructure: 4 years (Preferred)
Work Location: One location",$65.00 /hr (est.),501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,1996,$25 to $100 million (USD)
Fracsys Inc,#N/A,"Washington, DC",Data Engineer,"Fracsys Inc is hiring a Data Engineer position. The ideal candidate must have at least 12+ years of industry experience. He or she must be responsible for successful technical delivery of Data Warehousing and Business Intelligence projects using a formal project management methodology.
Responsibilities
Directs, motivates and develops key technical personnel
Provides technical direction across the full lifecycle system development process
Provides strategic planning and comprehensive senior level technical consulting to IT senior management and senior technical staff
Working independently within guidelines
Provides detailed inputs for plans and schedules, including goals, risks, and resource allocation
Conduct diagnostic, troubleshooting steps and present conclusions to a varied audience
comfortable slinging some code to solve a problem
Build Data Marts and Data warehouses to facilitate analytical capabilities across the varied sources of data
Design and implement data patterns on cloud using cloud Data warehouse and data virtualization tools
Evaluate new tools and technologies for target state on cloud
Learn and implement new ETL, BI tools and data warehousing technologies
Qualifications
Minimum twelve (12) years in a Technical Lead or Consulting role leading Information Management related initiatives (system integration, data warehouse build, data mart build, or similar)
MUST have proven leadership experience for team of at least 3 technical resources.
Experience creating Data Marts, Data warehouses on-premise and on cloud for real time and batch processing frameworks
Experience building and optimizing big-data data pipelines and data sets including Postgres, AWS Relational Data Service
extensive hands-on experience building and optimizing data structures for data analytics, data science and business intelligence
Experience building self-service data consumption patterns and knowledge of cloud-based data Lake platforms
Experience wrangling data (structured and unstructured), in-depth knowledge of database architecture
Experience utilizing or leading implementations leveraging ETL tools (Informatica / Talend), BI Reporting tools such as MicroStrategy, Microsoft Power BI, data modeling tools such as Erwin, Oracle, SQL server, NoSQL, JDBC, UNIX shell scripting, PERL and JAVA, XML/JSON files, SAS , Python, AWS cloud-native technologies, S3, Athena, Redshift
Familiarity with the following technologies: Hadoop, Kafka, Airflow, Hive, Presto, Athena, S3, Aurora, EMR, Spark
Ability to drive, contribute to, and communicate solutions to technical product challenges
Ability to roll-up your sleeves and work in process or resource gaps and fill it in yourself
Excellent written and oral communication skills in English
Education
Bachelor’s degree in computer science, Data Analytics, Information Systems, or a related degree or equivalent experience. Master’s degree is preferred.
Job Type: Full-time
Pay: From $96,336.72 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Experience level:
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Washington, DC: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person","$96,337 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Infosoft,#N/A,"Peachtree Corners, GA",Data Pipeline Engineer - 2173,"Here are the job position details for your review:
Job Title: Data Pipeline Engineer (GStreamer/ DeepStream)
Pay Rate: $90 to 120/Hr.
Duration: 3 Months
Location: Peachtree Corners, GA
This position is for the Building Automation division.
We are looking for Data Pipeline Engineer (GStreamer) to participate in the development of Streaming data pipelines using NVIDIA Deepstream, GStreamer, Python programming, messaging queues, and related tools.
Preference is for candidates located in Atlanta, GA metro area. However, ok with non-local candidates who are open to 75% travel to Atlanta, GA.
Understanding the basics of our architecture/setup to be able to do setup/support for demos, testing, etc.
Basic computer hardware assembly and set-up
Possible set-up/installation of the application(s)
Possible development tasks depending on what the rest of the development team has for them
Develops business software applications, system software, and/or IT solutions on the basis of functional specifications.
Scope of position embraces both the customization or reuse/modification of system software and/or solutions as well as their development from scratch.
Consults with customers in creating functional specifications and work estimates, and supports Make-or-Buy decisions.
Analyses performance gaps of existing software environments and proposes measures to address efficiency problems.
Is also responsible for the implementation of data-driven components supporting data modeling and engineering in the context of data analytics (Data Stewardship).
Skills:
Python
Linux
Docker and Docker Swarm
Zero MQ / a good understanding of messaging
Desired Skills:
NVIDIA Deepstream SDK
Web Sockets
Flask
Protobuf
Possible:
GStreamer (if different than RTSP / H264 / MJPEG)
Job Type: Contract
Salary: $90.00 - $120.00 per hour
Ability to commute/relocate:
Peachtree Corners, GA: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
""Please share your Email ID""
Work Location: One location",$105.00 /hr (est.),Unknown,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Fulcrum Analytics
4.3",4.3,Remote,Data Engineer,"At Fulcrum, we provide cutting-edge data science consulting and software solutions to help companies solve challenging business problems. Through the utilization of our innovative products and dedicated team, clients are able to operate more efficiently, manage risk, and make informed business decisions every day. Fulcrum Analytics has stood at the forefront of data, analytics, and innovative software for over 25 years, offering sophisticated solutions, groundbreaking technologies, and winning strategies that help companies achieve their targeted results.
Role Description:
We are seeking a highly-skilled data engineer with experience in data analytics and data science. The ideal candidate will be responsible for building and maintaining data pipelines, designing and implementing databases and data models, and developing data-driven solutions to drive business growth for our clients.
Responsibilities:
Design, build, and maintain efficient, scalable, and reliable data pipelines using SQL, or programming languages such as Python or Java.
Develop and maintain databases and data models to meet clients’ analytical needs.
Identify data quality issues and develop processes to ensure data accuracy, completeness, and consistency.
Work with cross-functional teams to define and implement data integration strategies for new and existing systems.
Develop and maintain automated testing and deployment of analytics code and data pipelines.
Collaborate with data scientists to develop and implement machine learning models, statistical models, and other advanced analytics techniques.
Communicate analytical results and recommendations to technical and non-technical stakeholders through presentations and visualizations.
Stay up to date with emerging technologies and industry best practices, and recommend tools and processes that can improve data engineering and analytics processes.
Qualifications:
Bachelor's or master's degree in computer science, statistics, data science, or a related field.
2+ years of experience in data engineering, data analytics, and data science.
Strong programming skills in Python, with experience in building data pipelines using frameworks such as Airflow, Glue, Lambda, Athena, and BigQuery as well as data platforms such as Hadoop, Snowflake, and GCP.
Experience working with SOAP or RESTful web services.
Familiarity with version control, in particular Git.
Experience in designing, building, and optimizing relational and NoSQL databases.
Familiarity with data visualization tools, such as Tableau or Power BI.
Ability to understand and transform complex data into actionable insights.
Strong analytical and problem-solving skills, with a focus on delivering practical solutions to business challenges.
Ability to learn and solve problems independently.
Excellent communication and collaboration skills, with the ability to explain complex concepts to both technical and non-technical audiences.
If you are passionate about data analytics and want to work in a dynamic environment with talented colleagues, we encourage you to apply for this role. We offer competitive salary and benefits packages, as well as opportunities for professional growth and development.


Position Type: Full-time salaried
Reports to: Senior Data Engineer
Base Compensation: $110,000 - $140,000
Benefits: Medical, Dental, Vision, 401k with Match, Bonus, Unlimited PTO
Location: Remote within the United States of America; Eastern time zone preferred","$125,000 /yr (est.)",1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,1993,$5 to $25 million (USD)
"Merkle
3.7",3.7,"Norristown, PA",Data Engineer,"Company Description

We are Dentsu International. We innovate the way brands are built. That means we do things differently so they’re better than before. In this way, we make our clients’ most important marketing assets—their brands—win in a changing world.
Dentsu International is a modern marketing solutions company. Our mission is to help clients navigate, progress, and thrive in a world of change. Businesses rely on our integrated network of agencies and specialized practices to champion meaningful progress through creative, media, commerce, data, and technology. Part of Dentsu Group, our global network comprises 66,000 diverse people in 143 countries, who are dedicated to teaming for growth and good. Follow us on Twitter @DentsuUSA and visit dentsu.com/us.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.

Job Description
Develop onboarding process to receive and process information related to onboarding a new subscriber
Convert Snowflake Stored Procedures to Python Scripts directly in Airflow
Develop metadata driven solutions for managing data processing and cadence
End to end automation development and testing
Building and maintaining stored procedures and scripts in Snowflake Data Cloud
Building and maintaining complex SQL queries for ETL/ELT processing
Develop Dashboard for data processed for audit and governance needs
Follow Agile methodologies to deliver iterative feature sets rapidly
Maintain codebase using source control methodologies, Git
Collaborate with business analysts, data analysts, and other developers to deliver powerful new features
Coordinate with teams as part of a larger data-sharing system
Contribute to a collaborative, positive, and enjoyable environment for your operations team
Performing quality assurance and testing
Writing user and technical documentation such as runbooks

Qualifications
Strong communication skills both verbal and written
Strong understanding of conditional programming logic
Knowledge of software engineering and development methodologies, techniques, and tools; includes Issue Tracking (JIRA) Software Development Lifecycle SDLC), and Continuous Integration / Continuous Development (CI/CD)
Experience working with large databases and data warehouses utilizing both relational and dimensional design concepts
Eagerness to learn new technologies, programming languages and platforms to solve complex data challenges
Experience with Python required
Experience with SQL required
Experience with Snowflake Data Cloud is highly desirable
Experience with Amazon Web Services (AWS) a plus
Experience with Airflow (or other orchestration) a plus
Experience in database marketing is a plus

Additional Information

Perks:
Great compensation package
Comprehensive healthcare plans
401(k) with employer match
Flexible time-off
16 weeks paid parental leave
Additional Information
The anticipated salary range for this position is $94,000-$146,000. Salary is based on a range of factors that include relevant experience, knowledge, skills, other job-related qualifications, and geography. A range of medical, dental, vision, 401(k) matching, paid time off, and/or other benefits also are available. For more information regarding dentsu benefits, please visit dentsubenefitsplus.com
Employees from diverse or underrepresented backgrounds encouraged to apply.
Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact recruiting@dentsuaegis.com if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.
About dentsu
Dentsu is the network designed for what’s next, helping clients predict and plan for disruptive future opportunities in the sustainable economy. Taking a people-centered approach to business transformation, dentsu combines Japanese innovation with a diverse, global perspective to drive client growth and to shape society www.dentsu.com.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.
Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact your recruiter if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.","$120,000 /yr (est.)",5001 to 10000 Employees,Company - Private,Media & Communication,Advertising & Public Relations,1971,$500 million to $1 billion (USD)
"Kastech Software Solutions Group
4.0",4.0,"Dallas, TX",Data Engineer,"In KastechSSG we are are seeking for a Cloud DB Platform Engineer with Data Enablement Services experience ($50/hour), to work in a Hybrid Schedule (3 days on-site, 2 days home office) for one of our partner companies!
Hybrid Schedule info: onsite 3 days per week from list of locations: Charlotte NC, Dallas TX, Chandler AZ, Newark, NJ - Must be local to one of these locations.
** Contract would be $50 per hour on W2 (18 months). American Citizens or GC Holders Only**
3-5 years experience in:
Terraform, Azure/GCP, DevOps Engineering, ETL, Client/AI, Data lake, Data architecture.
GCP Key Services Experience (ie Big Query, Composer, Vertex AI, DataProc, Data Catalog, Notebooks).
Azure Key Service Experience (ie Synapse, Purview, Data Lake, Logic Apps, Cognitive Services).
Job Type: Full-time
Salary: $50.00 per hour
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you American? If not, please specify your visa type
Would you be willing to work under W2?
Experience:
Terraform: 3 years (Preferred)
Azure Data Lake: 3 years (Preferred)
GCP Key Service Exp: 3 years (Preferred)
DevOps: 3 years (Preferred)
Work Location: In person",$50.00 /hr (est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2008,$25 to $100 million (USD)
Atalan Tech,#N/A,Remote,Data Engineer,"Atalan Tech is a start-up with a mission to mitigate the impact of clinician burnout and turnover in our nation’s health system. We are expanding our team and looking to hire the critical role of Data Engineer. The Data Engineer will work closely with the Chief Technology Officer, Chief Scientific Officer (UCLA Ph.D.), Chief Product Officer (Oxford Ph.D.), and Chief Executive Officer to deliver actionable insights from data in a variety of situations.
We are a small, close-knit team. This is an excellent opportunity for someone to join a mission-oriented, early-stage company at the ground floor and grow with us. There will be plenty of room for growth and creativity, and opportunities to tackle challenges that present one of the toughest problems in our society.
Requirements
Strong hands-on experience of designing, developing and managing data pipelines
Strong coding skills in Python (required) and SQL (required) and R (strongly preferred)
Experience building and maintaining data pipelines on AWS using EC2, EBS, Redshift, Postgres, S3, Data Pipeline, Kinesis, Lambda, etc.
Thorough understanding and experience with task management tools (Airflow, NiFi, etc.)
Knowledge of data management best practices for ETL, data governance, data warehousing, data modeling and data science
Basic knowledge of Linux (Redhat preferred) administration
Minimum B.S in Computer Science, software engineering or related field (or equivalent in related professional experience)
Strong communication skills
Strong problem-solving/critical thinking skills
Ability to work in a fast-paced, dynamic start-up environment
Some knowledge of healthcare, health tech, or healthcare data analytics (preferred)
Typical Tasks and Responsibilities include:
Working closely with CTO and CSO to build data pipelines for machine learning
Assisting the CPO in managing and supporting analysis of enterprise data to drive better products and solutions
Managing company’s data infrastructure on AWS (Postgres/RDS, Redshift, S3, etc.)
Synthesizing multiple data sources, such as internal medical records and HR data
Integrating with client systems including EHR (Epic, Cerner), HRIS, flat files, etc.
Working closely with other key members on other projects
Developing and designing data security standards and policies in line with industry best practices
About Us:
Atalan Tech’s mission is to improve the wellness of clinicians and their patients. We blend labor economics, machine learning, and clinical psychology with a team from Harvard, Yale, and UCLA. Atalan Tech is looking to contribute innovative and sustainable solutions to one of the most pressing problems in the healthcare industry. Building upon our proof-of-concept success, we have recently secured multiple engagements with nationwide healthcare organizations and have successfully raised institutional funding to expand our reach.
Contact Us:
If you’re interested in this position, please send us your CV/resume and cover letter.
Job Type: Full-time
Pay: $108,448.20 - $130,604.27 per year
Experience level:
2 years
Schedule:
Monday to Friday
Application Question(s):
Do you know the role is full time?
Can you start within the next month?
What are your salary expectations?
Have you held the title of a data engineer? If not, what experience do you have as a data engineer? Please be very specific.
Have you released Python code to a production environment in the last year?
Experience:
Data analytics: 2 years (Required)
Work Location: Remote","$119,526 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
National Group Corporation,#N/A,"Dallas, TX",Data Engineer,"Who are we?
ReAlign Insurance Holdings is a leading primary underwriter of program administrator managed specialty insurance programs in the U.S. With the extensive knowledge and experience our team has in building and managing carriers and programs we are able to reach decisions on program opportunities quicker than our competition.
Why choose ReAlign?
At ReAlign, we believe that we are only as strong as our people. We strive to create an inclusive and welcoming culture where employees feel comfortable and empowered to be themselves. We support a diverse and inclusive work environment, with a focus on a healthy work-life balance. We understand that comprehensive benefits are important which is why we offer a benefit package that includes: medical, dental, vision, company paid short and long-term disability, life insurance, health spending account, 401(k) match every pay period, PTO, paid holidays, floating holidays, tuition reimbursement, and referral bonuses.
Job Summary
The Data Engineer position will involve data exchange process analysis, design, development, unit testing, and support. This role would work with other data team members on database ETL processes and reporting needs in both the on-premise database environment as well as the Azure data environment. This position requires strong technical and analytical skills to review business processes and data, recommend solutions, conduct functional and code analysis, and lead and perform database development tasks. This position will also involve production support and require experience within a DevOps environment.
Essential Functions
· Design, development, and support data ETL processing for both external and internal customers
· Support the internal and Azure cloud-based data warehouse platforms
· Work with DBA/System Admin to develop and maintain automated data processing
· Design and analyze queries and stored procedures
· Write and support SSRS and PowerBI reports
· Work in a data development life cycle environment
Work with stakeholders and data analysts to understand business requirements
Work with end users and the testing team to perform unit, functional, and integration testing and validation prior to production implementation
Troubleshoot and resolve operational problems with a focus on delivery
· Communicate solutions and architectural design to IT leadership
· Work in both Kanban and Scrum Agile environments
· Continually improve processes and solutions, seeking ways to use new tools, features, and capabilities
Job Requirements
3-10 years of experience as a Database Developer
Highly proficient in writing advanced SQL queries, stored procedures, SSRS reports, and ETL processes
Strong knowledge of relational and transactional database architecture
Strong technical and analytical skills
Experience with the Microsoft Azure and PowerBI platforms and Agile development methodology a plus
Bachelor’s Degree in computer science, database development, or related field preferred
Detail oriented with the objective of delivering a solution to a problem in a timely and efficient manner
Self-driven, willing to take initiative when an opportunity presents itself
Strong collaboration and communication skills and the ability to work in a team environment a must
Ability and willingness to work in a partial on-prem and partial remote office environment
Experience in the Property and Casualty Insurance industry a plus
Proficient with standard desktop software used in a business environment (MS Word, Excel, Visio, etc.)
Must be able to work in a hybrid in-office and remote work environment
Must be authorized to work in the United States
Job Type: Full-time
Pay: $90,000.00 - $125,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Health savings account
Life insurance
Paid time off
Retirement plan
Vision insurance
Experience level:
3 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you currently local to the Dallas area and able to commute to the office 2 to 3 times per week? (Deal breaker)
Experience:
writing advanced SQL queries and ETL processes: 3 years (Required)
cloud database platform (i.e. Azure, AWS): 1 year (Preferred)
Azure Data Factory: 1 year (Preferred)
Work Location: In person","$107,500 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Quadrant Resource
4.1",4.1,Remote,Data Engineer,"Job Description:
4-6 years of experience as a Data Engineer in the Snowflake-Fivetran-dbt stack
• Possess a solid understanding of end to end Data ingestion, ETL, data analysis, Reporting processes
• Expert in ETL development using Fivetran and experienced in implementing data pipelines for Snowflake Cloud DWH
•Experienced in using dbt for data transformation
•Expertise on Tableau/Metabase for building reports/dashboards
•Expert in troubleshooting and resolving ETL issues, data load failures/problems and transformation translation problems
•Excellent communicator with the ability to work effectively with distributed teams
Job Type: Full-time
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Data Engineer: 9 years (Required)
Fivetran: 8 years (Required)
DBT: 7 years (Required)
Snowflake: 6 years (Required)
Work Location: Remote",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,2004,Unknown / Non-Applicable
"American Power & Gas
2.5",2.5,"Largo, FL",Data Engineer,"Because of expansive growth American Power & Gas is seeking a Data Engineer to add to our technical team. This is a fulltime permanent on-site role.
We have been offering Green Energy solutions to both residential and small commercial customers for over 20 years and have won the award for fastest growing company in the Tampa Bay Business Journal as well as being featured in Forbes and the Huffington Post.
**
**
Key Responsibilities
Support operational executives in solving business problems by designing, developing, troubleshooting, and implementing data driven solutions to complex technical objectives.
Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes.
Deploy sophisticated analytics programs, machine learning and statistical models to predict business outcomes and continually optimize performance through data science.
Gather and summarize technical requirements associated with strategic business initiatives.
Document, maintain, support and enhance company technology platforms and analytics applications.
Look for opportunities to streamline and automate various reporting processes to help support internal teams.
Assist with the research and development of internal business case studies to support onboarding new data tools.
Extract, manipulate and cleanse raw data from various data sources – call center, web, and CRM systems.
Work independently as well as collaboratively with other team members and key stakeholders as required to troubleshoot and resolve data issues.
Provide comprehensive technical and consultative services to support development and maintenance of internal and third-party platforms.
Create functional test cases/criteria to verify all functionality adheres to specifications and create end user manuals.
Routinely represent the Analytics department in cross-functional status & data strategy meetings.
Assist business analysts in provision of regular performance trends reporting, forecasts, and insights for marketing and sales team leaders as well as senior executive management to maintain a tight finger on the pulse of emerging performance trends and opportunities.
Partner with internal cross functional teams to identify business needs and analytics opportunities, developing tools and techniques to analyze and provide performance-improving recommendations.
Partner with the Operations team to optimize data workflows from sourcing to storage to reporting to deliverables to maximize for value-added and time-efficiency.
Develop, enhance, and manage various analytical solutions in support of business objectives.
Determine what data is needed and how to consume and store this data to support reporting needs and ad hoc performance-improving analysis for internal stakeholders.
Ensure deliverables are adapted properly to stakeholder audience; adapting terminology and visuals as needed to “speak the stakeholder’s language”, thus communicating with maximum effectiveness.
Troubleshoot and QA data, reporting, and tracking anomalies as needed, with proactive communication to stakeholders.
Relentlessly challenge the status quo. Always be critical of how we can be more effective or more efficient as an individual, as a team, and as a business.
Provide ongoing and proactive client service to your internal customers as required to continue elevating the performance of the business.
Regularly work with and analyze data across marketing channels and the customer journey through website analytics, call center activity, and CRM systems.
Requirements
University degree or college graduate in Engineering, Computer Science, Mathematics, Statistics, related technical/programming discipline, or the equivalent hands on experience in Data Engineering or Software Development.
Proficiency with coding in SQL is required. Ability to also write in Python, R, Java, or similar programming languages is preferred.
Strong technical prowess, including an understanding of algorithms, systems architecture and end user experience.
Experience with modern source, build, and deploy tools such as Git, Grub, Maven, Yeoman, etc. is a plus.
Ability to think unconventionally to derive innovative and creative solutions.
Competency in accurately estimating development timelines.
Experience with data warehouse design, relational databases, SQL/NoSQL data modeling, RESTful API standards and large scale data processing solutions.
Demonstrated skill in database development with solid understanding of schema design, stored procedure development, query optimization and ETL processes.
Excellent troubleshooting ability. Must be able to resolve issues tied to capturing and processing data in a timely manner.
Excellent English written and verbal communication skills, especially explaining technical concepts to non-technical business leaders.
Exceptional critical thinking and problem-solving skills; able to distill overall objectives into the actionable steps required to achieve those objectives.
Capable of effectively managing projects, priorities, timelines, and working relationships.
Must possess the intellectual curiosity to succeed in a dynamic, entrepreneurial, fast paced, sales driven organizational culture.
Excited by the opportunity to disrupt the status quo and uncover the eureka moment insights that will take the business to the next level – proactively searching for problems to solve, knowing there is so much to learn.
We offer Health, Dental, Optical and Life Insurance, PTO (paid time off) and the opportunity for promotions and room to advance.
For immediate consideration please send a resume to Carl Schumacher the Manager of Recruiting CarlS@goapg.com
Job Type: Full-time","$92,733 /yr (est.)",201 to 500 Employees,Company - Private,"Energy, Mining & Utilities",Energy & Utilities,2010,$100 to $500 million (USD)
"Contact Government Services, LLC
4.7",4.7,"Washington, DC",Data Engineer,"CGS is seeking a passionate and driven Data Engineer to support a rapidly growing Data Analytics and Business Intelligence platform focused on providing solutions that empower our Federal customers with the tools and capabilities needed to turn data into actionable insights. The ideal candidate is a critical thinker and perpetual learner; excited to gain exposure and build skillsets across a range of technologies while solving some of our clients’ toughest challenges. As a Data Engineer, you will be integral to data operations for the development and integration of multiple data types across a range of data sets and sources.

You will be responsible for the day-to-day operations of systems that depend on data, ensuring data is properly processed and securely transferred to its appropriate location, in a timely manner. Processing data will include managing, manipulating, storing and parsing data in a data pipeline for variety of target sources. You will also support maintenance of applications and tools that reside on these systems such as upgrades, patches, configuration changes, etc. The work is performed in a multidisciplinary team environment using agile methodologies. The candidate we seek must be highly motivated and enthusiastic about implementing new technologies and learning about new data in a small team environment where deadlines are important.
Responsibilities:
Complete development efforts across data pipeline to store, manage, store, and provision to data consumers
Being an active and collaborating member of an Agile/Scrum team and following all Agile/Scrum best practices
Write code to ensure the performance and reliability of data extraction and processing
Support continuous process automation for data ingest
Achieve technical excellence by advocating for and adhering to lean-agile engineering principles and practices such as API-first design, simple design, continuous integration, version control, and automated testing
Work with program management and engineers to implement and document complex and evolving requirements
Help cultivate an environment that promotes customer service excellence, innovation, collaboration, and teamwork
Collaborate with others as part of a cross-functional team that includes user experience researchers and designers, product managers, engineers, and other functional specialists
Required Skills:
Must be a US Citizen
Must be able to obtain a Public Trust Clearance
7+ years of IT experience including experience in design, management, and solutioning of large, complex data sets and models.
Experience with developing data pipelines from many sources from structured and unstructured data sets in a variety of formats
Proficiency developing ETL processes, and performing test and validation steps
Proficiency to manipulate data (Python, R, SQL, SAS)
Strong knowledge of big data analysis and storage tools and technologies
Strong understanding of the agile principles and ability to apply them
Strong understanding of the CI/CD pipelines and ability to apply them
Experience with relational database, such as, PostgreSQL
Work comfortably in version control systems, such as, Git Repositories
Contact Government Services strives to simplify and enhance government bureaucracy through the optimization of human, technical, and financial resources. We combine cutting edge technology with world class personnel to deliver customized solutions that fit specific needs and solve the most challenging, dynamic problems. Our team emphasizes technical expertise and an empathy to public service to provide unique top tier support for our government engagements.
SOME BENEFITS OF THIS POSITION INCLUDE:
Health, Dental, & Vision insurance
Life Insurance
401k
Flexible Spending Account (Health, Dependent Care, and Commuter)
Paid Time Off & Observance of State/Federal Holidays
For more information about CGS please visit: https://www.cgsfederal.com
39U0b7kJu8","$103,213 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Radiant System,#N/A,"Pleasanton, CA",Data Engineer,"First Priority: Pleasanton, CA –
Second Priority for a superstar: Plano, TX
Experience in developing and deploying data pipelines, preferably in the Cloud
Experience working with Spark, Databricks, and other relevant technologies in Microsoft Azure cloud
Experience in productionizing and deploying Big Data platforms and applications
Expert in writing SQL statements
Job Type: Contract
Application Question(s):
Are you willing to work on W2?
Work Location: In person",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Virtualan Software LLC
4.5",4.5,"Atlanta, GA","Sr. Data Engineer (Python,Spark & Databricks)","Role : Senior Data Engineer (Python, Spark & Databricks)
Location : Hybrid (Atlanta, GA)
Pay: 60-70$ on C2C / 1099
RESPONSIBILITIES:
Create or modify the conceptual, logical and physical data models.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of sources like Hadoop, Spark, AWS Lambda, etc.
Lead and/or mentor a small team of data engineers.
Design, develop, test, deploy, maintain and improve data integration pipeline.
Develop pipeline objects using Apache Spark , PySpark and/or Python.
Design and develop data pipeline architectures using Spark and related AWS Services.
Communicate effectively with client leadership and business stakeholders.
Participate in proposal and/or SOW development.
REQUIREMENTS:
5+ years of professional work experience designing and implementing data pipelines in a cloud environment is required.
2+ years of experience migrating/developing data solutions in the AWS cloud is required.
1+ years of experience building/implementing data pipelines using DataBricks is required.
Expert level knowledge of using SQL to write complex, highly-optimized queries across large volumes of data.
Hands-on object-oriented programming experience using Python is required.
Professional work experience building real-time data streams using Spark.
Knowledge or experience in architectural best practices in building data lakes.
Bachelor or Master degree in Computer Science, Engineering, Information Systems or relevant degree.
Hybrid work schedule, and must live in the Atlanta, GA metro area. Must be open to up to 25% national travel to client locations when on engagements outside of the Atlanta, GA area.
Job Type: Contract
Pay: $60.00 - $70.00 per hour
Schedule:
Monday to Friday
Application Question(s):
Apply only if you are interested in Hybrid Role in Atlanta, GA. Interested ?
Experience:
Data Engineer: 4 years (Required)
Databricks: 1 year (Required)
Willingness to travel:
25% (Required)
Work Location: On the road",$65.00 /hr (est.),1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,2019,Less than $1 million (USD)
"Quirch Foods
3.0",3.0,"Coral Gables, FL",Data Engineer,"About Quirch Foods
In business for over 50 years, Quirch Foods is a global distributor and exporter of fresh and frozen food products with a unique focus on ethnic cultures. We service our customers from 5 distribution centers strategically located in the United States. We have consistently been ranked among the top 50 exporters in the US and are one of the largest importers of seafood. Our customers include independent grocers, chain supermarkets, foodservice distributors, cruise lines, restaurants and food processors/manufacturers. We have an extensive product list with over 8,000 SKU's and carry niche brands such as Panamei, Mambo and Chiquita along with national brands like Tyson, IBP, Excel, JBS, Iowa Premium, Smithfield, Dietz & Watson, along with many others. We are headquartered in Coral Gables, FL.
Quirch is a portfolio company of Palladium Equity Partners, a leading mid-market private equity fund based in New York City, which partnered with the Executive Team to capitalize on Quirch Foods’ record of excellence and to grow it decisively beyond its current business.
Position Summary
At Quirch Foods we believe in leveraging technology as a source of competitive advantage. This includes customized ERP Solutions, Business Intelligence, EDI, Web and Mobile Solutions. We are currently looking for a Data Engineer to join our development efforts.
Essential Duties and Responsibilities:
Develop, test and maintain custom business solutions by leveraging state of the art software development practices using an agile methodology
Evaluate business requirement and help define problems and develop solutions
Participate in design and architecture discussions with business leaders, end users and IT team members
Document business requirements and solution, document code and provide support for creation of end user documentation
Update business knowledge, technical skills and soft skills, leverage educational opportunities, participate in professional organizations
Experience:
MS SQL Programming
Visual Studio\C#\WPS Experience
Microsoft SSAS, SSIS, SSRS desired but not required
Biztalk experience desired but not required
Skills and Requirements:
Bachelor’s degree in Computer Science, Engineering, Math or equivalent and\or related experience and training preferred
Strong analytical and problem-solving skills
Strong collaborator and team player
Great organizational skills, attention to detail and follow thru
Effective oral and written communication skills
Extensive familiarity with data management principles
Benefits:
Professional growth and developmental opportunities to grow your skills using state of the art technology
Comprehensive benefits package that includes: Medical, Dental, Prescription Drug Plan, Disability Plan, Life insurance Plan
401K savings Plan
Paid Holidays
Personal Time off
Employee Discounts
Quirch Foods is an Equal Opportunity Employer (EOE). Qualified applicants are considered for employment without regard to age, race, color, religion, sex, national origin, sexual orientation, disability, or veteran status. All applicants must be eligible to work in the United States.
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Vision insurance
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Coral Gables, FL 33134: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
What is your desired compensation?
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Data Engineering: 1 year (Preferred)
Work Location: In person","$89,340 /yr (est.)",201 to 500 Employees,Company - Private,Retail & Wholesale,Wholesale,1967,$500 million to $1 billion (USD)
Redzara.us,#N/A,"Atlanta, GA",Azure Data Engineer,"Responsible for developing new and modernizing existing applications to Azure Cloud Platform.
The ideal candidate must demonstrate in-depth knowledge and understanding of Database concepts.
Requirements:
7+ years of experience in Data Warehousing and Business Intelligence
5+ Years in BI development leveraging the Microsoft SQL Server. Needs to be an expert in T-SQL
5+ Years in SSIS and use of Stored Procedures.
1-2 Years in Azure Data Factory, SQL VMs, build and maintain ETL
Ingest and aggregate data from both internal and external data sources to build our data lake
Strong understanding data modeling (i.e., conceptual, logical, and physical model design, experience with Operation Data Stores, Enterprise Data warehouses and Data Marts).
Job Type: Contract
Pay: $43.49 - $57.00 per hour
Compensation package:
1099 contract
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Atlanta, GA 30334: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure Data Factory: 3 years (Required)
Azure: 3 years (Required)
Data warehouse: 1 year (Preferred)
Work Location: Hybrid remote in Atlanta, GA 30334",$50.25 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Integrated Technology Strategies, Inc.
4.0",4.0,"New York, NY",Data Engineer,"Integrated Technology Strategies is a provider of information technology consulting. Digital transformation, improving business performance, developing strategies and enhancing value is at the core of what we are.

Integrated Technology Strategies is looking to hire a Data Engineer to be a part of our Consulting team.

We are looking for someone passionate about data is focused on developing the platform for critical data products, including real time business metrics and analytics capabilities. The role requires supporting and collaborating with groups including Data Analytics/BI/Product, as well as our core backend API team. The individual will not be afraid to think out of the box and will play a key role in technical decision making. We are highly focused on giving ownership and responsibility to autonomous teams, using the right tools for the job, and building flexible architectures.

Responsibility:
Continue to evolve the internal Reporting and Analytics platform on top of Snowflake on AWS infrastructure.
Experience in Architect, design and implementing scalable ETL and data processing systems to handle the big data ecosystem including data collection, processing, ETL and Data warehouse.
Build soft real time capabilities and insight into product metrics to help product managers and BI/Analytics understand and optimize product features and guide product decisions.
Participate and contribute to the capabilities and engineering priorities across the organization.
Contribute to the codebase and participate in code review.
Build analytics tools that utilize the data pipeline to
provide actionable insights into operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product,
Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Reporting to the Senior Director, Software Engineering, you’ll be responsible for overseeing engineering product quality and delivery and setting and overseeing technical standards for teams who are working on everything from customer-facing applications.

Skill Requirements:
Solid understanding of real time data processing with Kafka, Spark and Flink and batch data processing frameworks on EMR and Snowflake.
Passion for building world-class data platforms that support a global customer base
Solid engineering background and understanding of programming languages such as Python, Java or equivalent
5+ years of progressive experience in data infrastructure development, with a track record of successful high-quality deliveries
Experience of working in an agile environment and embracing engineering best practices
Ability to apply both technical competence and interpersonal skills to achieve business outcomes
High emotional intelligence, sound temperament, and professional attitude
Strong understanding of SQL, experience with key databases such as Snowflake, MS-SQL and Postgres
Knowledge of the internals of how database systems work to design models for varied use cases.
Experience with CI and CD in an AWS environment with Terraforms
Experience with key Data technologies, such as Sqoop. Kafka will be a plus
Proven experience in building secure data platforms
Bachelor’s degree in Computer Science or equivalent","$110,128 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$5 to $25 million (USD)
"MoneyDolly
5.0",5.0,"Salt Lake City, UT",General Data Engineer,"Position Title: General Data Engineer
Location: Remote (US Based only)
Commitment: Full-Time

Job Brief

About Us
MoneyDolly is a fast growing Fintech Saas tech company, headquartered in Sandy, Utah. We are leading innovation in supporter relationship management, wherein teams meet all their fundraising goals. Teams can create their own page for your supporters to visit, offer products and incentives for their contributions, then simply invite their team to spread the word and watch the money roll in. MoneyDolly has already helped thousands of organizations, groups and teams nationwide raise over $20 million. We are still private, and our best work is still ahead of us. This is a massive industry with antiquated methods and no clear market leader. This is the spot for a qualified team player looking to build something new, make a real impact, and actually change the world.

Job Description:
We are looking for a highly motivated and talented Generalist Data Engineer to join our development team. The ideal candidate will be someone who loves to wear multiple hats and thrives in an environment of high autonomy to accomplish our business goals. You will be responsible for collaborating with our business team, creating flexible data models, and maintaining data pipelines to ensure the smooth flow of data in our organization.
Responsibilities:
Interact with our business team to gather requirements and understand data needs
Design and create flexible data models that allow for easy report generation and ad-hoc analysis
Write and maintain DBT transformations to generate flexible data models from our production database
Develop and maintain required data pipelines in Python to collect and process data from various sources
Develop and maintain Tableau and Hex.Tech dashboards to analyze the collected data
Ensure data quality, integrity, and security in all data processes
Collaborate with other team members to implement data solutions and integrate them into the existing infrastructure
Continuously monitor and optimize data models and pipelines to meet changing business requirements
Requirements:
Bachelor's degree in Computer Science, Engineering, or a related field
Strong experience in data modeling, ETL processes, and data pipeline development
Proficiency in Python and SQL
Familiarity with DBT transformations and best practices
Experience working with relational databases and big data technologies
Excellent problem-solving skills and attention to detail
Strong communication and collaboration skills
Ability to work independently and adapt to a fast-paced, dynamic environment
Nice to have:
Experience with data visualization tools (e.g., Tableau, Power BI)
Experience with AWS and GCP (RDS Postgres and BigQuery are our current stack)
Knowledge of the fundraising industry or a strong interest in learning more about it
What We Offer:
A competitive salary ($100k-$140k range) and benefits package
A supportive and collaborative work environment
Opportunities for professional growth and development
The chance to make a significant impact in a growing startup
If you are passionate about data engineering and excited about the prospect of revolutionizing the fundraising industry, we'd love to hear from you. Apply now to join the MoneyDolly team!","$120,000 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Iron Bow Technologies
3.5",3.5,"Washington, DC",Sr. Data Center Engineer,"Iron Bow Technologies is seeking a Sr. Data Center Engineer to support our Federal customer, The Department of Justice (DOJ), Criminal Division (CRM), in Washington, DC. The selected candidate will support our flexible and scalable solution with the DOJ CRM to extend cloud-native features to existing legacy and next-generation applications while also future proofing the infrastructure to easily accommodate future requirements, such as transitioning to a public cloud.
This position is hybrid and requires 2 days a week onsite at the customer location in NW Washington, DC and support at the customer data center as needed in Sterling, VA.
Essential Duties and Responsibilities:
Assess and provide VMware solutions to include vCenter, vSphere, SRM, ESXi
Primary storage solutions and data file storage management, NAS file storage solutions
Data Center blade servers and rack servers
Server virtualization
Experience:
10 years of related experience providing system administration support in a virtualized environment, including at least 6 years of VMWare solutioning, including vCenter, vSphere, SRM, and ESXi
Data Center operations experience, with SAN and NAS
Experience supporting primary storage solutions and data file storage management, NAS file storage solutions
Hands on experience with Data Center blade and rack servers
Ability to work staggered shift hours when needed to support critical coverage and/or time-sensitive needs
Be a member of our Team providing coordinated 24x7 IaaS on-call support
U.S. Citizen
Currently hold or obtain/maintain a Public Trust High clearance
Education:
BS in Computer Science, Information Science, or other relevant discipline is preferred – additional years of experience may be considered in lieu of the degree
Preferred Certifications:
VMware Certified Professional
Primary storage certifications such as HPE, Dell-EMC, NetApp, or Pure Storage
VCP-DCV
Travel Requirements:
No travel
PRINCIPALS ONLY. NO AGENCIES PLEASE. MUST BE A US CITIZEN ABLE TO OBTAIN A PUBLIC TRUST SECURITY CLEARANCE.
Iron Bow Technologies is a leading IT solution provider dedicated to successfully transforming technology investments into business capabilities for government, commercial and healthcare clients. Our depth of technical expertise, global reach and strategic partnerships with industry leaders as well as disruptive technology partners uniquely positions Iron Bow to target the right solution to ensure your successful business outcome. We partner with our clients from planning and implementation through ongoing maintenance and management to deliver solutions that are strong, flexible and on target .
Iron Bow Technologies is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.
#LI-TM1",#N/A,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,1983,$500 million to $1 billion (USD)
"Integration Developer Network LLC
4.9",4.9,Remote,Data Engineer with PL/SQL Background,"Requirements:
Recent PL/SQL development work experience with Oracle 19.
Must have working experience with tools like Airflow, Git.
Must have python coding experience and REST API development experience.
Plus would be experience with microservices, docker, Kubernetes.
Plus would be experience with accounting and financial data
Experience in event driven and cloud based architectures and working knowledge of containerization and streaming technologies.
Detailed work experience on Oracle, SingleStore or IBM I DB2 database, SQL & PL/SQL, including ANSI SQL.
Experience in database analysis, development and maintenance of business applications specific to oil & gas back-office operations. Expertise in various ETL tools such as SQL*Loader, Equalum, Python scripts.
Familiarity with No-SQL databases and pipelines (Elasticsearch, MongoDB).
2+ years’ experience in the design, implementation and support of full life-cycle data engineering projects.
Bachelor's degree in Computer Science, Engineering or other equivalent degree, with 5+ years data engineering experience or Master's degree in Computer Science, Engineering or other equivalent degree with 2+ years data engineering preferred.
High school diploma or GED is required.
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Experience level:
9 years
Schedule:
8 hour shift
Experience:
Python: 8 years (Required)
PL/SQL: 5 years (Required)
ETL: 5 years (Required)
Work Location: Remote",$62.50 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2011,$1 to $5 million (USD)
"Hiscox
4.1",4.1,"Atlanta, GA",Senior Data Engineer,"job description:
You will be a senior member of our US IT Team data engineering practice. You will be an early member of this team and will work closely with the Director of Data Engineering to help establish it. We will have our work cut out for us delivering planned work, building the practice and evolving our technical landscape. This is an exciting opportunity for the right individual to join at the very beginning and grow with the new team.
As a senior you will be expected to lead, mentor others and deputise for the director of data engineering when required. Yoi will also continue to contribute as a hands-on engineer. You will have the knowledge, best earned through 5+ years of experience, to truly inhabit this senior role in an ambitious team.
What you’ll be doing:
Working with business stakeholders and our delivery team to understand high value business problems that can be solved through the application of data processing and analytical systems.
Developing, expanding and evolving our existing databases and ETL pipelines.
Working with architects to design, build and support a transformational cloud-based data platform for the business.
Being a core and professional member of the new data engineering practice
Understand business requirements and help refine into development tasks and estimate their complexity.
Research, evaluate and adopt new technologies with a right tool for the job mentality.
Focus on both speed of delivery and quality, with suitable pragmatism – ensuring your solutions are always appropriate and not too complex or over-engineered.
Quick progression of projects from proof-of-concept to post-production stage.
Communication and presentation of ideas to colleagues in all parts of the wider tech team.
Participating in code reviews for the data engineering practice.
Act as a servant leader in our practice – ensuring team members have
person specification:
Knowledge of, or ability to rapidly adopt our core languages for data engineering – SQL and/or Python.
Experience working using a modern DevOps approach. You should not just have written data code but also the CI/CD pipelines necessary to test and deploy that code in a professional environment.
A robust understanding of core data engineering topics – ETL vs ELT, structured and unstructured data, data quality and data governance.
Ability to contribute to all aspects of a solution – design, infrastructure, development, testing and maintenance.
The ability to design and advocate for technical solutions to business problems.
Effective collaboration with technical and non-technical team members through agile ceremonies – roadmap planning, feature workshops, backlog elaboration, code review.
Understanding of cloud technology and significant hands-on experience working with it. You must understand how cloud-native solutions must be built differently from traditional ones.
Track record of taking initiative and delivering projects end-to-end; clear evidence of being self-driven and motivated
Immense curiosity, high energy and desire to go the extra mile to make a difference.

Desirable Skills
Beyond the required skills we are open to individuals of diverse talents. Experience with additional technologies, data science knowledge or a business background are all valued. We want to know how your unique abilities can contribute to our team.
Our Technology
We operate in a diverse technical landscape and are looking for flexible engineers who can adapt to and use many different tools. We would not expect any engineer to be familiar with the entire tech stack – no engineer can. Instead, we seek people with a good understanding of data structures and algorithms and the ability to apply this knowledge in learning new tools.
We have an existing data landscape involving both MS SQL Server and Oracle databases. We use SSIS for ETL. We will also need to source data from our integration layer – webMethods.
If some of that sounds like it needs bringing up to date, we know! We hope to start building a modern cloud data platform to realise the full value of our data. The technologies we use for that platform are still to be decided. Likely it will be based on Snowflake with Azure Data Factory for ELT. Other parts of our business have had success using Prefect for orchestration and dbt for transformation and we will reuse where it is sensible.
We prepare data for use by analysts working with a variety of tools – Tableau, PowerBI, and even Excel.
We take a DevOps approach and strive for continuous integration / continuous deployment. We use Azure Pipelines to deploy our code. We deploy infrastructure the same way using Terraform and Docker.
Our technology landscape is not fixed. Our engineering and architecture teams drive the technology we adopt.


Salary range $140,000 - $170,000","$155,000 /yr (est.)",1001 to 5000 Employees,Company - Public,Insurance,Insurance Carriers,1901,$1 to $5 billion (USD)
"Titan America
3.8",3.8,"Roanoke, VA",Data Engineer,"Data Engineer
We are seeking a motivated and detail-oriented Data Engineer to join our team based in Roanoke, Virginia. As a Data Engineer, you will play a crucial role in designing, developing and maintaining scalable data pipelines and transformations for business-critical use cases. In addition, you may take on various Software Development and Data Science responsibilities as we frequently share responsibilities and knowledge across our growing team.
Responsibilities:
Design, develop and maintain scalable data pipelines for extracting, transforming, and loading data from various sources
Collaborate with cross-functional teams to identify data needs and determine the best data solutions
Develop and implement data models to support business requirements and ensure data quality
Ensure the security and privacy of sensitive data by implementing appropriate access controls
Monitor and optimize data pipeline performance to ensure timely and accurate data delivery
Document data pipeline processes, data dictionaries, and data storage solutions
Requirements:
Bachelor's degree in Computer Science, Computer Engineering, or a related technical field
Minimum of five years of professional experience working as a Data Engineer or Software Developer
Strong hands-on experience with data warehouse and transformation solutions, i.e. Domo, Snowflake or similar
Proficient in at least one scripting language such as Python, JavaScript, or R
Understanding of data modeling, data integration and data quality processes
Familiarity with cloud platforms such as AWS, Azure, or Google Cloud Platform
Strong analytical and problem solving skills
Full Stack Software Development experience in a professional setting is highly desired, but not required
This is an excellent opportunity for a driven and collaborative individual to make a significant impact in a dynamic and growing team. If you have a passion for data and a desire to work in a fast-paced and dynamic environment, we want to hear from you!
Success begins with hiring the right people to partner with us as we grow and develop our businesses. People are central to everything we do. It is through their efforts and talents that Titan has been successful for over 100 years.
Titan America is an Equal Employment Opportunity (EEO) / Affirmative Action employer and welcomes all qualified applicants. Applicants will receive fair and impartial consideration without regard to race, sex, color, religion, national origin, age, disability, veteran status, genetic data, or other legally protected status.
Job Type: Full-time
Pay: $75,000.00 - $85,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Referral program
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Roanoke, VA 24019: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Do you currently or will you in the future need sponsorship to work in the United States?
This position requires the successful candidate to be in the office in Roanoke, VA at least 3 days a week. Can you meet this requirement?
This position requires the successful candidates to be a full time, W2 employee. Can you meet this requirement?
Education:
Bachelor's (Preferred)
Experience:
Data Engineering: 5 years (Preferred)
Work Location: Hybrid remote in Roanoke, VA 24019","$80,000 /yr (est.)",1001 to 5000 Employees,Company - Public,"Energy, Mining & Utilities",Mining & Metals,1902,Unknown / Non-Applicable
"Appsintegration INC
4.5",4.5,"California, KY",Data Engineer,"Role:- Data Engineer
Position:- Onsite (W2 position)
Experience:- 10+ Years
Responsibilities
Working closely with business teams to understand their requirements and develop reporting or analytic solutions.
Producing and automating the delivery of key metrics and KPIs to the business.
In some cases, this will focus on making data available and others will be developing full reports for end users.
Managing stakeholder demand and a backlog of reporting requirements and needs.
Actively working with users to understand data issues, tracing back data lineage, and helping the business put appropriate data cleansing and quality processes in place.
Managing secure access to data sets for the business.
Requirements
Bachelor's degree in Computer Science or equivalent.
Proficient in creating Data Factory pipelines for on-cloud ETL processing.
Experience implementing data pipelines using the latest technologies.
Experience developing using SQL, relational databases, and data warehousing technologies.
Extensive ETL experience.
Ability to scale systems in Cloud as per requirement.
Experience in handling structured and unstructured datasets.
Good to have experience in data modeling and Advanced SQL techniques.
Must-Have Skills: Azure Data Factory (ADF), SQL, Cloud-based ETL.
Good to Have Skills: Azure File Storage Services, Databricks, Azure Synapse.
you can reach me out on LinkedIn
here is my link :- linkedin.com/in/shravanthi-thakur-705a81231
Job Type: Contract
Salary: $42.00 - $45.00 per hour
Ability to commute/relocate:
California, KY 41007: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",$43.50 /hr (est.),1001 to 5000 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Hiscox
4.1",4.1,"Atlanta, GA",Senior Data Engineer,"job description:
You will be a senior member of our US IT Team data engineering practice. You will be an early member of this team and will work closely with the Director of Data Engineering to help establish it. We will have our work cut out for us delivering planned work, building the practice and evolving our technical landscape. This is an exciting opportunity for the right individual to join at the very beginning and grow with the new team.
As a senior you will be expected to lead, mentor others and deputise for the director of data engineering when required. Yoi will also continue to contribute as a hands-on engineer. You will have the knowledge, best earned through 5+ years of experience, to truly inhabit this senior role in an ambitious team.
What you’ll be doing:
Working with business stakeholders and our delivery team to understand high value business problems that can be solved through the application of data processing and analytical systems.
Developing, expanding and evolving our existing databases and ETL pipelines.
Working with architects to design, build and support a transformational cloud-based data platform for the business.
Being a core and professional member of the new data engineering practice
Understand business requirements and help refine into development tasks and estimate their complexity.
Research, evaluate and adopt new technologies with a right tool for the job mentality.
Focus on both speed of delivery and quality, with suitable pragmatism – ensuring your solutions are always appropriate and not too complex or over-engineered.
Quick progression of projects from proof-of-concept to post-production stage.
Communication and presentation of ideas to colleagues in all parts of the wider tech team.
Participating in code reviews for the data engineering practice.
Act as a servant leader in our practice – ensuring team members have
person specification:
Knowledge of, or ability to rapidly adopt our core languages for data engineering – SQL and/or Python.
Experience working using a modern DevOps approach. You should not just have written data code but also the CI/CD pipelines necessary to test and deploy that code in a professional environment.
A robust understanding of core data engineering topics – ETL vs ELT, structured and unstructured data, data quality and data governance.
Ability to contribute to all aspects of a solution – design, infrastructure, development, testing and maintenance.
The ability to design and advocate for technical solutions to business problems.
Effective collaboration with technical and non-technical team members through agile ceremonies – roadmap planning, feature workshops, backlog elaboration, code review.
Understanding of cloud technology and significant hands-on experience working with it. You must understand how cloud-native solutions must be built differently from traditional ones.
Track record of taking initiative and delivering projects end-to-end; clear evidence of being self-driven and motivated
Immense curiosity, high energy and desire to go the extra mile to make a difference.

Desirable Skills
Beyond the required skills we are open to individuals of diverse talents. Experience with additional technologies, data science knowledge or a business background are all valued. We want to know how your unique abilities can contribute to our team.
Our Technology
We operate in a diverse technical landscape and are looking for flexible engineers who can adapt to and use many different tools. We would not expect any engineer to be familiar with the entire tech stack – no engineer can. Instead, we seek people with a good understanding of data structures and algorithms and the ability to apply this knowledge in learning new tools.
We have an existing data landscape involving both MS SQL Server and Oracle databases. We use SSIS for ETL. We will also need to source data from our integration layer – webMethods.
If some of that sounds like it needs bringing up to date, we know! We hope to start building a modern cloud data platform to realise the full value of our data. The technologies we use for that platform are still to be decided. Likely it will be based on Snowflake with Azure Data Factory for ELT. Other parts of our business have had success using Prefect for orchestration and dbt for transformation and we will reuse where it is sensible.
We prepare data for use by analysts working with a variety of tools – Tableau, PowerBI, and even Excel.
We take a DevOps approach and strive for continuous integration / continuous deployment. We use Azure Pipelines to deploy our code. We deploy infrastructure the same way using Terraform and Docker.
Our technology landscape is not fixed. Our engineering and architecture teams drive the technology we adopt.


Salary range $140,000 - $170,000","$155,000 /yr (est.)",1001 to 5000 Employees,Company - Public,Insurance,Insurance Carriers,1901,$1 to $5 billion (USD)
Atika Technologies,#N/A,"Pittsburgh, PA",Data Engineer,"Role: Data Engineer/PySpark Developer, Pittsburgh, PA - Day One Onsite
Fulltime and Contract(W2)
LinkedIn Must
USC/GC/EADs
Three round of interview: First two on Zoom Video Call, End Client Round will be Onsite on the Client's Location (Pittsburgh, PA)
Job Description:
Python(PySpark), Jupiter hub, airflow, Java
Database knowledge - Oracle / SQL / Data Analytics
Regulatory reporting knowledge preferred
Good communication skills
Open / Ability to learn new technologies
Very good Analytical Skills
Experience in ETL is preferable
Job Types: Full-time, Contract
Salary: $120,000.00 - $130,000.00 per year
Benefits:
Dental insurance
Health insurance
Life insurance
Compensation package:
Bonus pay
Performance bonus
Yearly pay
Experience level:
10 years
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Pittsburgh, PA 15259: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Required)
Spark: 4 years (Required)
Regulatory reporting: 3 years (Required)
SQL: 4 years (Required)
IT: 10 years (Required)
Airflow: 4 years (Required)
Jupiter: 4 years (Required)
Work Location: One location","$125,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Redzara.us,#N/A,"Atlanta, GA",Azure Data Engineer,"Responsible for developing new and modernizing existing applications to Azure Cloud Platform.
The ideal candidate must demonstrate in-depth knowledge and understanding of Database concepts.
Requirements:
7+ years of experience in Data Warehousing and Business Intelligence
5+ Years in BI development leveraging the Microsoft SQL Server. Needs to be an expert in T-SQL
5+ Years in SSIS and use of Stored Procedures.
1-2 Years in Azure Data Factory, SQL VMs, build and maintain ETL
Ingest and aggregate data from both internal and external data sources to build our data lake
Strong understanding data modeling (i.e., conceptual, logical, and physical model design, experience with Operation Data Stores, Enterprise Data warehouses and Data Marts).
Job Type: Contract
Pay: $43.49 - $57.00 per hour
Compensation package:
1099 contract
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Atlanta, GA 30334: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure Data Factory: 3 years (Required)
Azure: 3 years (Required)
Data warehouse: 1 year (Preferred)
Work Location: Hybrid remote in Atlanta, GA 30334",$50.25 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"SECURE RPO
4.3",4.3,Manhattan,Senior data engineer,"Must have skills:
8+ years of experience building high performance scalable enterpris`e analytics or data centric solutions
8+ or 5+ years of experience in the design and build of data extraction, transformation, and loading processes by writing custom data pipelines
At least 5 years of experience implementing complex ETL pipelines preferably in connection with Glue and/or Spark
Exceptional coding and design skills in Python or Java/Scala
Hands-on experience with AWS (i.e. Glue, Aurora Postgres, Lambda, EMR, EKS, Redshift, etc.)
Experience with visualization tools like QuickSight, PowerBI, Looker or Tableau
Experience with Talend (ETL) is a big plus
Roles and Responsibility:
Drive a high impact and high visibility project that enables data availability, encompasses data analytics, machine learning, and petabyte scale datasets, and provides reliable and timely access to thousands of data sources
Design, architect and support systems for collecting, storing, and analyzing data at scale
Recommend improvements and modifications on new and existing data and ETL pipelines. Create optimal data pipeline architecture and systems using Apache Airflow
Create data analytics for d ata scientists to innovate, build and optimize our ecosystem
Assemble large, complex data sets that meet functional and non-functional business requirements
Be responsible for ingesting data into our data lake and providing frameworks and services for operating on that data including the use of Spark
Analyze, debug and correct issues with data pipelines
Operate on or build solution required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, AWS and Spark technologies
Job Type: Full-time
Salary: $56.80 - $80.16 per hour
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Informatica: 7 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: On the road",$68.48 /hr (est.),1 to 50 Employees,Unknown,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Apolis
3.9",3.9,"Saint Louis, MO",Data Engineer (Python Focused),"Role: Data Engineer (Python Focused) (W2 ONLY)
Location : St. Louis, MO 63102
Duration: 18 Months
The primary responsibilities of this role will be to work as a part of the data and analytics team of the client. The ideal candidate would have experience using ETL and other data engineering tools in a cloud environment. Additionally, this candidate would have coding and scripting experience creating code to move and manipulate data using tools such in Python such as Spark or Pandas. Microsoft Azure experience is a plus but relevant experience with other cloud-based environments will be considered.
This team uses Microsoft Azure to build and productionize data sets and processes to the product owners and product users and this role will be expected to contribute to those objectives. As a member of this team, you will work in conjunction with other data engineers, platform engineers, solution developers and others to execute and deliver working solutions to the end users.
Technical Skills
High focus on Python
More Programmer/Coder experience, less Cloud experience needed.
Must have experience with data analytics/engineering and cloud technologies and an in-depth working knowledge and understanding of data engineering, design, and integration.
Intermediate or higher level of proficiency with Python Must have the proven ability to learn quickly and apply cloud data engineering principles and skills to new tools and use cases.
Ability to define and take technical ownership of detailed requirements and collaborate effectively with various team members such as Business Analysts and Project Managers and end users.
Excellent communication (verbal and written) and problem-solving skills are essential for this position.
Initiative, Proactive Cooperation, and curiosity are mandatory behavioral skills required for this position.
Experience with several of the following technologies is preferred:
Experience with Microsoft Azure Platform. In particular, the data processing toolsets including Azure Data Factory Pipelines, Azure Databricks, Azure Data Lake Storage.
Experience using Notebooks (Such as Jupyter, DataBricks) also desired.
Experience using code languages such as Python and C#
Experience using query languages such as SQL, MDX and DAX
Experience using scripting languages such as PowerShell, M-Query (Power Query), and Windows batch commands.
Understanding of Agile and DevOps principles for software development and ownership.
Familiar with compliance and controls for software development and deployment.
Education Required
A bachelor's in computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field [or equivalent work experience] is not required but is preferred.
Job Type: Contract
Salary: $60.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Vision insurance
Compensation package:
Overtime pay
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
St. Louis, MO: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Preferred)
ETL: 5 years (Preferred)
Cloud infrastructure: 4 years (Preferred)
Work Location: One location",$65.00 /hr (est.),501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,1996,$25 to $100 million (USD)
"shaped.ai Inc.
4.0",4.0,"New York, NY",Data Engineer,"Hello! We’re building the future of content discovery infrastructure with AI — come join us!

Shaped (YC W22) is an API for developers to seamlessly add personalized ranking and recommendation to their products. These frictionless discovery experiences help end-users find what they want faster, and, consequently, grow conversion and engagement business metrics. Behind the scenes, we’re building state-of-the-art ML infrastructure for training and deploying large AI models to power our personalization engine.
We’re a seed stage start-up backed by top-tier investors (e.g. Y-Combinator, Susa Ventures, Tribe Capital etc…) and executives (e.g. Google, Amazon, Uber, Dropbox etc…). Our team comes from Meta, Google, Apple and Uber. We’re a remote team but have a small office in Brooklyn, New York.
We’ve been lucky to have a huge amount of success since our initial launch earlier this year and now looking to expand our real-time data engineering capabilities. We are looking for a data engineer to design, build and optimize Shaped's real-time streaming data ingestion endpoints. You will be a founding engineer working on state-of-the-art infrastructure. As one of Shaped’s early employees you will help shape our product, culture and vision.
This role is for a full-time data engineer at $120k - $200k a year and 0.1 - 0.5% equity.

Responsibilities:
Manage the unification of our batch and real-time data processing pipelines.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Requirements:
Bachelor's in computer science, data science or mathematics related field. Master's degree or PhD will be advantageous.
5+ years of data engineering experience, ideally applied to machine learning use-cases.
Experience with AWS or common cloud technologies
Proficient with Python, SQL.
Extensive knowledge of both offline and online large scale data processing frameworks: Spark, Beam, Flink.
Extensive knowledge with stream technologies like Kakfa or Kinesis.
Excellent written and verbal communication skills.
We’re excited to work with you. Come build the future of AI and discovery with us!","$160,000 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
WellTrust Medical Group,#N/A,"Charlotte, NC",Data Engineer,"The Data Engineer will assist the senior analyst in complex modeling and projects to ensure their completion within the allotted timeline. Helps track measures of success and develop reports that measure operational and customer metrics. Demonstrates ability to troubleshoot data consumption and data quality problems.

Responsibilities:
Build and maintain reports using My SQL Server
Identify optimal data sources, structure data validation, and cache optimization.
Performs business problem modeling and quantitative analytics including regression analysis, time-series, hierarchical cluster analysis, and data mining.
Helps track measures of success and develop reports that measure operational and customer metrics.
Demonstrates ability to troubleshoot data consumption and data quality problems.
Evaluates and recommend improvements to current models, metrics, and analytic processes to improve operational efficiency and insight.
Design, document, and communicate effective technical solutions meeting end-user requirements
Perform analyses with large amounts of complex data, and uses data visualization tools to communicate results.
Requirements:

Bachelor's degree or equivalent experience in a quantitative field (Mathematics, Statistics, Computer Science, Economics, Finance, Physics, Engineering)
3+ years of professional working experience as a Data Engineer or Analyst
1-3 years of data warehouse experience
Build and maintain reports using My SQL Server
Strong analytical and creative problem-solving skills
Design, document, and communicate effective technical solutions meeting end-user
SQL and Excel Macro development
Strong understanding of various data environments such as SQL, Oracle, Big Query, etc.
Experience with data visualization tools such as Looker Studio, Tableau, D3.js, etc.
Proficient in R, Python, MATLAB, Java, or another programming language.",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Tipico - North America
4.0",4.0,"Denver, CO",Data Engineer,"Company Description

Tipico is an energetic, passionate, bold tech company, and we love sports! Tipico has recently expanded to the US market, establishing offices in Hoboken, New Jersey and Denver, Colorado. We are driven by our core values: passion, progress and trust. Our goal is to build the best possible mobile sports betting product in the industry, in order to amplify the emotions of millions of sports fans every day. Let’s Make This Interesting ™
Please note: this role is located in our Denver, CO office; we work off a hybrid model and come into the office 2 days per week.

Job Description

The Data Engineer will report directly to the Data Engineering Manager and work together with rest of the data engineers, BI analysts, and product teams. The position holder ensures this is done by doing the following:
Create and maintain both batch (ETL/ELT) and real-time data pipelines and architecture
Ensure all data provided by Data Platform is of the highest quality and is delivered in a timely manner and in line with agreed SLAs
Assemble large and complex data sets that meet functional and non-functional business requirements
Identify, design, and implement internal process improvements, including automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies
Collaborate with stakeholders including data architects, Executive, Product, Data and IT team members from the beginning to the delivery of a project
Build a scalable, modern, and trusted data ecosystem, making sure it is responsive, available and in line with business requirements
Take part in sprints, workloads, and deliverables through agile methodologies
Manage quality assurance, verification of accuracy and consistency of data, specifically KPI measures sent to the business
Thrive working in a fast-paced and challenging environment
Open to continuous learning individually and with the team
Comfortable with being challenged / seeking challenge
Willing to work flexible hours if needed

Qualifications
3+ years in Data Platforms / Datawarehouse / BI experience
Must have experience in successfully implementing data-centric applications, such as data warehouses, operational data stores, and data integration projects
Strong analytical and problem-solving skills that balance with creative approaches
Ability to learn new and complex concepts quickly
Strong experience with large-scale production databases and SQL
Solid understanding of cloud data services: AWS services such as S3, Athena, EC2, RedShift, EMR, EKS, RDS and Lambda
Proficient with one or more big data & new data technologies such as DBT, NIFI, Airflow, Kafka
Experience with data modelling
Experience with ETL development (extractions, data load, aggregation, transformation)
Knowledge in working with time-series/analytics databases such as Elasticsearch
Understanding of containerization and orchestration technologies like Docker/Kubernetes
Must be able to take ownership of projects with a high level of attention to detail and priority
Team player with a hands-on approach and attitude
Must be comfortable with on call rotation
Nice to have:
Familiarity with Marketing tooling and/or CRM systems is preferred
Familiarity with using business intelligence tools such as Domo, Tableau or Looker
Knowledge about and / or experience with sports betting
Background in digital companies, online, gambling, gaming, e-commerce, marketing, or mobile apps
Familiarity with one scripting language, preferably Python

Additional Information

What's in it for you:
Work in an environment where you, your work and ideas matter and have an impact.
You will be a be part of the newest and up and coming USA online iGaming and sports betting companies in the market.
Work in a new and young business with high growth potential.
Build your own success story together with us.
Work with self-organized, self-responsible and entrepreneurial employees.
Start-up feeling, backed-up by a leading European sports betting house. We are a high-volume business and are taking off in the US!
Competitive salary, Medical, Vision, Dental Benefits, Unlimited PTO, and more
Salary for this position is $120k+ & annual performance bonus","$120,000 /yr (est.)",201 to 500 Employees,Company - Private,"Arts, Entertainment & Recreation",Gambling,2004,Unknown / Non-Applicable
"Fuge Technologies Inc
4.7",4.7,"Jersey City, NJ",Big Data Engineer,"Big Data Engineering Technical Lead
Mandatory Skills
Big Data – Spark, Kafka, Databricks, Hive, Nifi
AWS, Java, Scala
Client Interview Needed for Selection (yes / No)
Yes
Detailed JD (Pl share the Detailed Description, 1 liner JD will not work)
10+ yrs experience as Data Engineer
Extensive experience in spark on data processing
Kafka design experience like monitoring & parallelism.
Comfortable with Python/Java/Scala coding experience
Crisp / clear cut communication
Flexible with learning new technology
Flink experience is a plus
AWS experience is nice to have
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Jersey City, NJ 07302: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",$72.50 /hr (est.),1 to 50 Employees,Contract,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Take Command Health
4.9",4.9,"Dallas, TX",Data Engineer,"Company: Take Command
Position: Data Engineer
Salary: $120,000-$130,000 plus bonus
Location: Dallas or Austin, TX
About Take Command
Take Command is a start-up on a mission to improve the healthcare system, starting with health insurance. Pragmatically speaking, we help employers reimburse employees for individual insurance instead of offering a traditional one-size-fits-all group plan. We believe this model can empower employees (when they have the right support) to be savvy healthcare consumers and have a transformative impact on the entire healthcare system.
Let’s be honest—health insurance is usually a confusing, frustrating, and even emotional experience for people. We want to fix that with a new model, great technology, and a superior user experience. We have made a great start, but we need your help to fully realize our long-term vision.
As a part of the dev team, you'll build the framework for both our clients and our team to succeed. You will be a pillar in our company as we grow and make our product the best product on the market!
In this role, you will:
Take ownership for delivering multiple features, their quality and assist team members in delivering their features
Recommend improvements to data quality, processes and code to make it easier to use, manage and maintain
Experience designing and developing data models and data migrations
Experience managing and maintaining ETL or ELT processes (Hevo, BigQuery, Looker, etc.)
Hands-on development experience with SQL, relational database design (ERDs)
Must be able to write SQL & contribute code
Be product-minded; focus not only on what needs to be built but why something needs to be built
You may be a great fit for this role if you have:
3 or more years of professional experience as a data analyst or data engineer
Cloud-Based Data Tools e.g. Hevo, BigQuery, Looker, etc.
Database technologies: SQL, RDBMS / Relational databases (Postgres, Amazon RDS / Aurora, etc.)
Nice to have experience: Python, Java, Typescript or another modern OOP language
Cloud infrastructure: AWS
Source control: Git
Working at Take Command
We’re excited to build a team and culture that reflects our values! We offer competitive pay, health benefits, and equity options to share with this position. We want you to be an owner along with us in revolutionizing healthcare.
A generously funded ICHRA for medical, dental, and vision premiums and medical expenses. You get to use our own product and we think that’s so exciting and rare!
Unlimited personal vacation in addition to regular company holidays.
401(k): 90-day eligibility for 2% match that vests immediately!
Access to LinkedIn Learning.
We have two beautiful offices conveniently located near The Hill (Dallas) and Holly Commons (Austin). The kitchen is well-stocked and we've designed the space to have lots of different areas to work!
Paid parental leave for new parents.
Flexible on where you work – we hope to see you around the office on average 3 days a week especially when white-boarding or building relationships with your colleagues, but you also have the ability to work from home or wherever you'd like when you need to get focus work done.
More About Us
We secured our Series A funding in 2021 and are thrilled to be able to expand our team. Despite being a small startup in a land of health insurance giants, we’re the recognized industry leader for what we do (health insurance reimbursements) and passionate about bringing it to market because we know we can help fix a broken system and improve our clients’ wellbeing and health outcomes.
We’ve been featured in The New York Times, The Wall Street Journal, The Dallas Morning News, and other national healthcare publications and are excited about our growth opportunities.
Take Command knows diversity and inclusion among our teammates is integral to our company’s success and growth. Our vision is to recruit, develop, and retain the best team from a diverse candidate pool.
This has mostly been about us, but we’d love to hear from you-we can’t wait to hear your story!
Take Command is an equal opportunity employer! We celebrate diversity and are committed to creating an inclusive environment for all employees.
Xrygn4NXMr","$125,000 /yr (est.)",1 to 50 Employees,Company - Private,Insurance,Insurance Agencies & Brokerages,2014,Unknown / Non-Applicable
"Trella Health
4.8",4.8,"Philadelphia, PA",Senior Data Engineer,"At Trella Health, we are passionate and committed to our mission – empowering meaningful change in healthcare. Since our founding in 2015, we continue to grow our team, enhance our solution and services offerings, accelerate into new markets, and expand our customer base. We are rapidly growing and are looking for new Trellavators to join our team!
""What is a Trellavator?"" you ask. Innovate and elevate is the name of our game! We go above and beyond to collaborate with and support each other – we believe that when a colleague or customers succeeds, we succeed. By learning from others, building on our successes, and taking risks, we constantly raise the bar – continuous improvement is in our DNA. Our word is paramount, we keep our commitments, and we always follow through. We have a strong, reliable support system that fuels growth, collaboration, and passion – and together, we create a positive environment where everyone at Trella Health, including the customers we support, can thrive. Are you ready to learn more about the opportunities with our team? Trell-yeah you are!
Position Overview:
The Senior Data Engineer will be responsible for designing, implementing, and maintaining an effective data pipeline for Trella Health's applications. This position is responsible for turning data concepts into effective data models that are efficient and scalable. The Senior Data Engineer will ensure a smooth data release and deployment process throughout the data development lifecycle.
Successful candidate will be a highly organized, self-starter that can roll up their sleeves and solve problems efficiently. Ability to document procedures, manage time, and have a knack for optimizing data flows.
Location: Atlanta HQ; Raleigh, NC; Nashville, TN; Philadelphia, PA; Remote considered
Reports to: Manager, Data Engineering
As a member of the Data team at Trella, you will:
Champion a comprehensive data vision and operating principles that improve data quality, consistency, accessibility, decision making, use, and stewardship.
Manage data pipeline by developing processes for data input, transformation, and storage
Act as an interface between the Data Science team and Application Developers
Architect and implement front-end (application layer) and back-end (ETL layer) database technologies in a highly customized data environment
Develop automated processes for moving data between systems
Implement ETL processes that transform source data into functional data stores consumed by front-end applications
Scale data pipeline by utilizing the most efficient tools and technologies
This job might be a fit for you if you have:
Bachelor's degree in Computer Science, Engineering, Mathematics, or other technical discipline (proven professional experience will be considered in lieu of a technical degree)
2+ full years of recent advanced Snowflake development experience
Experience manipulating healthcare data
In-depth knowledge of Amazon Web Services (AWS) and Platform as a Service (PAAS)
Demonstrable experience with Snowflake utilities including SnowSQL & Snowflake Javascript
Analyze all aspects of the Snowflake Query Engine to improve bottlenecks that may exist
Ability to automate data ingestion from Amazon S3, FTP, and other technologies
Experience with scripting languages, extensive Python experience, Node.js experience a plus
Working knowledge of Bitbucket and Git
Experience managing and loading data in AWS RDS (MySQL/MariaDb)
Ability to optimize MySQL query execution plans for robust performance
Technical Job Requirements (Nice to Have)
Experience manipulating healthcare data including claims, ADT data
Foundational knowledge of CI/CD platforms, preferably TeamCity
Development and consumption of API services
Experience with data science tools (Jupyter, SageMaker, R)
Understanding of Atlassian suite of tools, including Jira and Confluence
Soft Skill Job Requirements (Nice to Have)
Strong analytical skills and ability to rapidly switch contexts
Excellent communication and problem-solving skills
Sole contributor that is willing to independently uncover and improve processes that are inefficient or sub-optimal
Track record of implementing creative solutions to complex problems
Experience working in a nimble, fast-paced, start-up environment
Knowledge of HIPPA, PHI, PII policies
About Trella Health:
Trella Health provides unmatched, actionable market intelligence to post-acute care and value-based care providers of all sizes. Our industry leading analytics paired with CRM and EHR integration workflows positions us as the most advanced sales enablement platform for the post-acute care market. One of only a few companies to be deemed both a Qualified Entity by CMS and an Innovator under its Virtual Research Data Center Program, Trella has elite access to billions of performance metrics and referral data to enable competitive positioning across the market. Its standardized insights, representing 90% of all lives 65+ U.S. population, help customers identify, engage, and manage critical relationships and advance their organizations with certainty.
Since launching our first product in 2016, we have experienced tremendous growth and have highlighted a few key data points to get an at-a-glance view of Trella:
Over 700 customers including 10 of the top 10 post-acute providers and 4 of the top 10 health systems
Over 120 team members and growing
Recognized as one of Modern Healthcare's ""Best Places to Work"" in consecutive years and were ranked in the Top 50 startups in the US by LinkedIn
Headquartered in Atlanta, GA with remote hubs in Nashville, Philadelphia, Raleigh, and the Philippines
The Trellavator Experience:
At Trella Health, you can expect to join a welcoming team that truly embodies our core values. Our collaborative culture is anchored by trust, transparency, and inclusion — and we also have a lot of fun. No matter where you work, at Trella Health you can expect an awesome team, frequent virtual gatherings, engaging events, casual attire, and more.
We offer competitive salaries with a comprehensive benefits package to all employees and provide an environment that fosters work-life harmonization with unlimited PTO, company-observed holidays, and Summer Fridays. As we continue to see exponential growth, our goal is to continue to put team members first and strive to offer our team members the best culture and benefits possible. Some of the benefits we provide are:
Health, Dental, Vision & Voluntary Benefits
Competitive Salary & Bonus Plans
401k Retirement Savings
Unlimited PTO & 10 Paid Holidays
Flexible Work Hours
Equity Shares
Paid Leave Programs
Marketplace for discounted retail and entertainment
Equal Opportunity Employer
Trella Health is an equal opportunity employer. All persons will receive consideration for employment without regard to race, color, gender, sexual orientation, gender identity or expression, religion, national origin, marital status, age, disability, handicap, veteran status, genetic information, or any other protected status as recognized by federal, state, or local laws.","$113,846 /yr (est.)",51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2015,Unknown / Non-Applicable
Siri info solutions inc,#N/A,"Beaverton, OR",Sr. Data Engineer,"Must Have Skills
PySpark
Python on AWS
Responsibilities:
Understanding of advanced data warehousing concepts, data modeling and extract, transform and load development
Advanced SQL skills with experience querying large datasets from multiple sources and developing automated reporting
Python skills for scripting, data manipulation, custom extract, transform and loads and statistical/regression analysis particularly, as they apply to sales and marketing operations and performance
Job Type: Full-time
Salary: $47.00 - $50.00 per hour
Experience level:
7 years
Work Location: On the road",$48.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Egrove Systems Corporation
4.4",4.4,"Dallas, TX",Data Engineer Flink,"Java, Bigdata, Azure & Flink
Flink Sr. Developer, who has implemented and dealt with failure scenarios of processing data through Flink.
A senior who can help his Flink development team, guiding and helping them implement custom solutions through Flink.
Seasoned Java developer who knows about all aspects of SDLC.
Worked on integrations of other technologies with Flink, eg: Kafka, MongoDB, etc
Azure experience in the areas of Messaging, Data processing, preferably on Flink or on Databricks.
Job Type: Contract
Salary: From $51.83 per hour
Compensation package:
Monthly bonus
Experience level:
11+ years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
Work Location: On the road
Speak with the employer
+91 732)-860-7733",$51.83 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Internet & Web Services,#N/A,Unknown / Non-Applicable
Proits Hub LLC,#N/A,Remote,Data Analytics Engineer,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote
Speak with the employer
+91 9417599933",$60.00 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Merkle
3.7",3.7,"Norristown, PA",Data Engineer,"Company Description

We are Dentsu International. We innovate the way brands are built. That means we do things differently so they’re better than before. In this way, we make our clients’ most important marketing assets—their brands—win in a changing world.
Dentsu International is a modern marketing solutions company. Our mission is to help clients navigate, progress, and thrive in a world of change. Businesses rely on our integrated network of agencies and specialized practices to champion meaningful progress through creative, media, commerce, data, and technology. Part of Dentsu Group, our global network comprises 66,000 diverse people in 143 countries, who are dedicated to teaming for growth and good. Follow us on Twitter @DentsuUSA and visit dentsu.com/us.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.

Job Description
Develop onboarding process to receive and process information related to onboarding a new subscriber
Convert Snowflake Stored Procedures to Python Scripts directly in Airflow
Develop metadata driven solutions for managing data processing and cadence
End to end automation development and testing
Building and maintaining stored procedures and scripts in Snowflake Data Cloud
Building and maintaining complex SQL queries for ETL/ELT processing
Develop Dashboard for data processed for audit and governance needs
Follow Agile methodologies to deliver iterative feature sets rapidly
Maintain codebase using source control methodologies, Git
Collaborate with business analysts, data analysts, and other developers to deliver powerful new features
Coordinate with teams as part of a larger data-sharing system
Contribute to a collaborative, positive, and enjoyable environment for your operations team
Performing quality assurance and testing
Writing user and technical documentation such as runbooks

Qualifications
Strong communication skills both verbal and written
Strong understanding of conditional programming logic
Knowledge of software engineering and development methodologies, techniques, and tools; includes Issue Tracking (JIRA) Software Development Lifecycle SDLC), and Continuous Integration / Continuous Development (CI/CD)
Experience working with large databases and data warehouses utilizing both relational and dimensional design concepts
Eagerness to learn new technologies, programming languages and platforms to solve complex data challenges
Experience with Python required
Experience with SQL required
Experience with Snowflake Data Cloud is highly desirable
Experience with Amazon Web Services (AWS) a plus
Experience with Airflow (or other orchestration) a plus
Experience in database marketing is a plus

Additional Information

Perks:
Great compensation package
Comprehensive healthcare plans
401(k) with employer match
Flexible time-off
16 weeks paid parental leave
Additional Information
The anticipated salary range for this position is $94,000-$146,000. Salary is based on a range of factors that include relevant experience, knowledge, skills, other job-related qualifications, and geography. A range of medical, dental, vision, 401(k) matching, paid time off, and/or other benefits also are available. For more information regarding dentsu benefits, please visit dentsubenefitsplus.com
Employees from diverse or underrepresented backgrounds encouraged to apply.
Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact recruiting@dentsuaegis.com if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.
About dentsu
Dentsu is the network designed for what’s next, helping clients predict and plan for disruptive future opportunities in the sustainable economy. Taking a people-centered approach to business transformation, dentsu combines Japanese innovation with a diverse, global perspective to drive client growth and to shape society www.dentsu.com.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.
Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact your recruiter if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.","$120,000 /yr (est.)",5001 to 10000 Employees,Company - Private,Media & Communication,Advertising & Public Relations,1971,$500 million to $1 billion (USD)
"DiNi Communications, Inc.
3.0",3.0,"New York, NY",Data Center Engineer,"DiNi Communications, Inc. is seeking a Data Center Engineer. This person needs to be a team player with exceptional communication skills. Maintaining the confidentiality of the client's information is of utmost importance.
Required Education and Experience
· At least 8 years of experience as a large data center technician
· Knowledge of Networking,
· Ability to lift equipment at least (50lbs) Rack and Stack (Network Switches, Servers, Storage,
Appliances)
· Knowledge of structured Cabling in a Data Center.
· General knowledge of Operating Systems, (Windows, Unix).
· General knowledge of networking in N Tier Environment (App, Web, DMZ,).
· Level 1 and Level 2 troubleshooting in a N Tier Environment.
· Ability to follow and execute commands to start up and shutdown systems.
· Physically travel to and be available at site B to perform technical changes that require a
person at Site B.
· Performing backup and restore activities.
· Asset Management.
· Maintaining document.
· Work with vendor on circuit issues, (WAN and Internet).
· Cable Patch the equipment in the environment.
· Participate in Site B fall-over testing
Preferred Network Engineering Experience
· Knowledge of N Tier networking.
· Certification in Networking (CISCO).
· Certification in various Operating systems (Linux and Windows), Storage, Messaging
(Microsoft).
· Certification in IT Security.
· Knowledge of Databases.
· Experience working with CISCO, EMC, Symantec, HP, Oracle, Bluecoat, MacAfee, and other vendors' equipment and software in a networked environment.
Responsibilities include but are not limited to:
Adding equipment to support agency applications in site B Environment,
Coordinating Site B testing with Site A technical resources.
Monitoring the B environment.
Rack and Stack new and replacement equipment
Troubleshooting and resolving incidents and problems in the
Physically travel to and be available at Site B to perform technical changes that require a person at Site B.
Upgrading and expanding the environment
Performing backup and restore activities
Manage Site B assets
Maintaining document
Cable Patch the equipment in the environment
Participate in Site B fall-over testing.
Job Type: Full-time
Pay: $60.00 - $70.00 per hour
Schedule:
Monday to Friday
Experience:
large data center technician: 8 years (Required)
Networking in N Tier Environment (App, Web, DMZ,): 3 years (Required)
Ability to lift equipment at least (50lbs): 1 year (Required)
structured Cabling in a Data Center: 3 years (Required)
Level 1 and 2 troubleshooting in a N Tier Environment: 3 years (Required)
Performing backup and restore activities: 3 years (Required)
Cable Patch the equipment: 3 years (Required)
Certification in Networking (CISCO): 1 year (Preferred)
Certification in IT Security: 3 years (Preferred)
CISCO, EMC, Symantec, HP, Oracle, Bluecoat, MacAfee: 3 years (Preferred)
Work Location: In person",$65.00 /hr (est.),Unknown,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"Cureatr
4.1",4.1,Remote,Data Engineer,"About Us
We are a thriving, medication management solutions company, headquartered in Manhattan, but largely virtually distributed, committed to tackling the $528B suboptimal medication management problem. Patient medication information is often incomplete for healthcare providers and systems – it’s our mission to fill in the gaps. We provide real-time, universal access to accurate medication data for over 265 million patients and high-quality interventions by board certified telepharmacists. Through our solutions, healthcare providers can reduce preventable hospitalizations, avoid readmissions, and ultimately, improve patient outcomes. We’re already used by dozens of hospital systems, with tens of thousands of clinicians and supporting staff using us to care for millions of patients.
Here at Cureatr, we are driven by a core set of values and strive to incorporate them into everything we do. We care deeply about our users, patients, and employees, and we aim to live and operate compassionately. We take pride in creating a diverse, inclusive, and equitable company culture, where our open and honest approach to work fosters innovation and encourages employees to achieve professional and personal growth.

The Role
We’re looking for a Data Engineer to join our growing Engineering team. Use the power of healthcare data to find insights that will demonstrate the value Cureatr is delivering to our customers, help improve patient outcomes, and expand the scope of Cureatr’s mission. As a Data Engineer at Cureatr, you will be responsible for designing, implementing and maintaining our data infrastructure, pipelines, automation, and ETL processes. You will work closely with our Data Science and Analytics teams to ensure that our data is clean, accurate, and readily available for analysis. You will also be responsible for ensuring that our data meets industry standards for security, privacy, and compliance.
This is a remote, work from home position, but candidates must be located within the U.S. Ideal candidates must be able to travel to our New York City office, or TBD company-wide offsite locations, up to 4x per year.

You’ll be part of a team working to solve problems like:
Increasing the number of patients who receive the opportunity to do a medication reconciliation, which can help keep them out of the hospital.
Streamlining and automating processes so that our pharmacists can serve as many patients as possible.
Predicting patient events likely to occur so that our clinical team can adequately prevent them before it’s too late.
Helping our pharmacists make the most of data and insights that will help them be as effective as possible in serving patients.
Evaluating the effectiveness of our clinical services impact at the population level.

What You’ll Do:
Design, implement, and maintain our data infrastructure, pipelines, automation, and ETL processes to support our data needs.
Build and maintain scalable data models and databases.
Collaborate with Data Science and Analytics teams to understand their data needs and provide data solutions accordingly.
Ensure data quality and integrity by implementing data validation and cleansing processes.
Ensure data security and compliance with healthcare regulations.
Monitor and optimize data performance and scalability.
Design and implement data governance policies and procedures.
Evaluate and implement new data technologies and tools to improve our data capabilities.

Your Skills:
3+ years of experience as a Data Engineer or similar role.
Strong programming skills in languages such as Python and SQL.
Experience using git to track changes.
Experience with data modeling and database design.
Experience with the “modern data stack” preferred
Experience with ETL and data pipeline development.
Knowledge of healthcare data and related standards (e.g., HL7, FHIR) is a plus.
Familiarity with AWS.
Strong problem-solving and analytical skills.
Excellent communication and collaboration skills.

Technology Stack:
Python
RedShift
dbt
Dagster
MongoDB
Looker
AWS

Educational Requirement: Bachelor’s Degree or relevant work experience

Salary: $110,000 - $140,000 plus full benefits

Why work with us?
Make a measurable impact on patients’ lives
Be part of a company dedicated to living its core values:
Empathy
Honesty
Integrity
Impact
Fun
Equity in Healthcare
Work in the company without micromanagement
Share your vision and make an impact on the product
Opportunities for career growth
Remote work
Competitive salary
Premium benefits (Health, Dental, Vision, 401k, unlimited PTO, more)

Equal Employment Opportunity Policy
Cureatr is an equal opportunity/affirmative action employer. All qualified applicants will receive consideration for employment without regard to sex, pregnancy, race, religion or religious creed, color, gender, gender identity, gender expression, national origin, ancestry, physical or mental disability, medical condition, genetic information, marital status, registered domestic partner status, age, sexual orientation, military or veteran status, protected veteran status, or any other basis protected by federal, state, local law, ordinance, or regulation and will not be discriminated against on these bases
Disclaimer: we are aware of a scam targeting applicants using a cureatrcareers.com email address. Please block and report this sender, as they are not affiliated with Cureatr. If you have any questions, please contact us at jobs@cureatr.com. Thank you!
uBajh3OfeT","$125,000 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2012,Less than $1 million (USD)
Fracsys Inc,#N/A,"Washington, DC",Data Engineer,"Fracsys Inc is hiring a Data Engineer position. The ideal candidate must have at least 12+ years of industry experience. He or she must be responsible for successful technical delivery of Data Warehousing and Business Intelligence projects using a formal project management methodology.
Responsibilities
Directs, motivates and develops key technical personnel
Provides technical direction across the full lifecycle system development process
Provides strategic planning and comprehensive senior level technical consulting to IT senior management and senior technical staff
Working independently within guidelines
Provides detailed inputs for plans and schedules, including goals, risks, and resource allocation
Conduct diagnostic, troubleshooting steps and present conclusions to a varied audience
comfortable slinging some code to solve a problem
Build Data Marts and Data warehouses to facilitate analytical capabilities across the varied sources of data
Design and implement data patterns on cloud using cloud Data warehouse and data virtualization tools
Evaluate new tools and technologies for target state on cloud
Learn and implement new ETL, BI tools and data warehousing technologies
Qualifications
Minimum twelve (12) years in a Technical Lead or Consulting role leading Information Management related initiatives (system integration, data warehouse build, data mart build, or similar)
MUST have proven leadership experience for team of at least 3 technical resources.
Experience creating Data Marts, Data warehouses on-premise and on cloud for real time and batch processing frameworks
Experience building and optimizing big-data data pipelines and data sets including Postgres, AWS Relational Data Service
extensive hands-on experience building and optimizing data structures for data analytics, data science and business intelligence
Experience building self-service data consumption patterns and knowledge of cloud-based data Lake platforms
Experience wrangling data (structured and unstructured), in-depth knowledge of database architecture
Experience utilizing or leading implementations leveraging ETL tools (Informatica / Talend), BI Reporting tools such as MicroStrategy, Microsoft Power BI, data modeling tools such as Erwin, Oracle, SQL server, NoSQL, JDBC, UNIX shell scripting, PERL and JAVA, XML/JSON files, SAS , Python, AWS cloud-native technologies, S3, Athena, Redshift
Familiarity with the following technologies: Hadoop, Kafka, Airflow, Hive, Presto, Athena, S3, Aurora, EMR, Spark
Ability to drive, contribute to, and communicate solutions to technical product challenges
Ability to roll-up your sleeves and work in process or resource gaps and fill it in yourself
Excellent written and oral communication skills in English
Education
Bachelor’s degree in computer science, Data Analytics, Information Systems, or a related degree or equivalent experience. Master’s degree is preferred.
Job Type: Full-time
Pay: From $96,336.72 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Experience level:
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Washington, DC: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person","$96,337 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Kastech Software Solutions Group
4.0",4.0,"Dallas, TX",Data Engineer,"In KastechSSG we are are seeking for a Cloud DB Platform Engineer with Data Enablement Services experience ($50/hour), to work in a Hybrid Schedule (3 days on-site, 2 days home office) for one of our partner companies!
Hybrid Schedule info: onsite 3 days per week from list of locations: Charlotte NC, Dallas TX, Chandler AZ, Newark, NJ - Must be local to one of these locations.
** Contract would be $50 per hour on W2 (18 months). American Citizens or GC Holders Only**
3-5 years experience in:
Terraform, Azure/GCP, DevOps Engineering, ETL, Client/AI, Data lake, Data architecture.
GCP Key Services Experience (ie Big Query, Composer, Vertex AI, DataProc, Data Catalog, Notebooks).
Azure Key Service Experience (ie Synapse, Purview, Data Lake, Logic Apps, Cognitive Services).
Job Type: Full-time
Salary: $50.00 per hour
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you American? If not, please specify your visa type
Would you be willing to work under W2?
Experience:
Terraform: 3 years (Preferred)
Azure Data Lake: 3 years (Preferred)
GCP Key Service Exp: 3 years (Preferred)
DevOps: 3 years (Preferred)
Work Location: In person",$50.00 /hr (est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2008,$25 to $100 million (USD)
DocsInk LLC,#N/A,"Wrightsville Beach, NC",Data Engineer,"DocsInk is on a mission to provide a collaborative and exciting alternative to the traditional healthcare model. Through the complete virtualization of healthcare organizations, we are equipping providers with the ability to deliver better patient care with speed, efficiency, and accuracy. With the widespread focus on providing value-based care, DocsInk is making it possible to provide superior care while also maximizing reimbursements. In today’s environment, shifting the way we approach healthcare is essential and DocsInk is on the cutting edge of where the industry is headed. We are excited to be growing our team and are looking for driven individuals with a passion for making a difference in the healthcare industry.

As our Data Engineer, you will be in the driver’s seat as our team creates elegant and forward-thinking software solutions for our healthcare customers. You will work with fellow developers, designers, training & support, quality assurance, and management in a fast-paced environment to take our products to the next level and exceed expectations. We are looking for someone who is incredibly talented to do what most cannot and yet humble enough to easily work with peers, accept guidance from more experienced staff, and mentor more junior staff. If designing and developing complex integration efforts is what gets you out of bed each morning, then we want you to help us reshape healthcare.

People are our most important asset and the number one reason we all love working at DocsInk. As a team, we value accountability, transparency and collaboration. We have agile teams, with clear, outcomes-focused goals. Delivering innovative solutions and superior service are at the forefront of what we do, and the collective ideas of the team allow us to continue to excel in a fun and energetic environment.

Professional Requirements and Responsibilities:

Role / Responsibilities:

Implement robust ETL solutions that integrate healthcare data feeds using tools such as Apache NiFi, NextGen Connect (JavaScript), PostgreSQL, MySQL, and Linux shell scripting
Configuring and managing deployments to AWS infrastructure consisting of both traditional servers and serverless components. CI/CD tools include Bitbucket and AWS Code Deploy
Building direct interfaces with hospitals and EMR vendors utilizing industry standards such as HL7 2.x, C-CDA, and FHIR as well as implementing interfaces that consume formats such as XML, JSON, and delimited files
Utilizing APIs and web services to permit bi-directional flows of data to and from our partner organizations
Monitoring operational ETL processes, triaging and resolving issues as they arise to meet SLAs
Writing PL/SQL functions and packages to automate workflows in a Postgres environment linked with web technologies
Maintaining data standards (including adherence to HIPAA), documenting work, and championing process improvements
Work as a project team member to jointly deliver high-quality business solutions consistent with project objectives and constraints

Knowledge and Skills Required:

4+ years experience with HL7 2.x
2+ years experience with Apache Nifi
2+ years experience with NextGen Connect
Advanced knowledge of PostgreSQL, MySQL, AWS Code Deploy, HL7 2.x, C-CDA, FHIR, and HIPAA data compliance

Qualifications and Other Attributes Required:

Degree in Computer Science or equivalent field, or equivalent work experience
Highly self-motivated team player who takes initiative
Strong sense of personal accountability; contributes beyond job role and responsibilities
Excellent problem-solving skills; meticulous & methodical","$85,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Loyal
3.9",3.9,"Atlanta, GA",Senior Cloud Data Operations Engineer,"Loyal is an organization centered on experience and building a platform that allows consumers to make meaningful decisions when it comes to healthcare. We deeply understand providers, locations, services, appointments, business rules, and moreover, we understand patients - who they are, the preferred method of communication, upcoming appointments, lapsed appointments, outstanding bills, health risks, and more. With this intelligence, our platform fuels highly relevant and personalized experiences across all mediums (website, email, voice…) allowing patients to get healthy, stay healthy, and have a better relationship with the health care provider.
**This is a remote role**
Summary
The Senior Cloud Data Operations Engineer is responsible for delivering Loyal's cloud-based data warehouse. They interface with groups across the organization to ensure we are collecting and storing data in a way that makes it easy to create and consume. They identify best practices for big data management, select the right tooling, create a workable data architecture, and drive an implementation roadmap others can depend on. The Senior Cloud Data Operations Engineer also works closely with our security group to ensure data security and privacy. They also mentor others on the team, providing technical leadership to those they work with.
You will
Create data streams and consolidate data sources into a single cloud-based data warehouse
Architect and catalog data warehouse storage schema
Work with product teams to enable easy data aggregation
Work with data consumers (analysts, data scientists, and AI/ML engineers) to ensure data is easily accessible and consumable
Create and execute a roadmap that meets data storage and availability goals
Ensure data is secure through data analysis, access control, and encryption
You have
Bachelor's degree in computer science, information systems, applied math or related field, or equivalent work experience
Minimum 5 years of experience working with cloud data processing services (AWS, Azure, or GCP).
Minimum 3 year experience with Big Data technologies, such as Databricks, Snowflake, Spark, Hadoop, etc.
Strong experience in creating data architecture and storage schemas required
Significant working knowledge of SQL and other query languages and tools
Significant experience with building relational database systems strongly preferred
Experience with ETL tools a plus
Knowledge of Python preferred
Software development, data analytics, or data science experience is a plus.
Experience working within a software as a service (SaaS) company preferred.
Experience working within a high growth and/or ambiguous environment, with proven experience to be dynamic preferred.
#LI-REMOTE
We know that potential candidates are often less likely to apply to a position if they don't match 100% of the job qualifications. Don't let that be why you miss out on this opportunity! We encourage you to apply if you can demonstrate many of these skills and competencies.
Loyal to our employees
We are a remote-friendly company! We encourage you to apply from anywhere in the United States. We also believe in a work/life balance that fulfills you while you're here and supports you when you're not. We built our benefits package to prove that we're committed to you having everything you need (including a little fun). Here is what we offer full-time employees:
Flexible paid time off, sick and personal days
At least one holiday per month (sometimes, more!)
Full health, dental, and vision insurance - Loyal pays the premium for all employees!
One Time Home Office Setup Stipend For Remote & Hybrid Roles
Monthly Internet Stipend for Remote & Hybrid Roles
Long term & short term disability
401[k] plan
16 Weeks Paid Parental Leave
2 Volunteer days per year
Matching Gift Program
Participation Grant Program
Annual Travel/Team Events up to twice per year (post-COVID)
Our Commitment
We believe that the key to Loyal's success is you. Your unique background, life experience, knowledge, self-expression, and talent make you uniquely you. Who you are, what you have experienced, and how you think inspires us to be innovative and bold.
Loyal is an equal opportunity employer. We hire great people from a wide variety of backgrounds, not just because it's the right thing to do, but because it makes our company stronger. We welcome the unique contributions that you can bring in terms of your education, opinions, culture, ethnicity, race, ancestry, sex, gender identity and expression, national origin, citizenship, marital status, age, languages spoken, veteran status, color, religion, disability, sexual orientation, and beliefs.
We consider qualified applicants regardless of criminal histories, consistent with legal requirements.
Further, consistent with applicable federal and state law, Loyal provides reasonable accommodations when requested by qualified applicants or employees with disabilities, unless doing so would cause an undue hardship. Loyal's policy regarding requests for reasonable accommodation applies to all aspects of employment, including the application process. If you require a reasonable accommodation in connection with the application process, please contact the Talent Acquisition Department at talentacquisition@loyalhealth.com.
E-Verify
This employer participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S. If E-Verify cannot confirm that you are authorized to work, this employer is required to give you written instructions and an opportunity to contact Department of Homeland Security (DHS) or Social Security Administration (SSA) so you can begin to resolve the issue before the employer can take any action against you, including terminating your employment. Employers can only use E-Verify once you have accepted a job offer and completed the I-9 Form.
COVID-19 Vaccinations
Consistent with Loyal policy, candidates performing in-person work will be required to show proof of being fully vaccinated against COVID-19 upon commencing employment. Reasonable accommodations will be considered on a case-by-case basis for exemptions to this requirement in accordance with applicable law. If you require a reasonable accommodation to Loyal's vaccination requirement, please contact the People Department at people@loyalhealth.com.",#N/A,51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2015,$5 to $25 million (USD)
Infosoft,#N/A,"Peachtree Corners, GA",Data Pipeline Engineer - 2173,"Here are the job position details for your review:
Job Title: Data Pipeline Engineer (GStreamer/ DeepStream)
Pay Rate: $90 to 120/Hr.
Duration: 3 Months
Location: Peachtree Corners, GA
This position is for the Building Automation division.
We are looking for Data Pipeline Engineer (GStreamer) to participate in the development of Streaming data pipelines using NVIDIA Deepstream, GStreamer, Python programming, messaging queues, and related tools.
Preference is for candidates located in Atlanta, GA metro area. However, ok with non-local candidates who are open to 75% travel to Atlanta, GA.
Understanding the basics of our architecture/setup to be able to do setup/support for demos, testing, etc.
Basic computer hardware assembly and set-up
Possible set-up/installation of the application(s)
Possible development tasks depending on what the rest of the development team has for them
Develops business software applications, system software, and/or IT solutions on the basis of functional specifications.
Scope of position embraces both the customization or reuse/modification of system software and/or solutions as well as their development from scratch.
Consults with customers in creating functional specifications and work estimates, and supports Make-or-Buy decisions.
Analyses performance gaps of existing software environments and proposes measures to address efficiency problems.
Is also responsible for the implementation of data-driven components supporting data modeling and engineering in the context of data analytics (Data Stewardship).
Skills:
Python
Linux
Docker and Docker Swarm
Zero MQ / a good understanding of messaging
Desired Skills:
NVIDIA Deepstream SDK
Web Sockets
Flask
Protobuf
Possible:
GStreamer (if different than RTSP / H264 / MJPEG)
Job Type: Contract
Salary: $90.00 - $120.00 per hour
Ability to commute/relocate:
Peachtree Corners, GA: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
""Please share your Email ID""
Work Location: One location",$105.00 /hr (est.),Unknown,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Intersec
4.5",4.5,Remote,Senior Data Engineer,"Architect and implement highly scalable data pipeline to streamline migration of data from source data repositories to the centralized data platform
Implement agreed upon data models (e.g., data marts) to enable faster and efficient consumption of data by downstream consumers
Conduct performance tuning to optimize data migration techniques and ensure adherence to identified service-level agreements (SLAs)
Define and implement automated validation checks to ensure data integrity between source and target
Assist in onboarding consumers with existing technology to connect to the platform
Setup predictive alert mechanism for pipeline degradation for IMF pipelines.
Enhance data quality framework for data pipelines and derived datasets
Enhance Data Audit Balance and Control mechanism for data pipelines
Enhance pipelines to be certified copies of authoritative source
Populate metadata for the Enterprise with particular focus on the Data Sources being onboarded to EDP (Technical and Business Glossaries) related to IMF Modernization Programs
Validate correct lineage for datasets
Required:
*8+ years of Databricks, Data Modeling, and/or Metadata management experience
* Familiarity with architecting and developing real and near-real-time solutions
* Bachelor's Degree in Engineering, Mathematics, computer Science, Statistics or other quantitative discipline, or 4 years equivalent professional experience
* US Citizenship and ability to gain a public trust
Preferred:
* Familiarity with IRS systems and ETL pipelines
* Ability to work independently and manage multiple tasks
* IRS MBI Clearance with IRS Laptop and Informatica Installed
Job Type: Full-time
Salary: $70.00 - $75.00 per hour
Schedule:
Monday to Friday
Application Question(s):
Are you a US/GC card holder?
How many years of Databricks, Data Modeling, and/or Metadata management experience do you have?
Work Location: Remote",$72.50 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,Unknown / Non-Applicable
OpenSecrets.org,#N/A,Remote,Snowflake Data Engineer,"About us
Our goal is to is to track the flow of money in American politics and provide the data and analysis to strengthen democracy. Our vision is for Americans to use this knowledge to create a more vibrant, representative and accountable democracy..
Our work environment includes:
Modern office setting
Work-from-home days
Wellness programs
Data Engineer
OpenSecrets is seeking to add a new member to our data team. The Data Engineer will be responsible for helping build and maintain our data systems.
Responsibilities include:
Implementing methods to improve data reliability and quality
Analyzing raw data and implementing that data into our system using data pipelines
Troubleshooting data errors and pipeline issues
REQUIRED QUALIFICATIONS:
Experience working with and analyzing large datasets, equivalent to 2-3 years in a similar position
Proven ability to build complex data pipelines using Snowpipe and SSIS
The ability to work with diverse populations
Experience with MSSQL Server and Snowflake
Advanced knowledge of ANSI SQL
PREFERRED QUALIFICATIONS:
Excellent written and verbal communication skills.
The ability to manage multiple priorities and projects both independently and as a member of a team
Snowflake SnowPro Data Engineer certification
Experience with Apache Spark
Experience with machine learning matching models
Knowledge of and interest in American government and money in politics.
SALARY AND BENEFITS:
We are a fully remote organization and this person can be based anywhere in the U.S.
OpenSecrets offers an excellent benefits package, including full medical, dental, 403(b) match, generous vacation and sick leave, and all federal holidays paid.
TO APPLY:
Please visit this link: https://airtable.com/shrGe2bugeapKVr8a
People of diverse backgrounds are strongly encouraged to apply. OpenSecrets is an equal opportunity employer committed to creating a diverse and inclusive organizational culture. We do not discriminate against candidates and employees because of their disability, sex, race, gender identity, sexual orientation, religion, national origin, age, veteran status, or any other protected status under the law.
ABOUT US: OpenSecrets is the nation's premier research group tracking money in U.S. politics and its effect on elections and public policy. Our mission is to track the flow of money in American politics and provide the data and analysis to strengthen democracy. Our vision is for Americans to use this knowledge to create a more vibrant, representative and accountable democracy.
Job Type: Full-time
Pay: $83,874.82 - $100,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Retirement plan
Vision insurance
Compensation package:
Yearly pay
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Experience:
SQL: 3 years (Required)
Snowflake: 1 year (Preferred)
ETL: 2 years (Required)
Work Location: Remote","$91,937 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Quadrant Resource
4.1",4.1,Remote,Data Engineer,"Job Description:
4-6 years of experience as a Data Engineer in the Snowflake-Fivetran-dbt stack
• Possess a solid understanding of end to end Data ingestion, ETL, data analysis, Reporting processes
• Expert in ETL development using Fivetran and experienced in implementing data pipelines for Snowflake Cloud DWH
•Experienced in using dbt for data transformation
•Expertise on Tableau/Metabase for building reports/dashboards
•Expert in troubleshooting and resolving ETL issues, data load failures/problems and transformation translation problems
•Excellent communicator with the ability to work effectively with distributed teams
Job Type: Full-time
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Data Engineer: 9 years (Required)
Fivetran: 8 years (Required)
DBT: 7 years (Required)
Snowflake: 6 years (Required)
Work Location: Remote",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,2004,Unknown / Non-Applicable
Atalan Tech,#N/A,Remote,Data Engineer,"Atalan Tech is a start-up with a mission to mitigate the impact of clinician burnout and turnover in our nation’s health system. We are expanding our team and looking to hire the critical role of Data Engineer. The Data Engineer will work closely with the Chief Technology Officer, Chief Scientific Officer (UCLA Ph.D.), Chief Product Officer (Oxford Ph.D.), and Chief Executive Officer to deliver actionable insights from data in a variety of situations.
We are a small, close-knit team. This is an excellent opportunity for someone to join a mission-oriented, early-stage company at the ground floor and grow with us. There will be plenty of room for growth and creativity, and opportunities to tackle challenges that present one of the toughest problems in our society.
Requirements
Strong hands-on experience of designing, developing and managing data pipelines
Strong coding skills in Python (required) and SQL (required) and R (strongly preferred)
Experience building and maintaining data pipelines on AWS using EC2, EBS, Redshift, Postgres, S3, Data Pipeline, Kinesis, Lambda, etc.
Thorough understanding and experience with task management tools (Airflow, NiFi, etc.)
Knowledge of data management best practices for ETL, data governance, data warehousing, data modeling and data science
Basic knowledge of Linux (Redhat preferred) administration
Minimum B.S in Computer Science, software engineering or related field (or equivalent in related professional experience)
Strong communication skills
Strong problem-solving/critical thinking skills
Ability to work in a fast-paced, dynamic start-up environment
Some knowledge of healthcare, health tech, or healthcare data analytics (preferred)
Typical Tasks and Responsibilities include:
Working closely with CTO and CSO to build data pipelines for machine learning
Assisting the CPO in managing and supporting analysis of enterprise data to drive better products and solutions
Managing company’s data infrastructure on AWS (Postgres/RDS, Redshift, S3, etc.)
Synthesizing multiple data sources, such as internal medical records and HR data
Integrating with client systems including EHR (Epic, Cerner), HRIS, flat files, etc.
Working closely with other key members on other projects
Developing and designing data security standards and policies in line with industry best practices
About Us:
Atalan Tech’s mission is to improve the wellness of clinicians and their patients. We blend labor economics, machine learning, and clinical psychology with a team from Harvard, Yale, and UCLA. Atalan Tech is looking to contribute innovative and sustainable solutions to one of the most pressing problems in the healthcare industry. Building upon our proof-of-concept success, we have recently secured multiple engagements with nationwide healthcare organizations and have successfully raised institutional funding to expand our reach.
Contact Us:
If you’re interested in this position, please send us your CV/resume and cover letter.
Job Type: Full-time
Pay: $108,448.20 - $130,604.27 per year
Experience level:
2 years
Schedule:
Monday to Friday
Application Question(s):
Do you know the role is full time?
Can you start within the next month?
What are your salary expectations?
Have you held the title of a data engineer? If not, what experience do you have as a data engineer? Please be very specific.
Have you released Python code to a production environment in the last year?
Experience:
Data analytics: 2 years (Required)
Work Location: Remote","$119,526 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"GTA (Global Technology Associates)
4.6",4.6,"San Francisco, CA",Data Communications Engineer - III,"Looking for a Data Communications Engineer - III
What you will be doing as a Data Communications Engineer – III
This member of technical staff will be responsible for managing the standalone IT network and 4G/5G infrastructure and mobile edge cloud environments for a test and development lab program.
This is a technical position focused on supporting the development of technology solutions testing and incubation of new platforms and the product engineering of greenlit projects.
In this role the candidate will work with internal technology and product groups and external partners to support the development of new solutions.
He/She will be responsible for the architecture design and deployment of new 4G/5G and MEC platforms and features and will work very closely with Device testing Product engineering Business groups and peer TPD teams.
The candidate will be a member of a small team of Sr. Network engineers who manage remote lab access configure and manage virtual environments to support projects monitor and troubleshoot problems and make recommendations on how to streamline development with internal and external partners.
What you will bring to the table as a Data Communications Engineer - III
BA in IT/EE or Computer Engineering
The ideal candidate needs to have a strong technical background with experience in wireless and wireline networks hands on integration experience with MEC/Cloud/IT
Experience working in a customer facing environment Expert understanding of computer technologies and architectures including O/S Database NFV Cloud and SDN.
Passion for working with cutting edge technology Ability to work well in a team environment.
What you didn’t know about us:
Competitive salary
Health, Dental and Vision Benefits
Short/Long Term Disability and Critical Care/Illness Protection
Life Insurance and Retirement Plans
Employee Assistance Program
With this position, you will get the opportunity to work with our game changing clients and further advance your already valuable experience in the telecom industry!
We are Connectors. We thrive on ‘quality over quantity’ and put in the work building strong relationships. We create connections, discover qualities, uncover skills, and place people with accuracy. We are your true partner!
We are Collaborators. You’ll be working with a wholly-owned subsidiary of Kelly and part of the Kelly Telecom division. It allows us to be as nimble and fiercely competitive as a startup while having the backing of a multibillion dollar publicly traded company which has been in business for 75 years. With direct access to hiring managers, services don’t stop at standard recruiting processes. We use our expertise to improve your application skills and provide ongoing career support.
We give 24/7 Support. We are in this together. We provide around the clock availability, competitive employee benefits, and continuously check-in to make sure things are going smoothly. Check out our Glassdoor page!
Kelly Telecom is an equal opportunity employer and will consider all applications without regard to race, genetic information, sex, age, color, religion, national origin, veteran status, disability, or any other characteristic protected by law. For more information click Equal Employment Opportunity is the law.
You should know: Your safety matters! Vaccination against COVID-19 may be a requirement for this job in compliance with current client and governmental policies. A recruiter will confirm and share more details with you during the interview process.
#JobsAtKellyTelecom
Job Types: Full-time, Contract
Salary: $91,144.17 - $180,998.90 per year
Work Location: In person","$136,072 /yr (est.)",Unknown,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"FreightWaves, Inc.
3.5",3.5,Remote,Senior Data Engineer,"Are you smart, driven, curious, resourceful, and not afraid to fail? Then we want to meet you! Our team of bold, innovative, and creative teammates is what makes us a top startup to work for. FreightWaves delivers news and commentary as well as data and analytics which empower risk management and actionable market insights in the logistics and supply chain industry. If you are ready to join our team, it is time for YOU to apply!
FreightWaves is on the hunt for a curious, tenacious, and team-oriented Senior Data Engineer to join our fast paced engineering team. The ideal candidate is inquisitive, versatile, team oriented, thrives on change, and has a positive attitude. If you are ready to be challenged, learn new and exciting technologies, and have the unique opportunity to work with some of the most talented developers in the country, we want you to apply!
**This position is fully remote.**
**Must RESIDE in the United States and be eligible to work.**
What you will be doing:
Implementing ingestion pipelines, using Airflow as the orchestration platform, for consuming data from a wide variety of sources (API, SFTP, Cloud Storage Bucket, etc.).
Implementing transformation pipelines using software engineering best practices and tools (DBT)
Working closely with Software Engineering and DevOps to maintain reproducible infrastructure and data that serves both API-only customers and in-house SaaS products
Defining and implementing data ingestion/transformation quality control processes using established frameworks (Pytest, DBT)
Building pipelines that use multiple technologies and cloud environments (for example, an Airflow pipeline pulling a file from an S3 bucket and loading the data into BigQuery)
Create and ensure data automation stability with associated monitoring tools.
Review existing and proposed infrastructure for architectural enhancements that follow both software engineering and data analytics best practices.
Working closely with Data Science and facilitating advanced data analysis (like Machine Learning)
What you bring to the table:
Strong working knowledge of Apache Airflow
Experience supporting a SaaS or DaaS product, bonus points if you were creating new data products/features
Strong in Linux environments and experience in scripting languages
Python Expert
Strong understanding of software best practices and associated tools.
Experience in any major RDBMS (MySQL, Postgres, SQL Server, etc.).
Strong SQL Skills, bonus points for having used both T-SQL and Standard SQL
Experience with NoSQL (Elasticsearch, MongoDB, etc.)
Multi-cloud and/or hybrid-cloud experience
Strong interpersonal skills
Comfortable working directly with data providers, including non-technical individuals
Experience with the following (or transitioning from equivalent platform services):
Cloud Storage
Cloud Pubsub
BigQuery
Apache Airflow
dbt
DataFlow
Bonus knowledge/experience:
Experience implementing cloud architecture changes
Working knowledge of how to build and maintain APIs using Python/FastAPI
Transforming similar data from disparate sources to create canonical data structures
Surfacing data to BI platforms such as Looker Studio
Data Migration experience, especially from one cloud platform to another
Certification: Professional Google Cloud Certified Data Engineer
Our Benefits:
An excellent work environment, flat hierarchies, and short decision paths.
Work from home
A generous benefits package including 100% employer-paid health, dental, vision and Life insurance, STD, LTD
Stock options
Appealing 401k matching plan
Career Mentorship Opportunities
Personal Development Credit (Can be used toward Student loans or relevant PD Courses)
Annual life achievement bonus of $2000 for having a baby, buying a house, or getting married (max one per year)
No set days off Vacation policy (our team takes time off as needed with supervisor approval)
Up to $50 for Gym or Virtual Gym membership.
Audible or Kindle Unlimited subscription
Discount on Ford vehicles
oYXkhYiWkU",#N/A,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2017,Unknown / Non-Applicable
"Titan America
3.8",3.8,"Roanoke, VA",Data Engineer,"Data Engineer
We are seeking a motivated and detail-oriented Data Engineer to join our team based in Roanoke, Virginia. As a Data Engineer, you will play a crucial role in designing, developing and maintaining scalable data pipelines and transformations for business-critical use cases. In addition, you may take on various Software Development and Data Science responsibilities as we frequently share responsibilities and knowledge across our growing team.
Responsibilities:
Design, develop and maintain scalable data pipelines for extracting, transforming, and loading data from various sources
Collaborate with cross-functional teams to identify data needs and determine the best data solutions
Develop and implement data models to support business requirements and ensure data quality
Ensure the security and privacy of sensitive data by implementing appropriate access controls
Monitor and optimize data pipeline performance to ensure timely and accurate data delivery
Document data pipeline processes, data dictionaries, and data storage solutions
Requirements:
Bachelor's degree in Computer Science, Computer Engineering, or a related technical field
Minimum of five years of professional experience working as a Data Engineer or Software Developer
Strong hands-on experience with data warehouse and transformation solutions, i.e. Domo, Snowflake or similar
Proficient in at least one scripting language such as Python, JavaScript, or R
Understanding of data modeling, data integration and data quality processes
Familiarity with cloud platforms such as AWS, Azure, or Google Cloud Platform
Strong analytical and problem solving skills
Full Stack Software Development experience in a professional setting is highly desired, but not required
This is an excellent opportunity for a driven and collaborative individual to make a significant impact in a dynamic and growing team. If you have a passion for data and a desire to work in a fast-paced and dynamic environment, we want to hear from you!
Success begins with hiring the right people to partner with us as we grow and develop our businesses. People are central to everything we do. It is through their efforts and talents that Titan has been successful for over 100 years.
Titan America is an Equal Employment Opportunity (EEO) / Affirmative Action employer and welcomes all qualified applicants. Applicants will receive fair and impartial consideration without regard to race, sex, color, religion, national origin, age, disability, veteran status, genetic data, or other legally protected status.
Job Type: Full-time
Pay: $75,000.00 - $85,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Referral program
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Roanoke, VA 24019: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Do you currently or will you in the future need sponsorship to work in the United States?
This position requires the successful candidate to be in the office in Roanoke, VA at least 3 days a week. Can you meet this requirement?
This position requires the successful candidates to be a full time, W2 employee. Can you meet this requirement?
Education:
Bachelor's (Preferred)
Experience:
Data Engineering: 5 years (Preferred)
Work Location: Hybrid remote in Roanoke, VA 24019","$80,000 /yr (est.)",1001 to 5000 Employees,Company - Public,"Energy, Mining & Utilities",Mining & Metals,1902,Unknown / Non-Applicable
"Appsintegration INC
4.5",4.5,"California, KY",Data Engineer,"Role:- Data Engineer
Position:- Onsite (W2 position)
Experience:- 10+ Years
Responsibilities
Working closely with business teams to understand their requirements and develop reporting or analytic solutions.
Producing and automating the delivery of key metrics and KPIs to the business.
In some cases, this will focus on making data available and others will be developing full reports for end users.
Managing stakeholder demand and a backlog of reporting requirements and needs.
Actively working with users to understand data issues, tracing back data lineage, and helping the business put appropriate data cleansing and quality processes in place.
Managing secure access to data sets for the business.
Requirements
Bachelor's degree in Computer Science or equivalent.
Proficient in creating Data Factory pipelines for on-cloud ETL processing.
Experience implementing data pipelines using the latest technologies.
Experience developing using SQL, relational databases, and data warehousing technologies.
Extensive ETL experience.
Ability to scale systems in Cloud as per requirement.
Experience in handling structured and unstructured datasets.
Good to have experience in data modeling and Advanced SQL techniques.
Must-Have Skills: Azure Data Factory (ADF), SQL, Cloud-based ETL.
Good to Have Skills: Azure File Storage Services, Databricks, Azure Synapse.
you can reach me out on LinkedIn
here is my link :- linkedin.com/in/shravanthi-thakur-705a81231
Job Type: Contract
Salary: $42.00 - $45.00 per hour
Ability to commute/relocate:
California, KY 41007: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",$43.50 /hr (est.),1001 to 5000 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"SECURE RPO
4.3",4.3,Manhattan,Senior data engineer,"Must have skills:
8+ years of experience building high performance scalable enterpris`e analytics or data centric solutions
8+ or 5+ years of experience in the design and build of data extraction, transformation, and loading processes by writing custom data pipelines
At least 5 years of experience implementing complex ETL pipelines preferably in connection with Glue and/or Spark
Exceptional coding and design skills in Python or Java/Scala
Hands-on experience with AWS (i.e. Glue, Aurora Postgres, Lambda, EMR, EKS, Redshift, etc.)
Experience with visualization tools like QuickSight, PowerBI, Looker or Tableau
Experience with Talend (ETL) is a big plus
Roles and Responsibility:
Drive a high impact and high visibility project that enables data availability, encompasses data analytics, machine learning, and petabyte scale datasets, and provides reliable and timely access to thousands of data sources
Design, architect and support systems for collecting, storing, and analyzing data at scale
Recommend improvements and modifications on new and existing data and ETL pipelines. Create optimal data pipeline architecture and systems using Apache Airflow
Create data analytics for d ata scientists to innovate, build and optimize our ecosystem
Assemble large, complex data sets that meet functional and non-functional business requirements
Be responsible for ingesting data into our data lake and providing frameworks and services for operating on that data including the use of Spark
Analyze, debug and correct issues with data pipelines
Operate on or build solution required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, AWS and Spark technologies
Job Type: Full-time
Salary: $56.80 - $80.16 per hour
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Informatica: 7 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: On the road",$68.48 /hr (est.),1 to 50 Employees,Unknown,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"shaped.ai Inc.
4.0",4.0,"New York, NY",Data Engineer,"Hello! We’re building the future of content discovery infrastructure with AI — come join us!

Shaped (YC W22) is an API for developers to seamlessly add personalized ranking and recommendation to their products. These frictionless discovery experiences help end-users find what they want faster, and, consequently, grow conversion and engagement business metrics. Behind the scenes, we’re building state-of-the-art ML infrastructure for training and deploying large AI models to power our personalization engine.
We’re a seed stage start-up backed by top-tier investors (e.g. Y-Combinator, Susa Ventures, Tribe Capital etc…) and executives (e.g. Google, Amazon, Uber, Dropbox etc…). Our team comes from Meta, Google, Apple and Uber. We’re a remote team but have a small office in Brooklyn, New York.
We’ve been lucky to have a huge amount of success since our initial launch earlier this year and now looking to expand our real-time data engineering capabilities. We are looking for a data engineer to design, build and optimize Shaped's real-time streaming data ingestion endpoints. You will be a founding engineer working on state-of-the-art infrastructure. As one of Shaped’s early employees you will help shape our product, culture and vision.
This role is for a full-time data engineer at $120k - $200k a year and 0.1 - 0.5% equity.

Responsibilities:
Manage the unification of our batch and real-time data processing pipelines.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Requirements:
Bachelor's in computer science, data science or mathematics related field. Master's degree or PhD will be advantageous.
5+ years of data engineering experience, ideally applied to machine learning use-cases.
Experience with AWS or common cloud technologies
Proficient with Python, SQL.
Extensive knowledge of both offline and online large scale data processing frameworks: Spark, Beam, Flink.
Extensive knowledge with stream technologies like Kakfa or Kinesis.
Excellent written and verbal communication skills.
We’re excited to work with you. Come build the future of AI and discovery with us!","$160,000 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Redzara.us,#N/A,"Atlanta, GA",Azure Data Engineer,"Responsible for developing new and modernizing existing applications to Azure Cloud Platform.
The ideal candidate must demonstrate in-depth knowledge and understanding of Database concepts.
Requirements:
7+ years of experience in Data Warehousing and Business Intelligence
5+ Years in BI development leveraging the Microsoft SQL Server. Needs to be an expert in T-SQL
5+ Years in SSIS and use of Stored Procedures.
1-2 Years in Azure Data Factory, SQL VMs, build and maintain ETL
Ingest and aggregate data from both internal and external data sources to build our data lake
Strong understanding data modeling (i.e., conceptual, logical, and physical model design, experience with Operation Data Stores, Enterprise Data warehouses and Data Marts).
Job Type: Contract
Pay: $43.49 - $57.00 per hour
Compensation package:
1099 contract
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Atlanta, GA 30334: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure Data Factory: 3 years (Required)
Azure: 3 years (Required)
Data warehouse: 1 year (Preferred)
Work Location: Hybrid remote in Atlanta, GA 30334",$50.25 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Tipico - North America
4.0",4.0,"Denver, CO",Data Engineer,"Company Description

Tipico is an energetic, passionate, bold tech company, and we love sports! Tipico has recently expanded to the US market, establishing offices in Hoboken, New Jersey and Denver, Colorado. We are driven by our core values: passion, progress and trust. Our goal is to build the best possible mobile sports betting product in the industry, in order to amplify the emotions of millions of sports fans every day. Let’s Make This Interesting ™
Please note: this role is located in our Denver, CO office; we work off a hybrid model and come into the office 2 days per week.

Job Description

The Data Engineer will report directly to the Data Engineering Manager and work together with rest of the data engineers, BI analysts, and product teams. The position holder ensures this is done by doing the following:
Create and maintain both batch (ETL/ELT) and real-time data pipelines and architecture
Ensure all data provided by Data Platform is of the highest quality and is delivered in a timely manner and in line with agreed SLAs
Assemble large and complex data sets that meet functional and non-functional business requirements
Identify, design, and implement internal process improvements, including automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies
Collaborate with stakeholders including data architects, Executive, Product, Data and IT team members from the beginning to the delivery of a project
Build a scalable, modern, and trusted data ecosystem, making sure it is responsive, available and in line with business requirements
Take part in sprints, workloads, and deliverables through agile methodologies
Manage quality assurance, verification of accuracy and consistency of data, specifically KPI measures sent to the business
Thrive working in a fast-paced and challenging environment
Open to continuous learning individually and with the team
Comfortable with being challenged / seeking challenge
Willing to work flexible hours if needed

Qualifications
3+ years in Data Platforms / Datawarehouse / BI experience
Must have experience in successfully implementing data-centric applications, such as data warehouses, operational data stores, and data integration projects
Strong analytical and problem-solving skills that balance with creative approaches
Ability to learn new and complex concepts quickly
Strong experience with large-scale production databases and SQL
Solid understanding of cloud data services: AWS services such as S3, Athena, EC2, RedShift, EMR, EKS, RDS and Lambda
Proficient with one or more big data & new data technologies such as DBT, NIFI, Airflow, Kafka
Experience with data modelling
Experience with ETL development (extractions, data load, aggregation, transformation)
Knowledge in working with time-series/analytics databases such as Elasticsearch
Understanding of containerization and orchestration technologies like Docker/Kubernetes
Must be able to take ownership of projects with a high level of attention to detail and priority
Team player with a hands-on approach and attitude
Must be comfortable with on call rotation
Nice to have:
Familiarity with Marketing tooling and/or CRM systems is preferred
Familiarity with using business intelligence tools such as Domo, Tableau or Looker
Knowledge about and / or experience with sports betting
Background in digital companies, online, gambling, gaming, e-commerce, marketing, or mobile apps
Familiarity with one scripting language, preferably Python

Additional Information

What's in it for you:
Work in an environment where you, your work and ideas matter and have an impact.
You will be a be part of the newest and up and coming USA online iGaming and sports betting companies in the market.
Work in a new and young business with high growth potential.
Build your own success story together with us.
Work with self-organized, self-responsible and entrepreneurial employees.
Start-up feeling, backed-up by a leading European sports betting house. We are a high-volume business and are taking off in the US!
Competitive salary, Medical, Vision, Dental Benefits, Unlimited PTO, and more
Salary for this position is $120k+ & annual performance bonus","$120,000 /yr (est.)",201 to 500 Employees,Company - Private,"Arts, Entertainment & Recreation",Gambling,2004,Unknown / Non-Applicable
"Trella Health
4.8",4.8,"Philadelphia, PA",Senior Data Engineer,"At Trella Health, we are passionate and committed to our mission – empowering meaningful change in healthcare. Since our founding in 2015, we continue to grow our team, enhance our solution and services offerings, accelerate into new markets, and expand our customer base. We are rapidly growing and are looking for new Trellavators to join our team!
""What is a Trellavator?"" you ask. Innovate and elevate is the name of our game! We go above and beyond to collaborate with and support each other – we believe that when a colleague or customers succeeds, we succeed. By learning from others, building on our successes, and taking risks, we constantly raise the bar – continuous improvement is in our DNA. Our word is paramount, we keep our commitments, and we always follow through. We have a strong, reliable support system that fuels growth, collaboration, and passion – and together, we create a positive environment where everyone at Trella Health, including the customers we support, can thrive. Are you ready to learn more about the opportunities with our team? Trell-yeah you are!
Position Overview:
The Senior Data Engineer will be responsible for designing, implementing, and maintaining an effective data pipeline for Trella Health's applications. This position is responsible for turning data concepts into effective data models that are efficient and scalable. The Senior Data Engineer will ensure a smooth data release and deployment process throughout the data development lifecycle.
Successful candidate will be a highly organized, self-starter that can roll up their sleeves and solve problems efficiently. Ability to document procedures, manage time, and have a knack for optimizing data flows.
Location: Atlanta HQ; Raleigh, NC; Nashville, TN; Philadelphia, PA; Remote considered
Reports to: Manager, Data Engineering
As a member of the Data team at Trella, you will:
Champion a comprehensive data vision and operating principles that improve data quality, consistency, accessibility, decision making, use, and stewardship.
Manage data pipeline by developing processes for data input, transformation, and storage
Act as an interface between the Data Science team and Application Developers
Architect and implement front-end (application layer) and back-end (ETL layer) database technologies in a highly customized data environment
Develop automated processes for moving data between systems
Implement ETL processes that transform source data into functional data stores consumed by front-end applications
Scale data pipeline by utilizing the most efficient tools and technologies
This job might be a fit for you if you have:
Bachelor's degree in Computer Science, Engineering, Mathematics, or other technical discipline (proven professional experience will be considered in lieu of a technical degree)
2+ full years of recent advanced Snowflake development experience
Experience manipulating healthcare data
In-depth knowledge of Amazon Web Services (AWS) and Platform as a Service (PAAS)
Demonstrable experience with Snowflake utilities including SnowSQL & Snowflake Javascript
Analyze all aspects of the Snowflake Query Engine to improve bottlenecks that may exist
Ability to automate data ingestion from Amazon S3, FTP, and other technologies
Experience with scripting languages, extensive Python experience, Node.js experience a plus
Working knowledge of Bitbucket and Git
Experience managing and loading data in AWS RDS (MySQL/MariaDb)
Ability to optimize MySQL query execution plans for robust performance
Technical Job Requirements (Nice to Have)
Experience manipulating healthcare data including claims, ADT data
Foundational knowledge of CI/CD platforms, preferably TeamCity
Development and consumption of API services
Experience with data science tools (Jupyter, SageMaker, R)
Understanding of Atlassian suite of tools, including Jira and Confluence
Soft Skill Job Requirements (Nice to Have)
Strong analytical skills and ability to rapidly switch contexts
Excellent communication and problem-solving skills
Sole contributor that is willing to independently uncover and improve processes that are inefficient or sub-optimal
Track record of implementing creative solutions to complex problems
Experience working in a nimble, fast-paced, start-up environment
Knowledge of HIPPA, PHI, PII policies
About Trella Health:
Trella Health provides unmatched, actionable market intelligence to post-acute care and value-based care providers of all sizes. Our industry leading analytics paired with CRM and EHR integration workflows positions us as the most advanced sales enablement platform for the post-acute care market. One of only a few companies to be deemed both a Qualified Entity by CMS and an Innovator under its Virtual Research Data Center Program, Trella has elite access to billions of performance metrics and referral data to enable competitive positioning across the market. Its standardized insights, representing 90% of all lives 65+ U.S. population, help customers identify, engage, and manage critical relationships and advance their organizations with certainty.
Since launching our first product in 2016, we have experienced tremendous growth and have highlighted a few key data points to get an at-a-glance view of Trella:
Over 700 customers including 10 of the top 10 post-acute providers and 4 of the top 10 health systems
Over 120 team members and growing
Recognized as one of Modern Healthcare's ""Best Places to Work"" in consecutive years and were ranked in the Top 50 startups in the US by LinkedIn
Headquartered in Atlanta, GA with remote hubs in Nashville, Philadelphia, Raleigh, and the Philippines
The Trellavator Experience:
At Trella Health, you can expect to join a welcoming team that truly embodies our core values. Our collaborative culture is anchored by trust, transparency, and inclusion — and we also have a lot of fun. No matter where you work, at Trella Health you can expect an awesome team, frequent virtual gatherings, engaging events, casual attire, and more.
We offer competitive salaries with a comprehensive benefits package to all employees and provide an environment that fosters work-life harmonization with unlimited PTO, company-observed holidays, and Summer Fridays. As we continue to see exponential growth, our goal is to continue to put team members first and strive to offer our team members the best culture and benefits possible. Some of the benefits we provide are:
Health, Dental, Vision & Voluntary Benefits
Competitive Salary & Bonus Plans
401k Retirement Savings
Unlimited PTO & 10 Paid Holidays
Flexible Work Hours
Equity Shares
Paid Leave Programs
Marketplace for discounted retail and entertainment
Equal Opportunity Employer
Trella Health is an equal opportunity employer. All persons will receive consideration for employment without regard to race, color, gender, sexual orientation, gender identity or expression, religion, national origin, marital status, age, disability, handicap, veteran status, genetic information, or any other protected status as recognized by federal, state, or local laws.","$113,846 /yr (est.)",51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2015,Unknown / Non-Applicable
WellTrust Medical Group,#N/A,"Charlotte, NC",Data Engineer,"The Data Engineer will assist the senior analyst in complex modeling and projects to ensure their completion within the allotted timeline. Helps track measures of success and develop reports that measure operational and customer metrics. Demonstrates ability to troubleshoot data consumption and data quality problems.

Responsibilities:
Build and maintain reports using My SQL Server
Identify optimal data sources, structure data validation, and cache optimization.
Performs business problem modeling and quantitative analytics including regression analysis, time-series, hierarchical cluster analysis, and data mining.
Helps track measures of success and develop reports that measure operational and customer metrics.
Demonstrates ability to troubleshoot data consumption and data quality problems.
Evaluates and recommend improvements to current models, metrics, and analytic processes to improve operational efficiency and insight.
Design, document, and communicate effective technical solutions meeting end-user requirements
Perform analyses with large amounts of complex data, and uses data visualization tools to communicate results.
Requirements:

Bachelor's degree or equivalent experience in a quantitative field (Mathematics, Statistics, Computer Science, Economics, Finance, Physics, Engineering)
3+ years of professional working experience as a Data Engineer or Analyst
1-3 years of data warehouse experience
Build and maintain reports using My SQL Server
Strong analytical and creative problem-solving skills
Design, document, and communicate effective technical solutions meeting end-user
SQL and Excel Macro development
Strong understanding of various data environments such as SQL, Oracle, Big Query, etc.
Experience with data visualization tools such as Looker Studio, Tableau, D3.js, etc.
Proficient in R, Python, MATLAB, Java, or another programming language.",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Siri info solutions inc,#N/A,"Beaverton, OR",Sr. Data Engineer,"Must Have Skills
PySpark
Python on AWS
Responsibilities:
Understanding of advanced data warehousing concepts, data modeling and extract, transform and load development
Advanced SQL skills with experience querying large datasets from multiple sources and developing automated reporting
Python skills for scripting, data manipulation, custom extract, transform and loads and statistical/regression analysis particularly, as they apply to sales and marketing operations and performance
Job Type: Full-time
Salary: $47.00 - $50.00 per hour
Experience level:
7 years
Work Location: On the road",$48.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Fuge Technologies Inc
4.7",4.7,"Jersey City, NJ",Big Data Engineer,"Big Data Engineering Technical Lead
Mandatory Skills
Big Data – Spark, Kafka, Databricks, Hive, Nifi
AWS, Java, Scala
Client Interview Needed for Selection (yes / No)
Yes
Detailed JD (Pl share the Detailed Description, 1 liner JD will not work)
10+ yrs experience as Data Engineer
Extensive experience in spark on data processing
Kafka design experience like monitoring & parallelism.
Comfortable with Python/Java/Scala coding experience
Crisp / clear cut communication
Flexible with learning new technology
Flink experience is a plus
AWS experience is nice to have
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Jersey City, NJ 07302: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",$72.50 /hr (est.),1 to 50 Employees,Contract,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"DiNi Communications, Inc.
3.0",3.0,"New York, NY",Data Center Engineer,"DiNi Communications, Inc. is seeking a Data Center Engineer. This person needs to be a team player with exceptional communication skills. Maintaining the confidentiality of the client's information is of utmost importance.
Required Education and Experience
· At least 8 years of experience as a large data center technician
· Knowledge of Networking,
· Ability to lift equipment at least (50lbs) Rack and Stack (Network Switches, Servers, Storage,
Appliances)
· Knowledge of structured Cabling in a Data Center.
· General knowledge of Operating Systems, (Windows, Unix).
· General knowledge of networking in N Tier Environment (App, Web, DMZ,).
· Level 1 and Level 2 troubleshooting in a N Tier Environment.
· Ability to follow and execute commands to start up and shutdown systems.
· Physically travel to and be available at site B to perform technical changes that require a
person at Site B.
· Performing backup and restore activities.
· Asset Management.
· Maintaining document.
· Work with vendor on circuit issues, (WAN and Internet).
· Cable Patch the equipment in the environment.
· Participate in Site B fall-over testing
Preferred Network Engineering Experience
· Knowledge of N Tier networking.
· Certification in Networking (CISCO).
· Certification in various Operating systems (Linux and Windows), Storage, Messaging
(Microsoft).
· Certification in IT Security.
· Knowledge of Databases.
· Experience working with CISCO, EMC, Symantec, HP, Oracle, Bluecoat, MacAfee, and other vendors' equipment and software in a networked environment.
Responsibilities include but are not limited to:
Adding equipment to support agency applications in site B Environment,
Coordinating Site B testing with Site A technical resources.
Monitoring the B environment.
Rack and Stack new and replacement equipment
Troubleshooting and resolving incidents and problems in the
Physically travel to and be available at Site B to perform technical changes that require a person at Site B.
Upgrading and expanding the environment
Performing backup and restore activities
Manage Site B assets
Maintaining document
Cable Patch the equipment in the environment
Participate in Site B fall-over testing.
Job Type: Full-time
Pay: $60.00 - $70.00 per hour
Schedule:
Monday to Friday
Experience:
large data center technician: 8 years (Required)
Networking in N Tier Environment (App, Web, DMZ,): 3 years (Required)
Ability to lift equipment at least (50lbs): 1 year (Required)
structured Cabling in a Data Center: 3 years (Required)
Level 1 and 2 troubleshooting in a N Tier Environment: 3 years (Required)
Performing backup and restore activities: 3 years (Required)
Cable Patch the equipment: 3 years (Required)
Certification in Networking (CISCO): 1 year (Preferred)
Certification in IT Security: 3 years (Preferred)
CISCO, EMC, Symantec, HP, Oracle, Bluecoat, MacAfee: 3 years (Preferred)
Work Location: In person",$65.00 /hr (est.),Unknown,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"Kastech Software Solutions Group
4.0",4.0,"Dallas, TX",Data Engineer,"In KastechSSG we are are seeking for a Cloud DB Platform Engineer with Data Enablement Services experience ($50/hour), to work in a Hybrid Schedule (3 days on-site, 2 days home office) for one of our partner companies!
Hybrid Schedule info: onsite 3 days per week from list of locations: Charlotte NC, Dallas TX, Chandler AZ, Newark, NJ - Must be local to one of these locations.
** Contract would be $50 per hour on W2 (18 months). American Citizens or GC Holders Only**
3-5 years experience in:
Terraform, Azure/GCP, DevOps Engineering, ETL, Client/AI, Data lake, Data architecture.
GCP Key Services Experience (ie Big Query, Composer, Vertex AI, DataProc, Data Catalog, Notebooks).
Azure Key Service Experience (ie Synapse, Purview, Data Lake, Logic Apps, Cognitive Services).
Job Type: Full-time
Salary: $50.00 per hour
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you American? If not, please specify your visa type
Would you be willing to work under W2?
Experience:
Terraform: 3 years (Preferred)
Azure Data Lake: 3 years (Preferred)
GCP Key Service Exp: 3 years (Preferred)
DevOps: 3 years (Preferred)
Work Location: In person",$50.00 /hr (est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2008,$25 to $100 million (USD)
"Apolis
3.9",3.9,"Saint Louis, MO",Data Engineer (Python Focused),"Role: Data Engineer (Python Focused) (W2 ONLY)
Location : St. Louis, MO 63102
Duration: 18 Months
The primary responsibilities of this role will be to work as a part of the data and analytics team of the client. The ideal candidate would have experience using ETL and other data engineering tools in a cloud environment. Additionally, this candidate would have coding and scripting experience creating code to move and manipulate data using tools such in Python such as Spark or Pandas. Microsoft Azure experience is a plus but relevant experience with other cloud-based environments will be considered.
This team uses Microsoft Azure to build and productionize data sets and processes to the product owners and product users and this role will be expected to contribute to those objectives. As a member of this team, you will work in conjunction with other data engineers, platform engineers, solution developers and others to execute and deliver working solutions to the end users.
Technical Skills
High focus on Python
More Programmer/Coder experience, less Cloud experience needed.
Must have experience with data analytics/engineering and cloud technologies and an in-depth working knowledge and understanding of data engineering, design, and integration.
Intermediate or higher level of proficiency with Python Must have the proven ability to learn quickly and apply cloud data engineering principles and skills to new tools and use cases.
Ability to define and take technical ownership of detailed requirements and collaborate effectively with various team members such as Business Analysts and Project Managers and end users.
Excellent communication (verbal and written) and problem-solving skills are essential for this position.
Initiative, Proactive Cooperation, and curiosity are mandatory behavioral skills required for this position.
Experience with several of the following technologies is preferred:
Experience with Microsoft Azure Platform. In particular, the data processing toolsets including Azure Data Factory Pipelines, Azure Databricks, Azure Data Lake Storage.
Experience using Notebooks (Such as Jupyter, DataBricks) also desired.
Experience using code languages such as Python and C#
Experience using query languages such as SQL, MDX and DAX
Experience using scripting languages such as PowerShell, M-Query (Power Query), and Windows batch commands.
Understanding of Agile and DevOps principles for software development and ownership.
Familiar with compliance and controls for software development and deployment.
Education Required
A bachelor's in computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field [or equivalent work experience] is not required but is preferred.
Job Type: Contract
Salary: $60.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Vision insurance
Compensation package:
Overtime pay
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
St. Louis, MO: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Preferred)
ETL: 5 years (Preferred)
Cloud infrastructure: 4 years (Preferred)
Work Location: One location",$65.00 /hr (est.),501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,1996,$25 to $100 million (USD)
"MyCare Medical Group
4.1",4.1,"Lutz, FL",Data Engineer,"Job Summary
The Data Engineer will design, develop, optimize, and maintain data architecture and pipelines that adhere to ELT principles and business goals.
Job Responsibilities:
Designs, develops, optimizes, and maintains data architecture and pipelines that adhere to ELT principles and business goals.
Solves complex data problems to deliver insights that helps business achieve its goals.
Creates data products for engineer, analyst, and data scientist team members to accelerate their productivity.
Engineers effective features for modelling in close collaboration with business analysts and stakeholders.
Leads the evaluation, implementation and deployment of emerging tools and process for analytics data engineering in order to improve productivity as a team and quality.
Partners with engineers, BI, and solutions architects to develop technical architectures for strategic enterprise projects and initiatives.
Fosters a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions.
Advises, consults, mentors, and coaches other data and analytic professionals on data standards and practices.
Develops and delivers communication and education plans on analytic data engineering capabilities, standards, and processes.
Job Qualifications:
Education and Work Experience: Bachelor’s degree in computer science, information systems, engineering, or a related field; Masters preferred.
3+ years of experience in data engineering role.
Expertise in ELT and data analysis, SQL and at least one programming language (e.g., SQL, Python).
Conceptual knowledge of data and analytics, such as dimensional modelling, reporting tools, data governance, and structured and unstructured data.
Skills: Strong understanding of agile methodologies and experience as a Data Engineer on a cross-functional agile team preferred.
Experience with designing and maintaining data warehouses and/or data lakes with technologies, such as Hadoop, Spark, or distributed databases, like Redshift and Snowflake, and experience with housing, accessing, and transforming data in a variety of relational databases.
Experience with cloud database technologies (e.g., Azure) and developing solutions on cloud computing services and infrastructure in the data and analytics space.
Experience in building data pipelines and deploying/maintaining them with best-in-class tools (e.g., Git and Jenkins).
Familiarity with the Linux operating system.
Basic familiarity with BI tools (e.g., Tableau, Power BI, Looker).
Experience working in data engineering or data architect role.
Candidate Qualities: Focuses on business agility and agile delivery with a “fail-fast, succeed early” mindset and emphasis on measurable outcomes.
Brings a high-energy and passionate outlook to the job and can influence those around them.
Able to build a sense of trust and rapport that creates a comfortable & effective workplace.
Passion for innovation and “can do” attitude.
BENEFITS
Comprehensive benefits package, including Health, Vision, Dental, and Life insurancesFSA and Life Assistance Program (EAP)
401(k) Retirement Plan
Health Advocacy, Travel Assistance, and My Secure Advantage
PTO Accrual and Holidays
#ZIP
Job Type: Full-time
Pay: $125,000.00 - $150,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Employee discount
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Referral program
Relocation assistance
Retirement plan
Tuition reimbursement
Vision insurance
Compensation package:
Performance bonus
Yearly bonus
Experience level:
3 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Lutz, FL 33558: Reliably commute or planning to relocate before starting work (Preferred)
Experience:
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person","$137,500 /yr (est.)",501 to 1000 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2018,Unknown / Non-Applicable
"Merkle
3.7",3.7,"Norristown, PA",Data Engineer,"Company Description

We are Dentsu International. We innovate the way brands are built. That means we do things differently so they’re better than before. In this way, we make our clients’ most important marketing assets—their brands—win in a changing world.
Dentsu International is a modern marketing solutions company. Our mission is to help clients navigate, progress, and thrive in a world of change. Businesses rely on our integrated network of agencies and specialized practices to champion meaningful progress through creative, media, commerce, data, and technology. Part of Dentsu Group, our global network comprises 66,000 diverse people in 143 countries, who are dedicated to teaming for growth and good. Follow us on Twitter @DentsuUSA and visit dentsu.com/us.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.

Job Description
Develop onboarding process to receive and process information related to onboarding a new subscriber
Convert Snowflake Stored Procedures to Python Scripts directly in Airflow
Develop metadata driven solutions for managing data processing and cadence
End to end automation development and testing
Building and maintaining stored procedures and scripts in Snowflake Data Cloud
Building and maintaining complex SQL queries for ETL/ELT processing
Develop Dashboard for data processed for audit and governance needs
Follow Agile methodologies to deliver iterative feature sets rapidly
Maintain codebase using source control methodologies, Git
Collaborate with business analysts, data analysts, and other developers to deliver powerful new features
Coordinate with teams as part of a larger data-sharing system
Contribute to a collaborative, positive, and enjoyable environment for your operations team
Performing quality assurance and testing
Writing user and technical documentation such as runbooks

Qualifications
Strong communication skills both verbal and written
Strong understanding of conditional programming logic
Knowledge of software engineering and development methodologies, techniques, and tools; includes Issue Tracking (JIRA) Software Development Lifecycle SDLC), and Continuous Integration / Continuous Development (CI/CD)
Experience working with large databases and data warehouses utilizing both relational and dimensional design concepts
Eagerness to learn new technologies, programming languages and platforms to solve complex data challenges
Experience with Python required
Experience with SQL required
Experience with Snowflake Data Cloud is highly desirable
Experience with Amazon Web Services (AWS) a plus
Experience with Airflow (or other orchestration) a plus
Experience in database marketing is a plus

Additional Information

Perks:
Great compensation package
Comprehensive healthcare plans
401(k) with employer match
Flexible time-off
16 weeks paid parental leave
Additional Information
The anticipated salary range for this position is $94,000-$146,000. Salary is based on a range of factors that include relevant experience, knowledge, skills, other job-related qualifications, and geography. A range of medical, dental, vision, 401(k) matching, paid time off, and/or other benefits also are available. For more information regarding dentsu benefits, please visit dentsubenefitsplus.com
Employees from diverse or underrepresented backgrounds encouraged to apply.
Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact recruiting@dentsuaegis.com if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.
About dentsu
Dentsu is the network designed for what’s next, helping clients predict and plan for disruptive future opportunities in the sustainable economy. Taking a people-centered approach to business transformation, dentsu combines Japanese innovation with a diverse, global perspective to drive client growth and to shape society www.dentsu.com.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.
Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact your recruiter if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.","$120,000 /yr (est.)",5001 to 10000 Employees,Company - Private,Media & Communication,Advertising & Public Relations,1971,$500 million to $1 billion (USD)
"Integrated Technology Strategies, Inc.
4.0",4.0,"New York, NY",Data Engineer,"Integrated Technology Strategies is a provider of information technology consulting. Digital transformation, improving business performance, developing strategies and enhancing value is at the core of what we are.

Integrated Technology Strategies is looking to hire a Data Engineer to be a part of our Consulting team.

We are looking for someone passionate about data is focused on developing the platform for critical data products, including real time business metrics and analytics capabilities. The role requires supporting and collaborating with groups including Data Analytics/BI/Product, as well as our core backend API team. The individual will not be afraid to think out of the box and will play a key role in technical decision making. We are highly focused on giving ownership and responsibility to autonomous teams, using the right tools for the job, and building flexible architectures.

Responsibility:
Continue to evolve the internal Reporting and Analytics platform on top of Snowflake on AWS infrastructure.
Experience in Architect, design and implementing scalable ETL and data processing systems to handle the big data ecosystem including data collection, processing, ETL and Data warehouse.
Build soft real time capabilities and insight into product metrics to help product managers and BI/Analytics understand and optimize product features and guide product decisions.
Participate and contribute to the capabilities and engineering priorities across the organization.
Contribute to the codebase and participate in code review.
Build analytics tools that utilize the data pipeline to
provide actionable insights into operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product,
Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Reporting to the Senior Director, Software Engineering, you’ll be responsible for overseeing engineering product quality and delivery and setting and overseeing technical standards for teams who are working on everything from customer-facing applications.

Skill Requirements:
Solid understanding of real time data processing with Kafka, Spark and Flink and batch data processing frameworks on EMR and Snowflake.
Passion for building world-class data platforms that support a global customer base
Solid engineering background and understanding of programming languages such as Python, Java or equivalent
5+ years of progressive experience in data infrastructure development, with a track record of successful high-quality deliveries
Experience of working in an agile environment and embracing engineering best practices
Ability to apply both technical competence and interpersonal skills to achieve business outcomes
High emotional intelligence, sound temperament, and professional attitude
Strong understanding of SQL, experience with key databases such as Snowflake, MS-SQL and Postgres
Knowledge of the internals of how database systems work to design models for varied use cases.
Experience with CI and CD in an AWS environment with Terraforms
Experience with key Data technologies, such as Sqoop. Kafka will be a plus
Proven experience in building secure data platforms
Bachelor’s degree in Computer Science or equivalent","$110,128 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$5 to $25 million (USD)
Atika Technologies,#N/A,"Pittsburgh, PA",Data Engineer,"Role: Data Engineer/PySpark Developer, Pittsburgh, PA - Day One Onsite
Fulltime and Contract(W2)
LinkedIn Must
USC/GC/EADs
Three round of interview: First two on Zoom Video Call, End Client Round will be Onsite on the Client's Location (Pittsburgh, PA)
Job Description:
Python(PySpark), Jupiter hub, airflow, Java
Database knowledge - Oracle / SQL / Data Analytics
Regulatory reporting knowledge preferred
Good communication skills
Open / Ability to learn new technologies
Very good Analytical Skills
Experience in ETL is preferable
Job Types: Full-time, Contract
Salary: $120,000.00 - $130,000.00 per year
Benefits:
Dental insurance
Health insurance
Life insurance
Compensation package:
Bonus pay
Performance bonus
Yearly pay
Experience level:
10 years
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Pittsburgh, PA 15259: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Required)
Spark: 4 years (Required)
Regulatory reporting: 3 years (Required)
SQL: 4 years (Required)
IT: 10 years (Required)
Airflow: 4 years (Required)
Jupiter: 4 years (Required)
Work Location: One location","$125,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Fracsys Inc,#N/A,"Washington, DC",Data Engineer,"Fracsys Inc is hiring a Data Engineer position. The ideal candidate must have at least 12+ years of industry experience. He or she must be responsible for successful technical delivery of Data Warehousing and Business Intelligence projects using a formal project management methodology.
Responsibilities
Directs, motivates and develops key technical personnel
Provides technical direction across the full lifecycle system development process
Provides strategic planning and comprehensive senior level technical consulting to IT senior management and senior technical staff
Working independently within guidelines
Provides detailed inputs for plans and schedules, including goals, risks, and resource allocation
Conduct diagnostic, troubleshooting steps and present conclusions to a varied audience
comfortable slinging some code to solve a problem
Build Data Marts and Data warehouses to facilitate analytical capabilities across the varied sources of data
Design and implement data patterns on cloud using cloud Data warehouse and data virtualization tools
Evaluate new tools and technologies for target state on cloud
Learn and implement new ETL, BI tools and data warehousing technologies
Qualifications
Minimum twelve (12) years in a Technical Lead or Consulting role leading Information Management related initiatives (system integration, data warehouse build, data mart build, or similar)
MUST have proven leadership experience for team of at least 3 technical resources.
Experience creating Data Marts, Data warehouses on-premise and on cloud for real time and batch processing frameworks
Experience building and optimizing big-data data pipelines and data sets including Postgres, AWS Relational Data Service
extensive hands-on experience building and optimizing data structures for data analytics, data science and business intelligence
Experience building self-service data consumption patterns and knowledge of cloud-based data Lake platforms
Experience wrangling data (structured and unstructured), in-depth knowledge of database architecture
Experience utilizing or leading implementations leveraging ETL tools (Informatica / Talend), BI Reporting tools such as MicroStrategy, Microsoft Power BI, data modeling tools such as Erwin, Oracle, SQL server, NoSQL, JDBC, UNIX shell scripting, PERL and JAVA, XML/JSON files, SAS , Python, AWS cloud-native technologies, S3, Athena, Redshift
Familiarity with the following technologies: Hadoop, Kafka, Airflow, Hive, Presto, Athena, S3, Aurora, EMR, Spark
Ability to drive, contribute to, and communicate solutions to technical product challenges
Ability to roll-up your sleeves and work in process or resource gaps and fill it in yourself
Excellent written and oral communication skills in English
Education
Bachelor’s degree in computer science, Data Analytics, Information Systems, or a related degree or equivalent experience. Master’s degree is preferred.
Job Type: Full-time
Pay: From $96,336.72 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Experience level:
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Washington, DC: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person","$96,337 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Intersec
4.5",4.5,Remote,Senior Data Engineer,"Architect and implement highly scalable data pipeline to streamline migration of data from source data repositories to the centralized data platform
Implement agreed upon data models (e.g., data marts) to enable faster and efficient consumption of data by downstream consumers
Conduct performance tuning to optimize data migration techniques and ensure adherence to identified service-level agreements (SLAs)
Define and implement automated validation checks to ensure data integrity between source and target
Assist in onboarding consumers with existing technology to connect to the platform
Setup predictive alert mechanism for pipeline degradation for IMF pipelines.
Enhance data quality framework for data pipelines and derived datasets
Enhance Data Audit Balance and Control mechanism for data pipelines
Enhance pipelines to be certified copies of authoritative source
Populate metadata for the Enterprise with particular focus on the Data Sources being onboarded to EDP (Technical and Business Glossaries) related to IMF Modernization Programs
Validate correct lineage for datasets
Required:
*8+ years of Databricks, Data Modeling, and/or Metadata management experience
* Familiarity with architecting and developing real and near-real-time solutions
* Bachelor's Degree in Engineering, Mathematics, computer Science, Statistics or other quantitative discipline, or 4 years equivalent professional experience
* US Citizenship and ability to gain a public trust
Preferred:
* Familiarity with IRS systems and ETL pipelines
* Ability to work independently and manage multiple tasks
* IRS MBI Clearance with IRS Laptop and Informatica Installed
Job Type: Full-time
Salary: $70.00 - $75.00 per hour
Schedule:
Monday to Friday
Application Question(s):
Are you a US/GC card holder?
How many years of Databricks, Data Modeling, and/or Metadata management experience do you have?
Work Location: Remote",$72.50 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,Unknown / Non-Applicable
Atalan Tech,#N/A,Remote,Data Engineer,"Atalan Tech is a start-up with a mission to mitigate the impact of clinician burnout and turnover in our nation’s health system. We are expanding our team and looking to hire the critical role of Data Engineer. The Data Engineer will work closely with the Chief Technology Officer, Chief Scientific Officer (UCLA Ph.D.), Chief Product Officer (Oxford Ph.D.), and Chief Executive Officer to deliver actionable insights from data in a variety of situations.
We are a small, close-knit team. This is an excellent opportunity for someone to join a mission-oriented, early-stage company at the ground floor and grow with us. There will be plenty of room for growth and creativity, and opportunities to tackle challenges that present one of the toughest problems in our society.
Requirements
Strong hands-on experience of designing, developing and managing data pipelines
Strong coding skills in Python (required) and SQL (required) and R (strongly preferred)
Experience building and maintaining data pipelines on AWS using EC2, EBS, Redshift, Postgres, S3, Data Pipeline, Kinesis, Lambda, etc.
Thorough understanding and experience with task management tools (Airflow, NiFi, etc.)
Knowledge of data management best practices for ETL, data governance, data warehousing, data modeling and data science
Basic knowledge of Linux (Redhat preferred) administration
Minimum B.S in Computer Science, software engineering or related field (or equivalent in related professional experience)
Strong communication skills
Strong problem-solving/critical thinking skills
Ability to work in a fast-paced, dynamic start-up environment
Some knowledge of healthcare, health tech, or healthcare data analytics (preferred)
Typical Tasks and Responsibilities include:
Working closely with CTO and CSO to build data pipelines for machine learning
Assisting the CPO in managing and supporting analysis of enterprise data to drive better products and solutions
Managing company’s data infrastructure on AWS (Postgres/RDS, Redshift, S3, etc.)
Synthesizing multiple data sources, such as internal medical records and HR data
Integrating with client systems including EHR (Epic, Cerner), HRIS, flat files, etc.
Working closely with other key members on other projects
Developing and designing data security standards and policies in line with industry best practices
About Us:
Atalan Tech’s mission is to improve the wellness of clinicians and their patients. We blend labor economics, machine learning, and clinical psychology with a team from Harvard, Yale, and UCLA. Atalan Tech is looking to contribute innovative and sustainable solutions to one of the most pressing problems in the healthcare industry. Building upon our proof-of-concept success, we have recently secured multiple engagements with nationwide healthcare organizations and have successfully raised institutional funding to expand our reach.
Contact Us:
If you’re interested in this position, please send us your CV/resume and cover letter.
Job Type: Full-time
Pay: $108,448.20 - $130,604.27 per year
Experience level:
2 years
Schedule:
Monday to Friday
Application Question(s):
Do you know the role is full time?
Can you start within the next month?
What are your salary expectations?
Have you held the title of a data engineer? If not, what experience do you have as a data engineer? Please be very specific.
Have you released Python code to a production environment in the last year?
Experience:
Data analytics: 2 years (Required)
Work Location: Remote","$119,526 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Argo Data
2.7",2.7,"Richardson, TX",Senior Data Engineer,"ARGO is a leading provider of software and analytics solutions for both the financial services and healthcare industries. ARGO transforms business processes for financial service providers and healthcare organizations using proven business models and software innovation informed by real customer challenges, breakthrough technology, and rich analytics.
ARGO- Data Engineer II (Markham, ON; Memphis, TN; ...) (theapplicantmanager.com)
JOB SUMMARY
The Data Engineer II is responsible for designing and implementing data solutions to support business needs. The successful candidate will have a strong background in software engineering and database design, as well as experience with working with cloud-based tools and technologies.
EXPECTED WORK AND PERFORMANCE
This section is intended to provide a more detailed overview of what the expectations of the job are to include: work to be performed, expected performance results and how they will be measured, as well as connection to corporate strategy, values, and mission. Job duties and responsibilities include the following. Other duties may be assigned as needed.
Design, build, and maintain data pipelines for data extraction, transformation, and loading (ETL) from various sources.
Ensure data security and privacy protocols are implemented and enforced.
Monitor and optimize data storage and processing performance to ensure high performance and cost-effectiveness.
Collaborate with data scientists, analysts, and other stakeholders to understand their data needs and translate them into technical solutions.
Maintain documentation of data systems, processes, and policies.
Stay current with the latest data tools, technologies, and trends and recommend improvements.
Mentor and train junior data engineers as needed.
QUALIFICATIONS
Bachelor's degree in Computer Science, Software Engineering, or related field.
3-5 years of experience in software engineering and SQL database design.
Proficiency in programming languages such as Python.
Strong understanding of data warehousing, ETL processes, and data modeling.
Experience with cloud infrastructure technologies such as Microsoft Azure.
Excellent problem-solving and analytical skills.
Ability to work well in a team-oriented environment and communicate effectively with both technical and non-technical stakeholders.
PHYSICAL DEMANDS
The candidate will work at our corporate headquarters in Richardson, TX. While performing the duties of this job, the employee is regularly required to stand; walk; sit; use hands to finger, handle, or feel; reach with hands and arms; talk or hear. Specific vision abilities required by this job include close vision, distance vision, color vision, peripheral vision, depth perception, and ability to adjust focus.
ADDITIONAL REQUIREMENTS:
Applicants for U.S. or Canadian based positions with ARGO must be legally authorized to work in the United States or Canada. Verification of employment eligibility will be required at the time of hire. Visa sponsorship is not available at this time for ARGO positions.
Pre-employment background screening will be conducted.
Job Type: Full-time
Pay: $70,745.42 - $141,624.04 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Life insurance
Paid time off
Professional development assistance
Referral program
Relocation assistance
Retirement plan
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Performance bonus
Yearly bonus
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Richardson, TX 75081: Reliably commute or willing to relocate with an employer-provided relocation package (Required)
Experience:
Informatica: 3 years (Required)
SQL: 5 years (Required)
Data warehouse: 3 years (Required)
Python: 3 years (Required)
Azure: 3 years (Required)
Work Location: In person","$106,185 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Computer Hardware Development,1980,$25 to $100 million (USD)
"Egrove Systems Corporation
4.4",4.4,"Dallas, TX",Data Engineer Flink,"Java, Bigdata, Azure & Flink
Flink Sr. Developer, who has implemented and dealt with failure scenarios of processing data through Flink.
A senior who can help his Flink development team, guiding and helping them implement custom solutions through Flink.
Seasoned Java developer who knows about all aspects of SDLC.
Worked on integrations of other technologies with Flink, eg: Kafka, MongoDB, etc
Azure experience in the areas of Messaging, Data processing, preferably on Flink or on Databricks.
Job Type: Contract
Salary: From $51.83 per hour
Compensation package:
Monthly bonus
Experience level:
11+ years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
Work Location: On the road
Speak with the employer
+91 732)-860-7733",$51.83 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Internet & Web Services,#N/A,Unknown / Non-Applicable
Proits Hub LLC,#N/A,Remote,Data Analytics Engineer,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote
Speak with the employer
+91 9417599933",$60.00 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Hiscox
4.1",4.1,"Atlanta, GA",Senior Data Engineer,"job description:
You will be a senior member of our US IT Team data engineering practice. You will be an early member of this team and will work closely with the Director of Data Engineering to help establish it. We will have our work cut out for us delivering planned work, building the practice and evolving our technical landscape. This is an exciting opportunity for the right individual to join at the very beginning and grow with the new team.
As a senior you will be expected to lead, mentor others and deputise for the director of data engineering when required. Yoi will also continue to contribute as a hands-on engineer. You will have the knowledge, best earned through 5+ years of experience, to truly inhabit this senior role in an ambitious team.
What you’ll be doing:
Working with business stakeholders and our delivery team to understand high value business problems that can be solved through the application of data processing and analytical systems.
Developing, expanding and evolving our existing databases and ETL pipelines.
Working with architects to design, build and support a transformational cloud-based data platform for the business.
Being a core and professional member of the new data engineering practice
Understand business requirements and help refine into development tasks and estimate their complexity.
Research, evaluate and adopt new technologies with a right tool for the job mentality.
Focus on both speed of delivery and quality, with suitable pragmatism – ensuring your solutions are always appropriate and not too complex or over-engineered.
Quick progression of projects from proof-of-concept to post-production stage.
Communication and presentation of ideas to colleagues in all parts of the wider tech team.
Participating in code reviews for the data engineering practice.
Act as a servant leader in our practice – ensuring team members have
person specification:
Knowledge of, or ability to rapidly adopt our core languages for data engineering – SQL and/or Python.
Experience working using a modern DevOps approach. You should not just have written data code but also the CI/CD pipelines necessary to test and deploy that code in a professional environment.
A robust understanding of core data engineering topics – ETL vs ELT, structured and unstructured data, data quality and data governance.
Ability to contribute to all aspects of a solution – design, infrastructure, development, testing and maintenance.
The ability to design and advocate for technical solutions to business problems.
Effective collaboration with technical and non-technical team members through agile ceremonies – roadmap planning, feature workshops, backlog elaboration, code review.
Understanding of cloud technology and significant hands-on experience working with it. You must understand how cloud-native solutions must be built differently from traditional ones.
Track record of taking initiative and delivering projects end-to-end; clear evidence of being self-driven and motivated
Immense curiosity, high energy and desire to go the extra mile to make a difference.

Desirable Skills
Beyond the required skills we are open to individuals of diverse talents. Experience with additional technologies, data science knowledge or a business background are all valued. We want to know how your unique abilities can contribute to our team.
Our Technology
We operate in a diverse technical landscape and are looking for flexible engineers who can adapt to and use many different tools. We would not expect any engineer to be familiar with the entire tech stack – no engineer can. Instead, we seek people with a good understanding of data structures and algorithms and the ability to apply this knowledge in learning new tools.
We have an existing data landscape involving both MS SQL Server and Oracle databases. We use SSIS for ETL. We will also need to source data from our integration layer – webMethods.
If some of that sounds like it needs bringing up to date, we know! We hope to start building a modern cloud data platform to realise the full value of our data. The technologies we use for that platform are still to be decided. Likely it will be based on Snowflake with Azure Data Factory for ELT. Other parts of our business have had success using Prefect for orchestration and dbt for transformation and we will reuse where it is sensible.
We prepare data for use by analysts working with a variety of tools – Tableau, PowerBI, and even Excel.
We take a DevOps approach and strive for continuous integration / continuous deployment. We use Azure Pipelines to deploy our code. We deploy infrastructure the same way using Terraform and Docker.
Our technology landscape is not fixed. Our engineering and architecture teams drive the technology we adopt.


Salary range $140,000 - $170,000","$155,000 /yr (est.)",1001 to 5000 Employees,Company - Public,Insurance,Insurance Carriers,1901,$1 to $5 billion (USD)
"FreightWaves, Inc.
3.5",3.5,Remote,Senior Data Engineer,"Are you smart, driven, curious, resourceful, and not afraid to fail? Then we want to meet you! Our team of bold, innovative, and creative teammates is what makes us a top startup to work for. FreightWaves delivers news and commentary as well as data and analytics which empower risk management and actionable market insights in the logistics and supply chain industry. If you are ready to join our team, it is time for YOU to apply!
FreightWaves is on the hunt for a curious, tenacious, and team-oriented Senior Data Engineer to join our fast paced engineering team. The ideal candidate is inquisitive, versatile, team oriented, thrives on change, and has a positive attitude. If you are ready to be challenged, learn new and exciting technologies, and have the unique opportunity to work with some of the most talented developers in the country, we want you to apply!
**This position is fully remote.**
**Must RESIDE in the United States and be eligible to work.**
What you will be doing:
Implementing ingestion pipelines, using Airflow as the orchestration platform, for consuming data from a wide variety of sources (API, SFTP, Cloud Storage Bucket, etc.).
Implementing transformation pipelines using software engineering best practices and tools (DBT)
Working closely with Software Engineering and DevOps to maintain reproducible infrastructure and data that serves both API-only customers and in-house SaaS products
Defining and implementing data ingestion/transformation quality control processes using established frameworks (Pytest, DBT)
Building pipelines that use multiple technologies and cloud environments (for example, an Airflow pipeline pulling a file from an S3 bucket and loading the data into BigQuery)
Create and ensure data automation stability with associated monitoring tools.
Review existing and proposed infrastructure for architectural enhancements that follow both software engineering and data analytics best practices.
Working closely with Data Science and facilitating advanced data analysis (like Machine Learning)
What you bring to the table:
Strong working knowledge of Apache Airflow
Experience supporting a SaaS or DaaS product, bonus points if you were creating new data products/features
Strong in Linux environments and experience in scripting languages
Python Expert
Strong understanding of software best practices and associated tools.
Experience in any major RDBMS (MySQL, Postgres, SQL Server, etc.).
Strong SQL Skills, bonus points for having used both T-SQL and Standard SQL
Experience with NoSQL (Elasticsearch, MongoDB, etc.)
Multi-cloud and/or hybrid-cloud experience
Strong interpersonal skills
Comfortable working directly with data providers, including non-technical individuals
Experience with the following (or transitioning from equivalent platform services):
Cloud Storage
Cloud Pubsub
BigQuery
Apache Airflow
dbt
DataFlow
Bonus knowledge/experience:
Experience implementing cloud architecture changes
Working knowledge of how to build and maintain APIs using Python/FastAPI
Transforming similar data from disparate sources to create canonical data structures
Surfacing data to BI platforms such as Looker Studio
Data Migration experience, especially from one cloud platform to another
Certification: Professional Google Cloud Certified Data Engineer
Our Benefits:
An excellent work environment, flat hierarchies, and short decision paths.
Work from home
A generous benefits package including 100% employer-paid health, dental, vision and Life insurance, STD, LTD
Stock options
Appealing 401k matching plan
Career Mentorship Opportunities
Personal Development Credit (Can be used toward Student loans or relevant PD Courses)
Annual life achievement bonus of $2000 for having a baby, buying a house, or getting married (max one per year)
No set days off Vacation policy (our team takes time off as needed with supervisor approval)
Up to $50 for Gym or Virtual Gym membership.
Audible or Kindle Unlimited subscription
Discount on Ford vehicles
oYXkhYiWkU",#N/A,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2017,Unknown / Non-Applicable
"Barracuda Networks Inc.
3.6",3.6,"Alpharetta, GA",Data Engineer,"Job ID: 23-579

Come Join Our Passionate Team! At Barracuda, we make the world a safer place. We believe every business deserves access to cloud-enabled, enterprise-grade security solutions that are easy to buy, deploy, and use. We protect email, networks, data and applications with innovative solutions that grow and adapt with our customers’ journey. More than 200,000 organizations worldwide trust Barracuda to protect them — in ways they may not even know they are at risk — so they can focus on taking their business to the next level.
We know a diverse workforce adds to our collective value and strength as an organization. Barracuda Networks is proud to be an Equal Opportunity Employer, committed to equal employment opportunity and equitable compensation regardless of race, gender, religion, sex, sexual orientation, national origin, or disability.
Envision yourself at Barracuda
We currently have an exciting new position open for a Data Engineer. This person will assist in driving technology innovation for engineering projects and work on architecture of AWS, Elastic and Kubernetes. They will also work closely with the Technology Architect to create and drive the Architecture Review Board.
What you’ll be working on
Determine internal / external customer data needs and identify system specifications
Analyze the needs of large systems and breaking them down into smaller manageable parts
Responsible for designing the journey of data from various sources to the database and everything in between
Plan and design the structure of data, brainstorm with team
Analyze best technology products and systems for internal teams
Help provide build vs buy decisions on data architecture
Communicate data requirements to engineers; explain data architecture to them and provide assistance throughout SDLC
Choose / maintain suitable software, hardware and integration methods
Help resolve technical problems as and when they arise
Review testing procedures to ensure data performant and error free
Ensure that database satisfy quality standards and procedures. Setup such standards.
Work with senior IT personnel to devise plans for future IT requirements of the organization
Produce progress reports on key infrastructures - Elastic and Data Pipelines
What you bring to the role
Bachelor Degree or Post Graduate in Computer Science, or relevant work experience
5+ years of experience in an Engineering / Operations heavy organization
1+ years of experience as a Data Analysis and Data Design responsible for data architecture of a SaaS application
Experience in understanding and designing data sharding in a distributed database
Some Hands-on experience in software development and system administration
Knowledge of strategic IT solutions
Familiarity with programming languages like JavaScript or Java or Python
Familiarity with Linux OS and Windows alike
Experience in cloud computing and cloud Technologies (AWS, GCP, Azure)
Excellent communication skills – you should be adept at listening to, understanding, and explaining key concepts to managerial and technical resources.
Good Anticipation and problem-solving skills
Ability to prioritize and manage time – responsible for multiple streams of work at the same time
Relationship building - Ability to form a good rapport with internal and external stakeholders
In-depth knowledge about quality standards and security best practices

Preferred Skills:
Expert level Understanding of Elastic eco system - Elasticsearch, Logstash, Kibana, Filebeats
Understanding of secure tunnel communication using VPN
Experience designing data at scale
AWS experience is preferred in Cloud Technologies
What you’ll get from us
A team where you can voice your opinion, make an impact, and where you and your experience are valued. Internal mobility – there are opportunities for cross training and the ability to attain your next career step within Barracuda.
Equity, in the form of non-qualified options
High-quality health benefits
Retirement Plan with employer match
Career-growth opportunities
Flexible Time Off and Paid Time Off benefits
Volunteer opportunities
#LI-Remote","$98,161 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,$100 to $500 million (USD)
Radiant System,#N/A,"Pleasanton, CA",Data Engineer,"First Priority: Pleasanton, CA –
Second Priority for a superstar: Plano, TX
Experience in developing and deploying data pipelines, preferably in the Cloud
Experience working with Spark, Databricks, and other relevant technologies in Microsoft Azure cloud
Experience in productionizing and deploying Big Data platforms and applications
Expert in writing SQL statements
Job Type: Contract
Application Question(s):
Are you willing to work on W2?
Work Location: In person",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Encore Technologies
4.4",4.4,"Atlanta, GA",Senior Data Engineer,"Encore Technologies is seeking a Senior Data Engineer to work for a client in Atlanta, GA (zip code 30339). This is a Direct Hire role that will be worked in a hybrid schedule, with 2-3 days a week onsite.
Summary:
The Senior Data Engineer is responsible for managing and organizing enterprise data. They will translate requirements and designs into functional data pipelines while ensuring the continued quality and completeness of the information. Senior Data Engineers will combine raw information from different sources to create consistent and machine-readable datasets that are easy to analyze and support company initiatives. They will support other Data Engineers and Data Analysts on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They will also implement methods to improve data reliability and quality, improve data visibility and reduce effort through automation.
Essential Duties:
Contribute to a team of data engineers through design, demand delivery, code reviews, release management, implementation, presentations, and meetings.
Mentor fellow data engineers and contribute to ongoing process improvements for the team
Evaluate business needs and objectives and align architecture/designs with business requirements
Build the data pipelines required for the optimal extraction, transformation, integration, and loading of raw data from a wide variety of data sources
Assemble large, complex data sets and model our data in a way that meets functional / non-functional business requirements
Create data tools for analytics team members that assist them in generating innovative industry insights that provide our business a competitive advantage
Implement data tagging mechanisms and metadata management so data is accurately classified and visible to the organization
Build processes to help identify and improve data quality, consistency, and effectiveness
Ensure our data is managed in a way that conforms to all information privacy and protection policies
Use agile software development processes to iteratively make improvements to our data management systems
Identify opportunities for automation
Be an advocate for best practices and continued learning
Requirements:
Bachelor's/Tech School degree in Computer Science, Information Systems, Engineering or equivalent and/or commensurate years of real-world experience in software engineering.
4+ years of relevant experience in data management
3+ years in data engineering with detailed knowledge of data warehouse technical architectures, infrastructure components, and ETL/ ELT.
Experience with performance analysis and optimization.
Experience in data acquisition, transformation, and storage design using design principles, patterns, and best practices.
Data engineering certification is a plus.
Experience with Informatica, Kafka, CDC, SQL, Irwin, Python, AWS (S3, Athena, Glue, Kinesis, Redshift), Spark, Scala, AI/ML, Modern data platforms, Snowflake, dbt, Fivetran, and Airflow.
Encore Technologies is an Equal Opportunity Employer. We respect and seek to empower each individual and support the diverse cultures, perspectives, skills, and experiences within our workforce.
Job Type: Full-time
Pay: $120,000.00 - $160,000.00 per year
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Atlanta, GA 30339: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Preferred)
Experience:
ETL/ELT: 3 years (Required)
data management: 4 years (Required)
Work Location: Hybrid remote in Atlanta, GA 30339","$140,000 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2014,$100 to $500 million (USD)
"GTA (Global Technology Associates)
4.6",4.6,"Plano, TX",Data Logging Engineer,"Looking for a Data Logging Engineer
What you will be doing as a Data Logging Engineer
· Perform system level log captures/analysis of several Network elements in the 4G/5G Network architecture to identify root causes of product and key performance issues.
· Document and maintain the troubleshooting steps including updating technical documentation for primary areas of responsibility
· Identify automation opportunities & implement scripts for log analysis, for discussions with external customers/internal customers
· Strong on customer support as well as data analysis of logs, etc.
· Must be able to drive for issue resolution with other Engineering team
· Initial analysis of Network KPI degradation to quantify and qualify the issue
· Be able to proactively perform Ad-hoc/periodic parameter audit, issue signature audit and SW/FW audit
· Ensure the latest ticket status ticket status is provided to the customers
· Software and patch upgrades on network element to support feature testing and issue resolution verification
· Provide 24X7 support as needed for commercial Network outages and network performance issues. Also provide technical reviews and provide support to customers as needed
What you will bring to the table as a Data Logging Engineer:
· Bachelor’s Degree in Telecommunication or Network Engineering with at least 2 years of experience in Wireless Telecom environment.
· Good knowledge of UNIX/LINUX operating systems
· Good Scripting skills using but not limited to Python, Shell or Perl
· Strong knowledge of MS office tools like Excel, power point and Word.
· Good analytical skills to investigate and evaluate trending data for issue resolution
· Good knowledge of 4G and 5G wireless network architecture concepts, RAN protocols and eNB interfaces S1-MME, S1-U is a plus
· Good knowledge of trouble ticket and knowledge management systems
· Willing to work during night to accommodate debugging involving offshore R&D teams
What you didn’t know about us:
· Competitive salary
· Health, Dental and Vision Benefits
· Short/Long Term Disability and Critical Care/Illness Protection
· Life Insurance and Retirement Plans
· Employee Assistance Program
· With this position, you will get the opportunity to work with our game changing clients and further advance your already valuable experience in the telecom industry!
We are Connectors. We thrive on ‘quality over quantity’ and put in the work building strong relationships. We create connections, discover qualities, uncover skills, and place people with accuracy. We are your true partner!
We are Collaborators. You’ll be working with a wholly-owned subsidiary of Kelly and part of the Kelly Telecom division. It allows us to be as nimble and fiercely competitive as a startup while having the backing of a multibillion dollar publicly traded company which has been in business for 75 years. With direct access to hiring managers, services don’t stop at standard recruiting processes. We use our expertise to improve your application skills and provide ongoing career support.
We give 24/7 Support. We are in this together. We provide around the clock availability, competitive employee benefits, and continuously check-in to make sure things are going smoothly. Check out our Glassdoor page!
Kelly Telecom is an equal opportunity employer and will consider all applications without regard to race, genetic information, sex, age, color, religion, national origin, veteran status, disability, or any other characteristic protected by law. For more information click Equal Employment Opportunity is the law.
You should know: Your safety matters! Vaccination against COVID-19 may be a requirement for this job in compliance with current client and governmental policies. A recruiter will confirm and share more details with you during the interview process.
#JobsAtKellyTelecom
Job Types: Full-time, Contract
Salary: From $50.00 per hour
Schedule:
8 hour shift
Work Location: In person",$50.00 /hr (est.),Unknown,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Virtualan Software LLC
4.5",4.5,"Atlanta, GA","Sr. Data Engineer (Python,Spark & Databricks)","Role : Senior Data Engineer (Python, Spark & Databricks)
Location : Hybrid (Atlanta, GA)
Pay: 60-70$ on C2C / 1099
RESPONSIBILITIES:
Create or modify the conceptual, logical and physical data models.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of sources like Hadoop, Spark, AWS Lambda, etc.
Lead and/or mentor a small team of data engineers.
Design, develop, test, deploy, maintain and improve data integration pipeline.
Develop pipeline objects using Apache Spark , PySpark and/or Python.
Design and develop data pipeline architectures using Spark and related AWS Services.
Communicate effectively with client leadership and business stakeholders.
Participate in proposal and/or SOW development.
REQUIREMENTS:
5+ years of professional work experience designing and implementing data pipelines in a cloud environment is required.
2+ years of experience migrating/developing data solutions in the AWS cloud is required.
1+ years of experience building/implementing data pipelines using DataBricks is required.
Expert level knowledge of using SQL to write complex, highly-optimized queries across large volumes of data.
Hands-on object-oriented programming experience using Python is required.
Professional work experience building real-time data streams using Spark.
Knowledge or experience in architectural best practices in building data lakes.
Bachelor or Master degree in Computer Science, Engineering, Information Systems or relevant degree.
Hybrid work schedule, and must live in the Atlanta, GA metro area. Must be open to up to 25% national travel to client locations when on engagements outside of the Atlanta, GA area.
Job Type: Contract
Pay: $60.00 - $70.00 per hour
Schedule:
Monday to Friday
Application Question(s):
Apply only if you are interested in Hybrid Role in Atlanta, GA. Interested ?
Experience:
Data Engineer: 4 years (Required)
Databricks: 1 year (Required)
Willingness to travel:
25% (Required)
Work Location: On the road",$65.00 /hr (est.),1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,2019,Less than $1 million (USD)
"Titan America
3.8",3.8,"Roanoke, VA",Data Engineer,"Data Engineer
We are seeking a motivated and detail-oriented Data Engineer to join our team based in Roanoke, Virginia. As a Data Engineer, you will play a crucial role in designing, developing and maintaining scalable data pipelines and transformations for business-critical use cases. In addition, you may take on various Software Development and Data Science responsibilities as we frequently share responsibilities and knowledge across our growing team.
Responsibilities:
Design, develop and maintain scalable data pipelines for extracting, transforming, and loading data from various sources
Collaborate with cross-functional teams to identify data needs and determine the best data solutions
Develop and implement data models to support business requirements and ensure data quality
Ensure the security and privacy of sensitive data by implementing appropriate access controls
Monitor and optimize data pipeline performance to ensure timely and accurate data delivery
Document data pipeline processes, data dictionaries, and data storage solutions
Requirements:
Bachelor's degree in Computer Science, Computer Engineering, or a related technical field
Minimum of five years of professional experience working as a Data Engineer or Software Developer
Strong hands-on experience with data warehouse and transformation solutions, i.e. Domo, Snowflake or similar
Proficient in at least one scripting language such as Python, JavaScript, or R
Understanding of data modeling, data integration and data quality processes
Familiarity with cloud platforms such as AWS, Azure, or Google Cloud Platform
Strong analytical and problem solving skills
Full Stack Software Development experience in a professional setting is highly desired, but not required
This is an excellent opportunity for a driven and collaborative individual to make a significant impact in a dynamic and growing team. If you have a passion for data and a desire to work in a fast-paced and dynamic environment, we want to hear from you!
Success begins with hiring the right people to partner with us as we grow and develop our businesses. People are central to everything we do. It is through their efforts and talents that Titan has been successful for over 100 years.
Titan America is an Equal Employment Opportunity (EEO) / Affirmative Action employer and welcomes all qualified applicants. Applicants will receive fair and impartial consideration without regard to race, sex, color, religion, national origin, age, disability, veteran status, genetic data, or other legally protected status.
Job Type: Full-time
Pay: $75,000.00 - $85,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Referral program
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Roanoke, VA 24019: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Do you currently or will you in the future need sponsorship to work in the United States?
This position requires the successful candidate to be in the office in Roanoke, VA at least 3 days a week. Can you meet this requirement?
This position requires the successful candidates to be a full time, W2 employee. Can you meet this requirement?
Education:
Bachelor's (Preferred)
Experience:
Data Engineering: 5 years (Preferred)
Work Location: Hybrid remote in Roanoke, VA 24019","$80,000 /yr (est.)",1001 to 5000 Employees,Company - Public,"Energy, Mining & Utilities",Mining & Metals,1902,Unknown / Non-Applicable
"Appsintegration INC
4.5",4.5,"California, KY",Data Engineer,"Role:- Data Engineer
Position:- Onsite (W2 position)
Experience:- 10+ Years
Responsibilities
Working closely with business teams to understand their requirements and develop reporting or analytic solutions.
Producing and automating the delivery of key metrics and KPIs to the business.
In some cases, this will focus on making data available and others will be developing full reports for end users.
Managing stakeholder demand and a backlog of reporting requirements and needs.
Actively working with users to understand data issues, tracing back data lineage, and helping the business put appropriate data cleansing and quality processes in place.
Managing secure access to data sets for the business.
Requirements
Bachelor's degree in Computer Science or equivalent.
Proficient in creating Data Factory pipelines for on-cloud ETL processing.
Experience implementing data pipelines using the latest technologies.
Experience developing using SQL, relational databases, and data warehousing technologies.
Extensive ETL experience.
Ability to scale systems in Cloud as per requirement.
Experience in handling structured and unstructured datasets.
Good to have experience in data modeling and Advanced SQL techniques.
Must-Have Skills: Azure Data Factory (ADF), SQL, Cloud-based ETL.
Good to Have Skills: Azure File Storage Services, Databricks, Azure Synapse.
you can reach me out on LinkedIn
here is my link :- linkedin.com/in/shravanthi-thakur-705a81231
Job Type: Contract
Salary: $42.00 - $45.00 per hour
Ability to commute/relocate:
California, KY 41007: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",$43.50 /hr (est.),1001 to 5000 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"MyCare Medical Group
4.1",4.1,"Lutz, FL",Data Engineer,"Job Summary
The Data Engineer will design, develop, optimize, and maintain data architecture and pipelines that adhere to ELT principles and business goals.
Job Responsibilities:
Designs, develops, optimizes, and maintains data architecture and pipelines that adhere to ELT principles and business goals.
Solves complex data problems to deliver insights that helps business achieve its goals.
Creates data products for engineer, analyst, and data scientist team members to accelerate their productivity.
Engineers effective features for modelling in close collaboration with business analysts and stakeholders.
Leads the evaluation, implementation and deployment of emerging tools and process for analytics data engineering in order to improve productivity as a team and quality.
Partners with engineers, BI, and solutions architects to develop technical architectures for strategic enterprise projects and initiatives.
Fosters a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions.
Advises, consults, mentors, and coaches other data and analytic professionals on data standards and practices.
Develops and delivers communication and education plans on analytic data engineering capabilities, standards, and processes.
Job Qualifications:
Education and Work Experience: Bachelor’s degree in computer science, information systems, engineering, or a related field; Masters preferred.
3+ years of experience in data engineering role.
Expertise in ELT and data analysis, SQL and at least one programming language (e.g., SQL, Python).
Conceptual knowledge of data and analytics, such as dimensional modelling, reporting tools, data governance, and structured and unstructured data.
Skills: Strong understanding of agile methodologies and experience as a Data Engineer on a cross-functional agile team preferred.
Experience with designing and maintaining data warehouses and/or data lakes with technologies, such as Hadoop, Spark, or distributed databases, like Redshift and Snowflake, and experience with housing, accessing, and transforming data in a variety of relational databases.
Experience with cloud database technologies (e.g., Azure) and developing solutions on cloud computing services and infrastructure in the data and analytics space.
Experience in building data pipelines and deploying/maintaining them with best-in-class tools (e.g., Git and Jenkins).
Familiarity with the Linux operating system.
Basic familiarity with BI tools (e.g., Tableau, Power BI, Looker).
Experience working in data engineering or data architect role.
Candidate Qualities: Focuses on business agility and agile delivery with a “fail-fast, succeed early” mindset and emphasis on measurable outcomes.
Brings a high-energy and passionate outlook to the job and can influence those around them.
Able to build a sense of trust and rapport that creates a comfortable & effective workplace.
Passion for innovation and “can do” attitude.
BENEFITS
Comprehensive benefits package, including Health, Vision, Dental, and Life insurancesFSA and Life Assistance Program (EAP)
401(k) Retirement Plan
Health Advocacy, Travel Assistance, and My Secure Advantage
PTO Accrual and Holidays
#ZIP
Job Type: Full-time
Pay: $125,000.00 - $150,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Employee discount
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Referral program
Relocation assistance
Retirement plan
Tuition reimbursement
Vision insurance
Compensation package:
Performance bonus
Yearly bonus
Experience level:
3 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Lutz, FL 33558: Reliably commute or planning to relocate before starting work (Preferred)
Experience:
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person","$137,500 /yr (est.)",501 to 1000 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2018,Unknown / Non-Applicable
Atika Technologies,#N/A,"Pittsburgh, PA",Data Engineer,"Role: Data Engineer/PySpark Developer, Pittsburgh, PA - Day One Onsite
Fulltime and Contract(W2)
LinkedIn Must
USC/GC/EADs
Three round of interview: First two on Zoom Video Call, End Client Round will be Onsite on the Client's Location (Pittsburgh, PA)
Job Description:
Python(PySpark), Jupiter hub, airflow, Java
Database knowledge - Oracle / SQL / Data Analytics
Regulatory reporting knowledge preferred
Good communication skills
Open / Ability to learn new technologies
Very good Analytical Skills
Experience in ETL is preferable
Job Types: Full-time, Contract
Salary: $120,000.00 - $130,000.00 per year
Benefits:
Dental insurance
Health insurance
Life insurance
Compensation package:
Bonus pay
Performance bonus
Yearly pay
Experience level:
10 years
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Pittsburgh, PA 15259: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Required)
Spark: 4 years (Required)
Regulatory reporting: 3 years (Required)
SQL: 4 years (Required)
IT: 10 years (Required)
Airflow: 4 years (Required)
Jupiter: 4 years (Required)
Work Location: One location","$125,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
WellTrust Medical Group,#N/A,"Charlotte, NC",Data Engineer,"The Data Engineer will assist the senior analyst in complex modeling and projects to ensure their completion within the allotted timeline. Helps track measures of success and develop reports that measure operational and customer metrics. Demonstrates ability to troubleshoot data consumption and data quality problems.

Responsibilities:
Build and maintain reports using My SQL Server
Identify optimal data sources, structure data validation, and cache optimization.
Performs business problem modeling and quantitative analytics including regression analysis, time-series, hierarchical cluster analysis, and data mining.
Helps track measures of success and develop reports that measure operational and customer metrics.
Demonstrates ability to troubleshoot data consumption and data quality problems.
Evaluates and recommend improvements to current models, metrics, and analytic processes to improve operational efficiency and insight.
Design, document, and communicate effective technical solutions meeting end-user requirements
Perform analyses with large amounts of complex data, and uses data visualization tools to communicate results.
Requirements:

Bachelor's degree or equivalent experience in a quantitative field (Mathematics, Statistics, Computer Science, Economics, Finance, Physics, Engineering)
3+ years of professional working experience as a Data Engineer or Analyst
1-3 years of data warehouse experience
Build and maintain reports using My SQL Server
Strong analytical and creative problem-solving skills
Design, document, and communicate effective technical solutions meeting end-user
SQL and Excel Macro development
Strong understanding of various data environments such as SQL, Oracle, Big Query, etc.
Experience with data visualization tools such as Looker Studio, Tableau, D3.js, etc.
Proficient in R, Python, MATLAB, Java, or another programming language.",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Fracsys Inc,#N/A,"Washington, DC",Data Engineer,"Fracsys Inc is hiring a Data Engineer position. The ideal candidate must have at least 12+ years of industry experience. He or she must be responsible for successful technical delivery of Data Warehousing and Business Intelligence projects using a formal project management methodology.
Responsibilities
Directs, motivates and develops key technical personnel
Provides technical direction across the full lifecycle system development process
Provides strategic planning and comprehensive senior level technical consulting to IT senior management and senior technical staff
Working independently within guidelines
Provides detailed inputs for plans and schedules, including goals, risks, and resource allocation
Conduct diagnostic, troubleshooting steps and present conclusions to a varied audience
comfortable slinging some code to solve a problem
Build Data Marts and Data warehouses to facilitate analytical capabilities across the varied sources of data
Design and implement data patterns on cloud using cloud Data warehouse and data virtualization tools
Evaluate new tools and technologies for target state on cloud
Learn and implement new ETL, BI tools and data warehousing technologies
Qualifications
Minimum twelve (12) years in a Technical Lead or Consulting role leading Information Management related initiatives (system integration, data warehouse build, data mart build, or similar)
MUST have proven leadership experience for team of at least 3 technical resources.
Experience creating Data Marts, Data warehouses on-premise and on cloud for real time and batch processing frameworks
Experience building and optimizing big-data data pipelines and data sets including Postgres, AWS Relational Data Service
extensive hands-on experience building and optimizing data structures for data analytics, data science and business intelligence
Experience building self-service data consumption patterns and knowledge of cloud-based data Lake platforms
Experience wrangling data (structured and unstructured), in-depth knowledge of database architecture
Experience utilizing or leading implementations leveraging ETL tools (Informatica / Talend), BI Reporting tools such as MicroStrategy, Microsoft Power BI, data modeling tools such as Erwin, Oracle, SQL server, NoSQL, JDBC, UNIX shell scripting, PERL and JAVA, XML/JSON files, SAS , Python, AWS cloud-native technologies, S3, Athena, Redshift
Familiarity with the following technologies: Hadoop, Kafka, Airflow, Hive, Presto, Athena, S3, Aurora, EMR, Spark
Ability to drive, contribute to, and communicate solutions to technical product challenges
Ability to roll-up your sleeves and work in process or resource gaps and fill it in yourself
Excellent written and oral communication skills in English
Education
Bachelor’s degree in computer science, Data Analytics, Information Systems, or a related degree or equivalent experience. Master’s degree is preferred.
Job Type: Full-time
Pay: From $96,336.72 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Experience level:
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Washington, DC: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person","$96,337 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Take Command Health
4.9",4.9,"Dallas, TX",Data Engineer,"Company: Take Command
Position: Data Engineer
Salary: $120,000-$130,000 plus bonus
Location: Dallas or Austin, TX
About Take Command
Take Command is a start-up on a mission to improve the healthcare system, starting with health insurance. Pragmatically speaking, we help employers reimburse employees for individual insurance instead of offering a traditional one-size-fits-all group plan. We believe this model can empower employees (when they have the right support) to be savvy healthcare consumers and have a transformative impact on the entire healthcare system.
Let’s be honest—health insurance is usually a confusing, frustrating, and even emotional experience for people. We want to fix that with a new model, great technology, and a superior user experience. We have made a great start, but we need your help to fully realize our long-term vision.
As a part of the dev team, you'll build the framework for both our clients and our team to succeed. You will be a pillar in our company as we grow and make our product the best product on the market!
In this role, you will:
Take ownership for delivering multiple features, their quality and assist team members in delivering their features
Recommend improvements to data quality, processes and code to make it easier to use, manage and maintain
Experience designing and developing data models and data migrations
Experience managing and maintaining ETL or ELT processes (Hevo, BigQuery, Looker, etc.)
Hands-on development experience with SQL, relational database design (ERDs)
Must be able to write SQL & contribute code
Be product-minded; focus not only on what needs to be built but why something needs to be built
You may be a great fit for this role if you have:
3 or more years of professional experience as a data analyst or data engineer
Cloud-Based Data Tools e.g. Hevo, BigQuery, Looker, etc.
Database technologies: SQL, RDBMS / Relational databases (Postgres, Amazon RDS / Aurora, etc.)
Nice to have experience: Python, Java, Typescript or another modern OOP language
Cloud infrastructure: AWS
Source control: Git
Working at Take Command
We’re excited to build a team and culture that reflects our values! We offer competitive pay, health benefits, and equity options to share with this position. We want you to be an owner along with us in revolutionizing healthcare.
A generously funded ICHRA for medical, dental, and vision premiums and medical expenses. You get to use our own product and we think that’s so exciting and rare!
Unlimited personal vacation in addition to regular company holidays.
401(k): 90-day eligibility for 2% match that vests immediately!
Access to LinkedIn Learning.
We have two beautiful offices conveniently located near The Hill (Dallas) and Holly Commons (Austin). The kitchen is well-stocked and we've designed the space to have lots of different areas to work!
Paid parental leave for new parents.
Flexible on where you work – we hope to see you around the office on average 3 days a week especially when white-boarding or building relationships with your colleagues, but you also have the ability to work from home or wherever you'd like when you need to get focus work done.
More About Us
We secured our Series A funding in 2021 and are thrilled to be able to expand our team. Despite being a small startup in a land of health insurance giants, we’re the recognized industry leader for what we do (health insurance reimbursements) and passionate about bringing it to market because we know we can help fix a broken system and improve our clients’ wellbeing and health outcomes.
We’ve been featured in The New York Times, The Wall Street Journal, The Dallas Morning News, and other national healthcare publications and are excited about our growth opportunities.
Take Command knows diversity and inclusion among our teammates is integral to our company’s success and growth. Our vision is to recruit, develop, and retain the best team from a diverse candidate pool.
This has mostly been about us, but we’d love to hear from you-we can’t wait to hear your story!
Take Command is an equal opportunity employer! We celebrate diversity and are committed to creating an inclusive environment for all employees.
Xrygn4NXMr","$125,000 /yr (est.)",1 to 50 Employees,Company - Private,Insurance,Insurance Agencies & Brokerages,2014,Unknown / Non-Applicable
"SECURE RPO
4.3",4.3,Manhattan,Senior data engineer,"Must have skills:
8+ years of experience building high performance scalable enterpris`e analytics or data centric solutions
8+ or 5+ years of experience in the design and build of data extraction, transformation, and loading processes by writing custom data pipelines
At least 5 years of experience implementing complex ETL pipelines preferably in connection with Glue and/or Spark
Exceptional coding and design skills in Python or Java/Scala
Hands-on experience with AWS (i.e. Glue, Aurora Postgres, Lambda, EMR, EKS, Redshift, etc.)
Experience with visualization tools like QuickSight, PowerBI, Looker or Tableau
Experience with Talend (ETL) is a big plus
Roles and Responsibility:
Drive a high impact and high visibility project that enables data availability, encompasses data analytics, machine learning, and petabyte scale datasets, and provides reliable and timely access to thousands of data sources
Design, architect and support systems for collecting, storing, and analyzing data at scale
Recommend improvements and modifications on new and existing data and ETL pipelines. Create optimal data pipeline architecture and systems using Apache Airflow
Create data analytics for d ata scientists to innovate, build and optimize our ecosystem
Assemble large, complex data sets that meet functional and non-functional business requirements
Be responsible for ingesting data into our data lake and providing frameworks and services for operating on that data including the use of Spark
Analyze, debug and correct issues with data pipelines
Operate on or build solution required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, AWS and Spark technologies
Job Type: Full-time
Salary: $56.80 - $80.16 per hour
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Informatica: 7 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: On the road",$68.48 /hr (est.),1 to 50 Employees,Unknown,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Fuge Technologies Inc
4.7",4.7,"Jersey City, NJ",Big Data Engineer,"Big Data Engineering Technical Lead
Mandatory Skills
Big Data – Spark, Kafka, Databricks, Hive, Nifi
AWS, Java, Scala
Client Interview Needed for Selection (yes / No)
Yes
Detailed JD (Pl share the Detailed Description, 1 liner JD will not work)
10+ yrs experience as Data Engineer
Extensive experience in spark on data processing
Kafka design experience like monitoring & parallelism.
Comfortable with Python/Java/Scala coding experience
Crisp / clear cut communication
Flexible with learning new technology
Flink experience is a plus
AWS experience is nice to have
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Jersey City, NJ 07302: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",$72.50 /hr (est.),1 to 50 Employees,Contract,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Cureatr
4.1",4.1,Remote,Data Engineer,"About Us
We are a thriving, medication management solutions company, headquartered in Manhattan, but largely virtually distributed, committed to tackling the $528B suboptimal medication management problem. Patient medication information is often incomplete for healthcare providers and systems – it’s our mission to fill in the gaps. We provide real-time, universal access to accurate medication data for over 265 million patients and high-quality interventions by board certified telepharmacists. Through our solutions, healthcare providers can reduce preventable hospitalizations, avoid readmissions, and ultimately, improve patient outcomes. We’re already used by dozens of hospital systems, with tens of thousands of clinicians and supporting staff using us to care for millions of patients.
Here at Cureatr, we are driven by a core set of values and strive to incorporate them into everything we do. We care deeply about our users, patients, and employees, and we aim to live and operate compassionately. We take pride in creating a diverse, inclusive, and equitable company culture, where our open and honest approach to work fosters innovation and encourages employees to achieve professional and personal growth.

The Role
We’re looking for a Data Engineer to join our growing Engineering team. Use the power of healthcare data to find insights that will demonstrate the value Cureatr is delivering to our customers, help improve patient outcomes, and expand the scope of Cureatr’s mission. As a Data Engineer at Cureatr, you will be responsible for designing, implementing and maintaining our data infrastructure, pipelines, automation, and ETL processes. You will work closely with our Data Science and Analytics teams to ensure that our data is clean, accurate, and readily available for analysis. You will also be responsible for ensuring that our data meets industry standards for security, privacy, and compliance.
This is a remote, work from home position, but candidates must be located within the U.S. Ideal candidates must be able to travel to our New York City office, or TBD company-wide offsite locations, up to 4x per year.

You’ll be part of a team working to solve problems like:
Increasing the number of patients who receive the opportunity to do a medication reconciliation, which can help keep them out of the hospital.
Streamlining and automating processes so that our pharmacists can serve as many patients as possible.
Predicting patient events likely to occur so that our clinical team can adequately prevent them before it’s too late.
Helping our pharmacists make the most of data and insights that will help them be as effective as possible in serving patients.
Evaluating the effectiveness of our clinical services impact at the population level.

What You’ll Do:
Design, implement, and maintain our data infrastructure, pipelines, automation, and ETL processes to support our data needs.
Build and maintain scalable data models and databases.
Collaborate with Data Science and Analytics teams to understand their data needs and provide data solutions accordingly.
Ensure data quality and integrity by implementing data validation and cleansing processes.
Ensure data security and compliance with healthcare regulations.
Monitor and optimize data performance and scalability.
Design and implement data governance policies and procedures.
Evaluate and implement new data technologies and tools to improve our data capabilities.

Your Skills:
3+ years of experience as a Data Engineer or similar role.
Strong programming skills in languages such as Python and SQL.
Experience using git to track changes.
Experience with data modeling and database design.
Experience with the “modern data stack” preferred
Experience with ETL and data pipeline development.
Knowledge of healthcare data and related standards (e.g., HL7, FHIR) is a plus.
Familiarity with AWS.
Strong problem-solving and analytical skills.
Excellent communication and collaboration skills.

Technology Stack:
Python
RedShift
dbt
Dagster
MongoDB
Looker
AWS

Educational Requirement: Bachelor’s Degree or relevant work experience

Salary: $110,000 - $140,000 plus full benefits

Why work with us?
Make a measurable impact on patients’ lives
Be part of a company dedicated to living its core values:
Empathy
Honesty
Integrity
Impact
Fun
Equity in Healthcare
Work in the company without micromanagement
Share your vision and make an impact on the product
Opportunities for career growth
Remote work
Competitive salary
Premium benefits (Health, Dental, Vision, 401k, unlimited PTO, more)

Equal Employment Opportunity Policy
Cureatr is an equal opportunity/affirmative action employer. All qualified applicants will receive consideration for employment without regard to sex, pregnancy, race, religion or religious creed, color, gender, gender identity, gender expression, national origin, ancestry, physical or mental disability, medical condition, genetic information, marital status, registered domestic partner status, age, sexual orientation, military or veteran status, protected veteran status, or any other basis protected by federal, state, local law, ordinance, or regulation and will not be discriminated against on these bases
Disclaimer: we are aware of a scam targeting applicants using a cureatrcareers.com email address. Please block and report this sender, as they are not affiliated with Cureatr. If you have any questions, please contact us at jobs@cureatr.com. Thank you!
uBajh3OfeT","$125,000 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2012,Less than $1 million (USD)
"Loyal
3.9",3.9,"Atlanta, GA",Senior Cloud Data Operations Engineer,"Loyal is an organization centered on experience and building a platform that allows consumers to make meaningful decisions when it comes to healthcare. We deeply understand providers, locations, services, appointments, business rules, and moreover, we understand patients - who they are, the preferred method of communication, upcoming appointments, lapsed appointments, outstanding bills, health risks, and more. With this intelligence, our platform fuels highly relevant and personalized experiences across all mediums (website, email, voice…) allowing patients to get healthy, stay healthy, and have a better relationship with the health care provider.
**This is a remote role**
Summary
The Senior Cloud Data Operations Engineer is responsible for delivering Loyal's cloud-based data warehouse. They interface with groups across the organization to ensure we are collecting and storing data in a way that makes it easy to create and consume. They identify best practices for big data management, select the right tooling, create a workable data architecture, and drive an implementation roadmap others can depend on. The Senior Cloud Data Operations Engineer also works closely with our security group to ensure data security and privacy. They also mentor others on the team, providing technical leadership to those they work with.
You will
Create data streams and consolidate data sources into a single cloud-based data warehouse
Architect and catalog data warehouse storage schema
Work with product teams to enable easy data aggregation
Work with data consumers (analysts, data scientists, and AI/ML engineers) to ensure data is easily accessible and consumable
Create and execute a roadmap that meets data storage and availability goals
Ensure data is secure through data analysis, access control, and encryption
You have
Bachelor's degree in computer science, information systems, applied math or related field, or equivalent work experience
Minimum 5 years of experience working with cloud data processing services (AWS, Azure, or GCP).
Minimum 3 year experience with Big Data technologies, such as Databricks, Snowflake, Spark, Hadoop, etc.
Strong experience in creating data architecture and storage schemas required
Significant working knowledge of SQL and other query languages and tools
Significant experience with building relational database systems strongly preferred
Experience with ETL tools a plus
Knowledge of Python preferred
Software development, data analytics, or data science experience is a plus.
Experience working within a software as a service (SaaS) company preferred.
Experience working within a high growth and/or ambiguous environment, with proven experience to be dynamic preferred.
#LI-REMOTE
We know that potential candidates are often less likely to apply to a position if they don't match 100% of the job qualifications. Don't let that be why you miss out on this opportunity! We encourage you to apply if you can demonstrate many of these skills and competencies.
Loyal to our employees
We are a remote-friendly company! We encourage you to apply from anywhere in the United States. We also believe in a work/life balance that fulfills you while you're here and supports you when you're not. We built our benefits package to prove that we're committed to you having everything you need (including a little fun). Here is what we offer full-time employees:
Flexible paid time off, sick and personal days
At least one holiday per month (sometimes, more!)
Full health, dental, and vision insurance - Loyal pays the premium for all employees!
One Time Home Office Setup Stipend For Remote & Hybrid Roles
Monthly Internet Stipend for Remote & Hybrid Roles
Long term & short term disability
401[k] plan
16 Weeks Paid Parental Leave
2 Volunteer days per year
Matching Gift Program
Participation Grant Program
Annual Travel/Team Events up to twice per year (post-COVID)
Our Commitment
We believe that the key to Loyal's success is you. Your unique background, life experience, knowledge, self-expression, and talent make you uniquely you. Who you are, what you have experienced, and how you think inspires us to be innovative and bold.
Loyal is an equal opportunity employer. We hire great people from a wide variety of backgrounds, not just because it's the right thing to do, but because it makes our company stronger. We welcome the unique contributions that you can bring in terms of your education, opinions, culture, ethnicity, race, ancestry, sex, gender identity and expression, national origin, citizenship, marital status, age, languages spoken, veteran status, color, religion, disability, sexual orientation, and beliefs.
We consider qualified applicants regardless of criminal histories, consistent with legal requirements.
Further, consistent with applicable federal and state law, Loyal provides reasonable accommodations when requested by qualified applicants or employees with disabilities, unless doing so would cause an undue hardship. Loyal's policy regarding requests for reasonable accommodation applies to all aspects of employment, including the application process. If you require a reasonable accommodation in connection with the application process, please contact the Talent Acquisition Department at talentacquisition@loyalhealth.com.
E-Verify
This employer participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S. If E-Verify cannot confirm that you are authorized to work, this employer is required to give you written instructions and an opportunity to contact Department of Homeland Security (DHS) or Social Security Administration (SSA) so you can begin to resolve the issue before the employer can take any action against you, including terminating your employment. Employers can only use E-Verify once you have accepted a job offer and completed the I-9 Form.
COVID-19 Vaccinations
Consistent with Loyal policy, candidates performing in-person work will be required to show proof of being fully vaccinated against COVID-19 upon commencing employment. Reasonable accommodations will be considered on a case-by-case basis for exemptions to this requirement in accordance with applicable law. If you require a reasonable accommodation to Loyal's vaccination requirement, please contact the People Department at people@loyalhealth.com.",#N/A,51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2015,$5 to $25 million (USD)
"Trella Health
4.8",4.8,"Philadelphia, PA",Senior Data Engineer,"At Trella Health, we are passionate and committed to our mission – empowering meaningful change in healthcare. Since our founding in 2015, we continue to grow our team, enhance our solution and services offerings, accelerate into new markets, and expand our customer base. We are rapidly growing and are looking for new Trellavators to join our team!
""What is a Trellavator?"" you ask. Innovate and elevate is the name of our game! We go above and beyond to collaborate with and support each other – we believe that when a colleague or customers succeeds, we succeed. By learning from others, building on our successes, and taking risks, we constantly raise the bar – continuous improvement is in our DNA. Our word is paramount, we keep our commitments, and we always follow through. We have a strong, reliable support system that fuels growth, collaboration, and passion – and together, we create a positive environment where everyone at Trella Health, including the customers we support, can thrive. Are you ready to learn more about the opportunities with our team? Trell-yeah you are!
Position Overview:
The Senior Data Engineer will be responsible for designing, implementing, and maintaining an effective data pipeline for Trella Health's applications. This position is responsible for turning data concepts into effective data models that are efficient and scalable. The Senior Data Engineer will ensure a smooth data release and deployment process throughout the data development lifecycle.
Successful candidate will be a highly organized, self-starter that can roll up their sleeves and solve problems efficiently. Ability to document procedures, manage time, and have a knack for optimizing data flows.
Location: Atlanta HQ; Raleigh, NC; Nashville, TN; Philadelphia, PA; Remote considered
Reports to: Manager, Data Engineering
As a member of the Data team at Trella, you will:
Champion a comprehensive data vision and operating principles that improve data quality, consistency, accessibility, decision making, use, and stewardship.
Manage data pipeline by developing processes for data input, transformation, and storage
Act as an interface between the Data Science team and Application Developers
Architect and implement front-end (application layer) and back-end (ETL layer) database technologies in a highly customized data environment
Develop automated processes for moving data between systems
Implement ETL processes that transform source data into functional data stores consumed by front-end applications
Scale data pipeline by utilizing the most efficient tools and technologies
This job might be a fit for you if you have:
Bachelor's degree in Computer Science, Engineering, Mathematics, or other technical discipline (proven professional experience will be considered in lieu of a technical degree)
2+ full years of recent advanced Snowflake development experience
Experience manipulating healthcare data
In-depth knowledge of Amazon Web Services (AWS) and Platform as a Service (PAAS)
Demonstrable experience with Snowflake utilities including SnowSQL & Snowflake Javascript
Analyze all aspects of the Snowflake Query Engine to improve bottlenecks that may exist
Ability to automate data ingestion from Amazon S3, FTP, and other technologies
Experience with scripting languages, extensive Python experience, Node.js experience a plus
Working knowledge of Bitbucket and Git
Experience managing and loading data in AWS RDS (MySQL/MariaDb)
Ability to optimize MySQL query execution plans for robust performance
Technical Job Requirements (Nice to Have)
Experience manipulating healthcare data including claims, ADT data
Foundational knowledge of CI/CD platforms, preferably TeamCity
Development and consumption of API services
Experience with data science tools (Jupyter, SageMaker, R)
Understanding of Atlassian suite of tools, including Jira and Confluence
Soft Skill Job Requirements (Nice to Have)
Strong analytical skills and ability to rapidly switch contexts
Excellent communication and problem-solving skills
Sole contributor that is willing to independently uncover and improve processes that are inefficient or sub-optimal
Track record of implementing creative solutions to complex problems
Experience working in a nimble, fast-paced, start-up environment
Knowledge of HIPPA, PHI, PII policies
About Trella Health:
Trella Health provides unmatched, actionable market intelligence to post-acute care and value-based care providers of all sizes. Our industry leading analytics paired with CRM and EHR integration workflows positions us as the most advanced sales enablement platform for the post-acute care market. One of only a few companies to be deemed both a Qualified Entity by CMS and an Innovator under its Virtual Research Data Center Program, Trella has elite access to billions of performance metrics and referral data to enable competitive positioning across the market. Its standardized insights, representing 90% of all lives 65+ U.S. population, help customers identify, engage, and manage critical relationships and advance their organizations with certainty.
Since launching our first product in 2016, we have experienced tremendous growth and have highlighted a few key data points to get an at-a-glance view of Trella:
Over 700 customers including 10 of the top 10 post-acute providers and 4 of the top 10 health systems
Over 120 team members and growing
Recognized as one of Modern Healthcare's ""Best Places to Work"" in consecutive years and were ranked in the Top 50 startups in the US by LinkedIn
Headquartered in Atlanta, GA with remote hubs in Nashville, Philadelphia, Raleigh, and the Philippines
The Trellavator Experience:
At Trella Health, you can expect to join a welcoming team that truly embodies our core values. Our collaborative culture is anchored by trust, transparency, and inclusion — and we also have a lot of fun. No matter where you work, at Trella Health you can expect an awesome team, frequent virtual gatherings, engaging events, casual attire, and more.
We offer competitive salaries with a comprehensive benefits package to all employees and provide an environment that fosters work-life harmonization with unlimited PTO, company-observed holidays, and Summer Fridays. As we continue to see exponential growth, our goal is to continue to put team members first and strive to offer our team members the best culture and benefits possible. Some of the benefits we provide are:
Health, Dental, Vision & Voluntary Benefits
Competitive Salary & Bonus Plans
401k Retirement Savings
Unlimited PTO & 10 Paid Holidays
Flexible Work Hours
Equity Shares
Paid Leave Programs
Marketplace for discounted retail and entertainment
Equal Opportunity Employer
Trella Health is an equal opportunity employer. All persons will receive consideration for employment without regard to race, color, gender, sexual orientation, gender identity or expression, religion, national origin, marital status, age, disability, handicap, veteran status, genetic information, or any other protected status as recognized by federal, state, or local laws.","$113,846 /yr (est.)",51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2015,Unknown / Non-Applicable
"Argo Data
2.7",2.7,"Richardson, TX",Senior Data Engineer,"ARGO is a leading provider of software and analytics solutions for both the financial services and healthcare industries. ARGO transforms business processes for financial service providers and healthcare organizations using proven business models and software innovation informed by real customer challenges, breakthrough technology, and rich analytics.
ARGO- Data Engineer II (Markham, ON; Memphis, TN; ...) (theapplicantmanager.com)
JOB SUMMARY
The Data Engineer II is responsible for designing and implementing data solutions to support business needs. The successful candidate will have a strong background in software engineering and database design, as well as experience with working with cloud-based tools and technologies.
EXPECTED WORK AND PERFORMANCE
This section is intended to provide a more detailed overview of what the expectations of the job are to include: work to be performed, expected performance results and how they will be measured, as well as connection to corporate strategy, values, and mission. Job duties and responsibilities include the following. Other duties may be assigned as needed.
Design, build, and maintain data pipelines for data extraction, transformation, and loading (ETL) from various sources.
Ensure data security and privacy protocols are implemented and enforced.
Monitor and optimize data storage and processing performance to ensure high performance and cost-effectiveness.
Collaborate with data scientists, analysts, and other stakeholders to understand their data needs and translate them into technical solutions.
Maintain documentation of data systems, processes, and policies.
Stay current with the latest data tools, technologies, and trends and recommend improvements.
Mentor and train junior data engineers as needed.
QUALIFICATIONS
Bachelor's degree in Computer Science, Software Engineering, or related field.
3-5 years of experience in software engineering and SQL database design.
Proficiency in programming languages such as Python.
Strong understanding of data warehousing, ETL processes, and data modeling.
Experience with cloud infrastructure technologies such as Microsoft Azure.
Excellent problem-solving and analytical skills.
Ability to work well in a team-oriented environment and communicate effectively with both technical and non-technical stakeholders.
PHYSICAL DEMANDS
The candidate will work at our corporate headquarters in Richardson, TX. While performing the duties of this job, the employee is regularly required to stand; walk; sit; use hands to finger, handle, or feel; reach with hands and arms; talk or hear. Specific vision abilities required by this job include close vision, distance vision, color vision, peripheral vision, depth perception, and ability to adjust focus.
ADDITIONAL REQUIREMENTS:
Applicants for U.S. or Canadian based positions with ARGO must be legally authorized to work in the United States or Canada. Verification of employment eligibility will be required at the time of hire. Visa sponsorship is not available at this time for ARGO positions.
Pre-employment background screening will be conducted.
Job Type: Full-time
Pay: $70,745.42 - $141,624.04 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Life insurance
Paid time off
Professional development assistance
Referral program
Relocation assistance
Retirement plan
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Performance bonus
Yearly bonus
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Richardson, TX 75081: Reliably commute or willing to relocate with an employer-provided relocation package (Required)
Experience:
Informatica: 3 years (Required)
SQL: 5 years (Required)
Data warehouse: 3 years (Required)
Python: 3 years (Required)
Azure: 3 years (Required)
Work Location: In person","$106,185 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Computer Hardware Development,1980,$25 to $100 million (USD)
"Hiscox
4.1",4.1,"Atlanta, GA",Senior Data Engineer,"job description:
You will be a senior member of our US IT Team data engineering practice. You will be an early member of this team and will work closely with the Director of Data Engineering to help establish it. We will have our work cut out for us delivering planned work, building the practice and evolving our technical landscape. This is an exciting opportunity for the right individual to join at the very beginning and grow with the new team.
As a senior you will be expected to lead, mentor others and deputise for the director of data engineering when required. Yoi will also continue to contribute as a hands-on engineer. You will have the knowledge, best earned through 5+ years of experience, to truly inhabit this senior role in an ambitious team.
What you’ll be doing:
Working with business stakeholders and our delivery team to understand high value business problems that can be solved through the application of data processing and analytical systems.
Developing, expanding and evolving our existing databases and ETL pipelines.
Working with architects to design, build and support a transformational cloud-based data platform for the business.
Being a core and professional member of the new data engineering practice
Understand business requirements and help refine into development tasks and estimate their complexity.
Research, evaluate and adopt new technologies with a right tool for the job mentality.
Focus on both speed of delivery and quality, with suitable pragmatism – ensuring your solutions are always appropriate and not too complex or over-engineered.
Quick progression of projects from proof-of-concept to post-production stage.
Communication and presentation of ideas to colleagues in all parts of the wider tech team.
Participating in code reviews for the data engineering practice.
Act as a servant leader in our practice – ensuring team members have
person specification:
Knowledge of, or ability to rapidly adopt our core languages for data engineering – SQL and/or Python.
Experience working using a modern DevOps approach. You should not just have written data code but also the CI/CD pipelines necessary to test and deploy that code in a professional environment.
A robust understanding of core data engineering topics – ETL vs ELT, structured and unstructured data, data quality and data governance.
Ability to contribute to all aspects of a solution – design, infrastructure, development, testing and maintenance.
The ability to design and advocate for technical solutions to business problems.
Effective collaboration with technical and non-technical team members through agile ceremonies – roadmap planning, feature workshops, backlog elaboration, code review.
Understanding of cloud technology and significant hands-on experience working with it. You must understand how cloud-native solutions must be built differently from traditional ones.
Track record of taking initiative and delivering projects end-to-end; clear evidence of being self-driven and motivated
Immense curiosity, high energy and desire to go the extra mile to make a difference.

Desirable Skills
Beyond the required skills we are open to individuals of diverse talents. Experience with additional technologies, data science knowledge or a business background are all valued. We want to know how your unique abilities can contribute to our team.
Our Technology
We operate in a diverse technical landscape and are looking for flexible engineers who can adapt to and use many different tools. We would not expect any engineer to be familiar with the entire tech stack – no engineer can. Instead, we seek people with a good understanding of data structures and algorithms and the ability to apply this knowledge in learning new tools.
We have an existing data landscape involving both MS SQL Server and Oracle databases. We use SSIS for ETL. We will also need to source data from our integration layer – webMethods.
If some of that sounds like it needs bringing up to date, we know! We hope to start building a modern cloud data platform to realise the full value of our data. The technologies we use for that platform are still to be decided. Likely it will be based on Snowflake with Azure Data Factory for ELT. Other parts of our business have had success using Prefect for orchestration and dbt for transformation and we will reuse where it is sensible.
We prepare data for use by analysts working with a variety of tools – Tableau, PowerBI, and even Excel.
We take a DevOps approach and strive for continuous integration / continuous deployment. We use Azure Pipelines to deploy our code. We deploy infrastructure the same way using Terraform and Docker.
Our technology landscape is not fixed. Our engineering and architecture teams drive the technology we adopt.


Salary range $140,000 - $170,000","$155,000 /yr (est.)",1001 to 5000 Employees,Company - Public,Insurance,Insurance Carriers,1901,$1 to $5 billion (USD)
"Egrove Systems Corporation
4.4",4.4,"Dallas, TX",Data Engineer Flink,"Java, Bigdata, Azure & Flink
Flink Sr. Developer, who has implemented and dealt with failure scenarios of processing data through Flink.
A senior who can help his Flink development team, guiding and helping them implement custom solutions through Flink.
Seasoned Java developer who knows about all aspects of SDLC.
Worked on integrations of other technologies with Flink, eg: Kafka, MongoDB, etc
Azure experience in the areas of Messaging, Data processing, preferably on Flink or on Databricks.
Job Type: Contract
Salary: From $51.83 per hour
Compensation package:
Monthly bonus
Experience level:
11+ years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
Work Location: On the road
Speak with the employer
+91 732)-860-7733",$51.83 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Internet & Web Services,#N/A,Unknown / Non-Applicable
Redzara.us,#N/A,"Atlanta, GA",Azure Data Engineer,"Responsible for developing new and modernizing existing applications to Azure Cloud Platform.
The ideal candidate must demonstrate in-depth knowledge and understanding of Database concepts.
Requirements:
7+ years of experience in Data Warehousing and Business Intelligence
5+ Years in BI development leveraging the Microsoft SQL Server. Needs to be an expert in T-SQL
5+ Years in SSIS and use of Stored Procedures.
1-2 Years in Azure Data Factory, SQL VMs, build and maintain ETL
Ingest and aggregate data from both internal and external data sources to build our data lake
Strong understanding data modeling (i.e., conceptual, logical, and physical model design, experience with Operation Data Stores, Enterprise Data warehouses and Data Marts).
Job Type: Contract
Pay: $43.49 - $57.00 per hour
Compensation package:
1099 contract
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Atlanta, GA 30334: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure Data Factory: 3 years (Required)
Azure: 3 years (Required)
Data warehouse: 1 year (Preferred)
Work Location: Hybrid remote in Atlanta, GA 30334",$50.25 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Intersec
4.5",4.5,Remote,Senior Data Engineer,"Architect and implement highly scalable data pipeline to streamline migration of data from source data repositories to the centralized data platform
Implement agreed upon data models (e.g., data marts) to enable faster and efficient consumption of data by downstream consumers
Conduct performance tuning to optimize data migration techniques and ensure adherence to identified service-level agreements (SLAs)
Define and implement automated validation checks to ensure data integrity between source and target
Assist in onboarding consumers with existing technology to connect to the platform
Setup predictive alert mechanism for pipeline degradation for IMF pipelines.
Enhance data quality framework for data pipelines and derived datasets
Enhance Data Audit Balance and Control mechanism for data pipelines
Enhance pipelines to be certified copies of authoritative source
Populate metadata for the Enterprise with particular focus on the Data Sources being onboarded to EDP (Technical and Business Glossaries) related to IMF Modernization Programs
Validate correct lineage for datasets
Required:
*8+ years of Databricks, Data Modeling, and/or Metadata management experience
* Familiarity with architecting and developing real and near-real-time solutions
* Bachelor's Degree in Engineering, Mathematics, computer Science, Statistics or other quantitative discipline, or 4 years equivalent professional experience
* US Citizenship and ability to gain a public trust
Preferred:
* Familiarity with IRS systems and ETL pipelines
* Ability to work independently and manage multiple tasks
* IRS MBI Clearance with IRS Laptop and Informatica Installed
Job Type: Full-time
Salary: $70.00 - $75.00 per hour
Schedule:
Monday to Friday
Application Question(s):
Are you a US/GC card holder?
How many years of Databricks, Data Modeling, and/or Metadata management experience do you have?
Work Location: Remote",$72.50 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,Unknown / Non-Applicable
Siri info solutions inc,#N/A,"Beaverton, OR",Sr. Data Engineer,"Must Have Skills
PySpark
Python on AWS
Responsibilities:
Understanding of advanced data warehousing concepts, data modeling and extract, transform and load development
Advanced SQL skills with experience querying large datasets from multiple sources and developing automated reporting
Python skills for scripting, data manipulation, custom extract, transform and loads and statistical/regression analysis particularly, as they apply to sales and marketing operations and performance
Job Type: Full-time
Salary: $47.00 - $50.00 per hour
Experience level:
7 years
Work Location: On the road",$48.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Fulcrum Analytics
4.3",4.3,Remote,Data Engineer,"At Fulcrum, we provide cutting-edge data science consulting and software solutions to help companies solve challenging business problems. Through the utilization of our innovative products and dedicated team, clients are able to operate more efficiently, manage risk, and make informed business decisions every day. Fulcrum Analytics has stood at the forefront of data, analytics, and innovative software for over 25 years, offering sophisticated solutions, groundbreaking technologies, and winning strategies that help companies achieve their targeted results.
Role Description:
We are seeking a highly-skilled data engineer with experience in data analytics and data science. The ideal candidate will be responsible for building and maintaining data pipelines, designing and implementing databases and data models, and developing data-driven solutions to drive business growth for our clients.
Responsibilities:
Design, build, and maintain efficient, scalable, and reliable data pipelines using SQL, or programming languages such as Python or Java.
Develop and maintain databases and data models to meet clients’ analytical needs.
Identify data quality issues and develop processes to ensure data accuracy, completeness, and consistency.
Work with cross-functional teams to define and implement data integration strategies for new and existing systems.
Develop and maintain automated testing and deployment of analytics code and data pipelines.
Collaborate with data scientists to develop and implement machine learning models, statistical models, and other advanced analytics techniques.
Communicate analytical results and recommendations to technical and non-technical stakeholders through presentations and visualizations.
Stay up to date with emerging technologies and industry best practices, and recommend tools and processes that can improve data engineering and analytics processes.
Qualifications:
Bachelor's or master's degree in computer science, statistics, data science, or a related field.
2+ years of experience in data engineering, data analytics, and data science.
Strong programming skills in Python, with experience in building data pipelines using frameworks such as Airflow, Glue, Lambda, Athena, and BigQuery as well as data platforms such as Hadoop, Snowflake, and GCP.
Experience working with SOAP or RESTful web services.
Familiarity with version control, in particular Git.
Experience in designing, building, and optimizing relational and NoSQL databases.
Familiarity with data visualization tools, such as Tableau or Power BI.
Ability to understand and transform complex data into actionable insights.
Strong analytical and problem-solving skills, with a focus on delivering practical solutions to business challenges.
Ability to learn and solve problems independently.
Excellent communication and collaboration skills, with the ability to explain complex concepts to both technical and non-technical audiences.
If you are passionate about data analytics and want to work in a dynamic environment with talented colleagues, we encourage you to apply for this role. We offer competitive salary and benefits packages, as well as opportunities for professional growth and development.


Position Type: Full-time salaried
Reports to: Senior Data Engineer
Base Compensation: $110,000 - $140,000
Benefits: Medical, Dental, Vision, 401k with Match, Bonus, Unlimited PTO
Location: Remote within the United States of America; Eastern time zone preferred","$125,000 /yr (est.)",1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,1993,$5 to $25 million (USD)
"Petrolink International
2.9",2.9,"Houston, TX",Data Analysis Engineer,"Location:
Houston, Texas, U.S.A.
Job Type:
Full-time
Job Description:
Petrolink is a global company that specializes in user-driven technology for the Energy Industry.
We are looking for a Data Analysis Engineer to assist with creating and delivering algorithms primarily used by our engineering solutions.
This position is based in our U.S. office in Houston, Texas.
Duties & Responsibilities:
Research and provide logic for engineering solutions algorithms.
Collaborate with Subject Matter Experts (Drilling Engineers, Mud loggers, Pore Pressure Analysts, and other specialists from the Drilling Industry) to understand different challenges in the Drilling Industry and the practices in place to address/solve these challenges.
Code the algorithms using C# to run on Petrolink’s proprietary analytics engine.
Develop, test, and maintain algorithms.
Support escalations related to issues in algorithms.
Learn existing algorithms to provide support when required.
Prepare training material and sessions for stakeholders as part of the production release.
Skills & Qualifications:
Proficient in C#.
Experience with Python is an advantage.
Knowledge in drilling practices, and ability to speak to other drilling experts and capture requirements and logic.
Knowledge in drilling performance KPIs is an advantage.
Prior experience with WITSML is an advantage.
Function and communicate well in a team having diverse expertise.
Bi-lingual (English and Spanish) preferred.
At least 2 years’ relevant experience.
Benefits:
Health insurance
Dental insurance
401(k) with matching
Health savings account
Life insurance
Long-term disability
This job description is intended to explain the general nature and level of work being performed. Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.
Applicants must be legally authorized to work in the United States for any employer.
To apply for this position, applicants must meet the above criteria.
Interested applicants fulfilling the above criteria, please send your resume to careers.usa@petrolink.com.","$81,133 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"Kastech Software Solutions Group
4.0",4.0,"Dallas, TX",Data Engineer,"In KastechSSG we are are seeking for a Cloud DB Platform Engineer with Data Enablement Services experience ($50/hour), to work in a Hybrid Schedule (3 days on-site, 2 days home office) for one of our partner companies!
Hybrid Schedule info: onsite 3 days per week from list of locations: Charlotte NC, Dallas TX, Chandler AZ, Newark, NJ - Must be local to one of these locations.
** Contract would be $50 per hour on W2 (18 months). American Citizens or GC Holders Only**
3-5 years experience in:
Terraform, Azure/GCP, DevOps Engineering, ETL, Client/AI, Data lake, Data architecture.
GCP Key Services Experience (ie Big Query, Composer, Vertex AI, DataProc, Data Catalog, Notebooks).
Azure Key Service Experience (ie Synapse, Purview, Data Lake, Logic Apps, Cognitive Services).
Job Type: Full-time
Salary: $50.00 per hour
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you American? If not, please specify your visa type
Would you be willing to work under W2?
Experience:
Terraform: 3 years (Preferred)
Azure Data Lake: 3 years (Preferred)
GCP Key Service Exp: 3 years (Preferred)
DevOps: 3 years (Preferred)
Work Location: In person",$50.00 /hr (est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2008,$25 to $100 million (USD)
Kanini,#N/A,"North Druid Hills, GA",Senior Data Engineer,"About Kanini
Kanini provides Agile Software Development, Cloud Computing, Data Science, and Location Intelligence services to public and private organizations. We have successfully served our clients in government, finance, transportation, utility, and software industries since 2003.
Why you should join
Working at Kanini is flexible and personal. We are a highly motivated, collaborative team experimenting with the latest technologies. We are committed to everyone having a healthy work/life balance, and we provide extensive mentorship and training resources to help you succeed.
Kanini is looking for Data Engineer who has a deep experience in Data Engineering, Agile planning, Snowflake, SQL, MySQL, PostgreSQL, Git, AWS Data Tools, Python, Jira
Job Description
The Senior Data Engineer is responsible for processing structured and unstructured data, validating data quality, and developing and supporting data products. The Senior Data Engineer also plays a role in Agile planning, providing advice and guidance, and monitoring emerging technologies.
Responsibilities
Gathers and processes raw, structured, semi-structured, and unstructured data using batch and real-time data processing frameworks.
Understands and enforces appropriate data master management techniques.
Ensures data quality and implements tools and frameworks for automating the identification of data quality issues.
Work with internal and external data providers on data validation providing feedback and making customized changes to data feeds and data mappings.
Understands the challenges that the analytics organization faces in their day-to-day work and partner with them to design viable data solutions.
Recommends improvements to processes, technology, and interfaces that improve the effectiveness of the team and reduce technical debt.
Implements and optimizes data solutions in enterprise data warehouses and big data repositories.
Installs, maintains, monitors, and supports business intelligence, distributed computation, and big data analytics tools.
Provides ongoing support, monitoring, and maintenance of deployed products.
Actively works with less experienced data engineers providing technical guidance and oversight.
Actively participates in the engineering community, staying up to date on new data technologies and best practices and shares insights with others in the organization.
Qualifications:
Bachelor’s degree in computer science or related field and 4 years work experience
Working experience with batch and real-time data processing frameworks
Working experience with data modelling, data access, schemas, and data storage techniques within Snowflake
Working experience in design, development, and implementation of highly scalable, high-volume software systems and components, client-facing web applications, and major Internet-oriented applications and systems
Working experience working with relational databases such as SQL, MySQL, Postgres/PostgreSQL
Working experience with business intelligence tools and platforms
Working experience with data quality tools
Working experience with application lifecycle methodologies (e.g. waterfall, agile, iterative)
Experience with ETL processes and tools
Experience working with Git.
Desirable Qualifications
Scrum Developer Certification or equivalent
Working experience with Sisense platform
Working experience with SQL Server Integration Services
Working experience with AWS data tools (Database Migration Service)
Working experience with Python
Experience working with Jira, Rally or similar tools
Kanini Software Solutions, Inc. does not discriminate in employment matters based on race, gender, religion, age, national origin, citizenship, veteran status, family status, disability status, or any other protected class. We support workplace diversity. If you have a disability, please let us know if there is anything we can do to improve the interview process for you; we’re happy to accommodate.
Kanini Software Solutions, Inc., 25 Century Blvd., Ste. 602, Nashville, TN 37214
Job Types: Full-time, Contract
Pay: $60.00 - $65.00 per hour
Application Question(s):
Current Location?
Work Location: Remote",$62.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Integrated Technology Strategies, Inc.
4.0",4.0,"New York, NY",Data Engineer,"Integrated Technology Strategies is a provider of information technology consulting. Digital transformation, improving business performance, developing strategies and enhancing value is at the core of what we are.

Integrated Technology Strategies is looking to hire a Data Engineer to be a part of our Consulting team.

We are looking for someone passionate about data is focused on developing the platform for critical data products, including real time business metrics and analytics capabilities. The role requires supporting and collaborating with groups including Data Analytics/BI/Product, as well as our core backend API team. The individual will not be afraid to think out of the box and will play a key role in technical decision making. We are highly focused on giving ownership and responsibility to autonomous teams, using the right tools for the job, and building flexible architectures.

Responsibility:
Continue to evolve the internal Reporting and Analytics platform on top of Snowflake on AWS infrastructure.
Experience in Architect, design and implementing scalable ETL and data processing systems to handle the big data ecosystem including data collection, processing, ETL and Data warehouse.
Build soft real time capabilities and insight into product metrics to help product managers and BI/Analytics understand and optimize product features and guide product decisions.
Participate and contribute to the capabilities and engineering priorities across the organization.
Contribute to the codebase and participate in code review.
Build analytics tools that utilize the data pipeline to
provide actionable insights into operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product,
Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Reporting to the Senior Director, Software Engineering, you’ll be responsible for overseeing engineering product quality and delivery and setting and overseeing technical standards for teams who are working on everything from customer-facing applications.

Skill Requirements:
Solid understanding of real time data processing with Kafka, Spark and Flink and batch data processing frameworks on EMR and Snowflake.
Passion for building world-class data platforms that support a global customer base
Solid engineering background and understanding of programming languages such as Python, Java or equivalent
5+ years of progressive experience in data infrastructure development, with a track record of successful high-quality deliveries
Experience of working in an agile environment and embracing engineering best practices
Ability to apply both technical competence and interpersonal skills to achieve business outcomes
High emotional intelligence, sound temperament, and professional attitude
Strong understanding of SQL, experience with key databases such as Snowflake, MS-SQL and Postgres
Knowledge of the internals of how database systems work to design models for varied use cases.
Experience with CI and CD in an AWS environment with Terraforms
Experience with key Data technologies, such as Sqoop. Kafka will be a plus
Proven experience in building secure data platforms
Bachelor’s degree in Computer Science or equivalent","$110,128 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$5 to $25 million (USD)
"spar information systems
3.5",3.5,Remote,Azure Data Engineer,"Role: Sr Azure Data Engineer
Location: Remote
Duration: 3 Months Contract to hire Full Time (W2 Only)
Must have 11+ IT Experience
Required Skills:
3+ years of experience in data software development, programming languages and developing with big data technologies
2+ years of experience designing and building on existing and new data applications
2+ years of experience in Cloud DevOps concepts, Cloud Services and Architecture, and Azure/AWS/GCP DevOps Operational Framework
1+ years of experience in open-source data tools and frameworks, or one of the following: .net Core, asp.Net, Angular, or Express.
Thanks & Regards,
Arvind Kumar Bind
Cell: 732 716 7403 (Text)
Direct Number:- 469-750-0607
Email : Arvind.B@sparinfosys.com
Job Types: Full-time, Contract, Permanent
Pay: $120,555.79 - $150,000.00 per year
Benefits:
401(k)
Health insurance
Paid time off
Experience level:
11+ years
Schedule:
8 hour shift
Experience:
Big data: 3 years (Required)
SQL: 1 year (Required)
Data lake: 3 years (Preferred)
Work Location: Remote","$135,278 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2012,$25 to $100 million (USD)
DocsInk LLC,#N/A,"Wrightsville Beach, NC",Data Engineer,"DocsInk is on a mission to provide a collaborative and exciting alternative to the traditional healthcare model. Through the complete virtualization of healthcare organizations, we are equipping providers with the ability to deliver better patient care with speed, efficiency, and accuracy. With the widespread focus on providing value-based care, DocsInk is making it possible to provide superior care while also maximizing reimbursements. In today’s environment, shifting the way we approach healthcare is essential and DocsInk is on the cutting edge of where the industry is headed. We are excited to be growing our team and are looking for driven individuals with a passion for making a difference in the healthcare industry.

As our Data Engineer, you will be in the driver’s seat as our team creates elegant and forward-thinking software solutions for our healthcare customers. You will work with fellow developers, designers, training & support, quality assurance, and management in a fast-paced environment to take our products to the next level and exceed expectations. We are looking for someone who is incredibly talented to do what most cannot and yet humble enough to easily work with peers, accept guidance from more experienced staff, and mentor more junior staff. If designing and developing complex integration efforts is what gets you out of bed each morning, then we want you to help us reshape healthcare.

People are our most important asset and the number one reason we all love working at DocsInk. As a team, we value accountability, transparency and collaboration. We have agile teams, with clear, outcomes-focused goals. Delivering innovative solutions and superior service are at the forefront of what we do, and the collective ideas of the team allow us to continue to excel in a fun and energetic environment.

Professional Requirements and Responsibilities:

Role / Responsibilities:

Implement robust ETL solutions that integrate healthcare data feeds using tools such as Apache NiFi, NextGen Connect (JavaScript), PostgreSQL, MySQL, and Linux shell scripting
Configuring and managing deployments to AWS infrastructure consisting of both traditional servers and serverless components. CI/CD tools include Bitbucket and AWS Code Deploy
Building direct interfaces with hospitals and EMR vendors utilizing industry standards such as HL7 2.x, C-CDA, and FHIR as well as implementing interfaces that consume formats such as XML, JSON, and delimited files
Utilizing APIs and web services to permit bi-directional flows of data to and from our partner organizations
Monitoring operational ETL processes, triaging and resolving issues as they arise to meet SLAs
Writing PL/SQL functions and packages to automate workflows in a Postgres environment linked with web technologies
Maintaining data standards (including adherence to HIPAA), documenting work, and championing process improvements
Work as a project team member to jointly deliver high-quality business solutions consistent with project objectives and constraints

Knowledge and Skills Required:

4+ years experience with HL7 2.x
2+ years experience with Apache Nifi
2+ years experience with NextGen Connect
Advanced knowledge of PostgreSQL, MySQL, AWS Code Deploy, HL7 2.x, C-CDA, FHIR, and HIPAA data compliance

Qualifications and Other Attributes Required:

Degree in Computer Science or equivalent field, or equivalent work experience
Highly self-motivated team player who takes initiative
Strong sense of personal accountability; contributes beyond job role and responsibilities
Excellent problem-solving skills; meticulous & methodical","$85,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Tipico - North America
4.0",4.0,"Denver, CO",Data Engineer,"Company Description

Tipico is an energetic, passionate, bold tech company, and we love sports! Tipico has recently expanded to the US market, establishing offices in Hoboken, New Jersey and Denver, Colorado. We are driven by our core values: passion, progress and trust. Our goal is to build the best possible mobile sports betting product in the industry, in order to amplify the emotions of millions of sports fans every day. Let’s Make This Interesting ™
Please note: this role is located in our Denver, CO office; we work off a hybrid model and come into the office 2 days per week.

Job Description

The Data Engineer will report directly to the Data Engineering Manager and work together with rest of the data engineers, BI analysts, and product teams. The position holder ensures this is done by doing the following:
Create and maintain both batch (ETL/ELT) and real-time data pipelines and architecture
Ensure all data provided by Data Platform is of the highest quality and is delivered in a timely manner and in line with agreed SLAs
Assemble large and complex data sets that meet functional and non-functional business requirements
Identify, design, and implement internal process improvements, including automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies
Collaborate with stakeholders including data architects, Executive, Product, Data and IT team members from the beginning to the delivery of a project
Build a scalable, modern, and trusted data ecosystem, making sure it is responsive, available and in line with business requirements
Take part in sprints, workloads, and deliverables through agile methodologies
Manage quality assurance, verification of accuracy and consistency of data, specifically KPI measures sent to the business
Thrive working in a fast-paced and challenging environment
Open to continuous learning individually and with the team
Comfortable with being challenged / seeking challenge
Willing to work flexible hours if needed

Qualifications
3+ years in Data Platforms / Datawarehouse / BI experience
Must have experience in successfully implementing data-centric applications, such as data warehouses, operational data stores, and data integration projects
Strong analytical and problem-solving skills that balance with creative approaches
Ability to learn new and complex concepts quickly
Strong experience with large-scale production databases and SQL
Solid understanding of cloud data services: AWS services such as S3, Athena, EC2, RedShift, EMR, EKS, RDS and Lambda
Proficient with one or more big data & new data technologies such as DBT, NIFI, Airflow, Kafka
Experience with data modelling
Experience with ETL development (extractions, data load, aggregation, transformation)
Knowledge in working with time-series/analytics databases such as Elasticsearch
Understanding of containerization and orchestration technologies like Docker/Kubernetes
Must be able to take ownership of projects with a high level of attention to detail and priority
Team player with a hands-on approach and attitude
Must be comfortable with on call rotation
Nice to have:
Familiarity with Marketing tooling and/or CRM systems is preferred
Familiarity with using business intelligence tools such as Domo, Tableau or Looker
Knowledge about and / or experience with sports betting
Background in digital companies, online, gambling, gaming, e-commerce, marketing, or mobile apps
Familiarity with one scripting language, preferably Python

Additional Information

What's in it for you:
Work in an environment where you, your work and ideas matter and have an impact.
You will be a be part of the newest and up and coming USA online iGaming and sports betting companies in the market.
Work in a new and young business with high growth potential.
Build your own success story together with us.
Work with self-organized, self-responsible and entrepreneurial employees.
Start-up feeling, backed-up by a leading European sports betting house. We are a high-volume business and are taking off in the US!
Competitive salary, Medical, Vision, Dental Benefits, Unlimited PTO, and more
Salary for this position is $120k+ & annual performance bonus","$120,000 /yr (est.)",201 to 500 Employees,Company - Private,"Arts, Entertainment & Recreation",Gambling,2004,Unknown / Non-Applicable
"shaped.ai Inc.
4.0",4.0,"New York, NY",Data Engineer,"Hello! We’re building the future of content discovery infrastructure with AI — come join us!

Shaped (YC W22) is an API for developers to seamlessly add personalized ranking and recommendation to their products. These frictionless discovery experiences help end-users find what they want faster, and, consequently, grow conversion and engagement business metrics. Behind the scenes, we’re building state-of-the-art ML infrastructure for training and deploying large AI models to power our personalization engine.
We’re a seed stage start-up backed by top-tier investors (e.g. Y-Combinator, Susa Ventures, Tribe Capital etc…) and executives (e.g. Google, Amazon, Uber, Dropbox etc…). Our team comes from Meta, Google, Apple and Uber. We’re a remote team but have a small office in Brooklyn, New York.
We’ve been lucky to have a huge amount of success since our initial launch earlier this year and now looking to expand our real-time data engineering capabilities. We are looking for a data engineer to design, build and optimize Shaped's real-time streaming data ingestion endpoints. You will be a founding engineer working on state-of-the-art infrastructure. As one of Shaped’s early employees you will help shape our product, culture and vision.
This role is for a full-time data engineer at $120k - $200k a year and 0.1 - 0.5% equity.

Responsibilities:
Manage the unification of our batch and real-time data processing pipelines.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Requirements:
Bachelor's in computer science, data science or mathematics related field. Master's degree or PhD will be advantageous.
5+ years of data engineering experience, ideally applied to machine learning use-cases.
Experience with AWS or common cloud technologies
Proficient with Python, SQL.
Extensive knowledge of both offline and online large scale data processing frameworks: Spark, Beam, Flink.
Extensive knowledge with stream technologies like Kakfa or Kinesis.
Excellent written and verbal communication skills.
We’re excited to work with you. Come build the future of AI and discovery with us!","$160,000 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Virtualan Software LLC
4.5",4.5,"Atlanta, GA","Sr. Data Engineer (Python,Spark & Databricks)","Role : Senior Data Engineer (Python, Spark & Databricks)
Location : Hybrid (Atlanta, GA)
Pay: 60-70$ on C2C / 1099
RESPONSIBILITIES:
Create or modify the conceptual, logical and physical data models.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of sources like Hadoop, Spark, AWS Lambda, etc.
Lead and/or mentor a small team of data engineers.
Design, develop, test, deploy, maintain and improve data integration pipeline.
Develop pipeline objects using Apache Spark , PySpark and/or Python.
Design and develop data pipeline architectures using Spark and related AWS Services.
Communicate effectively with client leadership and business stakeholders.
Participate in proposal and/or SOW development.
REQUIREMENTS:
5+ years of professional work experience designing and implementing data pipelines in a cloud environment is required.
2+ years of experience migrating/developing data solutions in the AWS cloud is required.
1+ years of experience building/implementing data pipelines using DataBricks is required.
Expert level knowledge of using SQL to write complex, highly-optimized queries across large volumes of data.
Hands-on object-oriented programming experience using Python is required.
Professional work experience building real-time data streams using Spark.
Knowledge or experience in architectural best practices in building data lakes.
Bachelor or Master degree in Computer Science, Engineering, Information Systems or relevant degree.
Hybrid work schedule, and must live in the Atlanta, GA metro area. Must be open to up to 25% national travel to client locations when on engagements outside of the Atlanta, GA area.
Job Type: Contract
Pay: $60.00 - $70.00 per hour
Schedule:
Monday to Friday
Application Question(s):
Apply only if you are interested in Hybrid Role in Atlanta, GA. Interested ?
Experience:
Data Engineer: 4 years (Required)
Databricks: 1 year (Required)
Willingness to travel:
25% (Required)
Work Location: On the road",$65.00 /hr (est.),1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,2019,Less than $1 million (USD)
"DataArt
4.6",4.6,Remote,Data Engineer,"CLIENT
Our client is a pioneer in US schools’ education since 2000, it is leading the way in next-generation curriculum and formative assessment. They develop a number of solutions and interactive web products for teachers, students and their parents.

The company is technology-driven with a huge number of software engineers involved in product development. They have a very solid approach to technology, employ best practices and processes, with a focus on cutting-edge frameworks, languages and tools.

We invite to the company, not a project
TEAM
There are multiple autonomous and empowered teams who work with their own part of the product and choose their own way of doing things and they expect people to be proactive and good team players.
Responsibilities
Building well-tested and optimized ETL data pipelines for both full and delta extraction
Collaborating with data analysts and learning scientists to store, aggregate, and calculate captured students’ work
Contributing to leading industry data standards, such as Caliper Analytics or xAPI
Improving our deployment and testing automation data pipelines
Requirements
3+ years of professional software development or data engineering experience
Strong CS and data engineering fundamentals
Proven fluency in SQL and a development language such as Python
Experience with the following storages: Snowflake, AWS Storage Services (S3, RDS, Glacier, DynamoDB)
Experience with ETL/BI: Airflow, DBT, Matillion, Looker
Experience with Cloud Infrastructure: AWS Kinesis, Lambda, API Gateway, Terraform
Understanding of ETL/ELT pipelines and Data Warehousing design, tooling, and support
Understanding of different data formatting (JSON, CSV, XML) and data storage techniques (3NF, EAV Model, Star Schema, Data Lake)
BS in Computer Science, Data Science, or equivalent
Experience in education or ed-tech
Proven passion and talent for teaching fellow engineers and non-engineers
Proven passion for building and learning: open source contributions, pet projects, self-education, Stack Overflow
Strong communication skills in writing, conversation",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,1997,$100 to $500 million (USD)
"FreightWaves, Inc.
3.5",3.5,Remote,Senior Data Engineer,"Are you smart, driven, curious, resourceful, and not afraid to fail? Then we want to meet you! Our team of bold, innovative, and creative teammates is what makes us a top startup to work for. FreightWaves delivers news and commentary as well as data and analytics which empower risk management and actionable market insights in the logistics and supply chain industry. If you are ready to join our team, it is time for YOU to apply!
FreightWaves is on the hunt for a curious, tenacious, and team-oriented Senior Data Engineer to join our fast paced engineering team. The ideal candidate is inquisitive, versatile, team oriented, thrives on change, and has a positive attitude. If you are ready to be challenged, learn new and exciting technologies, and have the unique opportunity to work with some of the most talented developers in the country, we want you to apply!
**This position is fully remote.**
**Must RESIDE in the United States and be eligible to work.**
What you will be doing:
Implementing ingestion pipelines, using Airflow as the orchestration platform, for consuming data from a wide variety of sources (API, SFTP, Cloud Storage Bucket, etc.).
Implementing transformation pipelines using software engineering best practices and tools (DBT)
Working closely with Software Engineering and DevOps to maintain reproducible infrastructure and data that serves both API-only customers and in-house SaaS products
Defining and implementing data ingestion/transformation quality control processes using established frameworks (Pytest, DBT)
Building pipelines that use multiple technologies and cloud environments (for example, an Airflow pipeline pulling a file from an S3 bucket and loading the data into BigQuery)
Create and ensure data automation stability with associated monitoring tools.
Review existing and proposed infrastructure for architectural enhancements that follow both software engineering and data analytics best practices.
Working closely with Data Science and facilitating advanced data analysis (like Machine Learning)
What you bring to the table:
Strong working knowledge of Apache Airflow
Experience supporting a SaaS or DaaS product, bonus points if you were creating new data products/features
Strong in Linux environments and experience in scripting languages
Python Expert
Strong understanding of software best practices and associated tools.
Experience in any major RDBMS (MySQL, Postgres, SQL Server, etc.).
Strong SQL Skills, bonus points for having used both T-SQL and Standard SQL
Experience with NoSQL (Elasticsearch, MongoDB, etc.)
Multi-cloud and/or hybrid-cloud experience
Strong interpersonal skills
Comfortable working directly with data providers, including non-technical individuals
Experience with the following (or transitioning from equivalent platform services):
Cloud Storage
Cloud Pubsub
BigQuery
Apache Airflow
dbt
DataFlow
Bonus knowledge/experience:
Experience implementing cloud architecture changes
Working knowledge of how to build and maintain APIs using Python/FastAPI
Transforming similar data from disparate sources to create canonical data structures
Surfacing data to BI platforms such as Looker Studio
Data Migration experience, especially from one cloud platform to another
Certification: Professional Google Cloud Certified Data Engineer
Our Benefits:
An excellent work environment, flat hierarchies, and short decision paths.
Work from home
A generous benefits package including 100% employer-paid health, dental, vision and Life insurance, STD, LTD
Stock options
Appealing 401k matching plan
Career Mentorship Opportunities
Personal Development Credit (Can be used toward Student loans or relevant PD Courses)
Annual life achievement bonus of $2000 for having a baby, buying a house, or getting married (max one per year)
No set days off Vacation policy (our team takes time off as needed with supervisor approval)
Up to $50 for Gym or Virtual Gym membership.
Audible or Kindle Unlimited subscription
Discount on Ford vehicles
oYXkhYiWkU",#N/A,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2017,Unknown / Non-Applicable
