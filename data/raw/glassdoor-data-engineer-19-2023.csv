company,company_rating,location,job_title,job_description,salary_estimate,company_size,company_type,company_sector,company_industry,company_founded,company_revenue
"Titan America
3.8",3.8,"Roanoke, VA",Data Engineer,"Data Engineer
We are seeking a motivated and detail-oriented Data Engineer to join our team based in Roanoke, Virginia. As a Data Engineer, you will play a crucial role in designing, developing and maintaining scalable data pipelines and transformations for business-critical use cases. In addition, you may take on various Software Development and Data Science responsibilities as we frequently share responsibilities and knowledge across our growing team.
Responsibilities:
Design, develop and maintain scalable data pipelines for extracting, transforming, and loading data from various sources
Collaborate with cross-functional teams to identify data needs and determine the best data solutions
Develop and implement data models to support business requirements and ensure data quality
Ensure the security and privacy of sensitive data by implementing appropriate access controls
Monitor and optimize data pipeline performance to ensure timely and accurate data delivery
Document data pipeline processes, data dictionaries, and data storage solutions
Requirements:
Bachelor's degree in Computer Science, Computer Engineering, or a related technical field
Minimum of five years of professional experience working as a Data Engineer or Software Developer
Strong hands-on experience with data warehouse and transformation solutions, i.e. Domo, Snowflake or similar
Proficient in at least one scripting language such as Python, JavaScript, or R
Understanding of data modeling, data integration and data quality processes
Familiarity with cloud platforms such as AWS, Azure, or Google Cloud Platform
Strong analytical and problem solving skills
Full Stack Software Development experience in a professional setting is highly desired, but not required
This is an excellent opportunity for a driven and collaborative individual to make a significant impact in a dynamic and growing team. If you have a passion for data and a desire to work in a fast-paced and dynamic environment, we want to hear from you!
Success begins with hiring the right people to partner with us as we grow and develop our businesses. People are central to everything we do. It is through their efforts and talents that Titan has been successful for over 100 years.
Titan America is an Equal Employment Opportunity (EEO) / Affirmative Action employer and welcomes all qualified applicants. Applicants will receive fair and impartial consideration without regard to race, sex, color, religion, national origin, age, disability, veteran status, genetic data, or other legally protected status.
Job Type: Full-time
Pay: $75,000.00 - $85,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Referral program
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Roanoke, VA 24019: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Do you currently or will you in the future need sponsorship to work in the United States?
This position requires the successful candidate to be in the office in Roanoke, VA at least 3 days a week. Can you meet this requirement?
This position requires the successful candidates to be a full time, W2 employee. Can you meet this requirement?
Education:
Bachelor's (Preferred)
Experience:
Data Engineering: 5 years (Preferred)
Work Location: Hybrid remote in Roanoke, VA 24019","$80,000 /yr (est.)",1001 to 5000 Employees,Company - Public,"Energy, Mining & Utilities",Mining & Metals,1902,Unknown / Non-Applicable
"RelMap Consulting
4.8",4.8,"Addison, TX",Sr. Data Engineer (Hybrid),"The data engineer entails:
Advanced SQL Server Development
Ability to create and modify views
Ability to create and modify custom functions
Ability to write complex queries, including joins, temp table processing, and common table expressions (CTE’s)
Ability to create and modify stored procedures using TSQL
Ability to analyze and optimize query performance using indices
Ability to use Red Gate’s SQL Compare to perform a schema (AKA “do a manual roll”) between one server an another, and between revision control and a database
Ability to stand-up new database server VM’s in Linux and Windows using VMWare
Ability to backup and restore a database in a Linux and Windows environment
Ability to manage and script automated database jobs
Ability to use PowerShell / Bash to interact with databases from the command line
Addition skills that would be desired but not required
· Experience with Redis
· Experience with MongoDB / CosmosDB
· Experience with Azure and Azure SQL
· C#, JavaScript (NodeJS / ES6), or Python experience
Job Type: Contract
Pay: $70.00 - $80.00 per hour
Compensation package:
Hourly pay
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Addison, TX 75001: Reliably commute or planning to relocate before starting work (Required)
Work Location: Hybrid remote in Addison, TX 75001",$75.00 /hr (est.),1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2010,$1 to $5 million (USD)
Proits Hub LLC,#N/A,Remote,Data Analytics Engineer,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote
Speak with the employer
+91 9417599933",$60.00 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Atika Technologies,#N/A,"Pittsburgh, PA",Data Engineer,"Role: Data Engineer/PySpark Developer, Pittsburgh, PA - Day One Onsite
Fulltime and Contract(W2)
LinkedIn Must
USC/GC/EADs
Three round of interview: First two on Zoom Video Call, End Client Round will be Onsite on the Client's Location (Pittsburgh, PA)
Job Description:
Python(PySpark), Jupiter hub, airflow, Java
Database knowledge - Oracle / SQL / Data Analytics
Regulatory reporting knowledge preferred
Good communication skills
Open / Ability to learn new technologies
Very good Analytical Skills
Experience in ETL is preferable
Job Types: Full-time, Contract
Salary: $120,000.00 - $130,000.00 per year
Benefits:
Dental insurance
Health insurance
Life insurance
Compensation package:
Bonus pay
Performance bonus
Yearly pay
Experience level:
10 years
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Pittsburgh, PA 15259: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Required)
Spark: 4 years (Required)
Regulatory reporting: 3 years (Required)
SQL: 4 years (Required)
IT: 10 years (Required)
Airflow: 4 years (Required)
Jupiter: 4 years (Required)
Work Location: One location","$125,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"DSMH LLC
5.0",5.0,Remote,"Data Engineer (AWS, Python)","Title: Data Engineer
Duration: 12+ Months
Technical Skills
(Required)
Four years or more of experience in data engineering and/or software
development.
Three years or more of experience with development, operations, or
architecture in AWS using Python.
Experience with development and delivery of microservices using serverless
AWS services (S3, RDS, Aurora, DynamoDB, Lambda, SNS, SQS, Kinesis,
IAM)
Strong competency in testing, including unit testing to coverage standards and
integration testing.
(Desired)
Software development experience with object-oriented development and design
patterns
Experience with serverless design in AWS
AWS technical certifications (Developer Associate, Solution Architect Associate)
Familiarity with machine learning and data design to support machine learning
Experience with productionizing machine learning workloads
Experience with ElasticSearch/ELK, Kibana, Grafana, and/or Prometheus
Experience with OpenTelemetry
Soft Skills
(Required)
Verbal and written communication skills, problem solving skills, customer
service and interpersonal skills.
Basic ability to work independently and manage one’s time.
(Desired)
Ability to work collaboratively in a complex, rapidly changing, and culturally
diverse environment.
Ability to clearly communicate complex technical ideas.
Comfortable working in a dynamic environment where digital is still evolving as
a core offering.
Job Type: Contract
Pay: $50.00 - $55.00 per hour
Experience level:
6 years
Schedule:
Monday to Friday
Experience:
AWS: 5 years (Required)
Python: 5 years (Required)
Work Location: Remote",$52.50 /hr (est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,$1 to $5 million (USD)
"shaped.ai Inc.
4.0",4.0,"New York, NY",Data Engineer,"Hello! We’re building the future of content discovery infrastructure with AI — come join us!

Shaped (YC W22) is an API for developers to seamlessly add personalized ranking and recommendation to their products. These frictionless discovery experiences help end-users find what they want faster, and, consequently, grow conversion and engagement business metrics. Behind the scenes, we’re building state-of-the-art ML infrastructure for training and deploying large AI models to power our personalization engine.
We’re a seed stage start-up backed by top-tier investors (e.g. Y-Combinator, Susa Ventures, Tribe Capital etc…) and executives (e.g. Google, Amazon, Uber, Dropbox etc…). Our team comes from Meta, Google, Apple and Uber. We’re a remote team but have a small office in Brooklyn, New York.
We’ve been lucky to have a huge amount of success since our initial launch earlier this year and now looking to expand our real-time data engineering capabilities. We are looking for a data engineer to design, build and optimize Shaped's real-time streaming data ingestion endpoints. You will be a founding engineer working on state-of-the-art infrastructure. As one of Shaped’s early employees you will help shape our product, culture and vision.
This role is for a full-time data engineer at $120k - $200k a year and 0.1 - 0.5% equity.

Responsibilities:
Manage the unification of our batch and real-time data processing pipelines.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Requirements:
Bachelor's in computer science, data science or mathematics related field. Master's degree or PhD will be advantageous.
5+ years of data engineering experience, ideally applied to machine learning use-cases.
Experience with AWS or common cloud technologies
Proficient with Python, SQL.
Extensive knowledge of both offline and online large scale data processing frameworks: Spark, Beam, Flink.
Extensive knowledge with stream technologies like Kakfa or Kinesis.
Excellent written and verbal communication skills.
We’re excited to work with you. Come build the future of AI and discovery with us!","$160,000 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
WellTrust Medical Group,#N/A,"Charlotte, NC",Data Engineer,"The Data Engineer will assist the senior analyst in complex modeling and projects to ensure their completion within the allotted timeline. Helps track measures of success and develop reports that measure operational and customer metrics. Demonstrates ability to troubleshoot data consumption and data quality problems.

Responsibilities:
Build and maintain reports using My SQL Server
Identify optimal data sources, structure data validation, and cache optimization.
Performs business problem modeling and quantitative analytics including regression analysis, time-series, hierarchical cluster analysis, and data mining.
Helps track measures of success and develop reports that measure operational and customer metrics.
Demonstrates ability to troubleshoot data consumption and data quality problems.
Evaluates and recommend improvements to current models, metrics, and analytic processes to improve operational efficiency and insight.
Design, document, and communicate effective technical solutions meeting end-user requirements
Perform analyses with large amounts of complex data, and uses data visualization tools to communicate results.
Requirements:

Bachelor's degree or equivalent experience in a quantitative field (Mathematics, Statistics, Computer Science, Economics, Finance, Physics, Engineering)
3+ years of professional working experience as a Data Engineer or Analyst
1-3 years of data warehouse experience
Build and maintain reports using My SQL Server
Strong analytical and creative problem-solving skills
Design, document, and communicate effective technical solutions meeting end-user
SQL and Excel Macro development
Strong understanding of various data environments such as SQL, Oracle, Big Query, etc.
Experience with data visualization tools such as Looker Studio, Tableau, D3.js, etc.
Proficient in R, Python, MATLAB, Java, or another programming language.",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Fracsys Inc,#N/A,"Washington, DC",Data Engineer,"Fracsys Inc is hiring a Data Engineer position. The ideal candidate must have at least 12+ years of industry experience. He or she must be responsible for successful technical delivery of Data Warehousing and Business Intelligence projects using a formal project management methodology.
Responsibilities
Directs, motivates and develops key technical personnel
Provides technical direction across the full lifecycle system development process
Provides strategic planning and comprehensive senior level technical consulting to IT senior management and senior technical staff
Working independently within guidelines
Provides detailed inputs for plans and schedules, including goals, risks, and resource allocation
Conduct diagnostic, troubleshooting steps and present conclusions to a varied audience
comfortable slinging some code to solve a problem
Build Data Marts and Data warehouses to facilitate analytical capabilities across the varied sources of data
Design and implement data patterns on cloud using cloud Data warehouse and data virtualization tools
Evaluate new tools and technologies for target state on cloud
Learn and implement new ETL, BI tools and data warehousing technologies
Qualifications
Minimum twelve (12) years in a Technical Lead or Consulting role leading Information Management related initiatives (system integration, data warehouse build, data mart build, or similar)
MUST have proven leadership experience for team of at least 3 technical resources.
Experience creating Data Marts, Data warehouses on-premise and on cloud for real time and batch processing frameworks
Experience building and optimizing big-data data pipelines and data sets including Postgres, AWS Relational Data Service
extensive hands-on experience building and optimizing data structures for data analytics, data science and business intelligence
Experience building self-service data consumption patterns and knowledge of cloud-based data Lake platforms
Experience wrangling data (structured and unstructured), in-depth knowledge of database architecture
Experience utilizing or leading implementations leveraging ETL tools (Informatica / Talend), BI Reporting tools such as MicroStrategy, Microsoft Power BI, data modeling tools such as Erwin, Oracle, SQL server, NoSQL, JDBC, UNIX shell scripting, PERL and JAVA, XML/JSON files, SAS , Python, AWS cloud-native technologies, S3, Athena, Redshift
Familiarity with the following technologies: Hadoop, Kafka, Airflow, Hive, Presto, Athena, S3, Aurora, EMR, Spark
Ability to drive, contribute to, and communicate solutions to technical product challenges
Ability to roll-up your sleeves and work in process or resource gaps and fill it in yourself
Excellent written and oral communication skills in English
Education
Bachelor’s degree in computer science, Data Analytics, Information Systems, or a related degree or equivalent experience. Master’s degree is preferred.
Job Type: Full-time
Pay: From $96,336.72 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Experience level:
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Washington, DC: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person","$96,337 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Infosoft,#N/A,"Peachtree Corners, GA",Data Pipeline Engineer - 2173,"Here are the job position details for your review:
Job Title: Data Pipeline Engineer (GStreamer/ DeepStream)
Pay Rate: $90 to 120/Hr.
Duration: 3 Months
Location: Peachtree Corners, GA
This position is for the Building Automation division.
We are looking for Data Pipeline Engineer (GStreamer) to participate in the development of Streaming data pipelines using NVIDIA Deepstream, GStreamer, Python programming, messaging queues, and related tools.
Preference is for candidates located in Atlanta, GA metro area. However, ok with non-local candidates who are open to 75% travel to Atlanta, GA.
Understanding the basics of our architecture/setup to be able to do setup/support for demos, testing, etc.
Basic computer hardware assembly and set-up
Possible set-up/installation of the application(s)
Possible development tasks depending on what the rest of the development team has for them
Develops business software applications, system software, and/or IT solutions on the basis of functional specifications.
Scope of position embraces both the customization or reuse/modification of system software and/or solutions as well as their development from scratch.
Consults with customers in creating functional specifications and work estimates, and supports Make-or-Buy decisions.
Analyses performance gaps of existing software environments and proposes measures to address efficiency problems.
Is also responsible for the implementation of data-driven components supporting data modeling and engineering in the context of data analytics (Data Stewardship).
Skills:
Python
Linux
Docker and Docker Swarm
Zero MQ / a good understanding of messaging
Desired Skills:
NVIDIA Deepstream SDK
Web Sockets
Flask
Protobuf
Possible:
GStreamer (if different than RTSP / H264 / MJPEG)
Job Type: Contract
Salary: $90.00 - $120.00 per hour
Ability to commute/relocate:
Peachtree Corners, GA: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
""Please share your Email ID""
Work Location: One location",$105.00 /hr (est.),Unknown,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"iManage
4.5",4.5,Remote,Senior Data Engineer (Azure),"We offer a flexible working policy that supports the health and well-being of our iManage employees. As an organization, we value collaborating and learning from our peers in person, while providing the necessary flexibility for our employees to have a meaningful work-life balance. Please reach out to learn more.
Being a Senior Data Engineer at iManage Means…
You are excited about data and believe in the democratization of data to support data driven decision-making. You will partner with our Information Technology team to implement, support, and extend our Enterprise Data Lake hosted on Azure and built using Azure Synapse. You will gather requirements from iManage business units and craft solutions which provide access to critical business data. You will develop data models and data pipelines for our Enterprise Data Lake, and provide integration with BI platforms and tools such as Totango and Power BI. You are passionate about lakehouse architecture and have experience using Delta Lake and bronze, silver, and gold data lake design.
Here is what one of our leaders, Cloud Services Director (Jacqueline Toepfer), has to say about the role: “As a Senior Data Engineer on our team, you will get the opportunity to showcase your expertise and make a real difference across the organization. You will be part of a truly collaborative team that is passionate about delivering quality solutions. You will be the in-house expert in the data models of multiple, disparate enterprise SaaS systems and utilize your wealth of knowledge to provide recommendations and solutions for consolidation, transformation, and integration of the disparate data sources.”
iM Responsible For…
Modeling, managing, and reporting of data stored in Azure Data Lake.
Gathering data requirements from various business units and translating these requirements into data models.
Using Python, PySpark, and system specific APIs to extract, transform, store and analyze data from a variety of systems.
Data modeling, defining data pipelines, and integrations necessary to present data in BI platforms such as Totango, or BI tools like Power BI.
Identifying and modeling all current disparate data sources and the data flows between these data sources.
Analyzing current repositories and proposing changes to data repositories and data flows to better support company objectives for the measurement of user experience and customer success.
Understanding the business needs of data integration and governance from disparate systems to drive the enhancement of the enterprise data lake.
Applying best practices to ensure the security and privacy of the data repositories.
Ensuring data repositories meet company standards for storage of PII.
Developing proficiency with the iManage product APIs for all iManage Cloud services.
iM Qualified Because I Have...
A Bachelor’s degree or higher in Computer Science or equivalent field.
3-5 years of experience working with data in a business setting.
Proficiency in data extraction, manipulation, and subsequent reporting with Spark and Python.
Experience designing data pipelines with a cloud-native mindset using Azure or AWS.
Knowledge and experience with architecting a data lake with Azure Synapse or adjacent technologies like Databricks.
Experience ingesting data from SaaS solutions and other services via API or other related technologies.
A passion to be a thought leader and work collaboratively within a team.
Commitment to understanding data requirements and delivering scalable, robust solutions that meet those requirements.
A creative mindset with a desire to explore new technologies and create innovative solutions.
Bonus Points If I Have…
Familiarity with Delta Lake.
A background with relational databases and data warehouse design using star schemas.
Experience with cloud-based data models for business solutions like Salesforce, Zendesk, and NetSuite.
Don't meet every qualification listed above? Studies show that women and people of color are less likely to apply to jobs unless they meet all qualifications. At iManage, we are committed to building a diverse and inclusive environment and encourage everyone to show up as their full authentic selves. We welcome those that come with a growth mindset and a hunger for learning; so, if you are excited about this role but your past experience doesn't align perfectly with every qualification, we encourage you to apply anyways!
iM Getting To…
Join a supportive, experienced team with an inclusive, encouraging, and vibrant culture.
Have flexible work hours that allow me to balance my ‘me time’ with my work commitments.
Collaborate in a modern open plan workspace, with a gaming area, free snacks, drinks and regular social events.
Focus on impactful work, solving complex, real challenges utilizing the latest technologies and protocols.
Own my career path with our internal development framework. Ask us more about this!
Learn new skills and earn certifications with access to unlimited courses in LinkedIn Learning.
Join an innovative, industry leading SaaS company that is continuing to grow & scale!
iManage Is Supporting Me By...
Creating an inclusive environment where I can help shape the culture not just by fitting in, but by adding to it.
Providing a market competitive salary that is applied through a consistent process, equitable for all our employees, and regularly reviewed based on industry data.
Rewarding me with an annual performance-based bonus.
Offering comprehensive Health/Vision/Dental/Life Insurance, and a 401k Retirement Savings Plan with a company match up to 4%.
Giving access to HealthJoy, a healthcare concierge service, to help me maximize my health benefits.
Granting enhanced leave for expecting parents; 20 weeks 100% paid for primary leave, and 10 weeks 100% paid for secondary leave.
Providing me with a flexible time off policy to take the time off that I need. Be it for vacation, volunteering, celebrating holidays, spending time with family, or simply taking time to recharge and reset.
Caring for my mental health and well-being with multiple company wellness days and free access to the Healthy Minds app for mindfulness, meditation and more.
About iManage…
iManage is dedicated to Making Knowledge WorkTM. Over one million professionals across 65+ countries rely on our intelligent, cloud-enabled, secure knowledge work platform to uncover and activate the knowledge that exists inside their business content and communications.
We are continuously innovating to solve the most complex professional challenges and enable better business outcomes; Our work is not always easy but it is ambitious and rewarding.
So we’re looking for people who love a challenge. People who are happiest when they’re solving problems and collaborating with the industry’s best and brightest. That’s the iManage way. It’s how we do things that might appear impossible. How we develop our employees’ strengths and unlock their potential. How we find meaning in everything we do.
Whoever you are, whatever you do, however you work. Make it mean something at iManage.
iManage provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.
Learn more at: www.imanage.com
Please see our privacy statement for more information on how we handle your personal data: https://imanage.com/privacy-policy/
#LI-LM1
#LI-Remote
V436F7WSwa",#N/A,501 to 1000 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,$100 to $500 million (USD)
"MyCare Medical Group
4.1",4.1,"Lutz, FL",Data Engineer,"Job Summary
The Data Engineer will design, develop, optimize, and maintain data architecture and pipelines that adhere to ELT principles and business goals.
Job Responsibilities:
Designs, develops, optimizes, and maintains data architecture and pipelines that adhere to ELT principles and business goals.
Solves complex data problems to deliver insights that helps business achieve its goals.
Creates data products for engineer, analyst, and data scientist team members to accelerate their productivity.
Engineers effective features for modelling in close collaboration with business analysts and stakeholders.
Leads the evaluation, implementation and deployment of emerging tools and process for analytics data engineering in order to improve productivity as a team and quality.
Partners with engineers, BI, and solutions architects to develop technical architectures for strategic enterprise projects and initiatives.
Fosters a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions.
Advises, consults, mentors, and coaches other data and analytic professionals on data standards and practices.
Develops and delivers communication and education plans on analytic data engineering capabilities, standards, and processes.
Job Qualifications:
Education and Work Experience: Bachelor’s degree in computer science, information systems, engineering, or a related field; Masters preferred.
3+ years of experience in data engineering role.
Expertise in ELT and data analysis, SQL and at least one programming language (e.g., SQL, Python).
Conceptual knowledge of data and analytics, such as dimensional modelling, reporting tools, data governance, and structured and unstructured data.
Skills: Strong understanding of agile methodologies and experience as a Data Engineer on a cross-functional agile team preferred.
Experience with designing and maintaining data warehouses and/or data lakes with technologies, such as Hadoop, Spark, or distributed databases, like Redshift and Snowflake, and experience with housing, accessing, and transforming data in a variety of relational databases.
Experience with cloud database technologies (e.g., Azure) and developing solutions on cloud computing services and infrastructure in the data and analytics space.
Experience in building data pipelines and deploying/maintaining them with best-in-class tools (e.g., Git and Jenkins).
Familiarity with the Linux operating system.
Basic familiarity with BI tools (e.g., Tableau, Power BI, Looker).
Experience working in data engineering or data architect role.
Candidate Qualities: Focuses on business agility and agile delivery with a “fail-fast, succeed early” mindset and emphasis on measurable outcomes.
Brings a high-energy and passionate outlook to the job and can influence those around them.
Able to build a sense of trust and rapport that creates a comfortable & effective workplace.
Passion for innovation and “can do” attitude.
BENEFITS
Comprehensive benefits package, including Health, Vision, Dental, and Life insurancesFSA and Life Assistance Program (EAP)
401(k) Retirement Plan
Health Advocacy, Travel Assistance, and My Secure Advantage
PTO Accrual and Holidays
#ZIP
Job Type: Full-time
Pay: $125,000.00 - $150,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Employee discount
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Referral program
Relocation assistance
Retirement plan
Tuition reimbursement
Vision insurance
Compensation package:
Performance bonus
Yearly bonus
Experience level:
3 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Lutz, FL 33558: Reliably commute or planning to relocate before starting work (Preferred)
Experience:
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person","$137,500 /yr (est.)",501 to 1000 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2018,Unknown / Non-Applicable
"Take Command Health
4.9",4.9,"Dallas, TX",Data Engineer,"Company: Take Command
Position: Data Engineer
Salary: $120,000-$130,000 plus bonus
Location: Dallas or Austin, TX
About Take Command
Take Command is a start-up on a mission to improve the healthcare system, starting with health insurance. Pragmatically speaking, we help employers reimburse employees for individual insurance instead of offering a traditional one-size-fits-all group plan. We believe this model can empower employees (when they have the right support) to be savvy healthcare consumers and have a transformative impact on the entire healthcare system.
Let’s be honest—health insurance is usually a confusing, frustrating, and even emotional experience for people. We want to fix that with a new model, great technology, and a superior user experience. We have made a great start, but we need your help to fully realize our long-term vision.
As a part of the dev team, you'll build the framework for both our clients and our team to succeed. You will be a pillar in our company as we grow and make our product the best product on the market!
In this role, you will:
Take ownership for delivering multiple features, their quality and assist team members in delivering their features
Recommend improvements to data quality, processes and code to make it easier to use, manage and maintain
Experience designing and developing data models and data migrations
Experience managing and maintaining ETL or ELT processes (Hevo, BigQuery, Looker, etc.)
Hands-on development experience with SQL, relational database design (ERDs)
Must be able to write SQL & contribute code
Be product-minded; focus not only on what needs to be built but why something needs to be built
You may be a great fit for this role if you have:
3 or more years of professional experience as a data analyst or data engineer
Cloud-Based Data Tools e.g. Hevo, BigQuery, Looker, etc.
Database technologies: SQL, RDBMS / Relational databases (Postgres, Amazon RDS / Aurora, etc.)
Nice to have experience: Python, Java, Typescript or another modern OOP language
Cloud infrastructure: AWS
Source control: Git
Working at Take Command
We’re excited to build a team and culture that reflects our values! We offer competitive pay, health benefits, and equity options to share with this position. We want you to be an owner along with us in revolutionizing healthcare.
A generously funded ICHRA for medical, dental, and vision premiums and medical expenses. You get to use our own product and we think that’s so exciting and rare!
Unlimited personal vacation in addition to regular company holidays.
401(k): 90-day eligibility for 2% match that vests immediately!
Access to LinkedIn Learning.
We have two beautiful offices conveniently located near The Hill (Dallas) and Holly Commons (Austin). The kitchen is well-stocked and we've designed the space to have lots of different areas to work!
Paid parental leave for new parents.
Flexible on where you work – we hope to see you around the office on average 3 days a week especially when white-boarding or building relationships with your colleagues, but you also have the ability to work from home or wherever you'd like when you need to get focus work done.
More About Us
We secured our Series A funding in 2021 and are thrilled to be able to expand our team. Despite being a small startup in a land of health insurance giants, we’re the recognized industry leader for what we do (health insurance reimbursements) and passionate about bringing it to market because we know we can help fix a broken system and improve our clients’ wellbeing and health outcomes.
We’ve been featured in The New York Times, The Wall Street Journal, The Dallas Morning News, and other national healthcare publications and are excited about our growth opportunities.
Take Command knows diversity and inclusion among our teammates is integral to our company’s success and growth. Our vision is to recruit, develop, and retain the best team from a diverse candidate pool.
This has mostly been about us, but we’d love to hear from you-we can’t wait to hear your story!
Take Command is an equal opportunity employer! We celebrate diversity and are committed to creating an inclusive environment for all employees.
Xrygn4NXMr","$125,000 /yr (est.)",1 to 50 Employees,Company - Private,Insurance,Insurance Agencies & Brokerages,2014,Unknown / Non-Applicable
"Intersec
4.5",4.5,Remote,Senior Data Engineer,"Architect and implement highly scalable data pipeline to streamline migration of data from source data repositories to the centralized data platform
Implement agreed upon data models (e.g., data marts) to enable faster and efficient consumption of data by downstream consumers
Conduct performance tuning to optimize data migration techniques and ensure adherence to identified service-level agreements (SLAs)
Define and implement automated validation checks to ensure data integrity between source and target
Assist in onboarding consumers with existing technology to connect to the platform
Setup predictive alert mechanism for pipeline degradation for IMF pipelines.
Enhance data quality framework for data pipelines and derived datasets
Enhance Data Audit Balance and Control mechanism for data pipelines
Enhance pipelines to be certified copies of authoritative source
Populate metadata for the Enterprise with particular focus on the Data Sources being onboarded to EDP (Technical and Business Glossaries) related to IMF Modernization Programs
Validate correct lineage for datasets
Required:
*8+ years of Databricks, Data Modeling, and/or Metadata management experience
* Familiarity with architecting and developing real and near-real-time solutions
* Bachelor's Degree in Engineering, Mathematics, computer Science, Statistics or other quantitative discipline, or 4 years equivalent professional experience
* US Citizenship and ability to gain a public trust
Preferred:
* Familiarity with IRS systems and ETL pipelines
* Ability to work independently and manage multiple tasks
* IRS MBI Clearance with IRS Laptop and Informatica Installed
Job Type: Full-time
Salary: $70.00 - $75.00 per hour
Schedule:
Monday to Friday
Application Question(s):
Are you a US/GC card holder?
How many years of Databricks, Data Modeling, and/or Metadata management experience do you have?
Work Location: Remote",$72.50 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,Unknown / Non-Applicable
Atalan Tech,#N/A,Remote,Data Engineer,"Atalan Tech is a start-up with a mission to mitigate the impact of clinician burnout and turnover in our nation’s health system. We are expanding our team and looking to hire the critical role of Data Engineer. The Data Engineer will work closely with the Chief Technology Officer, Chief Scientific Officer (UCLA Ph.D.), Chief Product Officer (Oxford Ph.D.), and Chief Executive Officer to deliver actionable insights from data in a variety of situations.
We are a small, close-knit team. This is an excellent opportunity for someone to join a mission-oriented, early-stage company at the ground floor and grow with us. There will be plenty of room for growth and creativity, and opportunities to tackle challenges that present one of the toughest problems in our society.
Requirements
Strong hands-on experience of designing, developing and managing data pipelines
Strong coding skills in Python (required) and SQL (required) and R (strongly preferred)
Experience building and maintaining data pipelines on AWS using EC2, EBS, Redshift, Postgres, S3, Data Pipeline, Kinesis, Lambda, etc.
Thorough understanding and experience with task management tools (Airflow, NiFi, etc.)
Knowledge of data management best practices for ETL, data governance, data warehousing, data modeling and data science
Basic knowledge of Linux (Redhat preferred) administration
Minimum B.S in Computer Science, software engineering or related field (or equivalent in related professional experience)
Strong communication skills
Strong problem-solving/critical thinking skills
Ability to work in a fast-paced, dynamic start-up environment
Some knowledge of healthcare, health tech, or healthcare data analytics (preferred)
Typical Tasks and Responsibilities include:
Working closely with CTO and CSO to build data pipelines for machine learning
Assisting the CPO in managing and supporting analysis of enterprise data to drive better products and solutions
Managing company’s data infrastructure on AWS (Postgres/RDS, Redshift, S3, etc.)
Synthesizing multiple data sources, such as internal medical records and HR data
Integrating with client systems including EHR (Epic, Cerner), HRIS, flat files, etc.
Working closely with other key members on other projects
Developing and designing data security standards and policies in line with industry best practices
About Us:
Atalan Tech’s mission is to improve the wellness of clinicians and their patients. We blend labor economics, machine learning, and clinical psychology with a team from Harvard, Yale, and UCLA. Atalan Tech is looking to contribute innovative and sustainable solutions to one of the most pressing problems in the healthcare industry. Building upon our proof-of-concept success, we have recently secured multiple engagements with nationwide healthcare organizations and have successfully raised institutional funding to expand our reach.
Contact Us:
If you’re interested in this position, please send us your CV/resume and cover letter.
Job Type: Full-time
Pay: $108,448.20 - $130,604.27 per year
Experience level:
2 years
Schedule:
Monday to Friday
Application Question(s):
Do you know the role is full time?
Can you start within the next month?
What are your salary expectations?
Have you held the title of a data engineer? If not, what experience do you have as a data engineer? Please be very specific.
Have you released Python code to a production environment in the last year?
Experience:
Data analytics: 2 years (Required)
Work Location: Remote","$119,526 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Radiant System,#N/A,"Pleasanton, CA",Data Engineer,"First Priority: Pleasanton, CA –
Second Priority for a superstar: Plano, TX
Experience in developing and deploying data pipelines, preferably in the Cloud
Experience working with Spark, Databricks, and other relevant technologies in Microsoft Azure cloud
Experience in productionizing and deploying Big Data platforms and applications
Expert in writing SQL statements
Job Type: Contract
Application Question(s):
Are you willing to work on W2?
Work Location: In person",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"CDNetworks Inc.
2.9",2.9,"Monrovia, CA",Jr. Data Center Engineer,"CDNetworks is one of the top leading CDN & Edge Service Providers with global offices in Korea, Japan, Singapore, Malaysia, China, Russia, London, and Canada. We focus on delivering integrated cloud and edge computing solutions with unparalleled speed, ultra-low latency, rigorous security, and reliability so that our clients can focus on what’s most important – growing their business.
The Jr. Data Center Engineer is a Full-Time position based in Monrovia, CA. Job is performed indoors in a Data Center or Warehouse environment. 75% Travel required for this position.
Job Responsibilities
Ensure all incidents are logged and resolved, gather all relevant data, and ensure all incidents and tasks follow the appropriate procedures.
Support data center activities and work closely with our system and network team to complete tasks/projects.
First responder to all alerts and problem reports while managing communications between departments and handling crisis documentation and dissemination after the fact.
Utilize internal systems such as JIRA/Wiki to manage project plans and progress.
Performing general system administration duties including OS patching and upgrades, batch job monitoring, system and hardware diagnostics, and other activities to ensure optimal health and performance of all systems as required.
Resolve complex problems related to Server and H/W areas.
Assisting/working closely with Network, System Engineers to configure customer requirements.
Physical deployment of network devices, servers, cables, etc.
Assembling/dissembling server hardware for deployment and OS installation and network equipment testing.
Maintain existing department and system documentation (update workflow, process, training documentation).
Other duties as assigned.
Abilities Required
Good verbal and written communication skills, and ability to work independently with minimum instruction.
Basic degree of mentorship, training, and direction team members skills.
Knowledge of IDC industry.
Intermediate degree of analytical and project management skills.
Other Features of Job
Job is performed indoors in a Data Center or Warehouse environment.
Language Skills
Excellent communication skills (English) – written and verbal. Bilingual Chinese or Korean is a plus!!
Job Type: Full-time
Salary: $20.00 - $25.00 per hour
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Health insurance
Life insurance
Paid time off
Professional development assistance
Referral program
Vision insurance
Schedule:
10 hour shift
8 hour shift
Evening shift
Monday to Friday
Night shift
On call
Overtime
Supplemental pay types:
Bonus pay
Ability to commute/relocate:
Monrovia, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Computer networking: 1 year (Preferred)
Shift availability:
Overnight Shift (Required)
Night Shift (Required)
Day Shift (Required)
Willingness to travel:
75% (Preferred)
Work Location: In person",$22.50 /hr (est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2000,$10+ billion (USD)
"Integration Developer Network LLC
4.9",4.9,Remote,Data Engineer with PL/SQL Background,"Requirements:
Recent PL/SQL development work experience with Oracle 19.
Must have working experience with tools like Airflow, Git.
Must have python coding experience and REST API development experience.
Plus would be experience with microservices, docker, Kubernetes.
Plus would be experience with accounting and financial data
Experience in event driven and cloud based architectures and working knowledge of containerization and streaming technologies.
Detailed work experience on Oracle, SingleStore or IBM I DB2 database, SQL & PL/SQL, including ANSI SQL.
Experience in database analysis, development and maintenance of business applications specific to oil & gas back-office operations. Expertise in various ETL tools such as SQL*Loader, Equalum, Python scripts.
Familiarity with No-SQL databases and pipelines (Elasticsearch, MongoDB).
2+ years’ experience in the design, implementation and support of full life-cycle data engineering projects.
Bachelor's degree in Computer Science, Engineering or other equivalent degree, with 5+ years data engineering experience or Master's degree in Computer Science, Engineering or other equivalent degree with 2+ years data engineering preferred.
High school diploma or GED is required.
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Experience level:
9 years
Schedule:
8 hour shift
Experience:
Python: 8 years (Required)
PL/SQL: 5 years (Required)
ETL: 5 years (Required)
Work Location: Remote",$62.50 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2011,$1 to $5 million (USD)
"N9 it solutions
4.6",4.6,Remote,Data Engineer,"Job title: Data Engineer
Visa's: CPT, OPT-EAD, H1B transfer
Employment: W2 position ( should be ok with Marketing)
Location: Hybrid Or Remote
(Authorized to work anywhere in the USA and for only those who are staying in the USA)
It's a long term project
Job description
Data Engineer (primary technologies: Azure Data Factory, Synapse, Synapse Pipelines, ADLS Gen 2, understanding of DataWarehouse concepts, ELT, Azure DevOps, Azure resource groups)
Develop architectural strategies for data modeling, design and implementation to meet stated requirements for metadata management, operational data stores and Extract Transform Load environments
Work with business leaders and teams to collect and translate information requirements into data to develop data-centric solutions
Apply industry-accepted data architecture principles and standards for modeling, stored procedures, replication, regulations, and security, among others, to meet technical and business goals.
Work to streamline data flows and models; improve consistency, quality, accessibility, and security; unify data architecture; remove unnecessary costs; and, optimize database activity across company needs.
Analyze and understand Data sources & APIs
Design and Develop methods to connect & collect data from different data sources
Design and Develop methods to filter/cleanse the data
Work closely with Data Scientists to ensure the source data is aggregated and cleansed
Work with Cloud and Data architects to define robust architecture in cloud setup pipelines and workflows
Benefits:
H1B and GC filling
Free training and Placement
E-verified
On-job technical support
Guesthouse facilities are also available
Skill Enhancement
Opportunity to work with Fortune 500 Companies
Job Type: Full-time
Salary: $40.26 - $56.00 per hour
Experience level:
6 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote
Speak with the employer
+91",$48.13 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2016,$5 to $25 million (USD)
Staff Bees Solutions,#N/A,"Dallas, TX",Data Engineer,"We are looking for OPT/CPT individuals, Helping Them to Train, Providing knowledge, and Placing Them in Fortune companies with the help of our Direct Clients in Big Data, Machine Learning, And Data Engineering Suitable Positions. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross-functional teams.
Responsibilities for Data Engineer
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics..
Qualifications for Data Engineer
An individual who has valid visa, Opt/Cpt are Applicable
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Strong project management and organizational skills.
Job Types: Full-time, Part-time, Contract
Salary: $70,000.00 - $80,000.00 per year
Benefits:
Employee assistance program
Health insurance
Professional development assistance
Relocation assistance
Compensation package:
Yearly pay
Experience level:
1 year
No experience needed
Under 1 year
Schedule:
10 hour shift
4 hour shift
8 hour shift
Monday to Friday
Ability to commute/relocate:
Dallas, TX 75243: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$75,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Quadrant Resource
4.1",4.1,Remote,Data Engineer,"Job Description:
4-6 years of experience as a Data Engineer in the Snowflake-Fivetran-dbt stack
• Possess a solid understanding of end to end Data ingestion, ETL, data analysis, Reporting processes
• Expert in ETL development using Fivetran and experienced in implementing data pipelines for Snowflake Cloud DWH
•Experienced in using dbt for data transformation
•Expertise on Tableau/Metabase for building reports/dashboards
•Expert in troubleshooting and resolving ETL issues, data load failures/problems and transformation translation problems
•Excellent communicator with the ability to work effectively with distributed teams
Job Type: Full-time
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Data Engineer: 9 years (Required)
Fivetran: 8 years (Required)
DBT: 7 years (Required)
Snowflake: 6 years (Required)
Work Location: Remote",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,2004,Unknown / Non-Applicable
"DiNi Communications, Inc.
3.0",3.0,"New York, NY",Data Center Engineer,"DiNi Communications, Inc. is seeking a Data Center Engineer. This person needs to be a team player with exceptional communication skills. Maintaining the confidentiality of the client's information is of utmost importance.
Required Education and Experience
· At least 8 years of experience as a large data center technician
· Knowledge of Networking,
· Ability to lift equipment at least (50lbs) Rack and Stack (Network Switches, Servers, Storage,
Appliances)
· Knowledge of structured Cabling in a Data Center.
· General knowledge of Operating Systems, (Windows, Unix).
· General knowledge of networking in N Tier Environment (App, Web, DMZ,).
· Level 1 and Level 2 troubleshooting in a N Tier Environment.
· Ability to follow and execute commands to start up and shutdown systems.
· Physically travel to and be available at site B to perform technical changes that require a
person at Site B.
· Performing backup and restore activities.
· Asset Management.
· Maintaining document.
· Work with vendor on circuit issues, (WAN and Internet).
· Cable Patch the equipment in the environment.
· Participate in Site B fall-over testing
Preferred Network Engineering Experience
· Knowledge of N Tier networking.
· Certification in Networking (CISCO).
· Certification in various Operating systems (Linux and Windows), Storage, Messaging
(Microsoft).
· Certification in IT Security.
· Knowledge of Databases.
· Experience working with CISCO, EMC, Symantec, HP, Oracle, Bluecoat, MacAfee, and other vendors' equipment and software in a networked environment.
Responsibilities include but are not limited to:
Adding equipment to support agency applications in site B Environment,
Coordinating Site B testing with Site A technical resources.
Monitoring the B environment.
Rack and Stack new and replacement equipment
Troubleshooting and resolving incidents and problems in the
Physically travel to and be available at Site B to perform technical changes that require a person at Site B.
Upgrading and expanding the environment
Performing backup and restore activities
Manage Site B assets
Maintaining document
Cable Patch the equipment in the environment
Participate in Site B fall-over testing.
Job Type: Full-time
Pay: $60.00 - $70.00 per hour
Schedule:
Monday to Friday
Experience:
large data center technician: 8 years (Required)
Networking in N Tier Environment (App, Web, DMZ,): 3 years (Required)
Ability to lift equipment at least (50lbs): 1 year (Required)
structured Cabling in a Data Center: 3 years (Required)
Level 1 and 2 troubleshooting in a N Tier Environment: 3 years (Required)
Performing backup and restore activities: 3 years (Required)
Cable Patch the equipment: 3 years (Required)
Certification in Networking (CISCO): 1 year (Preferred)
Certification in IT Security: 3 years (Preferred)
CISCO, EMC, Symantec, HP, Oracle, Bluecoat, MacAfee: 3 years (Preferred)
Work Location: In person",$65.00 /hr (est.),Unknown,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
Siri info solutions inc,#N/A,"Beaverton, OR",Sr. Data Engineer,"Must Have Skills
PySpark
Python on AWS
Responsibilities:
Understanding of advanced data warehousing concepts, data modeling and extract, transform and load development
Advanced SQL skills with experience querying large datasets from multiple sources and developing automated reporting
Python skills for scripting, data manipulation, custom extract, transform and loads and statistical/regression analysis particularly, as they apply to sales and marketing operations and performance
Job Type: Full-time
Salary: $47.00 - $50.00 per hour
Experience level:
7 years
Work Location: On the road",$48.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"ECI - Sacramento
4.4",4.4,Remote,Healthcare Data Quality/Engineer/SAS SME (FT),"Healthcare Data Quality / Data Engineer / SAS SME

Senior Healthcare SAS Data Quality Management SME
Location: remote
24-months.
Must have 10-years working in Information Technology or related field, and those years must include the following experiences:
Minimum of three (3) years of experience interpreting, analyzing and applying HIPAA health care transaction requirements.
Minimum of three (3) years of functional experience within an organization with a large, complex data set.
Minimum of three (3) years of experience gathering, documenting, and designing and applying data quality requirements and data standards.
Minimum of three (3) years of experience with implementing electronic tools/software to support data quality requirements.
Minimum of three (3) years of experience establishing a framework for enterprise data quality.

Others:
Experience working on Medi-Cal, or another Medicaid, as a Data Quality Analyst, Business Analyst, or Business Intelligence Analyst.
Experience with tools focused on business intelligence dashboards and reports, such as Statistical Analysis Software (SAS), Microsoft PowerBI or Tableau.
Experience applying data quality management concepts and the Data Management Body of Knowledge (DMBOK) or similar methodology.
Experience in the creation and implementation of data quality service level agreements.
A bachelor's degree from an accredited college or university.



About ECI - Sacramento:

Estrada Consulting, Inc. (ECI) delivers technology-enabled services and solutions to clients all over the USA and British Columbia. We provide system integration, custom application development, data warehouse and business intelligence, project management, custom reporting solutions and consulting services to mid-size and large enterprises in all major industries. The Company headquarters is in Sacramento, California, and was established in year 2000. Visit http://www.estradaci.com/ to learn about our projects, managed services, awards and certifications delivering value for a range of businesses and government agencies.",$70.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2000,Unknown / Non-Applicable
"DSFederal Inc
4.0",4.0,Remote,Junior Data Engineer*,"Description:
We are seeking a Junior Data Engineer to support the design, development, and maintenance of our government client’s data infrastructure. The Junior Data Engineer will work under the guidance of senior team members to implement and maintain data solutions to support business needs.
Requirements:
Develop, maintain, and optimize data pipelines.
Collaborate with cross-functional teams to design and implement data-driven solutions.
Assist in the development and maintenance of data storage systems.
Troubleshoot and debug data-related issues.
Participate in code reviews and contribute to best practices for data engineering.
Technical Skills Required:
Proficiency in at least one programming language (Python, Java, etc.).
Understanding of data modeling, database design, data marts, and data warehousing concepts.
Familiarity with ETL tools and techniques.
Experience with cloud-based data platforms such as AWS or Azure.
Strong problem-solving and analytical skills.
Desired Skills:
Familiarity with data visualization tools such as Tableau or Power BI.
Knowledge of machine learning concepts and frameworks.
Experience Required:
1-2 years of experience in data engineering or related field.
Education Required:
Bachelors in Engineering

Who We Are:
DSFederal is a leader in health IT and data analytics that delivers innovative solutions to improve healthcare delivery and address critical challenges, including cancer surveillance, HIV/AIDS, maternal and child health, and COVID-19 emergency response. Our highly experienced professionals serve over 50 different federal clients on over 80 projects. Our people are united by one mission – to improve human life through transformative solutions. www.dsfederal.com.
What We Can Offer You:
DSFederal attracts the best people in the business with our competitive benefits package that includes medical, dental and vision coverage, 401k plan with employer contribution, paid holidays, vacation, and more. If you enjoy being a part of a high performing, professional services, and technology focused organization, we encourage you to apply!
Our Mission: Improve human life through transformative solutions.
Our Core Values: Integrity, People First, Disciplined Business Practices, Customer Centric, Value Creation
EEO Statement: DS Federal is an Equal Opportunity / Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, or protected veteran status.
E-Verify Statement
DS Federal utilizes the E-Verify program for every potential new hire. This makes it possible for us to confirm that every employee who works for DS Federal is eligible to work in the United States. To learn more about E-Verify you can call 1-800-255-7688 or visit their website by clicking the logo below. E-Verify® is a registered trademark of the United States Department of Homeland Security.
#LI-LP1",#N/A,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$25 to $100 million (USD)
"Merkle
3.7",3.7,"Norristown, PA",Data Engineer,"Company Description

We are Dentsu International. We innovate the way brands are built. That means we do things differently so they’re better than before. In this way, we make our clients’ most important marketing assets—their brands—win in a changing world.
Dentsu International is a modern marketing solutions company. Our mission is to help clients navigate, progress, and thrive in a world of change. Businesses rely on our integrated network of agencies and specialized practices to champion meaningful progress through creative, media, commerce, data, and technology. Part of Dentsu Group, our global network comprises 66,000 diverse people in 143 countries, who are dedicated to teaming for growth and good. Follow us on Twitter @DentsuUSA and visit dentsu.com/us.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.

Job Description
Develop onboarding process to receive and process information related to onboarding a new subscriber
Convert Snowflake Stored Procedures to Python Scripts directly in Airflow
Develop metadata driven solutions for managing data processing and cadence
End to end automation development and testing
Building and maintaining stored procedures and scripts in Snowflake Data Cloud
Building and maintaining complex SQL queries for ETL/ELT processing
Develop Dashboard for data processed for audit and governance needs
Follow Agile methodologies to deliver iterative feature sets rapidly
Maintain codebase using source control methodologies, Git
Collaborate with business analysts, data analysts, and other developers to deliver powerful new features
Coordinate with teams as part of a larger data-sharing system
Contribute to a collaborative, positive, and enjoyable environment for your operations team
Performing quality assurance and testing
Writing user and technical documentation such as runbooks

Qualifications
Strong communication skills both verbal and written
Strong understanding of conditional programming logic
Knowledge of software engineering and development methodologies, techniques, and tools; includes Issue Tracking (JIRA) Software Development Lifecycle SDLC), and Continuous Integration / Continuous Development (CI/CD)
Experience working with large databases and data warehouses utilizing both relational and dimensional design concepts
Eagerness to learn new technologies, programming languages and platforms to solve complex data challenges
Experience with Python required
Experience with SQL required
Experience with Snowflake Data Cloud is highly desirable
Experience with Amazon Web Services (AWS) a plus
Experience with Airflow (or other orchestration) a plus
Experience in database marketing is a plus

Additional Information

Perks:
Great compensation package
Comprehensive healthcare plans
401(k) with employer match
Flexible time-off
16 weeks paid parental leave
Additional Information
The anticipated salary range for this position is $94,000-$146,000. Salary is based on a range of factors that include relevant experience, knowledge, skills, other job-related qualifications, and geography. A range of medical, dental, vision, 401(k) matching, paid time off, and/or other benefits also are available. For more information regarding dentsu benefits, please visit dentsubenefitsplus.com
Employees from diverse or underrepresented backgrounds encouraged to apply.
Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact recruiting@dentsuaegis.com if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.
About dentsu
Dentsu is the network designed for what’s next, helping clients predict and plan for disruptive future opportunities in the sustainable economy. Taking a people-centered approach to business transformation, dentsu combines Japanese innovation with a diverse, global perspective to drive client growth and to shape society www.dentsu.com.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.
Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact your recruiter if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.","$120,000 /yr (est.)",5001 to 10000 Employees,Company - Private,Media & Communication,Advertising & Public Relations,1971,$500 million to $1 billion (USD)
"Trella Health
4.8",4.8,"Philadelphia, PA",Senior Data Engineer,"At Trella Health, we are passionate and committed to our mission – empowering meaningful change in healthcare. Since our founding in 2015, we continue to grow our team, enhance our solution and services offerings, accelerate into new markets, and expand our customer base. We are rapidly growing and are looking for new Trellavators to join our team!
""What is a Trellavator?"" you ask. Innovate and elevate is the name of our game! We go above and beyond to collaborate with and support each other – we believe that when a colleague or customers succeeds, we succeed. By learning from others, building on our successes, and taking risks, we constantly raise the bar – continuous improvement is in our DNA. Our word is paramount, we keep our commitments, and we always follow through. We have a strong, reliable support system that fuels growth, collaboration, and passion – and together, we create a positive environment where everyone at Trella Health, including the customers we support, can thrive. Are you ready to learn more about the opportunities with our team? Trell-yeah you are!
Position Overview:
The Senior Data Engineer will be responsible for designing, implementing, and maintaining an effective data pipeline for Trella Health's applications. This position is responsible for turning data concepts into effective data models that are efficient and scalable. The Senior Data Engineer will ensure a smooth data release and deployment process throughout the data development lifecycle.
Successful candidate will be a highly organized, self-starter that can roll up their sleeves and solve problems efficiently. Ability to document procedures, manage time, and have a knack for optimizing data flows.
Location: Atlanta HQ; Raleigh, NC; Nashville, TN; Philadelphia, PA; Remote considered
Reports to: Manager, Data Engineering
As a member of the Data team at Trella, you will:
Champion a comprehensive data vision and operating principles that improve data quality, consistency, accessibility, decision making, use, and stewardship.
Manage data pipeline by developing processes for data input, transformation, and storage
Act as an interface between the Data Science team and Application Developers
Architect and implement front-end (application layer) and back-end (ETL layer) database technologies in a highly customized data environment
Develop automated processes for moving data between systems
Implement ETL processes that transform source data into functional data stores consumed by front-end applications
Scale data pipeline by utilizing the most efficient tools and technologies
This job might be a fit for you if you have:
Bachelor's degree in Computer Science, Engineering, Mathematics, or other technical discipline (proven professional experience will be considered in lieu of a technical degree)
2+ full years of recent advanced Snowflake development experience
Experience manipulating healthcare data
In-depth knowledge of Amazon Web Services (AWS) and Platform as a Service (PAAS)
Demonstrable experience with Snowflake utilities including SnowSQL & Snowflake Javascript
Analyze all aspects of the Snowflake Query Engine to improve bottlenecks that may exist
Ability to automate data ingestion from Amazon S3, FTP, and other technologies
Experience with scripting languages, extensive Python experience, Node.js experience a plus
Working knowledge of Bitbucket and Git
Experience managing and loading data in AWS RDS (MySQL/MariaDb)
Ability to optimize MySQL query execution plans for robust performance
Technical Job Requirements (Nice to Have)
Experience manipulating healthcare data including claims, ADT data
Foundational knowledge of CI/CD platforms, preferably TeamCity
Development and consumption of API services
Experience with data science tools (Jupyter, SageMaker, R)
Understanding of Atlassian suite of tools, including Jira and Confluence
Soft Skill Job Requirements (Nice to Have)
Strong analytical skills and ability to rapidly switch contexts
Excellent communication and problem-solving skills
Sole contributor that is willing to independently uncover and improve processes that are inefficient or sub-optimal
Track record of implementing creative solutions to complex problems
Experience working in a nimble, fast-paced, start-up environment
Knowledge of HIPPA, PHI, PII policies
About Trella Health:
Trella Health provides unmatched, actionable market intelligence to post-acute care and value-based care providers of all sizes. Our industry leading analytics paired with CRM and EHR integration workflows positions us as the most advanced sales enablement platform for the post-acute care market. One of only a few companies to be deemed both a Qualified Entity by CMS and an Innovator under its Virtual Research Data Center Program, Trella has elite access to billions of performance metrics and referral data to enable competitive positioning across the market. Its standardized insights, representing 90% of all lives 65+ U.S. population, help customers identify, engage, and manage critical relationships and advance their organizations with certainty.
Since launching our first product in 2016, we have experienced tremendous growth and have highlighted a few key data points to get an at-a-glance view of Trella:
Over 700 customers including 10 of the top 10 post-acute providers and 4 of the top 10 health systems
Over 120 team members and growing
Recognized as one of Modern Healthcare's ""Best Places to Work"" in consecutive years and were ranked in the Top 50 startups in the US by LinkedIn
Headquartered in Atlanta, GA with remote hubs in Nashville, Philadelphia, Raleigh, and the Philippines
The Trellavator Experience:
At Trella Health, you can expect to join a welcoming team that truly embodies our core values. Our collaborative culture is anchored by trust, transparency, and inclusion — and we also have a lot of fun. No matter where you work, at Trella Health you can expect an awesome team, frequent virtual gatherings, engaging events, casual attire, and more.
We offer competitive salaries with a comprehensive benefits package to all employees and provide an environment that fosters work-life harmonization with unlimited PTO, company-observed holidays, and Summer Fridays. As we continue to see exponential growth, our goal is to continue to put team members first and strive to offer our team members the best culture and benefits possible. Some of the benefits we provide are:
Health, Dental, Vision & Voluntary Benefits
Competitive Salary & Bonus Plans
401k Retirement Savings
Unlimited PTO & 10 Paid Holidays
Flexible Work Hours
Equity Shares
Paid Leave Programs
Marketplace for discounted retail and entertainment
Equal Opportunity Employer
Trella Health is an equal opportunity employer. All persons will receive consideration for employment without regard to race, color, gender, sexual orientation, gender identity or expression, religion, national origin, marital status, age, disability, handicap, veteran status, genetic information, or any other protected status as recognized by federal, state, or local laws.","$113,846 /yr (est.)",51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2015,Unknown / Non-Applicable
"Argo Data
2.7",2.7,"Richardson, TX",Senior Data Engineer,"ARGO is a leading provider of software and analytics solutions for both the financial services and healthcare industries. ARGO transforms business processes for financial service providers and healthcare organizations using proven business models and software innovation informed by real customer challenges, breakthrough technology, and rich analytics.
ARGO- Data Engineer II (Markham, ON; Memphis, TN; ...) (theapplicantmanager.com)
JOB SUMMARY
The Data Engineer II is responsible for designing and implementing data solutions to support business needs. The successful candidate will have a strong background in software engineering and database design, as well as experience with working with cloud-based tools and technologies.
EXPECTED WORK AND PERFORMANCE
This section is intended to provide a more detailed overview of what the expectations of the job are to include: work to be performed, expected performance results and how they will be measured, as well as connection to corporate strategy, values, and mission. Job duties and responsibilities include the following. Other duties may be assigned as needed.
Design, build, and maintain data pipelines for data extraction, transformation, and loading (ETL) from various sources.
Ensure data security and privacy protocols are implemented and enforced.
Monitor and optimize data storage and processing performance to ensure high performance and cost-effectiveness.
Collaborate with data scientists, analysts, and other stakeholders to understand their data needs and translate them into technical solutions.
Maintain documentation of data systems, processes, and policies.
Stay current with the latest data tools, technologies, and trends and recommend improvements.
Mentor and train junior data engineers as needed.
QUALIFICATIONS
Bachelor's degree in Computer Science, Software Engineering, or related field.
3-5 years of experience in software engineering and SQL database design.
Proficiency in programming languages such as Python.
Strong understanding of data warehousing, ETL processes, and data modeling.
Experience with cloud infrastructure technologies such as Microsoft Azure.
Excellent problem-solving and analytical skills.
Ability to work well in a team-oriented environment and communicate effectively with both technical and non-technical stakeholders.
PHYSICAL DEMANDS
The candidate will work at our corporate headquarters in Richardson, TX. While performing the duties of this job, the employee is regularly required to stand; walk; sit; use hands to finger, handle, or feel; reach with hands and arms; talk or hear. Specific vision abilities required by this job include close vision, distance vision, color vision, peripheral vision, depth perception, and ability to adjust focus.
ADDITIONAL REQUIREMENTS:
Applicants for U.S. or Canadian based positions with ARGO must be legally authorized to work in the United States or Canada. Verification of employment eligibility will be required at the time of hire. Visa sponsorship is not available at this time for ARGO positions.
Pre-employment background screening will be conducted.
Job Type: Full-time
Pay: $70,745.42 - $141,624.04 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Life insurance
Paid time off
Professional development assistance
Referral program
Relocation assistance
Retirement plan
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Performance bonus
Yearly bonus
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Richardson, TX 75081: Reliably commute or willing to relocate with an employer-provided relocation package (Required)
Experience:
Informatica: 3 years (Required)
SQL: 5 years (Required)
Data warehouse: 3 years (Required)
Python: 3 years (Required)
Azure: 3 years (Required)
Work Location: In person","$106,185 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Computer Hardware Development,1980,$25 to $100 million (USD)
Kanini,#N/A,"North Druid Hills, GA",Senior Data Engineer,"About Kanini
Kanini provides Agile Software Development, Cloud Computing, Data Science, and Location Intelligence services to public and private organizations. We have successfully served our clients in government, finance, transportation, utility, and software industries since 2003.
Why you should join
Working at Kanini is flexible and personal. We are a highly motivated, collaborative team experimenting with the latest technologies. We are committed to everyone having a healthy work/life balance, and we provide extensive mentorship and training resources to help you succeed.
Kanini is looking for Data Engineer who has a deep experience in Data Engineering, Agile planning, Snowflake, SQL, MySQL, PostgreSQL, Git, AWS Data Tools, Python, Jira
Job Description
The Senior Data Engineer is responsible for processing structured and unstructured data, validating data quality, and developing and supporting data products. The Senior Data Engineer also plays a role in Agile planning, providing advice and guidance, and monitoring emerging technologies.
Responsibilities
Gathers and processes raw, structured, semi-structured, and unstructured data using batch and real-time data processing frameworks.
Understands and enforces appropriate data master management techniques.
Ensures data quality and implements tools and frameworks for automating the identification of data quality issues.
Work with internal and external data providers on data validation providing feedback and making customized changes to data feeds and data mappings.
Understands the challenges that the analytics organization faces in their day-to-day work and partner with them to design viable data solutions.
Recommends improvements to processes, technology, and interfaces that improve the effectiveness of the team and reduce technical debt.
Implements and optimizes data solutions in enterprise data warehouses and big data repositories.
Installs, maintains, monitors, and supports business intelligence, distributed computation, and big data analytics tools.
Provides ongoing support, monitoring, and maintenance of deployed products.
Actively works with less experienced data engineers providing technical guidance and oversight.
Actively participates in the engineering community, staying up to date on new data technologies and best practices and shares insights with others in the organization.
Qualifications:
Bachelor’s degree in computer science or related field and 4 years work experience
Working experience with batch and real-time data processing frameworks
Working experience with data modelling, data access, schemas, and data storage techniques within Snowflake
Working experience in design, development, and implementation of highly scalable, high-volume software systems and components, client-facing web applications, and major Internet-oriented applications and systems
Working experience working with relational databases such as SQL, MySQL, Postgres/PostgreSQL
Working experience with business intelligence tools and platforms
Working experience with data quality tools
Working experience with application lifecycle methodologies (e.g. waterfall, agile, iterative)
Experience with ETL processes and tools
Experience working with Git.
Desirable Qualifications
Scrum Developer Certification or equivalent
Working experience with Sisense platform
Working experience with SQL Server Integration Services
Working experience with AWS data tools (Database Migration Service)
Working experience with Python
Experience working with Jira, Rally or similar tools
Kanini Software Solutions, Inc. does not discriminate in employment matters based on race, gender, religion, age, national origin, citizenship, veteran status, family status, disability status, or any other protected class. We support workplace diversity. If you have a disability, please let us know if there is anything we can do to improve the interview process for you; we’re happy to accommodate.
Kanini Software Solutions, Inc., 25 Century Blvd., Ste. 602, Nashville, TN 37214
Job Types: Full-time, Contract
Pay: $60.00 - $65.00 per hour
Application Question(s):
Current Location?
Work Location: Remote",$62.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Diamondpick,#N/A,"Alpharetta, GA",Java Software Engineer with Big-Data,"About us
There is someone out there who fits your requirements like a glove! We are an innovative Talent Solutions company with a vision to build talent supply chain models without compromise for the technology industry.
We are a new-age talent solutions company with a vision to build an enterprise recruitment model without compromise. We love to solve complex talent challenges and strive to be an enabler of business success for our clients using data, technology and our team’s vast experience in the technology market.
Senior Java Developer with Big data
Alpharetta, GA (Hybrid Work from Day1)
Job Description:
The ideal candidate must possess strong background on frontend and backend development technologies.
The candidate must possess excellent written and verbal communication skills with the ability to collaborate effectively with domain experts and technical experts in the team.
Responsibilities:
As a Java Senior Developer, you will
Maintain active relationships with Product Owner to understand business requirements, lead requirement gathering meetings and review designs with the product owner
Own his backlog items and coordinate with other team members to develop the features planned for each sprint
Perform technical design reviews and code reviews
Be Responsible for prototyping, developing, and troubleshooting software in the user interface or service layers
Perform peer reviews on source code to ensure reuse, scalability and the use of best practices
Participate in collaborative technical discussions that focus on software user experience, design, architecture, and development
Perform demonstrations for client stakeholders on project features and sub features, which utilizes the latest Front end and Backend development technologies
Requirements:
6+ years of experience in Java/JEE development
Skills in developing applications using multi-tier architecture
Knowledge of google/AWS cloud
Java/JEE, Spring, Spring boot, REST/SOAP web services, Hibernate, SQL, Tomcat, Application servers (WebSphere), SONAR, Agile, AJAX, Jenkins..etc
Skills in UML, application designing/architecture, Design Patterns..etc
Skills in Unit testing application using Junit or similar technologies
Good communication skills
Leadership skills
Provide overlap coverage with onsite/customer teams
Capability to support QA teams with test plans, root cause analysis and defect fixing
Strong experience in Responsive design, cross browser web applications
Strong knowledge of web service models
Strong knowledge in creating and working with APIs
Experience with Cloud services, specifically on Google cloud
Strong exposure in Agile, Scaled Agile based development models
Familiar with Interfaces such as REST web services, swagger profiles, JSON payloads.
Familiar with tools/utilities such as Bitbucket / Jira / Confluence
Job Types: Full-time, Contract, Temporary
Pay: From $60.00 per hour
Benefits:
Flexible schedule
Compensation package:
Hourly pay
Experience level:
8 years
Schedule:
8 hour shift
Day shift
Work Location: Hybrid remote in Alpharetta, GA 30005",$60.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Quirch Foods
3.0",3.0,"Coral Gables, FL",Data Engineer,"About Quirch Foods
In business for over 50 years, Quirch Foods is a global distributor and exporter of fresh and frozen food products with a unique focus on ethnic cultures. We service our customers from 5 distribution centers strategically located in the United States. We have consistently been ranked among the top 50 exporters in the US and are one of the largest importers of seafood. Our customers include independent grocers, chain supermarkets, foodservice distributors, cruise lines, restaurants and food processors/manufacturers. We have an extensive product list with over 8,000 SKU's and carry niche brands such as Panamei, Mambo and Chiquita along with national brands like Tyson, IBP, Excel, JBS, Iowa Premium, Smithfield, Dietz & Watson, along with many others. We are headquartered in Coral Gables, FL.
Quirch is a portfolio company of Palladium Equity Partners, a leading mid-market private equity fund based in New York City, which partnered with the Executive Team to capitalize on Quirch Foods’ record of excellence and to grow it decisively beyond its current business.
Position Summary
At Quirch Foods we believe in leveraging technology as a source of competitive advantage. This includes customized ERP Solutions, Business Intelligence, EDI, Web and Mobile Solutions. We are currently looking for a Data Engineer to join our development efforts.
Essential Duties and Responsibilities:
Develop, test and maintain custom business solutions by leveraging state of the art software development practices using an agile methodology
Evaluate business requirement and help define problems and develop solutions
Participate in design and architecture discussions with business leaders, end users and IT team members
Document business requirements and solution, document code and provide support for creation of end user documentation
Update business knowledge, technical skills and soft skills, leverage educational opportunities, participate in professional organizations
Experience:
MS SQL Programming
Visual Studio\C#\WPS Experience
Microsoft SSAS, SSIS, SSRS desired but not required
Biztalk experience desired but not required
Skills and Requirements:
Bachelor’s degree in Computer Science, Engineering, Math or equivalent and\or related experience and training preferred
Strong analytical and problem-solving skills
Strong collaborator and team player
Great organizational skills, attention to detail and follow thru
Effective oral and written communication skills
Extensive familiarity with data management principles
Benefits:
Professional growth and developmental opportunities to grow your skills using state of the art technology
Comprehensive benefits package that includes: Medical, Dental, Prescription Drug Plan, Disability Plan, Life insurance Plan
401K savings Plan
Paid Holidays
Personal Time off
Employee Discounts
Quirch Foods is an Equal Opportunity Employer (EOE). Qualified applicants are considered for employment without regard to age, race, color, religion, sex, national origin, sexual orientation, disability, or veteran status. All applicants must be eligible to work in the United States.
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Vision insurance
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Coral Gables, FL 33134: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
What is your desired compensation?
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Data Engineering: 1 year (Preferred)
Work Location: In person","$89,340 /yr (est.)",201 to 500 Employees,Company - Private,Retail & Wholesale,Wholesale,1967,$500 million to $1 billion (USD)
"Apolis
3.9",3.9,"Saint Louis, MO",Data Engineer (Python Focused),"Role: Data Engineer (Python Focused) (W2 ONLY)
Location : St. Louis, MO 63102
Duration: 18 Months
The primary responsibilities of this role will be to work as a part of the data and analytics team of the client. The ideal candidate would have experience using ETL and other data engineering tools in a cloud environment. Additionally, this candidate would have coding and scripting experience creating code to move and manipulate data using tools such in Python such as Spark or Pandas. Microsoft Azure experience is a plus but relevant experience with other cloud-based environments will be considered.
This team uses Microsoft Azure to build and productionize data sets and processes to the product owners and product users and this role will be expected to contribute to those objectives. As a member of this team, you will work in conjunction with other data engineers, platform engineers, solution developers and others to execute and deliver working solutions to the end users.
Technical Skills
High focus on Python
More Programmer/Coder experience, less Cloud experience needed.
Must have experience with data analytics/engineering and cloud technologies and an in-depth working knowledge and understanding of data engineering, design, and integration.
Intermediate or higher level of proficiency with Python Must have the proven ability to learn quickly and apply cloud data engineering principles and skills to new tools and use cases.
Ability to define and take technical ownership of detailed requirements and collaborate effectively with various team members such as Business Analysts and Project Managers and end users.
Excellent communication (verbal and written) and problem-solving skills are essential for this position.
Initiative, Proactive Cooperation, and curiosity are mandatory behavioral skills required for this position.
Experience with several of the following technologies is preferred:
Experience with Microsoft Azure Platform. In particular, the data processing toolsets including Azure Data Factory Pipelines, Azure Databricks, Azure Data Lake Storage.
Experience using Notebooks (Such as Jupyter, DataBricks) also desired.
Experience using code languages such as Python and C#
Experience using query languages such as SQL, MDX and DAX
Experience using scripting languages such as PowerShell, M-Query (Power Query), and Windows batch commands.
Understanding of Agile and DevOps principles for software development and ownership.
Familiar with compliance and controls for software development and deployment.
Education Required
A bachelor's in computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field [or equivalent work experience] is not required but is preferred.
Job Type: Contract
Salary: $60.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Vision insurance
Compensation package:
Overtime pay
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
St. Louis, MO: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Preferred)
ETL: 5 years (Preferred)
Cloud infrastructure: 4 years (Preferred)
Work Location: One location",$65.00 /hr (est.),501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,1996,$25 to $100 million (USD)
"GOBankingRates
3.3",3.3,"North, SC",Staff Data Engineer,"GOBankingRates™ is unique in the digital marketing and media industry - we combine marketing, digital, content and fintech. Our performance based approach increases brand awareness and generates targeted audience engagement on our internal web properties and partner sites.
Learn More About What We Do

What's interesting about this role?
GOBankingRates has big growth plans ahead and is looking for a strong Staff Data Engineer experienced in Data warehousing and Python to join our Data Engineering team. The GOBankingRates Data Engineering team is responsible for designing and developing the Data lake, enterprise database, data warehouse, reporting solutions, and pipelines for data processing. If you are a critical thinker with a solid track record of developing data solutions and solving complex problems with SQL and Python, we want you to join our team! You will play a vital role in designing and developing our next generation data pipelines and data platform. Join our team and prototype new data product ideas and concepts!
How will you make an impact?
Build and maintain multiple data pipelines to ingest new data sources (API and file-based) and support products used by external users and internal teams.
Optimize by building tools to evaluate and automatically monitor data quality and develop automated scheduling, testing, and distribution of feeds.
Work with data engineers, data scientists, and product managers to design, rapid prototype, and productize new data product ideas and capabilities.
Design and build cloud-based data lakes and data warehouses.
Conquer complex problems by finding new ways to solve them with simple, efficient approaches focusing on our platforms' reliability, scalability, quality, and cost.
Collaborate with the team to perform root cause analysis and audit internal and external data and processes to help answer specific business questions.
What will you bring to us?
Master's Degree (or a B.S. degree with relevant industry experience) in math, statistics, computer science, or equivalent technical field
Experience with dimensional data modeling and schema design in a database or data warehouse
Expertise with scripting languages such as Python and writing efficient and optimized SQL.
Working experience in building data warehouses and data lakes.
Experience working directly with data analytics to bridge business requirements with data engineering.
Experience with AWS infrastructure
Ability to operate in an agile, entrepreneurial start-up environment and prioritize
Excellent communication and teamwork, and a passion for learning
Curiosity and passion for data, visualization, and solving problems
Willingness to question the validity, accuracy of data, and assumptions
Preferred Qualifications:
Experience building data warehouse, data lake, and data pipeline using Snowflake/Redshift and other AWS Technologies.
Experience with large-scale distributed systems with large datasets.
Experience with event streams and stream processing (e.g., Kafka, Spark, Kinesis)
Hands-on experience with event streaming with modern event streaming tools like Pulsar, Kafka, and Kinesis. Understanding when streaming vs. batch processing is appropriate, and tradeoffs in a given context
Knowledge of advertising platforms.

Benefits
Competitive salary with excellent growth opportunity; we pride ourselves in having a team that exudes leadership, high initiative, creativity and passion.
Awesome medical, dental and vision plans with heavy employer contribution
Paid maternity leave and paternity leave programs
Paid vacation, sick days and holidays
Company funding for outside classes and conferences to help you improve your skills
Contribution to student loan debt payments after the first year of employment
401(k) - employees can start contributing immediately. After the first year, GOBankingRates matches your contribution up to 4% of your salary
A note about our response to COVID -19 and our new norm: The world has changed and we know it's important to adapt and to do our part to take care of our teams in this global pandemic. Our number one priority is to have our team feel safe, balanced and connected. We're committed to providing our teams with the best resources and tools to navigate this new virtual world that we're living in. We've also reinvented the ways in which we recognize, celebrate, and engage with each other to keep our culture strong!
Here's a peek into our world at GOBankingRates -
Our teams are working remotely 100% for the foreseeable future and have flex time. We're in the digital media space so we're mobile and flexible!
Option to work from an office (if you need to get away!)
Tools & resources are available to keep our team connected across North America. (JIRA, Trello, Airtable, Slack, Zoom and so much more!)
To keep our community engaged and connected, virtual team building events are held weekly and monthly.
For wellness and balance, weekly virtual fitness classes such as yoga are available.
To care for the local communities that we're a part of across the U.S our team members host socially distanced philanthropic events every quarter.
And most importantly, we've committed to consistent and transparent communication to help us all stay informed, engaged and to keep us on our path to success and #greatness.
We are an equal-opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.","$134,519 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Internet & Web Services,#N/A,$25 to $100 million (USD)
OpenSecrets.org,#N/A,Remote,Snowflake Data Engineer,"About us
Our goal is to is to track the flow of money in American politics and provide the data and analysis to strengthen democracy. Our vision is for Americans to use this knowledge to create a more vibrant, representative and accountable democracy..
Our work environment includes:
Modern office setting
Work-from-home days
Wellness programs
Data Engineer
OpenSecrets is seeking to add a new member to our data team. The Data Engineer will be responsible for helping build and maintain our data systems.
Responsibilities include:
Implementing methods to improve data reliability and quality
Analyzing raw data and implementing that data into our system using data pipelines
Troubleshooting data errors and pipeline issues
REQUIRED QUALIFICATIONS:
Experience working with and analyzing large datasets, equivalent to 2-3 years in a similar position
Proven ability to build complex data pipelines using Snowpipe and SSIS
The ability to work with diverse populations
Experience with MSSQL Server and Snowflake
Advanced knowledge of ANSI SQL
PREFERRED QUALIFICATIONS:
Excellent written and verbal communication skills.
The ability to manage multiple priorities and projects both independently and as a member of a team
Snowflake SnowPro Data Engineer certification
Experience with Apache Spark
Experience with machine learning matching models
Knowledge of and interest in American government and money in politics.
SALARY AND BENEFITS:
We are a fully remote organization and this person can be based anywhere in the U.S.
OpenSecrets offers an excellent benefits package, including full medical, dental, 403(b) match, generous vacation and sick leave, and all federal holidays paid.
TO APPLY:
Please visit this link: https://airtable.com/shrGe2bugeapKVr8a
People of diverse backgrounds are strongly encouraged to apply. OpenSecrets is an equal opportunity employer committed to creating a diverse and inclusive organizational culture. We do not discriminate against candidates and employees because of their disability, sex, race, gender identity, sexual orientation, religion, national origin, age, veteran status, or any other protected status under the law.
ABOUT US: OpenSecrets is the nation's premier research group tracking money in U.S. politics and its effect on elections and public policy. Our mission is to track the flow of money in American politics and provide the data and analysis to strengthen democracy. Our vision is for Americans to use this knowledge to create a more vibrant, representative and accountable democracy.
Job Type: Full-time
Pay: $83,874.82 - $100,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Retirement plan
Vision insurance
Compensation package:
Yearly pay
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Experience:
SQL: 3 years (Required)
Snowflake: 1 year (Preferred)
ETL: 2 years (Required)
Work Location: Remote","$91,937 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Tipico - North America
4.0",4.0,"Denver, CO",Data Engineer,"Company Description

Tipico is an energetic, passionate, bold tech company, and we love sports! Tipico has recently expanded to the US market, establishing offices in Hoboken, New Jersey and Denver, Colorado. We are driven by our core values: passion, progress and trust. Our goal is to build the best possible mobile sports betting product in the industry, in order to amplify the emotions of millions of sports fans every day. Let’s Make This Interesting ™
Please note: this role is located in our Denver, CO office; we work off a hybrid model and come into the office 2 days per week.

Job Description

The Data Engineer will report directly to the Data Engineering Manager and work together with rest of the data engineers, BI analysts, and product teams. The position holder ensures this is done by doing the following:
Create and maintain both batch (ETL/ELT) and real-time data pipelines and architecture
Ensure all data provided by Data Platform is of the highest quality and is delivered in a timely manner and in line with agreed SLAs
Assemble large and complex data sets that meet functional and non-functional business requirements
Identify, design, and implement internal process improvements, including automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies
Collaborate with stakeholders including data architects, Executive, Product, Data and IT team members from the beginning to the delivery of a project
Build a scalable, modern, and trusted data ecosystem, making sure it is responsive, available and in line with business requirements
Take part in sprints, workloads, and deliverables through agile methodologies
Manage quality assurance, verification of accuracy and consistency of data, specifically KPI measures sent to the business
Thrive working in a fast-paced and challenging environment
Open to continuous learning individually and with the team
Comfortable with being challenged / seeking challenge
Willing to work flexible hours if needed

Qualifications
3+ years in Data Platforms / Datawarehouse / BI experience
Must have experience in successfully implementing data-centric applications, such as data warehouses, operational data stores, and data integration projects
Strong analytical and problem-solving skills that balance with creative approaches
Ability to learn new and complex concepts quickly
Strong experience with large-scale production databases and SQL
Solid understanding of cloud data services: AWS services such as S3, Athena, EC2, RedShift, EMR, EKS, RDS and Lambda
Proficient with one or more big data & new data technologies such as DBT, NIFI, Airflow, Kafka
Experience with data modelling
Experience with ETL development (extractions, data load, aggregation, transformation)
Knowledge in working with time-series/analytics databases such as Elasticsearch
Understanding of containerization and orchestration technologies like Docker/Kubernetes
Must be able to take ownership of projects with a high level of attention to detail and priority
Team player with a hands-on approach and attitude
Must be comfortable with on call rotation
Nice to have:
Familiarity with Marketing tooling and/or CRM systems is preferred
Familiarity with using business intelligence tools such as Domo, Tableau or Looker
Knowledge about and / or experience with sports betting
Background in digital companies, online, gambling, gaming, e-commerce, marketing, or mobile apps
Familiarity with one scripting language, preferably Python

Additional Information

What's in it for you:
Work in an environment where you, your work and ideas matter and have an impact.
You will be a be part of the newest and up and coming USA online iGaming and sports betting companies in the market.
Work in a new and young business with high growth potential.
Build your own success story together with us.
Work with self-organized, self-responsible and entrepreneurial employees.
Start-up feeling, backed-up by a leading European sports betting house. We are a high-volume business and are taking off in the US!
Competitive salary, Medical, Vision, Dental Benefits, Unlimited PTO, and more
Salary for this position is $120k+ & annual performance bonus","$120,000 /yr (est.)",201 to 500 Employees,Company - Private,"Arts, Entertainment & Recreation",Gambling,2004,Unknown / Non-Applicable
"R1 RCM, Inc.
3.4",3.4,Remote,Data Engineer III,"The Software Engineer III serves as a Data Engineer focused on building next generation of healthcare products and services. As part of a collaborative, motivated Agile-based team, the Software Engineer III is responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.
Responsibilities
Create and maintain optimal data pipeline architecture to assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Work with stakeholders to optimize the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using ‘big data’ technologies.
Work with stakeholders including the Product, Data and Engineering teams to assist with data-related technical issues and support their data infrastructure needs.
Adhere to and ensure engineering best practices using modern SDLC that enables CI/CD and favors automation, auditability, automated testing, infrastructure, and policy as code.
Ability to collaborate effectively with other architects and engineers delivering end to end, high-performing, highly reliable, scalable, and operable solutions.
Required Qualifications
Bachelor’s degree in Computer Science, Engineering, or a related technical field; or equivalent experience.
3+ years of experience delivering data engineering applications and services.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets .
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Experience in any one of the big data technologies (Hadoop, MapReduce , Spark, Data Lake etc. )
Experience in any one of the stream-processing systems ( Kafka, Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages : Python, Java, Scala, etc.
Understanding of modern software development including version control, unit testing, and continuous integration and deployment.
Strong communication skills, with proven ability to present complex ideas and to document concisely.
Preferred Qualifications
Working knowledge of Cloud providers and services such as Amazon AWS or Microsoft Azure.
Experience with relational SQL and NoSQL databases, including MongoDB and ElasticSearch.
Knowledge of healthcare data standards such as HL7, FHIR, EDI X12.
Knowledge of the healthcare revenue cycle, EMRs, practice management systems.
Working in an evolving healthcare setting, we use our shared expertise to deliver innovative solutions. Our fast-growing team has opportunities to learn and grow through rewarding interactions, collaboration and the freedom to explore professional interests.

Our associates are given valuable opportunities to contribute, to innovate and create meaningful work that makes an impact in the communities we serve around the world. We also offer a culture of excellence that drives customer success and improves patient care. We believe in giving back to the community and offer a competitive benefits package including:
Comprehensive Medical, Dental, Vision & RX Coverage
Paid Time Off, Volunteer Time & Holidays
401K with Company Match
Company-Paid Life Insurance, Short-Term Disability & Long-Term Disability
Tuition Reimbursement
Parental Leave
R1 RCM Inc. (“the Company”) is dedicated to the fundamentals of equal employment opportunity. The Company’s employment practices , including those regarding recruitment, hiring, assignment, promotion, compensation, benefits, training, discipline, and termination shall not be based on any person’s age, color, national origin, citizenship status, physical or mental disability, medical condition, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status or any other characteristic protected by federal, state or local law. Furthermore, the Company is dedicated to providing a workplace free from harassment based on any of the foregoing protected categories.
If you have a disability and require a reasonable accommodation to complete any part of the job application process, please contact us at 312-496-7709 for assistance.
CA PRIVACY NOTICE: California resident job applicants can learn more about their privacy rights California Consent",#N/A,10000+ Employees,Company - Public,Healthcare,Health Care Services & Hospitals,2003,Unknown / Non-Applicable
"Petrolink International
2.9",2.9,"Houston, TX",Data Analysis Engineer,"Location:
Houston, Texas, U.S.A.
Job Type:
Full-time
Job Description:
Petrolink is a global company that specializes in user-driven technology for the Energy Industry.
We are looking for a Data Analysis Engineer to assist with creating and delivering algorithms primarily used by our engineering solutions.
This position is based in our U.S. office in Houston, Texas.
Duties & Responsibilities:
Research and provide logic for engineering solutions algorithms.
Collaborate with Subject Matter Experts (Drilling Engineers, Mud loggers, Pore Pressure Analysts, and other specialists from the Drilling Industry) to understand different challenges in the Drilling Industry and the practices in place to address/solve these challenges.
Code the algorithms using C# to run on Petrolink’s proprietary analytics engine.
Develop, test, and maintain algorithms.
Support escalations related to issues in algorithms.
Learn existing algorithms to provide support when required.
Prepare training material and sessions for stakeholders as part of the production release.
Skills & Qualifications:
Proficient in C#.
Experience with Python is an advantage.
Knowledge in drilling practices, and ability to speak to other drilling experts and capture requirements and logic.
Knowledge in drilling performance KPIs is an advantage.
Prior experience with WITSML is an advantage.
Function and communicate well in a team having diverse expertise.
Bi-lingual (English and Spanish) preferred.
At least 2 years’ relevant experience.
Benefits:
Health insurance
Dental insurance
401(k) with matching
Health savings account
Life insurance
Long-term disability
This job description is intended to explain the general nature and level of work being performed. Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.
Applicants must be legally authorized to work in the United States for any employer.
To apply for this position, applicants must meet the above criteria.
Interested applicants fulfilling the above criteria, please send your resume to careers.usa@petrolink.com.","$81,133 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"DataArt
4.6",4.6,Remote,Data Engineer,"CLIENT
Our client is a pioneer in US schools’ education since 2000, it is leading the way in next-generation curriculum and formative assessment. They develop a number of solutions and interactive web products for teachers, students and their parents.

The company is technology-driven with a huge number of software engineers involved in product development. They have a very solid approach to technology, employ best practices and processes, with a focus on cutting-edge frameworks, languages and tools.

We invite to the company, not a project
TEAM
There are multiple autonomous and empowered teams who work with their own part of the product and choose their own way of doing things and they expect people to be proactive and good team players.
Responsibilities
Building well-tested and optimized ETL data pipelines for both full and delta extraction
Collaborating with data analysts and learning scientists to store, aggregate, and calculate captured students’ work
Contributing to leading industry data standards, such as Caliper Analytics or xAPI
Improving our deployment and testing automation data pipelines
Requirements
3+ years of professional software development or data engineering experience
Strong CS and data engineering fundamentals
Proven fluency in SQL and a development language such as Python
Experience with the following storages: Snowflake, AWS Storage Services (S3, RDS, Glacier, DynamoDB)
Experience with ETL/BI: Airflow, DBT, Matillion, Looker
Experience with Cloud Infrastructure: AWS Kinesis, Lambda, API Gateway, Terraform
Understanding of ETL/ELT pipelines and Data Warehousing design, tooling, and support
Understanding of different data formatting (JSON, CSV, XML) and data storage techniques (3NF, EAV Model, Star Schema, Data Lake)
BS in Computer Science, Data Science, or equivalent
Experience in education or ed-tech
Proven passion and talent for teaching fellow engineers and non-engineers
Proven passion for building and learning: open source contributions, pet projects, self-education, Stack Overflow
Strong communication skills in writing, conversation",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,1997,$100 to $500 million (USD)
"Barracuda Networks Inc.
3.6",3.6,"Alpharetta, GA",Data Engineer,"Job ID: 23-579

Come Join Our Passionate Team! At Barracuda, we make the world a safer place. We believe every business deserves access to cloud-enabled, enterprise-grade security solutions that are easy to buy, deploy, and use. We protect email, networks, data and applications with innovative solutions that grow and adapt with our customers’ journey. More than 200,000 organizations worldwide trust Barracuda to protect them — in ways they may not even know they are at risk — so they can focus on taking their business to the next level.
We know a diverse workforce adds to our collective value and strength as an organization. Barracuda Networks is proud to be an Equal Opportunity Employer, committed to equal employment opportunity and equitable compensation regardless of race, gender, religion, sex, sexual orientation, national origin, or disability.
Envision yourself at Barracuda
We currently have an exciting new position open for a Data Engineer. This person will assist in driving technology innovation for engineering projects and work on architecture of AWS, Elastic and Kubernetes. They will also work closely with the Technology Architect to create and drive the Architecture Review Board.
What you’ll be working on
Determine internal / external customer data needs and identify system specifications
Analyze the needs of large systems and breaking them down into smaller manageable parts
Responsible for designing the journey of data from various sources to the database and everything in between
Plan and design the structure of data, brainstorm with team
Analyze best technology products and systems for internal teams
Help provide build vs buy decisions on data architecture
Communicate data requirements to engineers; explain data architecture to them and provide assistance throughout SDLC
Choose / maintain suitable software, hardware and integration methods
Help resolve technical problems as and when they arise
Review testing procedures to ensure data performant and error free
Ensure that database satisfy quality standards and procedures. Setup such standards.
Work with senior IT personnel to devise plans for future IT requirements of the organization
Produce progress reports on key infrastructures - Elastic and Data Pipelines
What you bring to the role
Bachelor Degree or Post Graduate in Computer Science, or relevant work experience
5+ years of experience in an Engineering / Operations heavy organization
1+ years of experience as a Data Analysis and Data Design responsible for data architecture of a SaaS application
Experience in understanding and designing data sharding in a distributed database
Some Hands-on experience in software development and system administration
Knowledge of strategic IT solutions
Familiarity with programming languages like JavaScript or Java or Python
Familiarity with Linux OS and Windows alike
Experience in cloud computing and cloud Technologies (AWS, GCP, Azure)
Excellent communication skills – you should be adept at listening to, understanding, and explaining key concepts to managerial and technical resources.
Good Anticipation and problem-solving skills
Ability to prioritize and manage time – responsible for multiple streams of work at the same time
Relationship building - Ability to form a good rapport with internal and external stakeholders
In-depth knowledge about quality standards and security best practices

Preferred Skills:
Expert level Understanding of Elastic eco system - Elasticsearch, Logstash, Kibana, Filebeats
Understanding of secure tunnel communication using VPN
Experience designing data at scale
AWS experience is preferred in Cloud Technologies
What you’ll get from us
A team where you can voice your opinion, make an impact, and where you and your experience are valued. Internal mobility – there are opportunities for cross training and the ability to attain your next career step within Barracuda.
Equity, in the form of non-qualified options
High-quality health benefits
Retirement Plan with employer match
Career-growth opportunities
Flexible Time Off and Paid Time Off benefits
Volunteer opportunities
#LI-Remote","$98,161 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,$100 to $500 million (USD)
"Contact Government Services, LLC
4.7",4.7,"Washington, DC",Data Engineer,"CGS is seeking a passionate and driven Data Engineer to support a rapidly growing Data Analytics and Business Intelligence platform focused on providing solutions that empower our Federal customers with the tools and capabilities needed to turn data into actionable insights. The ideal candidate is a critical thinker and perpetual learner; excited to gain exposure and build skillsets across a range of technologies while solving some of our clients’ toughest challenges. As a Data Engineer, you will be integral to data operations for the development and integration of multiple data types across a range of data sets and sources.

You will be responsible for the day-to-day operations of systems that depend on data, ensuring data is properly processed and securely transferred to its appropriate location, in a timely manner. Processing data will include managing, manipulating, storing and parsing data in a data pipeline for variety of target sources. You will also support maintenance of applications and tools that reside on these systems such as upgrades, patches, configuration changes, etc. The work is performed in a multidisciplinary team environment using agile methodologies. The candidate we seek must be highly motivated and enthusiastic about implementing new technologies and learning about new data in a small team environment where deadlines are important.
Responsibilities:
Complete development efforts across data pipeline to store, manage, store, and provision to data consumers
Being an active and collaborating member of an Agile/Scrum team and following all Agile/Scrum best practices
Write code to ensure the performance and reliability of data extraction and processing
Support continuous process automation for data ingest
Achieve technical excellence by advocating for and adhering to lean-agile engineering principles and practices such as API-first design, simple design, continuous integration, version control, and automated testing
Work with program management and engineers to implement and document complex and evolving requirements
Help cultivate an environment that promotes customer service excellence, innovation, collaboration, and teamwork
Collaborate with others as part of a cross-functional team that includes user experience researchers and designers, product managers, engineers, and other functional specialists
Required Skills:
Must be a US Citizen
Must be able to obtain a Public Trust Clearance
7+ years of IT experience including experience in design, management, and solutioning of large, complex data sets and models.
Experience with developing data pipelines from many sources from structured and unstructured data sets in a variety of formats
Proficiency developing ETL processes, and performing test and validation steps
Proficiency to manipulate data (Python, R, SQL, SAS)
Strong knowledge of big data analysis and storage tools and technologies
Strong understanding of the agile principles and ability to apply them
Strong understanding of the CI/CD pipelines and ability to apply them
Experience with relational database, such as, PostgreSQL
Work comfortably in version control systems, such as, Git Repositories
Contact Government Services strives to simplify and enhance government bureaucracy through the optimization of human, technical, and financial resources. We combine cutting edge technology with world class personnel to deliver customized solutions that fit specific needs and solve the most challenging, dynamic problems. Our team emphasizes technical expertise and an empathy to public service to provide unique top tier support for our government engagements.
SOME BENEFITS OF THIS POSITION INCLUDE:
Health, Dental, & Vision insurance
Life Insurance
401k
Flexible Spending Account (Health, Dependent Care, and Commuter)
Paid Time Off & Observance of State/Federal Holidays
For more information about CGS please visit: https://www.cgsfederal.com
39U0b7kJu8","$103,213 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Redzara.us,#N/A,"Atlanta, GA",Azure Data Engineer,"Responsible for developing new and modernizing existing applications to Azure Cloud Platform.
The ideal candidate must demonstrate in-depth knowledge and understanding of Database concepts.
Requirements:
7+ years of experience in Data Warehousing and Business Intelligence
5+ Years in BI development leveraging the Microsoft SQL Server. Needs to be an expert in T-SQL
5+ Years in SSIS and use of Stored Procedures.
1-2 Years in Azure Data Factory, SQL VMs, build and maintain ETL
Ingest and aggregate data from both internal and external data sources to build our data lake
Strong understanding data modeling (i.e., conceptual, logical, and physical model design, experience with Operation Data Stores, Enterprise Data warehouses and Data Marts).
Job Type: Contract
Pay: $43.49 - $57.00 per hour
Compensation package:
1099 contract
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Atlanta, GA 30334: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure Data Factory: 3 years (Required)
Azure: 3 years (Required)
Data warehouse: 1 year (Preferred)
Work Location: Hybrid remote in Atlanta, GA 30334",$50.25 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"LatentView Analytics
4.0",4.0,"Dallas, TX",Data Engineer,"Role : Data Engineer
Experience : 6 - 8+ Years
Location : Dallas,Tx (Onsite)
Position: FullTime Only
Skills :Python,SQL Server ,Scala, Hadoop, HPCC, Storm, Cloudera, Cassandra,Excel, R,Docker,Kubernetes,Snowflake,Azure,Kafka,Redshift,Hadoop,AWS.
Job Type: Full-time
Pay: $80,000.00 - $1,100,000.00 per year
Compensation package:
Yearly pay
Experience level:
6 years
Schedule:
8 hour shift
Experience:
data engineer: 6 years (Preferred)
Work Location: In person
Speak with the employer
+91 9876543210","$109,012 /yr (est.)",1001 to 5000 Employees,Company - Public,Management & Consulting,Business Consulting,2006,$25 to $100 million (USD)
"Loyal
3.9",3.9,"Atlanta, GA",Senior Cloud Data Operations Engineer,"Loyal is an organization centered on experience and building a platform that allows consumers to make meaningful decisions when it comes to healthcare. We deeply understand providers, locations, services, appointments, business rules, and moreover, we understand patients - who they are, the preferred method of communication, upcoming appointments, lapsed appointments, outstanding bills, health risks, and more. With this intelligence, our platform fuels highly relevant and personalized experiences across all mediums (website, email, voice…) allowing patients to get healthy, stay healthy, and have a better relationship with the health care provider.
**This is a remote role**
Summary
The Senior Cloud Data Operations Engineer is responsible for delivering Loyal's cloud-based data warehouse. They interface with groups across the organization to ensure we are collecting and storing data in a way that makes it easy to create and consume. They identify best practices for big data management, select the right tooling, create a workable data architecture, and drive an implementation roadmap others can depend on. The Senior Cloud Data Operations Engineer also works closely with our security group to ensure data security and privacy. They also mentor others on the team, providing technical leadership to those they work with.
You will
Create data streams and consolidate data sources into a single cloud-based data warehouse
Architect and catalog data warehouse storage schema
Work with product teams to enable easy data aggregation
Work with data consumers (analysts, data scientists, and AI/ML engineers) to ensure data is easily accessible and consumable
Create and execute a roadmap that meets data storage and availability goals
Ensure data is secure through data analysis, access control, and encryption
You have
Bachelor's degree in computer science, information systems, applied math or related field, or equivalent work experience
Minimum 5 years of experience working with cloud data processing services (AWS, Azure, or GCP).
Minimum 3 year experience with Big Data technologies, such as Databricks, Snowflake, Spark, Hadoop, etc.
Strong experience in creating data architecture and storage schemas required
Significant working knowledge of SQL and other query languages and tools
Significant experience with building relational database systems strongly preferred
Experience with ETL tools a plus
Knowledge of Python preferred
Software development, data analytics, or data science experience is a plus.
Experience working within a software as a service (SaaS) company preferred.
Experience working within a high growth and/or ambiguous environment, with proven experience to be dynamic preferred.
#LI-REMOTE
We know that potential candidates are often less likely to apply to a position if they don't match 100% of the job qualifications. Don't let that be why you miss out on this opportunity! We encourage you to apply if you can demonstrate many of these skills and competencies.
Loyal to our employees
We are a remote-friendly company! We encourage you to apply from anywhere in the United States. We also believe in a work/life balance that fulfills you while you're here and supports you when you're not. We built our benefits package to prove that we're committed to you having everything you need (including a little fun). Here is what we offer full-time employees:
Flexible paid time off, sick and personal days
At least one holiday per month (sometimes, more!)
Full health, dental, and vision insurance - Loyal pays the premium for all employees!
One Time Home Office Setup Stipend For Remote & Hybrid Roles
Monthly Internet Stipend for Remote & Hybrid Roles
Long term & short term disability
401[k] plan
16 Weeks Paid Parental Leave
2 Volunteer days per year
Matching Gift Program
Participation Grant Program
Annual Travel/Team Events up to twice per year (post-COVID)
Our Commitment
We believe that the key to Loyal's success is you. Your unique background, life experience, knowledge, self-expression, and talent make you uniquely you. Who you are, what you have experienced, and how you think inspires us to be innovative and bold.
Loyal is an equal opportunity employer. We hire great people from a wide variety of backgrounds, not just because it's the right thing to do, but because it makes our company stronger. We welcome the unique contributions that you can bring in terms of your education, opinions, culture, ethnicity, race, ancestry, sex, gender identity and expression, national origin, citizenship, marital status, age, languages spoken, veteran status, color, religion, disability, sexual orientation, and beliefs.
We consider qualified applicants regardless of criminal histories, consistent with legal requirements.
Further, consistent with applicable federal and state law, Loyal provides reasonable accommodations when requested by qualified applicants or employees with disabilities, unless doing so would cause an undue hardship. Loyal's policy regarding requests for reasonable accommodation applies to all aspects of employment, including the application process. If you require a reasonable accommodation in connection with the application process, please contact the Talent Acquisition Department at talentacquisition@loyalhealth.com.
E-Verify
This employer participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S. If E-Verify cannot confirm that you are authorized to work, this employer is required to give you written instructions and an opportunity to contact Department of Homeland Security (DHS) or Social Security Administration (SSA) so you can begin to resolve the issue before the employer can take any action against you, including terminating your employment. Employers can only use E-Verify once you have accepted a job offer and completed the I-9 Form.
COVID-19 Vaccinations
Consistent with Loyal policy, candidates performing in-person work will be required to show proof of being fully vaccinated against COVID-19 upon commencing employment. Reasonable accommodations will be considered on a case-by-case basis for exemptions to this requirement in accordance with applicable law. If you require a reasonable accommodation to Loyal's vaccination requirement, please contact the People Department at people@loyalhealth.com.",#N/A,51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2015,$5 to $25 million (USD)
"Tech Mahindra / Microsoft
3.2",3.2,Remote,Data Engineer - Cosmos,"Hi,
One of my direct client is looking for Data Engineer in Redmond, WA. If you are interested, please share me your updated resume.
Title: Data Engineer
Location: Redmond, WA
Direct Client: Microsoft
Job Description:
Primary Requirement:
1. Cosmos Scope Scripting
2. Azure Data Lake, Pipelines
3. ADLS, ADLA
4. SQL querying experience
5. Microsoft projects experience
Optional Requirement:
1. ADO
2. PowerShell or Python Scripting language
3. Project manager
Thanks & Regards
K. ManiMegalai
Phone: (206) 337-5702 Ext 241
Zen3 is now a Tech Mahindra Company.
Tech Mahindra is a strategic Microsoft Partner and is a proud partner in implementing the World’s largest Azure Migration program.
Job Type: Full-time
Schedule:
8 hour shift
Monday to Friday
Work Location: Remote",#N/A,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Iron Bow Technologies
3.5",3.5,"Washington, DC",Sr. Data Center Engineer,"Iron Bow Technologies is seeking a Sr. Data Center Engineer to support our Federal customer, The Department of Justice (DOJ), Criminal Division (CRM), in Washington, DC. The selected candidate will support our flexible and scalable solution with the DOJ CRM to extend cloud-native features to existing legacy and next-generation applications while also future proofing the infrastructure to easily accommodate future requirements, such as transitioning to a public cloud.
This position is hybrid and requires 2 days a week onsite at the customer location in NW Washington, DC and support at the customer data center as needed in Sterling, VA.
Essential Duties and Responsibilities:
Assess and provide VMware solutions to include vCenter, vSphere, SRM, ESXi
Primary storage solutions and data file storage management, NAS file storage solutions
Data Center blade servers and rack servers
Server virtualization
Experience:
10 years of related experience providing system administration support in a virtualized environment, including at least 6 years of VMWare solutioning, including vCenter, vSphere, SRM, and ESXi
Data Center operations experience, with SAN and NAS
Experience supporting primary storage solutions and data file storage management, NAS file storage solutions
Hands on experience with Data Center blade and rack servers
Ability to work staggered shift hours when needed to support critical coverage and/or time-sensitive needs
Be a member of our Team providing coordinated 24x7 IaaS on-call support
U.S. Citizen
Currently hold or obtain/maintain a Public Trust High clearance
Education:
BS in Computer Science, Information Science, or other relevant discipline is preferred – additional years of experience may be considered in lieu of the degree
Preferred Certifications:
VMware Certified Professional
Primary storage certifications such as HPE, Dell-EMC, NetApp, or Pure Storage
VCP-DCV
Travel Requirements:
No travel
PRINCIPALS ONLY. NO AGENCIES PLEASE. MUST BE A US CITIZEN ABLE TO OBTAIN A PUBLIC TRUST SECURITY CLEARANCE.
Iron Bow Technologies is a leading IT solution provider dedicated to successfully transforming technology investments into business capabilities for government, commercial and healthcare clients. Our depth of technical expertise, global reach and strategic partnerships with industry leaders as well as disruptive technology partners uniquely positions Iron Bow to target the right solution to ensure your successful business outcome. We partner with our clients from planning and implementation through ongoing maintenance and management to deliver solutions that are strong, flexible and on target .
Iron Bow Technologies is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.
#LI-TM1",#N/A,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,1983,$500 million to $1 billion (USD)
United Digestive,#N/A,"Atlanta, GA",Data Engineer,"GENERAL SUMMARY OF DUTIES: United Digestive’s Data Engineer will be responsible for developing and maintaining data pipelines, data warehouses, and data models to support the company's Business Intelligence reporting initiatives.
REPORTS TO: Director of Business Intelligence & Analytics
RESPONSIBILITIES:
Duties may include but are not limited to:
Implement, maintain, and improve data integration processes to collect, process, and manage large datasets
Collaborate with stakeholders to understand business requirements and design data solutions that meet their needs",#N/A,Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Vibrant Planet,#N/A,California,Data Engineer,"About Vibrant Planet
We are a team of leaders in science, forestry, policy, and tech, building a cloud-based, data-driven platform to increase the pace and scale of forest restoration and reduce catastrophic wildfire, tree mortality, forest degradation, and deforestation. Our current software modernizes land management planning and monitoring with AI-driven data development, user friendly scenario building and decision support, and forest resilience trends and treatment outcome detection. The system quantifies potential and actual treatment benefits, helping to grow markets for ecosystem services (carbon, water, biodiversity, sustainable forest-derived products).
Our initial platform (launched in September 2021) is focused on temperate, “fire adapted” landscapes in California and the Western US. Our mission is global; our roadmap expands into other geographies and forest types accordingly.
Driven by our sense of urgency to protect vital forests and the services they provide, we use our expertise to build sophisticated, AI/ML-driven, user friendly products to democratize the use of data and accelerate the process of stabilizing and restoring our forests.
Vibrant Planet is backed by climate and ecosystem resilience solutions leaders, including Grantham Foundation, Earthshot, Elemental Excelerator, Ecosystem Integrity Fund, Chris Cox (CPO at FB), Neil Hunt (ex CPO of Netflix), Cisco, Valia Ventures, and Halogen Ventures.
For further information please visit: VibrantPlanet.net
Equal Opportunity Employer: Vibrant Planet is committed to diversity. We encourage applicants from all cultures, races, colors, religions, sexes, national or regional origins, ages, disability status, sexual orientation, gender identity, military, or other status protected by law to apply.
We are most interested in finding the best candidate for the job, and that candidate may come from a less traditional background, but have capacity to grow into and thrive in the position after some mentoring. We do not require that you have experience with every job description task. We will consider any equivalent combination of knowledge, skills, education, and experience to meet minimum qualifications. We encourage each candidate to think broadly about their unique background and skill set and how it may relate to the role. This is important to us. We aren’t just saying this, we mean it.
For further information please visit: https://vibrantplanet.net
About the Role
We are looking for a versatile, hands-on data focused engineer to help us scale and build our geospatial pipelines and tooling. You will be part of a small team of engineers and scientists, tackling some of the most important climate change related issues facing the world today. This is a remote role (and company) so being comfortable and effective working in a distributed team is crucial.
Key Responsibilities:
Design, develop, scale, and maintain data pipelines that ingest, transform, and store large volumes of geospatial data from multiple sources.
Optimize data processing and storage performance and cost efficiency by leveraging cloud-based technologies and services.
Collaborate with engineers, scientists and other stakeholders to share knowledge and build expertise.
Lead and participate in development life cycle activities like design, coding, testing, and production release.
Contribute to our evolving engineering culture, standards, tooling, and processes.
Mentor and support other engineers and deeply review code.
Technical Qualifications
Strong software engineering skill set.
2+ years experience in data engineering or a related field.
Experience with distributed data processing frameworks and tools (e.g., Airflow, Hadoop, Spark).
Proficiency with Python or an equivalent language. Ability to write clean, high-quality code and tests to keep our system fast, reliable, and monitorable.
Bonus Qualifications - Absolutely not required, but nice to have
Airflow
Pandas / Geopandas
Jupyter notebooks
Geospatial processing
Lidar data
Satellite imagery
Experience with ArcGIS/QGIS
Other Expectations:
Strong communication skills; discussing complex technical concepts to engineers and non-engineers is no problem to you.
Collaborative and supportive team player, with a desire to enrich our engineering culture.
Eagerness to learn, think creatively, and share knowledge with others.
Ability to write understandable, testable code with an eye towards maintainability.
Proactive and empathetic mindset - you love to roll up your sleeves to fix problems
Location - We are a remote first company with the majority of the team in the US west coast time zone. We also have a small and growing presence in New Zealand. Location can be flexible, but we are primarily targeting those two time zones and anything in between.",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Hiscox
4.1",4.1,"Atlanta, GA",Senior Data Engineer,"job description:
You will be a senior member of our US IT Team data engineering practice. You will be an early member of this team and will work closely with the Director of Data Engineering to help establish it. We will have our work cut out for us delivering planned work, building the practice and evolving our technical landscape. This is an exciting opportunity for the right individual to join at the very beginning and grow with the new team.
As a senior you will be expected to lead, mentor others and deputise for the director of data engineering when required. Yoi will also continue to contribute as a hands-on engineer. You will have the knowledge, best earned through 5+ years of experience, to truly inhabit this senior role in an ambitious team.
What you’ll be doing:
Working with business stakeholders and our delivery team to understand high value business problems that can be solved through the application of data processing and analytical systems.
Developing, expanding and evolving our existing databases and ETL pipelines.
Working with architects to design, build and support a transformational cloud-based data platform for the business.
Being a core and professional member of the new data engineering practice
Understand business requirements and help refine into development tasks and estimate their complexity.
Research, evaluate and adopt new technologies with a right tool for the job mentality.
Focus on both speed of delivery and quality, with suitable pragmatism – ensuring your solutions are always appropriate and not too complex or over-engineered.
Quick progression of projects from proof-of-concept to post-production stage.
Communication and presentation of ideas to colleagues in all parts of the wider tech team.
Participating in code reviews for the data engineering practice.
Act as a servant leader in our practice – ensuring team members have
person specification:
Knowledge of, or ability to rapidly adopt our core languages for data engineering – SQL and/or Python.
Experience working using a modern DevOps approach. You should not just have written data code but also the CI/CD pipelines necessary to test and deploy that code in a professional environment.
A robust understanding of core data engineering topics – ETL vs ELT, structured and unstructured data, data quality and data governance.
Ability to contribute to all aspects of a solution – design, infrastructure, development, testing and maintenance.
The ability to design and advocate for technical solutions to business problems.
Effective collaboration with technical and non-technical team members through agile ceremonies – roadmap planning, feature workshops, backlog elaboration, code review.
Understanding of cloud technology and significant hands-on experience working with it. You must understand how cloud-native solutions must be built differently from traditional ones.
Track record of taking initiative and delivering projects end-to-end; clear evidence of being self-driven and motivated
Immense curiosity, high energy and desire to go the extra mile to make a difference.

Desirable Skills
Beyond the required skills we are open to individuals of diverse talents. Experience with additional technologies, data science knowledge or a business background are all valued. We want to know how your unique abilities can contribute to our team.
Our Technology
We operate in a diverse technical landscape and are looking for flexible engineers who can adapt to and use many different tools. We would not expect any engineer to be familiar with the entire tech stack – no engineer can. Instead, we seek people with a good understanding of data structures and algorithms and the ability to apply this knowledge in learning new tools.
We have an existing data landscape involving both MS SQL Server and Oracle databases. We use SSIS for ETL. We will also need to source data from our integration layer – webMethods.
If some of that sounds like it needs bringing up to date, we know! We hope to start building a modern cloud data platform to realise the full value of our data. The technologies we use for that platform are still to be decided. Likely it will be based on Snowflake with Azure Data Factory for ELT. Other parts of our business have had success using Prefect for orchestration and dbt for transformation and we will reuse where it is sensible.
We prepare data for use by analysts working with a variety of tools – Tableau, PowerBI, and even Excel.
We take a DevOps approach and strive for continuous integration / continuous deployment. We use Azure Pipelines to deploy our code. We deploy infrastructure the same way using Terraform and Docker.
Our technology landscape is not fixed. Our engineering and architecture teams drive the technology we adopt.


Salary range $140,000 - $170,000","$155,000 /yr (est.)",1001 to 5000 Employees,Company - Public,Insurance,Insurance Carriers,1901,$1 to $5 billion (USD)
"Dobbs Defense Solutions, LLC",#N/A,"Fort Meade, MD",Data/NiFi Engineer,"Our Mission
At Dobbs Defense, we deliver mission-centric IT, Cyber, and data analytics solutions for our government and commercial clients through the convergence of automation, innovation, training, and education. Delivering high-quality IT, cybersecurity, and data analytics solutions through proven and innovative methods is our vision.

Job Description
Dobbs Defense Solutions is seeking a Senior Data/NiFi Engineer to design, develop, implement, test, secure, and support data transport and brokering for multiple classified networks and between networks.

Duties
Design, develop, implement, test, secure, and support data transport and brokering.
Data flow engineering for one of DISA’s premier boundary defense systems.
Perform system administration of Niagara Files (NiFi) on Linux VMs.
Support backup/failover/load balancing of data brokering to improve availability.
Monitor availability & amp, usage.
Respond to outages, take corrective action, and issue outage reports.
Familiarity with DISA STIGs and SCAP.
Familiarity with Java programing.

Qualifications
Required education and experience:
5+ years of Data Engineering experience.
Significant experience with Niagara Files (NiFi) and MiNiFi or similar data engineering tools.
Excellent written and verbal communication skills.
IAM or IAT Level II Certification (e.g., GSEC, SEC+, SCNP, SSCP, etc.).
Active Secret clearance.

Required Clearance
Secret

Working Environment
Onsite

Our Equal Employment Opportunity Policy
Dobbs Defense Solutions is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, creed, sex (including pregnancy, childbirth, breastfeeding, or medical conditions related to pregnancy, childbirth or breastfeeding), sexual orientation, gender or gender identity (except where gender is a bona fide occupational qualification), gender expression and transgender status, national origin, ancestry, age, disability, military or veteran status, marital or domestic partner status, genetic information, citizenship, low-income status or any other status or characteristic protected by applicable law. We are committed to equal employment opportunity in all decisions related to employment, promotion, wages, benefits, and all other privileges, terms, and conditions of employment.
4Cmw9yg2oT","$83,799 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Trellis
4.4",4.4,Remote,Data Engineer,"Overview
As a Data Engineer for Trellis, you will be responsible for designing, building, and maintaining scalable data pipelines and infrastructure to support the deployment of machine learning models for educational applications. You will be working with large volumes of text data, and will be responsible for ensuring data quality, performing data cleaning, and implementing data transformation processes.
Responsibilities
Build and maintain data pipelines for processing large volumes of text data
Perform data cleaning and preprocessing to ensure data quality and consistency
Implement data transformation processes to convert raw data into formats suitable for machine learning models
Collaborate with data scientists and other stakeholders to understand project requirements and provide data engineering support as needed
Develop and maintain data infrastructure to support machine learning workflows
Monitor and troubleshoot data pipelines to ensure high availability and reliability
Qualifications
Bachelor's or Master's degree in Computer Science, Data Science, or a related field
Strong programming skills in Python and experience with data processing frameworks such as Spark and Hadoop
Experience with text data processing and natural language processing (NLP) techniques
Familiarity with machine learning workflows and frameworks such as TensorFlow and PyTorch
Experience with cloud-based data storage and processing technologies such as AWS, Google Cloud, or Azure
Strong problem-solving skills and attention to detail
Excellent communication and collaboration skills
Benefits
SF office, but remote-friendly. Come into the office 60% of the time — you’ll want to! It’ll be built as a library in a way that’s anti-fatigue. We will also have offices in NYC and Montreal.
Health insurance with 100% premium covered
Generous PTO / sick leave
401(k) plan with employer match
Free lunch and snacks
Annual company retreat in Montreal
Bring your dog to work",#N/A,51 to 200 Employees,Company - Private,Media & Communication,Advertising & Public Relations,2012,$5 to $25 million (USD)
"NexTek, LLC",#N/A,"Shoreview, MN",Sr. Data Engineer,"Nextek, LLC is a software development company in the financial services industry that creates web applications and solutions for financial services professionals. We are seeking a motivated Senior Data Engineer to join the team.
What we provide:
A laptop and other equipment/software necessary for work
Health Insurance, Dental Insurance, Vision Insurance
Paid time off (PTO), Volunteer time off (VTO)
Hybrid remote work options
401K
Life Insurance
Flexible schedules
Monthly Social Activities
Casual dress code
Professional development
Here is what we are looking for:
Experience in database administration, security, backup, and recovery
Preferred Knowledge:4+ years of experience in data/software engineering
Experience in the financial services industry
Experience with a variety of relational databases (particularly MySQL & MSSQL)
Experience with collecting and presenting analytics
Here is what you will be doing:
Display proficiency in relevant technical skills.
Maintain a deep understanding of the data structure.
Provide evaluations and recommendations for our data environment based on potential issues, tradeoffs, and risks.
Scope and implement solutions.
Make good estimates and set realistic expectations for delivery.
Ask for guidance when necessary.
Maintain and monitor the health of databases and associated solutions.
Make use of the same environments setup by the Software Engineering team
Thoroughly test code for bugs and security issues before deploying to production environments
Maintain and monitor actively deployed solutions
Follow and help define best practices
Peer reviews
Maintain awareness of industry trends
Share experience and insights to help your team grow
Increase understanding of impact on other teams
Increase understanding of the customer
All other duties as assigned
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Shoreview, MN: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Staffbee Solutions Pvt Ltd
5.0",5.0,"Dallas, TX",Data Engineer,"Dear OPT/CPT candidates,
We currently hiring OPT/CPT Individuals , we also specialized in training ( data engineering course , How to handle the interviews etc...) and placement for Data Engineering positions, With many benefits like
Free accomodation untill you get free accomodation
100% placement
Modification of resumes according to the market standards
Best Package
H1b sponsership
Travel allowances
+1469 902 8976 (wapp/call)
Or share resume at vijay.komma@staffbees.com
Job Type: Full-time
Salary: $60,000.00 - $70,000.00 per year
Application Question(s):
This requirment is for only OPT/CPT visa status candidates , Do you aware?
Are you sure you have read the JD?
Work Location: One location","$65,000 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2018,Unknown / Non-Applicable
"Swish Analytics
5.0",5.0,United States,Data Engineer,"Company Overview
Swish Analytics is a sports analytics, betting and fantasy startup building the next generation of predictive sports analytics data products. We believe that oddsmaking is a challenge rooted in engineering, mathematics, and sports betting expertise; not intuition. We're looking for team-oriented individuals with an authentic passion for accurate and predictive real-time data who can execute in a fast-paced, creative, and continually-evolving environment without sacrificing technical excellence. Our challenges are unique, so we hope you are comfortable in uncharted territory and passionate about building systems to support products across a variety of industries and consumer/enterprise clients.
Job Description
The Swish Analytics team is seeking Data Engineers to have direct impact on the infrastructure and delivery of our core consumer and enterprise data offerings. We're a team passionate about accurate predictions and real-time data, and hope you find satisfaction in building new products with the latest and greatest technologies. This is a remote position.
Duties
Architect low-latency, real-time analytics systems including raw data collection, feature development and endpoint production
Build new sports betting data products and predictions offerings Integrate large and complex real-time datasets into new consumer and enterprise products
Develop production-level predictive analytics into enterprise-grade APIs
Contribute to the design and implementation of new, fully-automated sports data delivery frameworks
Requirements
BS/BA degree in Mathematics, Computer Science, or related STEM field
Experience writing production level code
Proficiency in Python
Proficiency in SQL (preferably MySQL)
Experience building end-to-end ETL pipelines
Experience utilizing REST APIs
Experience in SQL database management, shema design, index structuring
Experience with version control (git), continuous integration and deployment, shell scripting, and cloud-computing infrastructures (AWS)
Experience with web scraping and cleaning unstructured data
Knowledge of data science and machine learning concepts
A strong interest for US sports and sports betting. You love sports, particularly the NFL, NBA, MLB, NHL, College Football, College Basketball, and Tennis, and can use your knowledge of the sport to inform your work with complex datasets
Base salary:$90-145,000
Swish Analytics is an Equal Opportunity Employer. All candidates who meet the qualifications will be considered without regard to race, color, religion, sex, national origin, age, disability, sexual orientation, pregnancy status, genetic, military, veteran status, marital status, or any other characteristic protected by law. The position responsibilities are not limited to the responsibilities outlined above and are subject to change. At the employer's discretion, this position may require successful completion of background and reference checks.","$117,500 /yr (est.)",1 to 50 Employees,Company - Private,"Arts, Entertainment & Recreation",Gambling,2014,Unknown / Non-Applicable
"Intertech, Inc
4.4",4.4,Minnesota,Sr. Data Engineer,"Sr. Data Engineer
US Citizenship Required
Contract to Hire Opportunity
Fully Remote

The Senior Data Engineer will oversee the department's data integration work, including developing data models, maintaining a data warehouse and analytics environment, and writing scripts for data integration and analysis. This role will work closely and collaboratively with members of other areas to define requirements, mine and analyze data, integrate data from a variety of sources, and deploy high quality data pipelines in support of the analytics needs.

Responsibilities
Maintain and build our data warehouse and analytics environment
Design, implement, test, deploy, and maintain stable, secure, and scalable data engineering solutions and pipelines in support of data and analytics projects, including integrating new sources of data into our central data warehouse, and moving data out to applications and affiliates as needed
Make data available for the reporting and analytics teams
Produce scalable, replicable code and engineering solutions that help automate repetitive data management tasks
Implement and monitor best in class security measures in our data warehouse and analytics environment, with an eye towards the evolving threat landscape
Maintain high quality documentation of data definitions, transformations, and processes to ensure data governance and security
Other duties as assigned
Provide technical assistance in an on-call rotation



Job Qualifications

Required:
Bachelor’s Degree in Computer Science or Management Information Systems (MIS) or Business, Finance or Accounting with an emphasis in MIS
Minimum 5 years experience of developing and supporting enterprise level data warehouse systems
Strong knowledge of relational databases and SQL. Extract, Transform, and Load (ETL) data into a relational database
General data manipulation skills: read in data, process and clean it, transform and recode it, merge different data sets together, reformat data between wide and long, etc.
Demonstrated ability to learn new techniques and troubleshoot code without support, ex. find answers to common programming challenges
Strong knowledge of T-SQL language as evidenced by ability to write complex SQL queries, Microsoft SQL Management Studio, SQL Analysis Services and SQL Server Integration Services
Demonstrated ability to work independently and be a self-starter
Demonstrated ability to work effectively in teams, in both a lead and support role
Strong analytical and problem-solving skills with the ability to work independently with minimal supervision

Preferred:
Experience working with Data Vault 2.0
Experience working with cloud infrastructure services like Amazon Web Services and Google Cloud
Experience with advanced data visualization and mapping",#N/A,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,1991,$5 to $25 million (USD)
"Leadstack Inc
4.3",4.3,Remote,Lead Data Engineer with Sql/Python,"LeadStack Inc. is an award winning, one of the nation's fastest growing, certified minority owned (MBE) staffing services provider of contingent workforce. As a recognized industry leader in contingent workforce solutions and Certified as a Great Place to Work, we're proud to partner with some of the most admired Fortune 500 brands in the world.
Title: Lead Data Engineer with Sql/Python
Location: Remote
Duration: 6+months
Direct Client
Immediate interview
Top Skills: Azure, SQL/Python Developer.
Job Description:
Design and develop data system integrations and data models for reporting and alerting.
Essential Job Functions:
Drive digital innovation by leveraging innovative new technologies and approaches to renovate, extend, and transform the existing core data assets, including SQL-based, NoSQL-based, and Cloud-based data platforms
Define high-level migration plans to address the gaps between the current and future state
Present opportunities with cost/benefit analysis to leadership to shape sound architectural decisions
Lead the analysis of the technology environment to detect critical deficiencies and recommend solutions for improvement
Interpreting data, analyzing results using statistical techniques
Developing and implementing data analyses, data collection systems and other strategies that optimize statistical efficiency and quality
Acquiring data from primary or secondary data sources and maintaining databases
Promote the reuse of data assets, including the management of the data catalog for reference
Key Responsibilities:
Innovate, develop, and drive the development and communication of data strategy and roadmaps across the technology organization to support project portfolio and business strategy
Drive the development and communication of enterprise standards for data domains and data solutions, focusing on simplified integration and streamlined operational and analytical uses
Drive digital innovation by leveraging innovative new technologies and approaches to renovate, extend, and transform the existing core data assets, including SQL-based, NoSQL-based, and Cloud-based data platforms
Define high-level migration plans to address the gaps between the current and future state, typically in sync with the budgeting or other capital planning processes Lead the analysis of the technology environment to detect critical deficiencies and recommend solutions for improvement
Mentor team members in data principles, patterns, processes and practices
Promote the reuse of data assets, including the management of the data catalog for referenceDraft and review architectural diagrams, interface specifications and other design documents
To know more about current opportunities at LeadStack , please visit us on https://leadstackinc.com/careers/
Job Type: Contract
Salary: $70.00 - $80.00 per hour
Experience level:
11+ years
Schedule:
8 hour shift
Experience:
Azure: 5 years (Preferred)
Data warehouse: 1 year (Preferred)
SQL developer: 6 years (Preferred)
Work Location: Remote",$75.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2016,Unknown / Non-Applicable
"Seamless.AI
3.4",3.4,"Columbus, OH",Senior Data Engineer - Remote US,"Role: Senior Data Engineer
The Opportunity
We are seeking a Senior Data Engineer with a strong background in data ingestion and processing for large data sets. The ideal candidate is maniacal about data accuracy and has a proven track record of building robust and scalable data pipelines. This role requires an experienced professional who can work collaboratively with cross-functional teams to design and implement solutions that meet business requirements and adhere to data security and privacy policies.
If you are a senior data engineer with a passion for data accuracy and a proven track record of building robust and scalable data pipelines, we encourage you to apply for this role. We offer competitive compensation, a flexible work environment, and opportunities for growth and advancement within the organization.
About Seamless
Seamless delivers the world's best sales leads. Through our product, we help sales teams maximize revenue, increase sales, and easily acquire their total addressable market using artificial intelligence; by development of a robust real-time contact and company search engine as well as a suite of technically-advanced tools to support sales and lead generation. We have been recognized as one of Ohio's fastest growing companies and have been recently ranked No. 7 in LinkedIn's Top 50 Startups of 2022, featured in Forbes as #1 Software company in Ohio in 2022, and on G2's ""Top 100 Highest Satisfaction Products for 2022"" list!
The Seamless Team
We have an amazing culture and work environment that anyone would want to be a part of. We encourage a culture of positivity. We thrive off of continuous feedback and do whatever it takes to help our team and customers be successful. You will grow as an individual, professionally, and be able to see and feel the impact you are making to the growth of Seamless every day.
Role Responsibilities
Design and develop data ingestion and processing pipelines that ensure data accuracy and completeness.
Work closely with cross-functional teams to understand business requirements and design solutions that meet those requirements.
Lead efforts to identify and implement data quality control measures to ensure consistency and reliability of data.
Monitor data pipelines to identify issues and proactively resolve them.
Evaluate and recommend new technologies and techniques to improve data pipeline performance and scalability.
Ensure compliance with data security and privacy policies and procedures.
Mentor junior engineers and provide technical leadership in data engineering best practices.

Candidate Requirements
Bachelor's or Master's degree in Computer Science, Software Engineering, or a related field.
3+ years of experience in data engineering, with a focus on data ingestion and accuracy.
Strong experience with large-scale data processing frameworks such as Hadoop, Spark, or Flink.
Proficiency in programming languages such as Python, Java, or Scala.
Experience with SQL and NoSQL databases.
Familiarity with cloud-based data processing and storage platforms such as AWS, Google Cloud, or Azure.
Strong experience in designing and implementing data quality control measures.
Excellent problem-solving and analytical skills.
Strong communication and collaboration skills.
Experience mentoring and leading teams of data engineers.
Seamless.AI has been delivering the world's best sales leads since 2015. Our product is the first real time, B2B search engine helping sales teams maximize revenue, increase sales, and easily acquire their total addressable market using artificial intelligence. We have been recognized as one of Ohio's fastest growing companies and won 2020 Best Places to Work and LinkedIn's Top 50 Tech Startups in 2020 and 2022. We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. Visa Sponsorship is not included in our hiring package. Applicants will need to be authorized to work in the U.S.","$93,575 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable
"Purpose Financial
4.2",4.2,Remote,Data Engineer,"Address : 322 Rhett St, Greenville, South Carolina, United States - 29601
Purpose Financial, Inc. is an innovative consumer financial services company that offers a diverse suite of credit products, promoting financial inclusion and meeting consumers wherever they are. Through its brands, the company is committed to helping customers achieve their version of financial stability in the moment and in the future. Since 1997, Purpose Financial has been a pioneer in the consumer credit and financial services market offering money solutions in over 1,000 storefronts locations and online lending. Providing services in over 20 states, Purpose Financial employs over 2,700 team members.
At Purpose Financial, our people are our greatest asset. We are inclusive and encourage different points of view. We all belong here. We respect our colleagues. We value our customers, and we support our communities.
What do we do? Our sole purpose is to help each customer achieve their version of financial stability in the moment and in the future.
We have a history! Consumers know that when they choose Purpose, they’re backed by a trusted, reliable financial services company with 25 years in the business, 2,700+ employees, 1,000+ store locations, and operating in 24 states.
Benefits and Perks. We believe our employee benefits should make life better. Purpose Financial offers a comprehensive benefits package to everyone on our team.
Health & Welfare Benefits. When you take care of yourself, you’re a better you – which is better for everyone!
Health Savings & Flexible Spending Accounts. Pre-tax money for your family’s out-of-pocket expenses.
401k with Matching Contributions. Think bigger! You can build a better future.
Wellness & Employee Assistance Programs. Take care of yourself in every way that matters.
Generous Vacation Policy & Paid Time Off. Time off to take care of yourself, your family – and your bucket list!
Volunteer Days. We want you to care. Always. Take 2 days off each year to volunteer and make a difference in your community.
Tuition Reimbursement. Pursue your goals without fretting over all the costs.
Employee Discounts. Save on home and auto insurance, cell phone service, entertainment, shopping, pet insurance and even travel.
Performance Incentives & Awards. We want you to feel valued for all your contributions.
Hybrid Workforce Support. We believe in getting it done – in whatever way works best for you and your team.
Want to meet our executive team? Visit our home page at www.havepurpose.com.
Position Summary
The job function is responsible for building a scalable data infrastructure. The role will be part of the effort to enable Purpose Financial to democratize data by developing self-serving and curated data models for our single source of truth. The job function includes understanding of business requirements, defining data requirements, identifying enterprise data sources, planning, designing, and building data acquisition from sources, defining data model, designing data ingestion pipelines, data normalization, data transformation, and consumption using platforms such as Snowflake and AWS.
Job Responsibility
Take ownership of building/optimizing data pipelines to increase efficiency and performance
Develop and maintain data ingestion and integration processes that focus on data quality and monitoring
Use AWS and Snowflake tools such as Glue, Lambda, Snowpark, and Snowpipe to build data ingestion pipelines from various sources
Incorporate standards and best practices into data pipeline solutions
Participate in troubleshooting and problem-solving design discussions
Provide logical and physical data design, and database modeling
Solve complex data issues around data integration, unusable data elements, unstructured data sets, and other data processing incidents
Continuously design the future state of our data & reporting process by collaborating with IT and business partners.
Partner with enterprise teams, data scientist, architects to define development standards and solutions.
Manage cloud infrastructure as code (IaC), using Terraform.
Work closely with other members of the data team to better design and build metrics that enhance our analytical capabilities
Manage workflows and data build processes that help the BI team build key business metrics and design dashboard visualizations
Assess the effectiveness and accuracy of new data sources, data points, and data gathering techniques
Troubleshoot problems with data (missing, corrupted, invalid) and build data recovery strategy
Apply best practices for AWS & Snowflake architecture, ELT and data models
Be a leader for our data journey, act as SME for data and database related topics, and assist with development of data lakes and warehouses
Education Required
Bachelor’s degree (or foreign equivalent degree) in any Computer, Engineering, or other technical field.
Experience Required
5+ years experience in data warehousing
3+ years of experience in data architecture and design, with a good understanding of various data systems and structures (i.e. SQL, NoSQL, Key-Value, Streams)
3+ years of experience with building data pipelines and using data orchestration tools and implementing end to end Monitoring & Alerting solutions
3+ years of experience working with AWS or public cloud provider.
Strong understanding of SQL, data structures, data integrity, and schema design best practices
Programming experience required, preferably Python.
A strong advocate of data governance and data quality
Experience with database internals, database design, SQL and database programming
Experience in financial services (Banking and consumer lending preferred)
Experience of authoring CI/CD pipelines, automation elements related to infrastructure composition, deployment orchestration, and monitoring.
Familiarity and experience with Agile/SCRUM based development methodology.
Knowledge Required
Excellent written and verbal communications skills as would be needed to communicate in person, by phone, and through email; adaptability and flexibility to changing environment; and comfortable working in a dynamic, high volume, fast-paced environment. Ability to read, write, evaluate, and apply information. Ability to interact professionally and exhibit appropriate social skills. Ability to understand and ensure compliance with policies, procedures, and laws governing our industry/business and products. Ability to develop and maintain business relationships.
Physical Requirements
Sitting for long periods of time; standing occasionally; walking; bending; squatting; kneeling; pushing/pulling; reaching; twisting; frequent lifting of less than 10 lbs., occasional lifting of up to 20 lbs.; driving and having access during the workday to an insured and reliable transportation; typing; data entry; grasping; transferring items between hands and/or to another person or receptacle; use of office equipment to include computers; ability to travel to, be physically present at, and complete the physical requirements of the position at any assigned location.
Competencies
Integrity
Customer Advocacy
Interpersonal Skills
Results/Accountability
Inclusiveness
Technical Proficiency/Leadership
Big Data Trends
Data Management
Adaptability
Travel - None
Attire - Business Casual
Other - Must be eligible to work in the USA and able to pass a background check.
Job Type: Full-time
Compensation package:
Yearly pay
Experience level:
5 years
Schedule:
Day shift
Work Location: Remote",#N/A,Unknown,Company - Private,Financial Services,Financial Transaction Processing,#N/A,Unknown / Non-Applicable
"Kroenke Sports Enterprises
3.4",3.4,"Denver, CO",Data Engineer,"Job Title: Data Engineer
Department: Hockey Operations
Business Unit: Colorado Avalanche
Location: Denver, CO or Remote
Reports To: Director of Analytics
Employment Type: Full Time – Salaried - Exempt
Supervisor Position: No
_____________________________________________________________________________________
Kroenke Sports & Entertainment (KSE) is an American Sports and Entertainment holding company based in Denver, Colorado. KSE is committed to providing world class sports and entertainment for both live and broadcast audiences. We are the employer of choice as the owner and operator of Ball Arena, DICK’S Sporting Goods Park, the Paramount Theatre, 1STBANK Center, Denver Nuggets (NBA), the Colorado Avalanche (NHL), Colorado Mammoth (NLL), Colorado Rapids (MLS), KIMN,KXKL, KKSE (FM/AM), Altitude Sports & Entertainment, Major League Fishing/Fishing League Worldwide (MLFLW), Winnercomm, Outdoor Sportsman Group and SkyCam.

Nature of Work:
The Colorado Avalanche are looking to hire a full-time Data Engineer to work within the team’s Hockey Operations Department. This person will be responsible for maintaining and expanding the Avalanche hockey operations database. They will be tasked with importing and integrating data from external providers and interacting with the rest of the hockey operations department to ensure optimal dissemination of information to the appropriate parties. This person will also have the opportunity to analyze data and share insights with members of the Analytics department as well as the broader Hockey Operations department if desired. They will report to the Director of Analytics.

Examples of work performed:
Manage and improve the organization’s data storage, structure, and ETL pipeline while optimizing query performance for large datasets
Design automated processes to oversee data integrity and query performance on a regular basis, as well as being available to spot and resolve data issues that may arise at any time
Ensure that our automated processes run on schedule without issue
Incorporate expansive new datasets from disparate sources into our structure in a seamless fashion

This description is a summary only and is describing the general level of work being performed, it is not intended to be all-inclusive. The duties of this position may change from time to time and/or based on business needs. We reserve the right to add or delete duties and responsibilities at the discretion of the supervisor and/or hiring authority.

Working Conditions & Physical Demands:
Typical Office Conditions
Travel may be required

Qualifications:
Required
Strong knowledge of ETL architecture and development in a cloud-based environment
Academic and/or industry experience in database architecture, back-end software design, and query optimization
Expertise in SQL and Spark
Excellent attention to detail, problem-solving abilities, and work ethic
Preferred
1-3 years of work experience in a database-related position
An advanced degree in software engineering, computer science, information technology, or a related field
Familiarity with R and Python, as well as R Studio / Posit
Experience with databricks and Linux operating system
Knowledge of NHL players and teams as well as a passion for hockey and hockey analytics
Some front-end development and/or data analysis experience is considered a plus

Competencies/Knowledge, Skills & Abilities:
Ability to maintain positive attitude and demonstrate professionalism
Ability to maintain a high level of confidentiality
Ability to complete work accurately and in a timely manner
Ability to work independently & in a group setting and demonstrate good judgment skills
Ability to communicate effectively orally and in writing
Possesses excellent interpersonal skills
Ability to multi-task, prioritize and adapt to changing environments

Compensation:
Salary Range $80,000 - $110,000 per annum

Benefits Include:
12 Paid Company Holidays
Health Insurance (Medical, Dental, Vision)
Paid Time Off (PTO)
Life Insurance
Short and Long-term Disability
Health Savings Account (HSA)
Flexible Spending plans (FSAs)
401K/Employer Match

Equal Employment Opportunity
Kroenke Sports & Entertainment (KSE) provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.","$95,000 /yr (est.)",1001 to 5000 Employees,Company - Private,"Arts, Entertainment & Recreation",Sports & Recreation,#N/A,Unknown / Non-Applicable
"CareFirst BlueCross BlueShield
3.5",3.5,"Washington, DC",Principal Data Engineer (Remote),"Resp & Qualifications
PURPOSE:
Applies data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources. Creates data collection frameworks for structured and unstructured data. Develops and maintains infrastructure systems (e.g., data warehouses, data lakes) including data access APIs. Prepares and manipulates data using Hadoop or equivalent. Employee is recognized as an expert within the organization and has in-depth and/or breadth of expertise in own discipline and broad knowledge of other disciplines within the function. Anticipates internal and/or external business challenges and/or regulatory issues; recommends process, product or service improvements. Solves unique and complex problems that have a broad impact on the business. Contributes to the development of functional strategy. Leads project teams to achieve milestones and objectives.

ESSENTIAL FUNCTIONS:
Leads project teams to achieve milestones and objectives. Creates data collection frameworks for structured and unstructured data. Applies data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources. Develops data models by studying existing data warehouse architecture; evaluating alternative logical data models including planning and execution tables; applying metadata and modeling standards, guidelines, conventions, and procedures; planning data classes and sub-classes, indexes, directories, repositories, messages, sharing, hiding, replication, back-up, retention, and recovery.
Solves the most complex problems; takes a new perspective on existing solutions. Develops and maintains infrastructure systems (e.g., data warehouses, data lakes) including data access APIs. Recommends process, product or service improvements. Solves unique and complex problems that have a broad impact on the business. Contributes to the development of functional strategy. Acts as a mentor for colleagues with less experience.
Manage the data collection process providing interpretation and recommendations to management. Executes quantitative analyses that translate data into actionable insights. Provides analytical and data-driven decision-making support for key projects. Designs, manages, and conducts quality control procedures for data sets using data from multiple systems.
Develops and maintains infrastructure systems (e.g., data warehouses, data lakes) including data access APIs. Prepares and manipulates data using various technologies (e.g. Hadoop or equivalent MapReduce platform). Defines, designs and build dimensional databases and data pipelines to support analytics projects.
Oversees the delivery of engineering data initiatives and projects. Supports long term data initiatives as well as Ad-Hoc analysis and ETL activities. Creates data collection frameworks for structured and unstructured data. Applies data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources.
Improves data delivery engineering job knowledge by attending educational workshops; reviewing professional publications; establishing personal networks; benchmarking state-of-the-art practices; participating in professional societies. Provide technical guidance and support to developers, data engineers and data administrators. Develop strategies for data acquisitions, recovery and implementations.

SUPERVISORY RESPONSIBILITY:
Position does not have direct reports but is expected to assist in guiding and mentoring less experienced staff. May lead a team of matrixed resources.

QUALIFICATIONS:

Education Level: Bachelor's Degree in Information Technology or Computer Science OR inlieu of a Bachelor's degree, an additional 4 years of relevant work experience is required in addition to the required work experience.
Licenses/Certifications Upon Hire:
Data Management\Certified Analytics Professional (CAP)
Experience:
10 years Experience leading database design and developing modeling tools.
Experience in leading data engineering and cross functional teams to implement scalable and fine tuned ETL/ELT solutions for optimal performance.
Experience developing and updating ETL/ELT scripts.
Hands-on experience with application development, relational database layout, development, data modeling.
Knowledge, Skills and Abilities (KSAs)
Expert in at least one programming language (i.e., SQL, NoSQL, Python).
Knowledge of multiple database technologies - structured and un-structured.
Ability to quick learn new technology and take direction.
Strong customer service orientation.
Provide direction to and lead technical teams.
Requires strong organizational with the ability to handle multiple priorities.
Excellent communication skills both written and verbal.
Must be able to meet established deadlines and handle multiple customer service demands from internal and external customers, within set expectations for service excellence. Must be able to effectively communicate and provide positive customer service to every internal and external customer, including customers who may be demanding or otherwise challenging.

Department
Department: CBIW Mandates Development
Equal Employment Opportunity
CareFirst BlueCross BlueShield is an Equal Opportunity (EEO) employer. It is the policy of the Company to provide equal employment opportunities to all qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information.
Hire Range Disclaimer
Actual salary will be based on relevant job experience and work history.
Where To Apply
Please visit our website to apply: www.carefirst.com/careers
Federal Disc/Physical Demand
Note: The incumbent is required to immediately disclose any debarment, exclusion, or other event that makes him/her ineligible to perform work directly or indirectly on Federal health care programs.
PHYSICAL DEMANDS:
The associate is primarily seated while performing the duties of the position. Occasional walking or standing is required. The hands are regularly used to write, type, key and handle or feel small controls and objects. The associate must frequently talk and hear. Weights up to 25 pounds are occasionally lifted.
Sponsorship in US
Must be eligible to work in the U.S. without Sponsorship
#LI-LD1","$125,053 /yr (est.)",5001 to 10000 Employees,Company - Private,Insurance,Insurance Carriers,1942,$100 to $500 million (USD)
PENTAFOUR GROUP,#N/A,"New York, NY",Sr. Data Integration Engineer,"Job description:
Major Responsibilities:
· Experience designing and developing Enterprise Data Warehouse solutions.
· Demonstrated proficiency with Data Analytics, Data Insights
· Proficient writing SQL queries and programming including stored procedures and reverse engineering existing process
· Leverage SQL, programming language (Python or similar) and/or ETL Tools (Azure Data Factory, Data Bricks, Talend and SnowSQL) to develop data pipeline solutions to ingest and exploit new and existing data sources.•
· Perform code reviews to ensure fit to requirements, optimal execution patterns and adherence to established standards.
Skills
· 12+ years - Enterprise Data Management
· 10+ years - SQL Server based development of large datasets
· 8+ years with Data Architecture
· 5+ years’ experience in Finance / Banking industry – some understanding of Securities and Banking products and their data footprints.
· 2+ years Python coding experience
· Proficient with Data Visualization tools
· Hands-on experience with Snowflake utilities such as SnowSQL and SnowPipe
· Working knowledge of MS Azure configuration items with respect to Snowflake.
· Hands-on experience with Tasks, Streams, Time travel, Optimizer, Metadata Manager, data sharing
· Experience in Data warehousing - OLTP, OLAP, Dimensions, Facts, and Data modeling.
· Previous experience leading an enterprise-wide Cloud Data Platform migration with strong architectural and design skills
· Capable of discussing enterprise level services independent of technology stack
· Experience with Cloud based data architectures, messaging, analytics
· Superior communication skills
· Cloud certification(s)
· Any experience with Regulatory Reporting is a Plus
Education
Minimally a BA degree within an engineering and/or computer science discipline
Master’s degree strongly preferred
Job Type: Full-time
Salary: $88,089.11 - $106,085.80 per year
Experience level:
10 years
9 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: On the road","$97,087 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Braintrust
4.6",4.6,"San Francisco, CA",Data Engineer,"ABOUT US:
Braintrust is a user-owned talent network that connects you with great jobs with no fees or membership costs—so you keep 100% of what you earn.

JOB TYPE: Freelance, Contract Position (no agencies/C2C - see notes below)
LOCATION: Remote - United States only (TimeZone: EST | Partial overlap)
HOURLY RANGE: Our client is looking to pay $100 – $105/hr
ESTIMATED DURATION: 40h/week - Long term

ABOUT THE HIRING PROCESS:
When you join Braintrust, you will be invited to a screening process for Braintrust to learn more about your previous work experiences. Once completed, you will have access to the employer for this role and other top companies that seek high-quality talent. Apply to this job to kick off the process.
THE OPPORTUNITY
Requirements
Summary:
Advanced analytics SQL - minimum 2 years, preferable 5
Python - minimum 1 year, preferable 2
Ability to work in cloud platform
Qualities that will help you thrive in this role:
You understand that being an effective engineer is about communicating with people as much as it is about writing code.
You are willing to work with and improve code you did not originally write, primarily in SQL and Python.
You are generous with your time and experience and can mentor and learn from other engineers.
You are comfortable with best practices for traditional data warehousing.
You love SQL and writing efficient and optimized ETL pipelines.
You are familiar with building and monitoring cloud services and infrastructure.
What you’ll be working on
What’s the role?
As a member of our client's Data Applications, Data Warehouse team, you’ll help us improve the stability, performance, and usability of their BigQuery data warehouse while advising their stakeholders on best practices and optimizations. Your work will enable other developers, data scientists, and analysts to write the high-performing pipelines that power data science, machine learning, and product development.
In addition to BigQuery SQL, our client's toolset includes Looker, Java, Python, and Spark, as well as Airflow, Terraform, and Kubernetes, and GCP services like Dataproc and Dataflow.
About The Team
They build highly-performant systems and data warehouses that are maintainable and cost effective.
They develop robust, highly available, well-monitored data infrastructure.
They stay in close communication with the internal customers and make strategic improvements to ensure those that depend on us have a great experience using data
What does the day-to-day look like?
You should have experience building data warehouses, data marts, and aggregate tables - supporting them at scale, and collaborating with other teams that depend on them.
Experience building applications and managing infrastructure using one of the major cloud providers is preferred but not required. (Our client uses Google Cloud).
Our client values curiosity, passion, responsibility, and generosity of spirit.
Apply Now!
Braintrust Job ID: 6590


C2C Candidates: This role is not available to C2C candidates working with an agency. If you are a professional contractor who has created an LLC/corp around their consulting practice, this is well aligned with Braintrust and we’d welcome your application.

Braintrust values the multitude of talents and perspectives that a diverse workforce brings. All qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status.

This is a remote position.",$102.50 /hr (est.),51 to 200 Employees,Company - Private,Management & Consulting,Business Consulting,2018,$5 to $25 million (USD)
"Apolis
3.9",3.9,"Saint Louis, MO",Data Engineer (Python Focused),"Role: Data Engineer (Python Focused) (W2 ONLY)
Location : St. Louis, MO 63102
Duration: 18 Months
The primary responsibilities of this role will be to work as a part of the data and analytics team of the client. The ideal candidate would have experience using ETL and other data engineering tools in a cloud environment. Additionally, this candidate would have coding and scripting experience creating code to move and manipulate data using tools such in Python such as Spark or Pandas. Microsoft Azure experience is a plus but relevant experience with other cloud-based environments will be considered.
This team uses Microsoft Azure to build and productionize data sets and processes to the product owners and product users and this role will be expected to contribute to those objectives. As a member of this team, you will work in conjunction with other data engineers, platform engineers, solution developers and others to execute and deliver working solutions to the end users.
Technical Skills
High focus on Python
More Programmer/Coder experience, less Cloud experience needed.
Must have experience with data analytics/engineering and cloud technologies and an in-depth working knowledge and understanding of data engineering, design, and integration.
Intermediate or higher level of proficiency with Python Must have the proven ability to learn quickly and apply cloud data engineering principles and skills to new tools and use cases.
Ability to define and take technical ownership of detailed requirements and collaborate effectively with various team members such as Business Analysts and Project Managers and end users.
Excellent communication (verbal and written) and problem-solving skills are essential for this position.
Initiative, Proactive Cooperation, and curiosity are mandatory behavioral skills required for this position.
Experience with several of the following technologies is preferred:
Experience with Microsoft Azure Platform. In particular, the data processing toolsets including Azure Data Factory Pipelines, Azure Databricks, Azure Data Lake Storage.
Experience using Notebooks (Such as Jupyter, DataBricks) also desired.
Experience using code languages such as Python and C#
Experience using query languages such as SQL, MDX and DAX
Experience using scripting languages such as PowerShell, M-Query (Power Query), and Windows batch commands.
Understanding of Agile and DevOps principles for software development and ownership.
Familiar with compliance and controls for software development and deployment.
Education Required
A bachelor's in computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field [or equivalent work experience] is not required but is preferred.
Job Type: Contract
Salary: $60.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Vision insurance
Compensation package:
Overtime pay
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
St. Louis, MO: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Preferred)
ETL: 5 years (Preferred)
Cloud infrastructure: 4 years (Preferred)
Work Location: One location",$65.00 /hr (est.),501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,1996,$25 to $100 million (USD)
"Loyal
3.9",3.9,"Atlanta, GA",Senior Cloud Data Operations Engineer,"Loyal is an organization centered on experience and building a platform that allows consumers to make meaningful decisions when it comes to healthcare. We deeply understand providers, locations, services, appointments, business rules, and moreover, we understand patients - who they are, the preferred method of communication, upcoming appointments, lapsed appointments, outstanding bills, health risks, and more. With this intelligence, our platform fuels highly relevant and personalized experiences across all mediums (website, email, voice…) allowing patients to get healthy, stay healthy, and have a better relationship with the health care provider.
**This is a remote role**
Summary
The Senior Cloud Data Operations Engineer is responsible for delivering Loyal's cloud-based data warehouse. They interface with groups across the organization to ensure we are collecting and storing data in a way that makes it easy to create and consume. They identify best practices for big data management, select the right tooling, create a workable data architecture, and drive an implementation roadmap others can depend on. The Senior Cloud Data Operations Engineer also works closely with our security group to ensure data security and privacy. They also mentor others on the team, providing technical leadership to those they work with.
You will
Create data streams and consolidate data sources into a single cloud-based data warehouse
Architect and catalog data warehouse storage schema
Work with product teams to enable easy data aggregation
Work with data consumers (analysts, data scientists, and AI/ML engineers) to ensure data is easily accessible and consumable
Create and execute a roadmap that meets data storage and availability goals
Ensure data is secure through data analysis, access control, and encryption
You have
Bachelor's degree in computer science, information systems, applied math or related field, or equivalent work experience
Minimum 5 years of experience working with cloud data processing services (AWS, Azure, or GCP).
Minimum 3 year experience with Big Data technologies, such as Databricks, Snowflake, Spark, Hadoop, etc.
Strong experience in creating data architecture and storage schemas required
Significant working knowledge of SQL and other query languages and tools
Significant experience with building relational database systems strongly preferred
Experience with ETL tools a plus
Knowledge of Python preferred
Software development, data analytics, or data science experience is a plus.
Experience working within a software as a service (SaaS) company preferred.
Experience working within a high growth and/or ambiguous environment, with proven experience to be dynamic preferred.
#LI-REMOTE
We know that potential candidates are often less likely to apply to a position if they don't match 100% of the job qualifications. Don't let that be why you miss out on this opportunity! We encourage you to apply if you can demonstrate many of these skills and competencies.
Loyal to our employees
We are a remote-friendly company! We encourage you to apply from anywhere in the United States. We also believe in a work/life balance that fulfills you while you're here and supports you when you're not. We built our benefits package to prove that we're committed to you having everything you need (including a little fun). Here is what we offer full-time employees:
Flexible paid time off, sick and personal days
At least one holiday per month (sometimes, more!)
Full health, dental, and vision insurance - Loyal pays the premium for all employees!
One Time Home Office Setup Stipend For Remote & Hybrid Roles
Monthly Internet Stipend for Remote & Hybrid Roles
Long term & short term disability
401[k] plan
16 Weeks Paid Parental Leave
2 Volunteer days per year
Matching Gift Program
Participation Grant Program
Annual Travel/Team Events up to twice per year (post-COVID)
Our Commitment
We believe that the key to Loyal's success is you. Your unique background, life experience, knowledge, self-expression, and talent make you uniquely you. Who you are, what you have experienced, and how you think inspires us to be innovative and bold.
Loyal is an equal opportunity employer. We hire great people from a wide variety of backgrounds, not just because it's the right thing to do, but because it makes our company stronger. We welcome the unique contributions that you can bring in terms of your education, opinions, culture, ethnicity, race, ancestry, sex, gender identity and expression, national origin, citizenship, marital status, age, languages spoken, veteran status, color, religion, disability, sexual orientation, and beliefs.
We consider qualified applicants regardless of criminal histories, consistent with legal requirements.
Further, consistent with applicable federal and state law, Loyal provides reasonable accommodations when requested by qualified applicants or employees with disabilities, unless doing so would cause an undue hardship. Loyal's policy regarding requests for reasonable accommodation applies to all aspects of employment, including the application process. If you require a reasonable accommodation in connection with the application process, please contact the Talent Acquisition Department at talentacquisition@loyalhealth.com.
E-Verify
This employer participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S. If E-Verify cannot confirm that you are authorized to work, this employer is required to give you written instructions and an opportunity to contact Department of Homeland Security (DHS) or Social Security Administration (SSA) so you can begin to resolve the issue before the employer can take any action against you, including terminating your employment. Employers can only use E-Verify once you have accepted a job offer and completed the I-9 Form.
COVID-19 Vaccinations
Consistent with Loyal policy, candidates performing in-person work will be required to show proof of being fully vaccinated against COVID-19 upon commencing employment. Reasonable accommodations will be considered on a case-by-case basis for exemptions to this requirement in accordance with applicable law. If you require a reasonable accommodation to Loyal's vaccination requirement, please contact the People Department at people@loyalhealth.com.",#N/A,51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2015,$5 to $25 million (USD)
OpenSecrets.org,#N/A,Remote,Snowflake Data Engineer,"About us
Our goal is to is to track the flow of money in American politics and provide the data and analysis to strengthen democracy. Our vision is for Americans to use this knowledge to create a more vibrant, representative and accountable democracy..
Our work environment includes:
Modern office setting
Work-from-home days
Wellness programs
Data Engineer
OpenSecrets is seeking to add a new member to our data team. The Data Engineer will be responsible for helping build and maintain our data systems.
Responsibilities include:
Implementing methods to improve data reliability and quality
Analyzing raw data and implementing that data into our system using data pipelines
Troubleshooting data errors and pipeline issues
REQUIRED QUALIFICATIONS:
Experience working with and analyzing large datasets, equivalent to 2-3 years in a similar position
Proven ability to build complex data pipelines using Snowpipe and SSIS
The ability to work with diverse populations
Experience with MSSQL Server and Snowflake
Advanced knowledge of ANSI SQL
PREFERRED QUALIFICATIONS:
Excellent written and verbal communication skills.
The ability to manage multiple priorities and projects both independently and as a member of a team
Snowflake SnowPro Data Engineer certification
Experience with Apache Spark
Experience with machine learning matching models
Knowledge of and interest in American government and money in politics.
SALARY AND BENEFITS:
We are a fully remote organization and this person can be based anywhere in the U.S.
OpenSecrets offers an excellent benefits package, including full medical, dental, 403(b) match, generous vacation and sick leave, and all federal holidays paid.
TO APPLY:
Please visit this link: https://airtable.com/shrGe2bugeapKVr8a
People of diverse backgrounds are strongly encouraged to apply. OpenSecrets is an equal opportunity employer committed to creating a diverse and inclusive organizational culture. We do not discriminate against candidates and employees because of their disability, sex, race, gender identity, sexual orientation, religion, national origin, age, veteran status, or any other protected status under the law.
ABOUT US: OpenSecrets is the nation's premier research group tracking money in U.S. politics and its effect on elections and public policy. Our mission is to track the flow of money in American politics and provide the data and analysis to strengthen democracy. Our vision is for Americans to use this knowledge to create a more vibrant, representative and accountable democracy.
Job Type: Full-time
Pay: $83,874.82 - $100,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Retirement plan
Vision insurance
Compensation package:
Yearly pay
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Experience:
SQL: 3 years (Required)
Snowflake: 1 year (Preferred)
ETL: 2 years (Required)
Work Location: Remote","$91,937 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Ikigai Labs, Inc.
4.9",4.9,"Cambridge, MA","Software Engineer, Data Engineering","Ikigai Labs is a fast growing startup founded out of MIT to empower data operators. We are building an easy to use AI augmented data processing and analytics platform on the cloud. Our users depend on us to automate, maintain, and enhance day-to-day mission critical operations. We are a team of talented, hardworking and fun-loving engineers, data scientists, and data analysts working towards the goal of building the next generation of data tools.
Job Description
JOB TITLE: Software Engineer, Data Engineer [Full-time]
LOCATION: Cambridge, MA
SUMMARY:
Ikigai Labs is seeking a dynamic and passionate engineer with strong software fundamentals to join a high-performing data platform development team. We are looking for a team player who is a quick learner, performs in a rapid development cycle, has a drive to surpass expectations, and an eagerness to share their work and knowledge.
We encourage applicants from all backgrounds and communities. We are committed to having a team that is made up of diverse skills, experiences, and abilities.
Technologies
Languages: Python3, SQL
Databases: Postgres, Elasticsearch, DynamoDB, RDS
Cloud: Kubernetes, Helm, EKS, Terraform, AWS
Data Engineering: Apache Arrow, Dremio, Ray
Misc.: Apache Superset, Plotly Dash, Metabase, Jupyterhub, Stripe, Fivetran
The Position
Design and develop scalable data integration (ETL/ELT) processes
Design and develop an on-demand predictive modeling platform with gRPC
Utilize Kubernetes to orchestrate the deployment, scaling and management of Docker containers
Utilize and learn various AWS services to solve cloud-native problems
Implement a testing platform which performs sanity check, load test, scale test, heartbeat test, and performance test
Provide periodic support to our customer success team
Qualifications
0-3 years of experience with a bachelor's degree in Computer Science, Math, or Engineering; or a master's degree
Experience with Python, AWS services, and/or ETL/ELT pipeline experiences
Experience with Kubernetes and/or EKS (optional)
Understanding of the fundamentals of design patterns and testing best practices
The ability to learn quickly in a fast-paced environment
Excellent organizational, time management, and communication skills
The desire to work in an AGILE environment with a focus on pair programming
Willingness to discuss obstacles, find creative solutions, and take initiative
The ability to receive and give both constructive and encouragement feedback","$102,419 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Barracuda Networks Inc.
3.6",3.6,"Alpharetta, GA",Data Engineer,"Job ID: 23-579

Come Join Our Passionate Team! At Barracuda, we make the world a safer place. We believe every business deserves access to cloud-enabled, enterprise-grade security solutions that are easy to buy, deploy, and use. We protect email, networks, data and applications with innovative solutions that grow and adapt with our customers’ journey. More than 200,000 organizations worldwide trust Barracuda to protect them — in ways they may not even know they are at risk — so they can focus on taking their business to the next level.
We know a diverse workforce adds to our collective value and strength as an organization. Barracuda Networks is proud to be an Equal Opportunity Employer, committed to equal employment opportunity and equitable compensation regardless of race, gender, religion, sex, sexual orientation, national origin, or disability.
Envision yourself at Barracuda
We currently have an exciting new position open for a Data Engineer. This person will assist in driving technology innovation for engineering projects and work on architecture of AWS, Elastic and Kubernetes. They will also work closely with the Technology Architect to create and drive the Architecture Review Board.
What you’ll be working on
Determine internal / external customer data needs and identify system specifications
Analyze the needs of large systems and breaking them down into smaller manageable parts
Responsible for designing the journey of data from various sources to the database and everything in between
Plan and design the structure of data, brainstorm with team
Analyze best technology products and systems for internal teams
Help provide build vs buy decisions on data architecture
Communicate data requirements to engineers; explain data architecture to them and provide assistance throughout SDLC
Choose / maintain suitable software, hardware and integration methods
Help resolve technical problems as and when they arise
Review testing procedures to ensure data performant and error free
Ensure that database satisfy quality standards and procedures. Setup such standards.
Work with senior IT personnel to devise plans for future IT requirements of the organization
Produce progress reports on key infrastructures - Elastic and Data Pipelines
What you bring to the role
Bachelor Degree or Post Graduate in Computer Science, or relevant work experience
5+ years of experience in an Engineering / Operations heavy organization
1+ years of experience as a Data Analysis and Data Design responsible for data architecture of a SaaS application
Experience in understanding and designing data sharding in a distributed database
Some Hands-on experience in software development and system administration
Knowledge of strategic IT solutions
Familiarity with programming languages like JavaScript or Java or Python
Familiarity with Linux OS and Windows alike
Experience in cloud computing and cloud Technologies (AWS, GCP, Azure)
Excellent communication skills – you should be adept at listening to, understanding, and explaining key concepts to managerial and technical resources.
Good Anticipation and problem-solving skills
Ability to prioritize and manage time – responsible for multiple streams of work at the same time
Relationship building - Ability to form a good rapport with internal and external stakeholders
In-depth knowledge about quality standards and security best practices

Preferred Skills:
Expert level Understanding of Elastic eco system - Elasticsearch, Logstash, Kibana, Filebeats
Understanding of secure tunnel communication using VPN
Experience designing data at scale
AWS experience is preferred in Cloud Technologies
What you’ll get from us
A team where you can voice your opinion, make an impact, and where you and your experience are valued. Internal mobility – there are opportunities for cross training and the ability to attain your next career step within Barracuda.
Equity, in the form of non-qualified options
High-quality health benefits
Retirement Plan with employer match
Career-growth opportunities
Flexible Time Off and Paid Time Off benefits
Volunteer opportunities
#LI-Remote","$98,161 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,$100 to $500 million (USD)
"Petrolink International
2.9",2.9,"Houston, TX",Data Analysis Engineer,"Location:
Houston, Texas, U.S.A.
Job Type:
Full-time
Job Description:
Petrolink is a global company that specializes in user-driven technology for the Energy Industry.
We are looking for a Data Analysis Engineer to assist with creating and delivering algorithms primarily used by our engineering solutions.
This position is based in our U.S. office in Houston, Texas.
Duties & Responsibilities:
Research and provide logic for engineering solutions algorithms.
Collaborate with Subject Matter Experts (Drilling Engineers, Mud loggers, Pore Pressure Analysts, and other specialists from the Drilling Industry) to understand different challenges in the Drilling Industry and the practices in place to address/solve these challenges.
Code the algorithms using C# to run on Petrolink’s proprietary analytics engine.
Develop, test, and maintain algorithms.
Support escalations related to issues in algorithms.
Learn existing algorithms to provide support when required.
Prepare training material and sessions for stakeholders as part of the production release.
Skills & Qualifications:
Proficient in C#.
Experience with Python is an advantage.
Knowledge in drilling practices, and ability to speak to other drilling experts and capture requirements and logic.
Knowledge in drilling performance KPIs is an advantage.
Prior experience with WITSML is an advantage.
Function and communicate well in a team having diverse expertise.
Bi-lingual (English and Spanish) preferred.
At least 2 years’ relevant experience.
Benefits:
Health insurance
Dental insurance
401(k) with matching
Health savings account
Life insurance
Long-term disability
This job description is intended to explain the general nature and level of work being performed. Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.
Applicants must be legally authorized to work in the United States for any employer.
To apply for this position, applicants must meet the above criteria.
Interested applicants fulfilling the above criteria, please send your resume to careers.usa@petrolink.com.","$81,133 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"Kroenke Sports Enterprises
3.4",3.4,"Denver, CO",Data Engineer,"Job Title: Data Engineer
Department: Hockey Operations
Business Unit: Colorado Avalanche
Location: Denver, CO or Remote
Reports To: Director of Analytics
Employment Type: Full Time – Salaried - Exempt
Supervisor Position: No
_____________________________________________________________________________________
Kroenke Sports & Entertainment (KSE) is an American Sports and Entertainment holding company based in Denver, Colorado. KSE is committed to providing world class sports and entertainment for both live and broadcast audiences. We are the employer of choice as the owner and operator of Ball Arena, DICK’S Sporting Goods Park, the Paramount Theatre, 1STBANK Center, Denver Nuggets (NBA), the Colorado Avalanche (NHL), Colorado Mammoth (NLL), Colorado Rapids (MLS), KIMN,KXKL, KKSE (FM/AM), Altitude Sports & Entertainment, Major League Fishing/Fishing League Worldwide (MLFLW), Winnercomm, Outdoor Sportsman Group and SkyCam.

Nature of Work:
The Colorado Avalanche are looking to hire a full-time Data Engineer to work within the team’s Hockey Operations Department. This person will be responsible for maintaining and expanding the Avalanche hockey operations database. They will be tasked with importing and integrating data from external providers and interacting with the rest of the hockey operations department to ensure optimal dissemination of information to the appropriate parties. This person will also have the opportunity to analyze data and share insights with members of the Analytics department as well as the broader Hockey Operations department if desired. They will report to the Director of Analytics.

Examples of work performed:
Manage and improve the organization’s data storage, structure, and ETL pipeline while optimizing query performance for large datasets
Design automated processes to oversee data integrity and query performance on a regular basis, as well as being available to spot and resolve data issues that may arise at any time
Ensure that our automated processes run on schedule without issue
Incorporate expansive new datasets from disparate sources into our structure in a seamless fashion

This description is a summary only and is describing the general level of work being performed, it is not intended to be all-inclusive. The duties of this position may change from time to time and/or based on business needs. We reserve the right to add or delete duties and responsibilities at the discretion of the supervisor and/or hiring authority.

Working Conditions & Physical Demands:
Typical Office Conditions
Travel may be required

Qualifications:
Required
Strong knowledge of ETL architecture and development in a cloud-based environment
Academic and/or industry experience in database architecture, back-end software design, and query optimization
Expertise in SQL and Spark
Excellent attention to detail, problem-solving abilities, and work ethic
Preferred
1-3 years of work experience in a database-related position
An advanced degree in software engineering, computer science, information technology, or a related field
Familiarity with R and Python, as well as R Studio / Posit
Experience with databricks and Linux operating system
Knowledge of NHL players and teams as well as a passion for hockey and hockey analytics
Some front-end development and/or data analysis experience is considered a plus

Competencies/Knowledge, Skills & Abilities:
Ability to maintain positive attitude and demonstrate professionalism
Ability to maintain a high level of confidentiality
Ability to complete work accurately and in a timely manner
Ability to work independently & in a group setting and demonstrate good judgment skills
Ability to communicate effectively orally and in writing
Possesses excellent interpersonal skills
Ability to multi-task, prioritize and adapt to changing environments

Compensation:
Salary Range $80,000 - $110,000 per annum

Benefits Include:
12 Paid Company Holidays
Health Insurance (Medical, Dental, Vision)
Paid Time Off (PTO)
Life Insurance
Short and Long-term Disability
Health Savings Account (HSA)
Flexible Spending plans (FSAs)
401K/Employer Match

Equal Employment Opportunity
Kroenke Sports & Entertainment (KSE) provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.","$95,000 /yr (est.)",1001 to 5000 Employees,Company - Private,"Arts, Entertainment & Recreation",Sports & Recreation,#N/A,Unknown / Non-Applicable
"Direct Line
3.8",3.8,"Ashburn, VA",Data Center Infrastructure Engineer,"SUMMARY:
Direct Line (“DL”) is a high growth global technology services company with primary focus in providing design, integration, installation, maintenance and managed services to well-known data center operators and technology companies. Direct Line deploys decades of experience and knowledge through key partnerships with hyperscale technology companies and multi-tenant data center operators that give its clients a competitive marketplace advantage. Direct Line is committed to continually improving our industry through certified training of cutting-edge technicians that deliver superior results with a passion for detail. Direct Line is headquartered in Fremont, California with additional locations in Virginia, Tennessee, North Carolina, New Mexico, the Pacific Northwest, Asia-Pacific, and Europe.
POSITION: Data Center Infrastructure Engineer
LOCATION: MUST BE LOCATED WITHIN A COMMUTABLE DISTANCE TO SANTA CLARA, CA OR ASHBURN, VA / MUST BE WILLING TO TRAVEL (75-80% travel)
Job description
We are now looking for a Data Center Infrastructure Engineer!
We are looking to grow our company and grow with the hardest working people in the world. Academic and commercial groups around the world are powering a revolution in artificial intelligence using deep learning techniques running on GPUs, enabling breakthroughs in the most complex problems from autonomous driving to medial image processing to natural language processing. Come work on an innovative company's AI technologies!
What You’ll Be Doing:
You will lead all aspects of and personally implement complex architectures in one of several data centers.
Solutions will include network, storage, and compute resources to meet customer requirements, SLAs and high levels of uptime. As a key member of the engineering team you will develop, implement, and lead rack-level elevation designs to ensure velocity and scale while efficiently utilizing space, power, and cooling.
Review, evaluate and improve the design and implementation of structured cable solutions to support network topologies
Establish continuous improvements in the design, implementation, deployment and operation of large-scale cloud-based solutions in power-dense air- and water-cooled environments
Develop and maintain processes and procedures associated with the management and deployment of data center infrastructure including asset management and RMAs
Support and expand data center monitoring applications, with a strong focus on CI/CD automation
You will ensure standards supporting operating procedures and engineering issues for problem incident management are followed, including all safety requirements
Handle network, electrical and mechanical operations at data centers focusing on availability, service delivery, and internal customer relationship management
Analyze and resolve critical engineering issues, often under tight timeframe pressures; Off hours and on-call hours are to be encouraged
What We Need to See:
You love solving hard problems and can work independently or as part of a team under tight timelines
You are passionate about providing outstanding support to customers
Bachelor’s degree in Math, Computer Science, or Engineering subject area. Equivalent background in Military Technical School also acceptable or equivalent experience in datacenter engineering operations.
6+ years’ experience as datacenter operations engineers with critical systems and telecommunications Infrastructure Standards, network certification is very desirable
Deep knowledge of data center operations including network, power, rack layouts, cabling, Raised Floor Systems, HOT/COLD aisle containment. Operational experience with compute, storage and GPU servers in both air- and water-cooled environments
Install, config, and maintain all NW and 3rd party HW
Experience with ERMA, Break-fix, etc.
Reading and understanding P2P cabling, labeling and cable mgt/dressing etc.
Ways to stand out of the crowd:
An obvious passion for getting things done in a fast-paced technology environment
Deep understanding of data center power and cooling infrastructure, of network and cabling infrastructure.
Experience with NetBox, CMMS, SNOW and Inventory Management tools.
You're a self-starter with an attitude for growth, continuous learning, and constantly looking to improve the team.
Attention to detail with superb interpersonal skills and the ability to effectively manage multiple priorities.
A positive attitude with a strategic outlook.
Constantly look to improve the team and build strong business relationships
Direct Line is a proud equal opportunity employer. We are a drug free, EEO employer committed to a diverse workforce. We will consider all qualified candidates regardless of race, color, national origin, sex, age, marital status, personal appearance, sexual orientation, gender identity, family responsibilities, disability, political affiliation, or veteran status.
Job Type: Full-time
Pay: $45.00 - $50.00 per hour
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible spending account
Health insurance
Life insurance
Paid time off
Referral program
Vision insurance
Schedule:
Monday to Friday
Experience:
CI/CD: 3 years (Required)
Linux: 3 years (Preferred)
Server Support: 3 years (Required)
Break/Fix / ERMA: 3 years (Preferred)
Data center Engineering: 3 years (Required)
Work Location: On the road",$47.50 /hr (est.),501 to 1000 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,1997,Unknown / Non-Applicable
"Presidio
4.0",4.0,"Tampa, FL","Engineer, Data Center (FL)","SEIZE THE OPPORTUNITY TO BE A PART OF SOMETHING GREAT!
Presidio is on the leading edge of a technology-driven movement to transform the way business is done, for our customers and our customers' customers. Joining Presidio means immersing yourself in a culture of self-starters, collaborators and innovators who make real, lasting change in the marketplace via cutting-edge technology and business solutions. At Presidio, we know that it’s our people that make the connections happen.
WHY YOU SHOULD JOIN US? You will set your career on track for outstanding achievement with a company that knows no limits. Presidio is a leading a global digital services and solutions provider focused on Digital Infrastructure, Business Analytics, Cloud, Security & Emerging solutions.
THE ROLE: Engineer, Data Center
Job Summary: Presidio is looking for a Data Center Engineer to join our talented Staff Augmented team at the Hillsborough County Courthouse. Primary responsibilities include engineering, design, installation, monitoring and troubleshooting of a complex data center environment.
Travel Requirements: This role is an onsite role with up to 2 days remote work per week. There is no travel required for this opportunity.
Job Responsibilities:
Administration, maintenance and troubleshooting of all servers in the Courts. This includes: monitoring of the servers, verification of all services, upgrades to servers and applications, permission changes, drive shares and folders, and associated other tasks
Review application logs and server resources to identify any preventative maintenance required .
Daily verification of the previous backup jobs
Perform any user-oriented moves, adds and changes to the information in Active Directory. This includes usernames, full names, locations, phone number, accounts and associated information
Keeping up with expiration dates and quotes for all maintenance contracts.
Develop comprehensive graphical and text-based design documentation and effectively manage the implementation process from design to customer acceptance.
Administration, maintenance and troubleshooting of all storage devices in the Courts. This includes: EMC, NetApp, Nimble
Administration, maintenance and troubleshooting of all virtualization infrastructure. This includes: VMware, VCenter, Site Recovery Manager
Administer and Maintain the CrowdStrike server to deploy and monitor CrowdStrike on all servers at the Courts.
Research, testing and staging of the Windows Updates using the WSUS server
Verification and final installation of the server patches. This includes all servers in the Courts.
Required Skills:
Systems Administration
Switches, Routers, Firewalls
Build out, OS install, security scans and remediation of Enterprise servers in a Data Center solution
Build/configure VMware host machines with integration into vCenter
Build/configure windows 2003-2016 virtual servers, patching, security compliance, AD, GPOs and user management
Education and Experience: (List out)
Bachelor's degree or equivalent experience and/or military experience
Over all 5 years of experience in design, installation and management of all Data Center Operations
*****
ABOUT PRESIDIO
Presidio is committed to Diversity, Equity, and Inclusion at the highest levels and has strengthened its drive to build and drive systemic DEI change process across all levels of the organization. Cultivating a culture of inclusion where the expression of all our differences are valued, celebrated, and add to our collective achievements.
Presidio is a global digital services and solutions provider accelerating business transformation through secured technology modernization. Highly skilled teams of engineers and solutions architects with deep expertise across cloud, security, networking and modern data center infrastructure help customers acquire, deploy and operate technology that delivers impactful business outcomes. Presidio is a trusted strategic advisor with a flexible full life cycle model of professional, managed, and support and staffing services to help execute, secure, operationalize and maintain technology solutions. We serve as an extension of our clients' IT teams, providing deep expertise and letting them focus on their core business. Presidio operates in 40+ US offices and offices in Ireland, London, Singapore, and India.
For more information visit: http://presidio.com
*****
Presidio is an Equal Opportunity / Affirmative Action Employer / VEVRAA Federal Contractor. All qualified candidates will receive consideration for this position regardless of race, color, creed, religion, national origin, age, sex, citizenship, ethnicity, veteran status, marital status, disability, sexual orientation, gender identification or any other characteristic protected by applicable federal, state and local statutes, regulations and ordinances.
To read more about discrimination protections under Federal Law, please visit: https://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf
If you have any difficulty using our online system and need an accommodation in the job application process due to a disability, please send an email to recruitment@presidio.com for assistance.
Presidio is a VEVRAA Federal Contractor requesting priority referrals of protected veterans for its openings. State Employment Services, please provide priority referrals to recruitment@presidio.com.
RECRUITMENT AGENCIES PLEASE NOTE:
Agencies/3 Parties may not solicit to any employee of Presidio. Any candidate information received from any Agency/3 Party will be considered a gift and property of Presidio, unless the Agency/3 Party is an Authorized Vendor of Presidio with an up-to-date Presidio Contract in hand signed by Presidio Talent Acquisition. No payment will be made to any Agency/3 Party who is not an Authorized Vendor, nor has specific approval in writing from Presidio Talent Acquisition to engage in recruitment efforts for Presidio.","$77,293 /yr (est.)",1001 to 5000 Employees,Subsidiary or Business Segment,Information Technology,Information Technology Support Services,2006,Unknown / Non-Applicable
"Crystal Management
4.8",4.8,Remote,Big Data Platform Engineer,"About
Since 2005, Crystal Management provides information technology (IT) infrastructure, systems integration, cybersecurity, facility design and transition, and professional services to customers in the defense, civilian federal agencies, homeland security, intelligence, and commercial sectors. We understand the mission demands innovative approaches, technology, and people. With talented professionals deployed worldwide, Crystal Management delivers IT enterprise solutions, systems engineering, and management consulting services for the largest transformation and restationing programs in defense history. Crystal Management is a service-disabled veteran-owned small business.

Position Summary
DCSA is taking steps to strengthen its position as a key provider of cloud-based and big data services to various Consumers. Transformation of its large-scale Big Data infrastructure is a key element of the overall information technology strategy. The Big Data Platform is comprised of an integrated set of platform technologies that provide industry-leading data persistence and analysis capabilities. As a member of the infrastructure team, the Senior Big Data Platform Engineer will play a significant role in helping DCSA transform its Big Data ecosystem that supports applications including Data Analytics Services and other key initiatives.
This telework position is fully remote with occasional travel to the client worksite.
Responsibilities
Deploy, configure, upgrade, and sustain the development, testing, and production of BDP environments.
Work with the applications team to deploy and upgrade applications running on the Big Data Platform.
Provide subject matter expertise to the organization and will perform hands-on engineering and configuration tasks in complex, interdependent environments, and will lead triage, diagnosis, and remediation of platform-related issues.
Work with our key partners and industry peers to understand and influence industry technical direction in order that big data requirements are accommodated.

Education/Certification Requirements
High School diploma or equivalent education
DoD 8570 compliant IAT Level 2 baseline certification (e.g., CCNA-Security, CySA+, GICSP, GSEC, Security+ CE, CND, or SSCP)

Required Qualifications
9+ years (or commensurate experience) of demonstrated expertise in technology selection, architecture design, engineering and configuration, deployment, operational support, and issue resolution.
Full stack technical expertise from OS and configuration management through to Big Data ecosystem components such as:
Data processing and streaming analytics using Kafka and Storm
Platform management plan using RDAs, Containers, Consul, and Airflow
Data storage and analytics using Hadoop with YARN, Accumulo, and Apache Spark
Manage user accounts and authentication using Citadel, Kerberos, LDAP, and SAML
Proxy services using NGINX
Deployment automation and configuration management using Puppet and Ansible
Expertise with integration and administration of PostgreSQL databases
Experience operating in an AWS Cloud environment, including administration of EC2 resources, and understanding of key networking concepts such as VPC, subnet, and security groups.
Understanding of storage concepts such as EBS volumes and S3.
Strong understanding of security vulnerability detection and remediation using ACAS and DISA STIGs.
Strong understanding of various encryption technologies.
Strong experience in Linux-based scripting and systems administration.
Demonstrated ability to develop sizing and capacity planning for large-scale big data platforms and to periodically evaluate and optimize performance and platform throughput.
Excellent communications skills, including the ability to handle detailed technical communications and distill complex technical details into executive-level oral and written communications.

Preferred Qualifications
Cloudera Certified Data Engineer (CCDE) certification
Hortonworks Certified Associate (HCA) certification
AWS Certified Big Data - Specialty certification
Experience with Atlassian tools such as Jira and Confluence
Experience working in an Agile Framework environment

Clearance Requirement
Active Secret clearance

COVID-19 Safety Protocols: To protect the health and safety of its employees and to comply with customer requirements, employees in certain positions may be required to be fully vaccinated against COVID-19 or subject to facility entry safety protocols (e.g., testing, masking, physical distancing), subject to the status of the federal contractor mandate and customer site requirements.

Crystal Management, LLC provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws.

This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.
#NowHiring
#CoronaVirusHiring",#N/A,1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2005,$5 to $25 million (USD)
"Leadstack Inc
4.3",4.3,Remote,Lead Data Engineer with Sql/Python,"LeadStack Inc. is an award winning, one of the nation's fastest growing, certified minority owned (MBE) staffing services provider of contingent workforce. As a recognized industry leader in contingent workforce solutions and Certified as a Great Place to Work, we're proud to partner with some of the most admired Fortune 500 brands in the world.
Title: Lead Data Engineer with Sql/Python
Location: Remote
Duration: 6+months
Direct Client
Immediate interview
Top Skills: Azure, SQL/Python Developer.
Job Description:
Design and develop data system integrations and data models for reporting and alerting.
Essential Job Functions:
Drive digital innovation by leveraging innovative new technologies and approaches to renovate, extend, and transform the existing core data assets, including SQL-based, NoSQL-based, and Cloud-based data platforms
Define high-level migration plans to address the gaps between the current and future state
Present opportunities with cost/benefit analysis to leadership to shape sound architectural decisions
Lead the analysis of the technology environment to detect critical deficiencies and recommend solutions for improvement
Interpreting data, analyzing results using statistical techniques
Developing and implementing data analyses, data collection systems and other strategies that optimize statistical efficiency and quality
Acquiring data from primary or secondary data sources and maintaining databases
Promote the reuse of data assets, including the management of the data catalog for reference
Key Responsibilities:
Innovate, develop, and drive the development and communication of data strategy and roadmaps across the technology organization to support project portfolio and business strategy
Drive the development and communication of enterprise standards for data domains and data solutions, focusing on simplified integration and streamlined operational and analytical uses
Drive digital innovation by leveraging innovative new technologies and approaches to renovate, extend, and transform the existing core data assets, including SQL-based, NoSQL-based, and Cloud-based data platforms
Define high-level migration plans to address the gaps between the current and future state, typically in sync with the budgeting or other capital planning processes Lead the analysis of the technology environment to detect critical deficiencies and recommend solutions for improvement
Mentor team members in data principles, patterns, processes and practices
Promote the reuse of data assets, including the management of the data catalog for referenceDraft and review architectural diagrams, interface specifications and other design documents
To know more about current opportunities at LeadStack , please visit us on https://leadstackinc.com/careers/
Job Type: Contract
Salary: $70.00 - $80.00 per hour
Experience level:
11+ years
Schedule:
8 hour shift
Experience:
Azure: 5 years (Preferred)
Data warehouse: 1 year (Preferred)
SQL developer: 6 years (Preferred)
Work Location: Remote",$75.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2016,Unknown / Non-Applicable
"Purple Drive Technologies
4.2",4.2,"Chicago, IL",AWS Data Engineer,"Looking for a strong AWS Data Engineer who has hands on experience with AWS data stack (AWS Glue, Glue Catalog, S3, AWS Hudi, EMR, Appflow and airflow). Successful candidate will be responsible for the following activities:
i. Identify, Build the foundational blocks of the Data Lake Platform such as Processing layer, CI/CD pipelines, Orchestration, template pipeline etc.
ii. Work closely with Infrastructure/Architects/key stakeholders to finalize the foundational blocks on various aspects of the platform
iii. Automate pipeline for Moxie data source by using the foundation that is built on.
iv. Work with Business stakeholders to understand the data needs and build data tech debt.
v. Build reusable data pipelines/framework as a solution accelerator
vi. Configure automated pipelines using the framework for the data sources (CC1, MedForce, Nice).
vii. Build Data Quality/Governance framework to make data complete and trustworthy.
viii. Reference Architecture, Documentation of Data Platform
ix. Configure automated pipelines for the data sources identified from tech debt.
x. Production roll-out, knowledge sharing and hyper-care
Job Type: Full-time
Schedule:
Monday to Friday
Work Location: One location
Speak with the employer
+91 (609) 796-2792","$97,051 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"PSEG
3.9",3.9,"Edison, NJ",AWS Data Engineer,"Requisition: 73755
PSEG Company: PSEG Services Corp.
Salary Range: $ 78,600 - $ 149,400
Incentive: PIP 10%
Work Location Category: Remote Local
PSEG operates under a Flexible Work Model where flexible work is offered when job requirements allow. In support of this model, roles have been categorized into one of four work location categories: onsite roles, hybrid roles that are a blend of onsite and remote work, remote local roles that are primarily home-based but require some level of purpose-driven in-person interaction and living within a commutable distance, and remote non-local roles that can be effectively performed remotely with the ability to work in approved states.
PSEG offers a unique experience to our more than 12,000 employees – we provide the resources and opportunities for career development that come with being a Fortune 500 company, as well as the attention, camaraderie and care for one another you might typically associate with a small business. Our focus on combatting climate change through clean energy technology, our new net zero climate vision for 2030 and enhanced commitment to diversity, equity and inclusion; and supporting the communities we serve make this a particularly exciting time to join PSEG.
Job Summary
The Technology Engineer is a direct report to the Senior Technology Engineer with understanding of business goals, business processes, technology expertise in one or more domains, technology solution design and implementation.
The Technology Engineer is involved in the full technology life cycle and responsible for designing, coding, configuring, testing, implementing and supporting application software. Technology Engineers work closely with Analysts and Product Managers to understand the business requirements that drive the analysis and physical design of technical solutions. Technology Engineers may be assigned to either implementation or support functions. Implementation involves creating new technology based solutions whereas support involves upgrades, maintenance or issue resolution to existing technology solutions.

Job Responsibilities
Primary Responsibilities:
Design, configuration, development, documentation and testing of technology solutions to meet business or technology requirements. Evaluation of existing technology solutions to determine fit for purpose for the new business or technology requirements. Recommendation of technology alternatives. Collaboration with other individuals to ensure proper integration of the new technology solution with the existing technology solutions.
Analysis of end user’s needs, business and technology requirements. Translation of these requirements into technology solution capabilities and design. Alternatively, using the user needs, business and technology knowledge to peer review designs, implementation plans, software code, configuration settings or other artifacts to ensure technology solution being created by others meets the user needs, business and technology requirements.
Provides support toward resolution of escalated support tickets. May perform the needed analysis to identify root cause of reported incidents, identify the short and long term remediation for the identified incidents
Specific responsibilities include:
Analyzes end-user needs and designs, configures, develops and tests technology solutions to satisfy demand. Performs build versus buy analysis for business demand.
Partners with business analysts to translate business needs to technology solution requirements.
Evaluates existing technology solutions and platforms and provides recommendations for improving technology performance by conducting gap analysis, identifying feasible alternative solutions, and assisting in the scope of needed modifications.
Collaborates with all stakeholders such as enterprise architects, software development, operations, cybersecurity and infrastructure to integrate applications and hardware.
Ensures that the design and technology solution implementation meets security and QA standards. Suggests fixes to issues by doing a thorough analysis of root cause and impact of any defect(s).
Provides support toward resolution of escalated support tickets. Applies operation break fixes and performs other proactive maintenance activities until permanent solutions can be implemented. Supports and participates in the solution deployment process for new functionality/ subsystems/modules, upgrades, updates and fixes to the production environment.
Makes solutions production-ready by following the standard change management processes, completing required forms, following procedures, completing version control documents, etc.

Job Specific Qualifications
Bachelor’s degree in Computer Science or a related field 4 years of professional technology solution engineering
Demonstrated technology solution ownership and adoption, projects or other work experiences.
Demonstrated experience in analysis of end-user needs to configure, develop and test low to medium complexity technology solutions to satisfy business demand.
Demonstrated track record of implementing technology solutions using structured methodologies such as agile (SCRUM, Kanban etc.) and waterfall.

Required Competencies:
AWS Data Cloud development experience
Experience developing and sustaining data load jobs using Extract, Transform, Load (ETL/ELT) methodologies and tools such as, Python, SQL, Shell Scripts and AWS Services.
Developing and sustaining a data ingestion pipeline to AWS leveraging services such as Athena, Glue, Lambda, S3, Relational Database Service (RDS) and Redshift.
Integration of AWS Data Lake with reporting tools such as PowerBI.
Experience using databases such as Oracle, MS SQL Server, and developing software code in one or more programming languages (Java, Python, etc.)
Experience with production support of mission critical technology solutions.
Experience documenting technical solutions and system process flow techniques.
Ability to work independently, multi-task effectively, and be flexible to accommodate change in priorities as necessary.
Strong analytical ability, communication skills, excellent problem solving skills, and ability to learn new technical concepts.
Ability to foster working relationships with Client departments, IT Management and Software Service Providers.
Desired Qualifications:
Graduate degree or MBA
AWS Developer/Architect certification
Minimum Years of Experience
4 years of experience
Education
Bachelors in Engineering or Computer Science
Bachelor in Information Technology
Certifications
None Noted
Disclaimer
Certain positions at the Company may require you to have access to Part 810-Controlled Information. Under the law, the Company is limited in who it can share this information with and in certain circumstances it is necessary to obtain specific authorization before the Company can share this information. Accordingly, if the position does require access to this information, you must complete a 10 CFR Part 810 Export Control Compliance Nationality Request Form, a copy of which will be provided to you by Talent Acquisition if an offer is made. If there is a need for specific authorization, due to the time it takes to obtain authorization from the government, we will likely not be able to further proceed with an offer
Candidates must foster an inclusive work environment and respect all aspects of diversity. Successful candidates must demonstrate and value differences in others' strengths, perspectives, approaches, and personal choices.
As an employee of PSE&G or PSEG LI, you should be aware that during storm restoration efforts, you may be required to perform functions outside of your routine duties and on a schedule that may be different from normal operations.
Certain positions at the Company may require you to have access to 10 CFR Part 810 controlled information. If the position does require access to this information, the Talent Acquisition representative will provide further details upon making an offer.
PSEG is an equal opportunity employer, dedicated to a policy of non-discrimination in employment, including the hiring process, based on any legally protected characteristic. Legally protected characteristics include race, color, religion, national origin, sex, age, marital status, sexual orientation, disability or veteran status or any other characteristic protected by federal, state, or local law in locations where PSEG employs individuals.
Business needs may cause PSEG to cancel or delay filling position at any time during the selection process.
This site (http://www.pseg.com) is strictly for candidates who are not currently PSEG employees. PSEG employees must apply for jobs internally through emPower which can be accessed through sharepoint.pseg.com by clicking on the emPower icon, then selecting careers.

PEOPLE WITH DISABILITIES:
PSEG is committed to providing reasonable accommodations to individuals with disabilities. If you have a disability and need assistance applying for a position, please call 973-430-3845 or email accommodations@pseg.com. If you need to request a reasonable accommodation to perform the essential functions of the job, email accommodations@pseg.com. Any information provided regarding a disability will be kept strictly confidential and will not be shared with anyone involved in making a hiring decision.

ADDITIONAL EEO/AA INFORMATION (Click link below)
Know your Rights: Workplace Discrimination is Illegal
Pay Transparency Nondiscrimination Provision","$114,000 /yr (est.)",10000+ Employees,Company - Public,"Energy, Mining & Utilities",Energy & Utilities,1903,$5 to $10 billion (USD)
"OpenGov
4.5",4.5,Remote,Senior Data Engineer,"Imagine yourself here!

OpenGov is a mission driven fast-growth, Series D, venture backed startup (includes Andreessen Horowitz, Formation 8, and Emerson Collective). Our Board of Directors includes iconic Silicon Valley executives John Chambers (former Cisco Chairman and CEO) and Marc Andreessen (Time Magazine’s list of the 100 most influential people in the world).

OpenGov is the leader in modern cloud software for local governments and state agencies. We have surpassed 1,600+ governments (and growing fast!) using our products in our mission to power more effective and accountable government.

OpenGov is a 2022 Top Workplaces USA award winner and a Forbes 2022 America's Best Startup Employer!

The Senior Data Engineers primary responsibility will be to balance the health of our data platform with the development of new integrations, pipelines, and data models. In this role, you will also manage the production support and enhancement of our existing BI and Data Warehouse tech stack. This role will be expected to learn and support Enterprise Data processes and software practices to proactively manage platform processes and ensure the highest quality of data. Developing strong relationships with business partners in the GTM, G&A, and R&D organizations is essential to success in this role. Prior experience in Data Warehousing and Data Integration/ELT technologies, coupled with your ability to understand complex data models and flows, will enable the incumbent to excel in this role.

Responsibilities:

Design and implement data solutions and related integrations with industry best practices
Design and implement efficient ETL and ELT processes to manage the data flowing into and out of the Data Warehouse in accordance with data governance and security standards
•Monitor and maintain data pipelines and system integrations proactively to ensure high service availability. •Systems may include Salesforce, Jira, Intacct, Netsuite, Zuora, Mavenlink (PSA), Zendesk, Pendo, ProductBoard, MixMax, Gainsight, Gong (CI), and Marketo
Help to define and improve our internal standards for style, maintainability, and best practices for a reliable data infrastructure
Build and leverage internal and external partnerships across GTM, G&A and R&D organizations, turning business goals into technical solutions
Assist in driving instrumentation and optimization of the data warehouse and systems integrations processes
Ensure data accuracy by following standardized structures and practices for dissemination and communication with appropriate departments
Comfortable breaking down complex data and integrations to technical and non-technical staff members
Drive business stakeholder meetings to gather requirements and make system integration, pipeline and reporting/BI recommendations
Conduct analysis and ongoing monitoring of available data, refinement, data and BI roadmap and feedback, scrubbing to improve existing data integrity
Understand what good looks like and the change management required to get there
Develop a deep understanding of the various business processes, data sets and data team deliverables

Requirements and Preferred Experience:

Bachelor’s degree in Information Technology, Computer Sciences or related field (equivalent experience may be substituted for formal education)
8+ years of experience in Business Systems, IT, Data Engineering or similar technical role
5+ years of experience in Data Engineering, Warehousing and Systems Integrations; 2+ years of experience with DBT
Experience with Salesforce, Support Platforms (Zendesk or Service Cloud), Mixmax, Outreach or Salesloft, Gainsight, JIRA, Intacct or Netsuite, Zuora, Professional Services Platforms (Mavenlink, FinancialForce, etc.)
Knowledge of enterprise SaaS applications and integrations
Experience working with
business intelligence tools and concepts (Domo, Tableau, Looker, etc.)
data warehouses (Redshift or Snowflake)
working with ETL and/or ELT tools (Snaplogic, Workato, and Fivetran)
iPasS experience maintaining and building integrations (Mulesoft, Snaplogic, Dell Boomi, etc.)
Demonstrated proficiency working with SQL, Python, GIT and CI/CD pipeline
An “end-to-end” data owner who believes in owning the whole stack from data source to cleaning to dashboard and stakeholder communication
Propensity and passion for problem solving, both conceptually and analytically, focused on optimizing strategy and process to continually provide valuable insights.
Organized professional, with the ability to manage multiple projects for multiple stakeholders with varying specifications
Ability to remain focused and flexible, working in a fast-paced environment with rapid change
Strong process and documentation skills, with the ability to connect process gaps with data integrity needs and to manage across teams to refine workflows
Strong written and verbal communication skills
Experience working with SaaS applications built on AWS or Azure platforms is preferred
#LI-DNI
What makes OpenGov unique

» Leadership: CEO Zac Bookman (MPA from Harvard and JD from Yale) is truly a mission-driven CEO. He was named one of the 100 most Intriguing Entrepreneurs by Goldman Sachs, a Tech Pioneer by the World Economic Forum, and SF and Silicon Valley Business Times' 40 under 40 class of 2018!

» Funding: Over $250 million, Series D company, from top tier investors including Andreessen Horowitz, 8VC, Cox Enterprises, and Emerson Collective.

» Board of Directors: Includes iconic executives John Chambers (former Cisco Chairman and CEO), Marc Andreessen (Time Magazine’s list of the 100 most influential people in the world), Katherine August-deWilde (Vice Chair of First Republic Bank), and Amy Pressman (co-founder, former president, and a current board member of Medallia).

» Growth: Record breaking growth with 1,600+ governments (and counting) using our products and seven acquisitions in the past six years! Click here to read more.

» Culture: Winner of Forbes 2022 Best Startup Employers, Winner of 2022 Top Workplaces USA award, 50 Best Workplaces award. Check out our Careers Video!

» Perks: 90% paid Medical/Dental/Vision premium for employees, fully paid Life and Short/Long term disability insurance, Unlimited PTO, Parental Leave policy, monthly fitness stipend, anniversary awards, and more!

» Product: Named to the GovTech 100 (seven consecutive years), we are the leader in cloud software for our nation's cities, counties, and state agencies.

» Mission Driven: We are a technology company with a passion for the mission. We're powering more effective and accountable government.

Come join us and make a positive social impact!

OpenGov is an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.",#N/A,201 to 500 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2012,Unknown / Non-Applicable
TEKletics,#N/A,"Scottsdale, AZ",Azure Data Engineer,"Azure Data Engineer
Tekletics is an information technology company focused on providing quality resources to support our customers needs. Successful candidates will work with world class organizations to deliver projects. Each candidate is required to have a strong work ethic and the ability to handle high pressure situations.
A little about this role:
As the Azure Data Engineer, you are primarily responsible for the collection and transformation of data across a multitude of data sources. This individual is also responsible for the optimization of the environment, structure, and processes associated with said data.
A day in the life:
Data Warehouse - As the Azure Data Engineer, you are responsible for the data warehouse design, development, testing, support, and configuration. You will review business requests for data warehouse data and data warehouse usage. You will also research data sources for new and better data feeds ensuring consistency and integration with existing warehouse structure.
Data Collection – You will be responsible for developing automated data pipelines and/or data integrations within the Azure Synapse environment. You will use SQL, Python scripts and Azure Functions to automate data collection from a wide variety of sources.
o API utilization – ability to leverage REST APIs as needed.
Data Transformation – You will create BI (Business Intelligence) and Data Warehousing cube design. You will create and manage ETL/ELT processes to transform and load data into data warehouse for reporting and analytics.
Data Optimization – As the Azure Data Engineer, you will create and maintain standards and policies. You will identify, design, and implement internal process improvements, including automation of manual processes and optimization of data delivery. You will continuously improve data reliability, efficiency, and quality.
Training – Identify and demonstrate techniques to optimize reporting for Data Visualization Analyst, provide and participate in internal and external training sessions, and produce documentation to support understanding and learnings around the Presence data/reporting environment.
This job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Projects and responsibilities may change at any time with or without notice due to our business, industry, and/or market changes.
What we are looking for:
Previous experience using SQL, Python scripts and Azure Functions
Experienced in Azure Synapse environment
Experience designing, building, and maintaining data processing system
Dependable, extroverted, diplomatic person, able to problem-solve successfully with a wide variety of people and issues
Attention to detail and strong organizational skills, self-motivated
Ability to work independently while being a strong team player
Ability to mentor junior level developers
Passion for innovation and “can do” attitude to thrive in a fast-paced environment
Proficient in time management and adhering to deadlines
Knowledge and interest of the natural products/brands and retail landscape is a plus
Proficient computer (MS Office applications) and data-mining skills
Flexibility to successfully multi-task in a fast-paced environment with a positive attitude
Regular and predictable attendance is required
Ability to manage time and deadlines
Job Type: Full-time
Pay: $79,947.00 - $142,509.31 per year
Experience level:
3 years
Schedule:
8 hour shift
Ability to commute/relocate:
Scottsdale, AZ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure: 3 years (Required)
SQL: 3 years (Required)
Data warehouse: 3 years (Required)
Work Location: In person","$111,228 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Purpose Financial
4.2",4.2,Remote,Data Engineer,"Address : 322 Rhett St, Greenville, South Carolina, United States - 29601
Purpose Financial, Inc. is an innovative consumer financial services company that offers a diverse suite of credit products, promoting financial inclusion and meeting consumers wherever they are. Through its brands, the company is committed to helping customers achieve their version of financial stability in the moment and in the future. Since 1997, Purpose Financial has been a pioneer in the consumer credit and financial services market offering money solutions in over 1,000 storefronts locations and online lending. Providing services in over 20 states, Purpose Financial employs over 2,700 team members.
At Purpose Financial, our people are our greatest asset. We are inclusive and encourage different points of view. We all belong here. We respect our colleagues. We value our customers, and we support our communities.
What do we do? Our sole purpose is to help each customer achieve their version of financial stability in the moment and in the future.
We have a history! Consumers know that when they choose Purpose, they’re backed by a trusted, reliable financial services company with 25 years in the business, 2,700+ employees, 1,000+ store locations, and operating in 24 states.
Benefits and Perks. We believe our employee benefits should make life better. Purpose Financial offers a comprehensive benefits package to everyone on our team.
Health & Welfare Benefits. When you take care of yourself, you’re a better you – which is better for everyone!
Health Savings & Flexible Spending Accounts. Pre-tax money for your family’s out-of-pocket expenses.
401k with Matching Contributions. Think bigger! You can build a better future.
Wellness & Employee Assistance Programs. Take care of yourself in every way that matters.
Generous Vacation Policy & Paid Time Off. Time off to take care of yourself, your family – and your bucket list!
Volunteer Days. We want you to care. Always. Take 2 days off each year to volunteer and make a difference in your community.
Tuition Reimbursement. Pursue your goals without fretting over all the costs.
Employee Discounts. Save on home and auto insurance, cell phone service, entertainment, shopping, pet insurance and even travel.
Performance Incentives & Awards. We want you to feel valued for all your contributions.
Hybrid Workforce Support. We believe in getting it done – in whatever way works best for you and your team.
Want to meet our executive team? Visit our home page at www.havepurpose.com.
Position Summary
The job function is responsible for building a scalable data infrastructure. The role will be part of the effort to enable Purpose Financial to democratize data by developing self-serving and curated data models for our single source of truth. The job function includes understanding of business requirements, defining data requirements, identifying enterprise data sources, planning, designing, and building data acquisition from sources, defining data model, designing data ingestion pipelines, data normalization, data transformation, and consumption using platforms such as Snowflake and AWS.
Job Responsibility
Take ownership of building/optimizing data pipelines to increase efficiency and performance
Develop and maintain data ingestion and integration processes that focus on data quality and monitoring
Use AWS and Snowflake tools such as Glue, Lambda, Snowpark, and Snowpipe to build data ingestion pipelines from various sources
Incorporate standards and best practices into data pipeline solutions
Participate in troubleshooting and problem-solving design discussions
Provide logical and physical data design, and database modeling
Solve complex data issues around data integration, unusable data elements, unstructured data sets, and other data processing incidents
Continuously design the future state of our data & reporting process by collaborating with IT and business partners.
Partner with enterprise teams, data scientist, architects to define development standards and solutions.
Manage cloud infrastructure as code (IaC), using Terraform.
Work closely with other members of the data team to better design and build metrics that enhance our analytical capabilities
Manage workflows and data build processes that help the BI team build key business metrics and design dashboard visualizations
Assess the effectiveness and accuracy of new data sources, data points, and data gathering techniques
Troubleshoot problems with data (missing, corrupted, invalid) and build data recovery strategy
Apply best practices for AWS & Snowflake architecture, ELT and data models
Be a leader for our data journey, act as SME for data and database related topics, and assist with development of data lakes and warehouses
Education Required
Bachelor’s degree (or foreign equivalent degree) in any Computer, Engineering, or other technical field.
Experience Required
5+ years experience in data warehousing
3+ years of experience in data architecture and design, with a good understanding of various data systems and structures (i.e. SQL, NoSQL, Key-Value, Streams)
3+ years of experience with building data pipelines and using data orchestration tools and implementing end to end Monitoring & Alerting solutions
3+ years of experience working with AWS or public cloud provider.
Strong understanding of SQL, data structures, data integrity, and schema design best practices
Programming experience required, preferably Python.
A strong advocate of data governance and data quality
Experience with database internals, database design, SQL and database programming
Experience in financial services (Banking and consumer lending preferred)
Experience of authoring CI/CD pipelines, automation elements related to infrastructure composition, deployment orchestration, and monitoring.
Familiarity and experience with Agile/SCRUM based development methodology.
Knowledge Required
Excellent written and verbal communications skills as would be needed to communicate in person, by phone, and through email; adaptability and flexibility to changing environment; and comfortable working in a dynamic, high volume, fast-paced environment. Ability to read, write, evaluate, and apply information. Ability to interact professionally and exhibit appropriate social skills. Ability to understand and ensure compliance with policies, procedures, and laws governing our industry/business and products. Ability to develop and maintain business relationships.
Physical Requirements
Sitting for long periods of time; standing occasionally; walking; bending; squatting; kneeling; pushing/pulling; reaching; twisting; frequent lifting of less than 10 lbs., occasional lifting of up to 20 lbs.; driving and having access during the workday to an insured and reliable transportation; typing; data entry; grasping; transferring items between hands and/or to another person or receptacle; use of office equipment to include computers; ability to travel to, be physically present at, and complete the physical requirements of the position at any assigned location.
Competencies
Integrity
Customer Advocacy
Interpersonal Skills
Results/Accountability
Inclusiveness
Technical Proficiency/Leadership
Big Data Trends
Data Management
Adaptability
Travel - None
Attire - Business Casual
Other - Must be eligible to work in the USA and able to pass a background check.
Job Type: Full-time
Compensation package:
Yearly pay
Experience level:
5 years
Schedule:
Day shift
Work Location: Remote",#N/A,Unknown,Company - Private,Financial Services,Financial Transaction Processing,#N/A,Unknown / Non-Applicable
"NexTek, LLC",#N/A,"Shoreview, MN",Sr. Data Engineer,"Nextek, LLC is a software development company in the financial services industry that creates web applications and solutions for financial services professionals. We are seeking a motivated Senior Data Engineer to join the team.
What we provide:
A laptop and other equipment/software necessary for work
Health Insurance, Dental Insurance, Vision Insurance
Paid time off (PTO), Volunteer time off (VTO)
Hybrid remote work options
401K
Life Insurance
Flexible schedules
Monthly Social Activities
Casual dress code
Professional development
Here is what we are looking for:
Experience in database administration, security, backup, and recovery
Preferred Knowledge:4+ years of experience in data/software engineering
Experience in the financial services industry
Experience with a variety of relational databases (particularly MySQL & MSSQL)
Experience with collecting and presenting analytics
Here is what you will be doing:
Display proficiency in relevant technical skills.
Maintain a deep understanding of the data structure.
Provide evaluations and recommendations for our data environment based on potential issues, tradeoffs, and risks.
Scope and implement solutions.
Make good estimates and set realistic expectations for delivery.
Ask for guidance when necessary.
Maintain and monitor the health of databases and associated solutions.
Make use of the same environments setup by the Software Engineering team
Thoroughly test code for bugs and security issues before deploying to production environments
Maintain and monitor actively deployed solutions
Follow and help define best practices
Peer reviews
Maintain awareness of industry trends
Share experience and insights to help your team grow
Increase understanding of impact on other teams
Increase understanding of the customer
All other duties as assigned
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Shoreview, MN: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Axos Bank
3.6",3.6,"San Diego, CA",Junior Business & Technology Analyst - Data Engineer,"Job Summary and Opportunity:
This is an exciting opportunity to join a unique and immersive rotational program as a first step in your career in technology. This full-time rotational program is geared toward providing multi-software platform exposure that focuses on the expansion of knowledge and real-life application within each. We are seeking innovative and energetic individuals who are excited about expanding their skillsets and accelerating their career path with immediate exposure to software applications.
For this position, you will be in the Data Engineer Rotational Program where you will be joining the Axos' Center of ExcellenceTeam. You will get to be a part of a team responsible for the implementation of cutting-edge software driven solutions. As you progress through the program, you will rotate into different complimentary areas within the Data program where roles and responsibilities will change. The final goal of the program is permanent placement within your area of focus. For those looking to make an impact this is where it begins.
In this role you will be focused on SQL related software or software built on direct interactions with SQL. Through the different rotations completed, you will gain the knowledge and skills database development, data quality, and business intelligence reporting to provide enterprise level solutions.
This position is on-site and will be located at our HQ in San Diego, California.
Responsibilities:
Define, prepare, execute and implement data validation and unit and integration testing methods to ensure data quality
Create SSIS packages for data transformation, cleansing, caching, aggregation, staging, and transfer
Analyze and define data flow requirements and prepare applicable system documentation and operation manuals as needed
Code, test and maintain new and existing SQL jobs, stored procedures and functions
Performance tune existing stored procedures, tables and indexes
Troubleshoot problems that may come up with database environments: performance issues, replication issues, or operational issues
Review SQL code written by other developers to ensure compliance to coding standards and best practices as well as maximum performance
Perform data analysis and data profiling tasks to provide support and recommendations for development and design decisions
Develop standardized reporting dashboards to meet the needs of the multiple business units across the Bank
Apply advanced modeling, data mining, machine learning and/or statistical techniques to data and dashboards to generate actionable insights enabling informed decision-making for optimized business and operational performance
Create mock-ups of reporting products, scorecards, dashboards, etc. to provide visualization to the end user
Work with teams within the organization to gather and document reporting requirements
Join client meetings to communicate status, give demos, provide timelines and offer insights
Participate in daily meetings that go over testing, and code reviews
Work with IT, Enterprise Data Management, Project Managers, Business Analysts, stakeholders across multiple business units to systematically plan the launch of new or enhanced dashboards, prepare launch collateral/documentation and work closely with users during through the different phases of a project
Develop deep understanding of the Bank's databases, identify appropriate data sources, relationships and logic needed to produce consistently reliable reports
Contribute to the overall strategy and quality of dashboarding
Document process steps of repetitive tasks performed
Partner with IT and other Infrastructure teams to tackle software upgrades, and coordinate testing
Perform any additional duties as assigned
Requirements:
Bachelor's degree in Information Technology, Computer Science, Business Administration, Mathematics or a related discipline
Customer Obsession: ""Good enough"" isn't good enough for you. You're obsessed with perfecting the customer experience
Leadership: A confident person with the ability to connect and inspire others to achieve success, whether or not they directly report to you
Results Oriented: A driver who possess the ability to take actions and implement effective solutions in a timely manner. Excuses aren’t in your vocabulary because you always find alternative solutions when issues arise
Ethics: Highest level of professional integrity and honesty as well as personal credibility. Your reputation for precedes you in this regard
Innovation: Dedication to maintaining cutting edge talent with the courage to implement new ideas, technology, and aggressively challenge the status quo. You don’t accept responses to new ideas like “That’s the way it’s always been done” because you use facts, data, and people skills to implement meaningful change
Immersion: A propensity to rapidly master the understanding and application of new technology
Excellent verbal and written communication skills, including ability to simplify complex concepts for technical and non-technical audience
Preferred:
Basic to intermediate knowledge in SQL server database development and testing
Working knowledge of Tableau
1+ year's working in an office environment or recent college graduate
APPLY DIRECTLY FOR CONSIDERATION:
Born digital, Axos Bank has reinvented the banking model and grown to over $18.4 billion in assets since our founding in 2000. With a broad and ever-growing range of financial products, Axos Bank is rated among the top 5 digital banks in the country! Axos Financial is our holding company and publicly traded on the New York Stock Exchange under the symbol ""AX"" (NYSE: AX).

We bring together human insight and digital expertise to anticipate the needs of our customers. Our team members are innovative, technologically sophisticated, and motivated to achieve.

Learn more about working here!

A targeted annual base salary range of USD $24/HR - $30/HR, based on the experience, skills, and education/certification required for this position. Eligibility for a discretionary semi-annual incentive compensation plan, based upon performance, payable in cash and/or share grants (RSU’s) that may vest over time. The annual discretionary target bonus percentage is up to 20%.

Axos benefits and perks include:
3 weeks’ Vacation, Sick leave, and Holidays (about 11 a year); Medical, Dental, Vision, Life insurance and more
HSA or FSA account and other voluntary benefits
401(k) Retirement Saving Plan with Employer Match Program and 529 Savings Plan
Employee Mortgage Loan Program and free access to Self-Directed Trading

Pre-Employment Drug Test:

All offers are contingent upon the candidate successfully passing a credit check, criminal background check, and pre-employment drug screening, which includes screening for marijuana. Axos Bank is a federally regulated banking institution. At the federal level, marijuana is an illegal schedule 1 drug; therefore, we will not employ any person who tests positive for marijuana, regardless of state legalization.

Equal Employment Opportunity:

Axos Bank is an Equal Opportunity employer. We are committed to providing equal employment opportunities to all employees and applicants without regard to race, religious creed, color, sex (including pregnancy, breast feeding and related medical conditions), gender, gender identity, gender expression, sexual orientation, national origin, ancestry, citizenship status, military and veteran status, marital status, age, protected medical condition, genetic information, physical disability, mental disability, or any other protected status in accordance with all applicable federal, state and local laws.

Job Functions and Work Environment:

While performing the duties of this position, the employee is required to sit for extended periods of time. Manual dexterity and coordination are required while operating standard office equipment such as computer keyboard and mouse, calculator, telephone, copiers, etc.

The work environment characteristics described here are representative of those an employee may encounter while performing the essential functions of this position. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions of this position.

#LI-Onsite",$27.00 /hr (est.),1001 to 5000 Employees,Company - Public,Financial Services,Banking & Lending,2000,$500 million to $1 billion (USD)
Cox powered by Atrium,#N/A,"Atlanta, GA",AWS Data Engineer- Hybrid,"Minimum Qualifications:
Bachelor’s degree or equivalent work experience
A minimum of 3+ years’ experience in Microsoft Windows/ SQL Server Technologies, .Net development, AWS Administration.
Experience working on 24x7 environments oriented towards a zero downtime target.
Working knowledge or previous administration of SQL 2016-SQL 2022 and Windows Server 2012+ preferred.
Ability to work with minimal direction, in a team environment.
Performance tuning for AWS/DataLake systems.
Some Experience with SQL in virtual, physical and cloud-based environments.
Experience with Athena and data modeling for cloud technologies.
Proven ability to quickly learn and implement new technologies.
Experience with Administration, Security/Identity Management and Terraforms in AWS.
Preferred Qualifications:
Experience with SentryOne, a plus.
Ability to code Powershell commands and maintain code in GitHub, a plus.
Some Experience with Metabase and Collibra, a plus.
Experience with ETL in AWS, a plus.
Pay Range:
$60-$68/hr
Requisition Disclaimer:
This job posting is for a temporary role as an employee of Atrium on assignment at Cox. The individual selected for this role will be offered the role as an employee of Atrium; compensation, medical benefits, fringe benefits and other terms and conditions of employment shall be presented by Atrium upon offer. The pay rate range provided is a reasonable estimate of the anticipated compensation range for this job at the time of posting. The actual pay rate will be based on a number of factors, including skills, competencies, experience, location and/or being pursued and other job-related factors permitted by law. In addition, this role will be eligible for overtime pay, in accordance with federal and state requirements

By applying for this position you agree to the Atrium Terms and Conditions. Agreeing to these terms, includes permission to use the email address and mobile phone number you provide during the application process or throughout the duration of your prospective or actual employees to notify you of job openings, profiles, articles, news, and other employment-related information, as well as to notify you of special promotions or additional products and services offered by us or our affiliates and partners (collectively, “Atrium Alerts”). Atrium Alerts may be sent by email, phone or text message. Your personal information will be safely stored in our database. Atrium does not sell your personal information to third parties. Text message and data rates may apply. To OPT OUT of text messaging or to modify your communication preferences for Atrium Alerts at any time, please contact us at privacyadministrator@atriumstaff.com.

If you do not agree with the Atrium Terms and Conditions, you can still complete your application for this position by emailing your resume to our team at coxrecruit@atriumworks.com. Please include the job title in the subject of your email.

As a woman-owned firm, Atrium values diversity. We are an equal opportunity employer and will consider all applications without regard to race, sex, age, color, religion, national origin, veteran status, disability, genetic information or any other characteristic protected by law. We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.
Posting: #zip",$64.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Seamless.AI
3.4",3.4,"Columbus, OH",Senior Data Engineer - Remote US,"Role: Senior Data Engineer
The Opportunity
We are seeking a Senior Data Engineer with a strong background in data ingestion and processing for large data sets. The ideal candidate is maniacal about data accuracy and has a proven track record of building robust and scalable data pipelines. This role requires an experienced professional who can work collaboratively with cross-functional teams to design and implement solutions that meet business requirements and adhere to data security and privacy policies.
If you are a senior data engineer with a passion for data accuracy and a proven track record of building robust and scalable data pipelines, we encourage you to apply for this role. We offer competitive compensation, a flexible work environment, and opportunities for growth and advancement within the organization.
About Seamless
Seamless delivers the world's best sales leads. Through our product, we help sales teams maximize revenue, increase sales, and easily acquire their total addressable market using artificial intelligence; by development of a robust real-time contact and company search engine as well as a suite of technically-advanced tools to support sales and lead generation. We have been recognized as one of Ohio's fastest growing companies and have been recently ranked No. 7 in LinkedIn's Top 50 Startups of 2022, featured in Forbes as #1 Software company in Ohio in 2022, and on G2's ""Top 100 Highest Satisfaction Products for 2022"" list!
The Seamless Team
We have an amazing culture and work environment that anyone would want to be a part of. We encourage a culture of positivity. We thrive off of continuous feedback and do whatever it takes to help our team and customers be successful. You will grow as an individual, professionally, and be able to see and feel the impact you are making to the growth of Seamless every day.
Role Responsibilities
Design and develop data ingestion and processing pipelines that ensure data accuracy and completeness.
Work closely with cross-functional teams to understand business requirements and design solutions that meet those requirements.
Lead efforts to identify and implement data quality control measures to ensure consistency and reliability of data.
Monitor data pipelines to identify issues and proactively resolve them.
Evaluate and recommend new technologies and techniques to improve data pipeline performance and scalability.
Ensure compliance with data security and privacy policies and procedures.
Mentor junior engineers and provide technical leadership in data engineering best practices.

Candidate Requirements
Bachelor's or Master's degree in Computer Science, Software Engineering, or a related field.
3+ years of experience in data engineering, with a focus on data ingestion and accuracy.
Strong experience with large-scale data processing frameworks such as Hadoop, Spark, or Flink.
Proficiency in programming languages such as Python, Java, or Scala.
Experience with SQL and NoSQL databases.
Familiarity with cloud-based data processing and storage platforms such as AWS, Google Cloud, or Azure.
Strong experience in designing and implementing data quality control measures.
Excellent problem-solving and analytical skills.
Strong communication and collaboration skills.
Experience mentoring and leading teams of data engineers.
Seamless.AI has been delivering the world's best sales leads since 2015. Our product is the first real time, B2B search engine helping sales teams maximize revenue, increase sales, and easily acquire their total addressable market using artificial intelligence. We have been recognized as one of Ohio's fastest growing companies and won 2020 Best Places to Work and LinkedIn's Top 50 Tech Startups in 2020 and 2022. We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. Visa Sponsorship is not included in our hiring package. Applicants will need to be authorized to work in the U.S.","$93,575 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable
"Abile Group, Inc.
5.0",5.0,"Saint Louis, MO",Data Cloud Engineer - Master,"Overview:
Abile Group has an exciting and challenging opportunity for a Data Cloud Engineer-Master on a 10 year contract providing User Facing and Data Center Services supporting an Intelligence Community customer. All the personnel on the team will work together to support innovative design, engineering, procurement, implementation, operations, sustainment and disposal of user facing and data center information technology (IT) services on multiple networks and security domains, at multiple locations worldwide, to support the IC mission.

The right candidate will possess the below skills and qualifications and be ready to handle all responsibilities independently and professionally.
Responsibilities:
Provides technical/management leadership on major tasks or technology assignments.
Establishes goals and plans that meet project objectives. Has domain and expert technical knowledge.
Directs and controls activities for a client, having overall responsibility for financial management, methods, and staffing to ensure that technical requirements are met.
Interactions involve client negotiations and interfacing with senior management.
Decision making and domain knowledge may have a critical impact on overall project implementation.
May supervise others.
Qualifications:
Clearance Required: TS/SCI

Degree and Years of Experience: BS/BA and 10 -15 years of relevant experience

Required Skills:
Experience in the various aspects of hybrid cloud activities.
Supports procurement and deployment of Platform Services to enable application portability across the private and public cloud environments offered by NGA.
Readies NGA's Hybrid Cloud Environment for system migration to IC ITE and oversee the future expansion of NGA Hybrid Cloud to additional public clouds.
In concert with DCS Government, supports standardization of DCS operations in a NGA Hybrid Cloud Management environment.
Transforms Government cloud requirements into appropriate technological alternatives and provides expertise in hybrid virtualization and cloud environments.
Experience developing systems, products, and/or processes based on a total systems perspective.
Consults, plans, analyzes designs, develops tests, assures quality, configures, installs, implements, integrates, maintains, and manages systems.
Has and maintains a diverse set of skills across multiple technical disciplines with recognized expertise in multiple disciplines and possess advanced knowledge of multiple mature and emerging technologies.
Works across organizational boundaries, both internally and externally and helps to drive the relationship between technical solutions and business needs of customers. Analyzes, defines and documents customer needs and required functionality.
Designs, develops and tests theoretical and/or physical models and develops the system design, considering operational impacts, performance, testing, manufacturing, cost and schedule, training, maintenance, and support.
Performs system level design trade analysis, reviews and approves system specifications and description documents, determines how a system is to be built, tested, and implemented, plans the system development execution and ensures adherence to appropriate standards, policies, principles, and practices.
Analyzes system capacity and performance to support problem resolution and system enhancements and monitors systems tests.
Responds to inquiries from a variety of sources for the purpose of providing technical assistance, consultation, advice and support, and regularly provides advice and recommends actions and solutions involving complex issues.

About Abile Group, Inc.:
Abile Group, Inc. was formed in July 2004 to partner with the Intelligence Community and their Contractors in the areas of Enterprise Analytics & Performance Management, IT & Systems Engineering and Program & Project Management. We have significant experience with the Federal Government and are an EDWOSB dedicated to our employees and clients. We are looking for high performing employees who enjoy providing advice and guidance along with solutions development and implementation support, crafted by combining industry best practices with the clients’ subject matter experience and Abile’s breadth of expertise.
EEO Statement:
Abile Group, Inc. is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability. Anyone requiring reasonable accommodations should email careers@abilegroup.com with requested details. A member of the HR team will respond to your request within 2 business days.

Please review our current job openings and apply for the positions you believe may be a fit. If you are not an immediate fit, we will also keep your resume in our database for future opportunities.",#N/A,51 to 200 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"JTEC Consulting
5.0",5.0,"Crystal City, VA",Data Engineer,"Data Engineer
Crystal City, VA 22202

JTEC Consulting LLC focuses on successfully delivering solutions to meet our clients’ most critical needs. Our founding members have decades of experience delivering a wide range of solutions to Air Force and DOD clients. We are a Veteran-Owned Small Business.

Security Clearance Requirement: Current TS/SCI
Location Note: Must reside in Wash DC Metro area and provide on-site support, relocation will be considered



Position Description:
JTEC Consulting is hiring a Mid-Senior level Data Engineer to work in support of a large-scale data analytics platform. This position requires excellent analytical skills and the ability to work directly with the government customer in a dynamic mission driven environment. Qualified candidates will have experience developing solutions for high volume, low latency applications and abilities to operate in a fast-paced environment.

Duties and responsibilities may include (but are not limited to):
Work as part of an enterprise-wide team collaborating with other data engineers, data scientists and product leads to develop innovative solutions for data requirements.
Build pipelines to collect data from disparate external sources.
Implement rules and perform validation and analytics to ensure expected data is received, cleansed, transformed, and in an optimized output format.
Support automation and performance optimization.
Monitor pipeline status and performance.
Support troubleshooting and issue resolution.
Other duties as assigned.
Education and qualifications include (but are not limited to):
U.S. citizenship
Current TS/SCI security clearance
Understanding of distributed computer systems
Experience working in big data environments and understanding of data challenges (DoD command experience preferred)
Experience working in cloud environments
4+ years of experience in the following areas:
Experience with modern programming languages such as Java and Spark
Experience using SQL and ETL
Experience working in a distributed environment using Apache Kafka and Apache Airflow
Experience with stream processing using Apache Flink or StreamSets
Experience working in an Agile development environment is preferred
Proficiency working across MS Office Suite including advanced Excel skills
Experience working with data visualization tools and dashboards is a plus
Effective written and verbal communication skills to work in a collaborative environment with government and contractor teams
Availability to work on-site
JTEC Consulting LLC provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.

Salary:
JTEC offers competitive compensation and benefits to all employees. The estimated salary range displayed in this posting is dependent upon position requirements, individual qualifications, education, experience, location, and other job related factors. Our recruiting team will review candidates' related experience and salary targets during the evaluation process. All qualified candidates will be considered.","$145,000 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Talent Group
2.3",2.3,"Austin, TX",data engineer with ML,"Required Skills :
A background in computer science, engineering, mathematics, or similar quantitative field with a minimum of 2 years professional experience .
Experience in implementing data pipelines using python.
Experience with workflow scheduling / orchestration such as Kubernetes, Airflow or Oozie.
Extract Transform Load (ETL) experience using Spark, Kafka, Hadoop, or similar technologies.
Experience with query APIs using JSON, Protocol Buffers, or XML.
Experience with Unix-based command line interface and Bash scripts
Job Type: Full-time
Salary: Up to $130,000.00 per year
Benefits:
401(k)
Experience level:
11+ years
Ability to commute/relocate:
Austin, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$130,000 /yr (est.)",1 to 50 Employees,Company - Public,Media & Communication,Broadcast Media,#N/A,Less than $1 million (USD)
"Mastery Logistics Systems, Inc
3.9",3.9,"Omaha, NE",Senior Data Engineer (Kafka),"About the Role
In the world of transportation, data is constantly moving, and Kafka is the roadway that keeps that traffic running smoothly to its destination. As a technical expert, you must be comfortable working across teams on multiple, high impact projects. You will be a valued part of a team that is constantly maturing Kafka use and event-driven architecture. Members of this team are responsible for the overall use and implementation of Kafka components including the Confluent platform, observability, governance, best practices, and solution development. An understanding of Kafka principles and enterprise integration patterns is required.
In order to be successful:
You are a self-directed person who can identify priorities.
You are a detail-oriented person who takes pride in keeping data correct and always having a backup plan.
You are a problem-solver who might write a script or find a tool to get things done when there isn't an established solution.
You want to learn and grow in the event-driven world.
You love Kafka! When you hear terms like ""event-driven"" or ""real-time streaming"" you're ready ready to dive in!
Responsibilities
Lead a team of Kafka engineers in an operational capacity
Develop and implement solutions using Kafka.
Administer and improve use of Kafka across the organization including Kafka Connect, ksqlDB, Streams, and custom implementations.
Work with multiple teams to ensure best use of Kafka and data-safe event streaming.
Understand and apply event-driven architecture patterns and Kafka best practices. Enable development teams to do the same.
Assist developers in choosing correct patterns, event modeling, and ensuring data integrity.
Continuous learning to be a Confluent/Kafka subject matter expert.
Work with Kafka and Confluent API's (e.g. metadata, metrics, admin) to provide pro-active insights and automation.
Work with SRE's to ensure Kafka-related metrics are exported to New Relic.
Perform regular reviews of performance data to ensure efficiency and resiliency.
Contribute regularly to event-driven patterns, best practices, and guidance.
Review feature release and change logs for Kafka, Confluent, and other related components to ensure best use of these systems across the organization.
Work with lead to ensure all teams are aware of technology changes and impact.
Acquire a deep understanding of source and sink connector technical details for a variety of platforms including PostgreSQL, MS SQL Server, Snowflake, and others as required.
Requirements
Be able to describe the primary components of Kafka and their function (brokers, zookeeper, topics).
At least two years of experience supporting applications in a production environment.
You will be expected to read and navigate code in multiple languages. Multi-language fluency and writing is not required.
Experience in a microservice architecture
Experience with event driven architecture
Proficiency in at least one programming language and one scripting language.
Proficiency with Docker containers.
Ability to participate in and contribute to code management in Github including actively collaborating in peer-reviews, feature branches, and resolving conflicts and commits.
Excellent written and verbal communication skills.
Strong sense of responsibility with a bias towards action.
Comfortable self-directing and prioritizing your own work.
Microservices experience is a plus.
Distributed tracing experience a plus.
An understanding of any cloud (Azure preferred) infrastructure and components is a plus, but is not required.
Create reference solutions.","$96,493 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
United Digestive,#N/A,"Atlanta, GA",Data Engineer,"GENERAL SUMMARY OF DUTIES: United Digestive’s Data Engineer will be responsible for developing and maintaining data pipelines, data warehouses, and data models to support the company's Business Intelligence reporting initiatives.
REPORTS TO: Director of Business Intelligence & Analytics
RESPONSIBILITIES:
Duties may include but are not limited to:
Implement, maintain, and improve data integration processes to collect, process, and manage large datasets
Collaborate with stakeholders to understand business requirements and design data solutions that meet their needs
Implement and maintain data quality checks and validations to ensure data accuracy and completeness
Optimize data processing and storage to ensure high availability, scalability, and performance
Develop and maintain automated data workflows and scheduling tools
Code ETL process using Azure Data Factory workflows and other technologies as appropriate
Troubleshoot and resolve data-related issues and incidents
Stay up to date with the latest technologies and trends in data engineering, specifically the Azure suite, and recommend improvements and optimizations to the data architecture and infrastructure
Ability to develop solutions and manage existing Azure DevOps pipelines
Maintains a high level of confidentiality with sensitive information
Performs any other duties and/or special tasks as assigned by United Digestive leadership
REQUIRED EDUCATION, SKILLS & EXPERIENCE:
Bachelor’s degree in Computer Science, Software Engineering, Information Technology, or in a related field required; healthcare-related Data Analytics experience preferred; 2+ years of data pipeline management required. Proficiency in SQL and experience with relational databases (e.g., Oracle, SQL Server, PostgreSQL); experience with data modeling, data warehousing, and ETL processes, proficiency in various programming language (e.g., Python, SQL, Java), experience with cloud platforms (e.g., AWS, Azure, GCP) and big data technologies (e.g., Databricks, Hadoop, Spark, Kafka, Cassandra). Must have knowledge of data visualization and BI tools (e.g., Tableau, Power BI) and how to support these tools in an efficient manner.
ADDITIONAL SKILLS AND EXPERIENCE:
Data Engineer must have:
Excellent verbal and written communication skills.
Excellent organizational skills and attention to detail.
Ability to multi-task and prioritize job needs
Ability to deal with patients, visitors, co-workers, and physicians with courtesy and respect
Work under pressure; assess, respond to, and communicate issues in a timely manner.
Self-motivation and be a proactive individual with a professional can-do attitude
PHYSICAL/MENTAL/ENVIRONMENTAL DEMANDS
Requires sitting and standing associated with a normal office environment. Travel as business needs dictate.
DRUG FREE WORKPLACE
United Digestive is a drug free workplace. All offers of employment are contingent upon passing a pre-employment drug screening.
EQUAL OPPORTUNITY EMPLOYER
United Digestive is an Equal Opportunity Employer and does not discriminate on the basis of race, religion, gender, color, or national origin in its employment practices.
Location: 1355 Peachtree Street NE STE 1600, Atlanta, GA 30309
Job Type: Full-time",#N/A,Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Point Predictive, Inc.",#N/A,"San Diego, CA",Senior Data Engineer,"Senior Data Engineer, San Diego, CA based OR willing to relocate to San Diego in 60 Days.
Company
PointPredictive is a fast-growing technology start-up that leverages a patented combination of artificial and natural intelligence [Ai+Ni] to provide risk assessments in the auto lending, mortgage, and retail space. The platform has been proven to reduce lender loan losses by 40-60% with review rates of 5-10% of their applications, resulting in higher productivity of lender risk management departments, significantly lower losses to their bottom lines, and improved customer experience. The company was founded in 2013 by a seasoned team of technology entrepreneurs with over 20 years of experience in the startup space (including several acquisitions) and has financial backing from top tier investors.
Role:
The company is looking for an outstanding Senior Data Engineer to focus on its Data Asset, scale the Database and Data Asset for high performance and reliability. You will also serve as an engineering resource for data related questions, issues, and bugs. Core skills include Python, Snowflake, database systems and SQL, and Amazon Web Services (AWS).
Responsibilities:
· Developing new extract-transform-load (ETL) processes and pipelines. Must be able to manage large volumes of data flowing in from a variety of formats and into a variety of location.
· Identifying, designing and implementing internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes. Automation experience highly desired.
Write complex SQL; be fluent in relational based systems; have strong analytical and problem-solving skills.
Ability to represent complex algorithms in software; bring strong understanding of database technologies, management systems, data structures, and algorithms; a deep understanding in database architecture testing methodology.
Develop and execute test plan, debugging, and testing scripts and tools.
Building real-time streaming data pipelines; building and deploying data pipelines, data streams, and extract-transform-load (ETL) processes.
Manage Data Governance and Data Cleansing, as well as supporting production issues and customer requests.
Provide engineering support to customer issues and bugs. Research and implement fixes.
About you:
You have 5+ years of relevant software development industry experience building and operating scalable, fault-tolerant, distributed systems.
Database and software development experience with Python, SQL, Redshift, and experience with Amazon Web Services, Snowflake along with pub/sub streaming technologies like Kafka, Kinesis, Spark Streaming, etc.
Experience with container services.
Fluid with Amazon Web Services.
Experience with concurrency, multithreading, and the deployment of distributed system architectures.
Experience leading and shipping large scope technical projects in collaboration with multiple experienced engineers.
You have excellent communication skills and the ability to work well within a team and across engineering teams.
You are a strong problem solver and have solid production debugging skills.
You Thrive in a fast-paced environment and see yourself as a partner with the business with the shared goal of moving the business forward.
You have a high level of responsibility, ownership, and accountability.
Job Type: Full-time
Competitive pay, bonus, equity, and benefits:
Benefits:
401(k)
Health insurance
Dental insurance
Flexible spending account
Life insurance
Paid time off
Vision insurance
Schedule:
Monday to Friday
Supplemental pay types:
Bonus pay
Education:
Bachelor's or Master’s (Preferred)
Work Location: San Diego (Del Mar)
Job Type: Full-time
Pay: $105,000.00 - $160,000.00 per year
Benefits:
401(k)
Dental insurance
Flexible spending account
Health insurance
Life insurance
Paid time off
Vision insurance
Compensation package:
Bonus pay
Stock options
Yearly pay
Experience level:
5 years
Schedule:
Monday to Friday
Application Question(s):
Are you willing to be in the office 5 days a week to influence culture and capabilities ?
Are you willing to move to San Diego within 60 days if not within the commuting region ?
Work Location: Hybrid remote in San Diego, CA 92130","$132,500 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Financial Information Technologies LLC
3.7",3.7,"Tampa, FL",Senior Data Engineer,"Join Fintech as a Senior Data Engineer!
Fintech is the leading business solutions provider for the beverage alcohol industry, empowering alcohol suppliers, distributors, and retailers with smart solutions that simplify beverage alcohol management. Our unique, thriving company culture promotes collaboration and growth at every level, and our comprehensive employee benefits have earned Fintech the title of a Tampa Bay Times Top 100 Workplaces for 2020 and 2021.
Fintech’s Senior Data Engineer brings a depth of relational database modeling and an understanding of transactional processing across a myriad of database types. They can analyze and assess new data sets to understand nuances of content in the context of purpose with an ability to conceptualize cleansing, harmonization, and modeling efforts. Working under the direction of the principal process architect the senior data engineer will lead a small team of experienced data wranglers to tackle a myriad of ad-hoc custom projects as well as service the development needs within our warehouse and app abstraction layers.
Essential Functions:
Collaborates with ELT/process automation, data insights, and data science teams
Builds data models in accordance with prescribed methodologies
Serves as knowledgeable backstop for level III ticket resolution
Guides and instructs junior developers and engineers on how to implement directives in accordance with project needs within adopted framework
Gains a familiarity with and contributes to the core meta-data driven data processing engine
Advising on data model consumption in analytics layers
Contributes to knowledge base
Qualifications:
8 + years of experience with SQL in multiple database flavors (SQL Server, Oracle, Snowflake, Postgres, Greenplum)
5 + years of experience with data ingest transformations and harmonization
5 + years of experience with database object creation and modeling
Analytical thinker that can adapt and problem solve in a fast-paced environment
Team oriented
Must be able to consume, understand, and implement a complicated but flexible processing back-end in a short time frame
Our Benefits:
Employer Matched 401K (Up to 10% of Employee Salary)
Company Paid Medical Insurance Option (Employee and Dependent Children)
Company Paid Dental Insurance Option (Employee only)
Company Paid Vision Insurance Option (Employee only)
Company Paid Long and Short-Term Disability
Company Paid Life and AD&D Insurance
Employee Recognition Program
18 PTO Days a Year
Six Paid Holidays
Business Casual Dress Code
Check out www.fintech.com for more information!
We E-Verify.
Fintech is an Equal Opportunity Employer that does not discriminate on the basis of actual or perceived race, color, creed, religion, national origin, ancestry, citizenship status, age, sex or gender (including pregnancy, childbirth and pregnancy-related conditions), gender identity or expression (including transgender status), sexual orientation, marital status, military service and veteran status, physical or mental disability, genetic information, or any other characteristic protected by applicable federal, state, or local laws and ordinances. Fintech’s management team is dedicated to this policy with respect to recruitment, hiring, placement, promotion, transfer, training, compensation, benefits, employee activities, access to facilities and programs and general treatment during employment.
Fintech is a Drug-Free Workplace.","$109,213 /yr (est.)",51 to 200 Employees,Company - Private,Financial Services,Financial Transaction Processing,1991,$25 to $100 million (USD)
"Unilever
4.1",4.1,Remote,Senior Data Engineer (Multiple Openings),"We have multiple Data Engineer roles open across the US to be located preferably in Eastern Standard Timezone; near our HQ office in New Jersey; or near satellite offices in Rogers, AR; Minneapolis, MN and Cincinnati, OH.
Background & Purpose of the Job
The North America Data Engineer will work with peer data engineers to develop data solutions to support product analytics managers and data scientists. This role will be partnering with internal data scientists, product managers and external stakeholders across the business to deliver advanced analytics. The ideal candidate is an experienced data engineer with vast hands-on experience and leadership experience in leading projects and teams from the ground up.
Who You Are & What You’ll Do
Facilitate the team’s delivery of scalable, maintainable, performant data engineering solutions. Work through your teams to define/design solution options, evaluate technical feasibility, and provide estimates on effort and risk. Drive the design and implementation of new data projects and the optimization of existing solutions. Work with architecture and engineers to ensure quality solutions are implemented, and engineering best practices are defined and followed. Drive collaborative reviews of designs, code, and test plans. Define the team’s technical roadmap and influence its adoption. Anticipate, identify and solve issues concerning data management to improve data quality. Work across teams to resolve operational & performance issues. Identify and remove technical bottlenecks for your engineering teams.
In this role you’ll be working closely with passionate and driven Product Management team, Architects, Engineers, Data Analysts, Internal partners and other departments. They will span across various functional groups across the enterprise. You will be in contact with a wide range of great people. In all interactions, you will find success in teamwork, a positive attitude, and hard work. As a Data Engineer, in partnership with diverse team of smart, intelligent, and creative people. This allows us to be a driving force for building first-class solutions for North America Data & Analytics team and its business partners, working on development projects related to customer development, supply chain, commerce, consumer behavior and web analytics among others.
You’re a dot connector and storyteller: You like to unravel complex data, - building analytical tools that utilize the data pipeline to provide actionable insights, operational efficiency, and other key metrics.
You’re a changemaker: You are a self-starter you can work independently to identify, design and implement internal process improvement; driving change with new innovative solutions, optimizing data delivery, re-designing for greater scalability.
You’re a paradox navigator: Enable data driven decision-making across customer development, supply chain, consumer marketing insights, and disrupting data silos.
You’re a culture & change champion: Be an advocate and set an example of digital cultural.
You love to win, and have fun doing it: Transform the way Unilever operates with a proactive, informed risk-taking, winning attitude!
What You Will Bring
Bachelor’s Degree in Computer Science, Information Systems, Business or related field or equivalent combination of education and experience with 3+ years of experience in large-scale software development and data engineering
Ability to define/design solution options, evaluate technical feasibility, and provide estimates on effort and risk
Experience working with data sources such as IRI, Nielsen, Retailer POS systems, Panel data such as Numerator and Nielsen/IRI Panel required, in support of retail such as Walmart, Target, Amazon, Dollar General, Kroger a huge plus!
Solid foundation in data structures, algorithms, and architecture patterns
Experience building distributed / cloud scalable, high-performance data solutions
Experience with batch processing frameworks and programming in tools such as Python, SQL, Spark and Hive
Experience with messaging/streaming/complex event processing tooling and frameworks with an emphasis on Spark Streaming or Structured Streaming or Apache Nifi
Experience with data warehousing related concepts - e.g. SQL and SQL Analytical functions
Experience in Agile/Scrum application development
Familiarity with the principles of Domain Driven Design
The following skills and experience are also relevant to our overall environment, and nice to have:
Experience working in a public cloud environment, particularly Microsoft Azure
Experience building RESTful API’s to enable data consumption
Experience in developing Power BI dashboards using DAX, Power Automate flows, Web Applications
Experience with practices like Continuous Development, Continuous Integration and Automated Testing
Pay: The pay range for this position is $83,200 to $124,700. Unilever takes into consideration a wide range of factors that are utilized in making compensation decisions including, but not limited to, skill sets, experience and training, licensure and certifications, qualifications and education, and other business and organizational needs.
Bonus: This position is bonus eligible.
Long-Term Incentive (LTI): This position is LTI eligible.
Benefits: Unilever employees are eligible to participate in our benefits plan. Should the employee choose to participate, they can choose from a range of benefits to include, but is not limited to, health insurance (including prescription drug, dental, and vision coverage), retirement savings benefits, life insurance and disability benefits, parental leave, sick leave, paid vacation and holidays, as well as access to numerous voluntary benefits. Any coverages for health insurance and retirement benefits will be in accordance with the terms and conditions of the applicable plans and associated governing plan documents.
-
Unilever is an organization committed to diversity and inclusion to drive our business results and create a better future every day for our diverse employees, global consumers, partners, and communities. We believe a diverse workforce allows us to match our growth ambitions and drive inclusion across the business. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, age, national origin, or protected veteran status and will not be discriminated against on the basis of disability.

Employment is subject to verification of pre-screening tests, which may include drug screening, background check, credit check and DMV check.

If you are an individual with a disability in need of assistance at any time during our recruitment process, please contact us at
NA.Accommodations@unilever.com
. Please note: This email is reserved for individuals with disabilities in need of assistance and is not a means of inquiry about positions or application statuses.
#LI-Remote","$103,950 /yr (est.)",10000+ Employees,Company - Public,Manufacturing,Consumer Product Manufacturing,1872,$10+ billion (USD)
"Shutterfly
3.3",3.3,"Eden Prairie, MN",Senior Data Engineer,"At Shutterfly, we make life’s experiences unforgettable. We believe there is extraordinary power in the self-expression. That’s why our family of brands helps customers create products and capture moments that reflect who they uniquely are.

As part of the Data Engineering team you will tackle the scalability, performance and distributed computing challenges needed to collect, process and store data for Shutterfly. You will enhance the Data Warehouse at Shutterfly on AWS, supporting the breadth and depth of the company’s analytic needs.
What You'll Do as a Sr. Data Engineer II
Build design, develop, test, deploy, maintain and enhance full-stack data engineering solutions
Provide technical leadership to the Data Warehouse and Shutterfly teams who use Shutterfly’s Enterprise Data Warehouse
With your technical expertise own and manage project priorities, deadlines and deliverables
Evangelize the benefits of existing solutions and new technologies to drive the use and push the technology of the Data Warehouse forward
Work closely with Data Operations to improve CI/CD pipelines am improve the operation and performance of the Data Warehouse
The Skills You'll Bring
Expert knowledge Python, Spark, SQL scripting and Java
7+ years of hands-on experience with building data ingress frameworks and experience with dealing with a wide variety of source data formats
Hands-on experience in building data applications, including design, implementation, debugging, and support.
Supporting a diverse and inclusive workforce is important to Shutterfly not only because it directly reflects our value of Embracing our Differences, but also because it’s the right thing to do for our business and for our people. Learn more about our commitment to Diversity, Equity and Inclusion at Shutterfly DE&I.","$132,354 /yr (est.)",10000+ Employees,Company - Private,Information Technology,Internet & Web Services,1999,$1 to $5 billion (USD)
"Apolis
3.9",3.9,"Saint Louis, MO",Data Engineer (Python Focused),"Role: Data Engineer (Python Focused) (W2 ONLY)
Location : St. Louis, MO 63102
Duration: 18 Months
The primary responsibilities of this role will be to work as a part of the data and analytics team of the client. The ideal candidate would have experience using ETL and other data engineering tools in a cloud environment. Additionally, this candidate would have coding and scripting experience creating code to move and manipulate data using tools such in Python such as Spark or Pandas. Microsoft Azure experience is a plus but relevant experience with other cloud-based environments will be considered.
This team uses Microsoft Azure to build and productionize data sets and processes to the product owners and product users and this role will be expected to contribute to those objectives. As a member of this team, you will work in conjunction with other data engineers, platform engineers, solution developers and others to execute and deliver working solutions to the end users.
Technical Skills
High focus on Python
More Programmer/Coder experience, less Cloud experience needed.
Must have experience with data analytics/engineering and cloud technologies and an in-depth working knowledge and understanding of data engineering, design, and integration.
Intermediate or higher level of proficiency with Python Must have the proven ability to learn quickly and apply cloud data engineering principles and skills to new tools and use cases.
Ability to define and take technical ownership of detailed requirements and collaborate effectively with various team members such as Business Analysts and Project Managers and end users.
Excellent communication (verbal and written) and problem-solving skills are essential for this position.
Initiative, Proactive Cooperation, and curiosity are mandatory behavioral skills required for this position.
Experience with several of the following technologies is preferred:
Experience with Microsoft Azure Platform. In particular, the data processing toolsets including Azure Data Factory Pipelines, Azure Databricks, Azure Data Lake Storage.
Experience using Notebooks (Such as Jupyter, DataBricks) also desired.
Experience using code languages such as Python and C#
Experience using query languages such as SQL, MDX and DAX
Experience using scripting languages such as PowerShell, M-Query (Power Query), and Windows batch commands.
Understanding of Agile and DevOps principles for software development and ownership.
Familiar with compliance and controls for software development and deployment.
Education Required
A bachelor's in computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field [or equivalent work experience] is not required but is preferred.
Job Type: Contract
Salary: $60.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Vision insurance
Compensation package:
Overtime pay
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
St. Louis, MO: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Preferred)
ETL: 5 years (Preferred)
Cloud infrastructure: 4 years (Preferred)
Work Location: One location",$65.00 /hr (est.),501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,1996,$25 to $100 million (USD)
"DSFederal Inc
4.0",4.0,Remote,Data Engineer*,"Description:
We are seeking an experienced Data Engineer to design, build, and maintain our government client’s data infrastructure. The Data Engineer will work closely with cross-functional teams to develop scalable data solutions that support our client’s business needs.
Requirements:
Design, develop, and maintain data pipelines and data storage systems.
Collaborate with cross-functional teams to design and implement data-driven solutions.
Optimize and tune data systems for performance and scalability.
Implement and maintain data quality and validation processes.
Troubleshoot and debug data-related issues.
Participate in code reviews and contribute to best practices for data engineering.
Technical Skills Required:
Proficiency in one or more programming languages (Python, Java, etc.)
Experience with data modeling, database design, data marts, and data warehousing concepts
Knowledge of ETL tools and techniques
Experience with cloud-based data platforms such as AWS or Azure
Strong problem-solving and analytical skills
Desired Skills:
Familiarity with data visualization tools such as Tableau or Power BI.
Knowledge of machine learning concepts and frameworks.
Experience Required:
3+ years of experience in data engineering or related field
Education Required:
Bachelor's in engineering

Who We Are:
DSFederal is a leader in health IT and data analytics that delivers innovative solutions to improve healthcare delivery and address critical challenges, including cancer surveillance, HIV/AIDS, maternal and child health, and COVID-19 emergency response. Our highly experienced professionals serve over 50 different federal clients on over 80 projects. Our people are united by one mission – to improve human life through transformative solutions. www.dsfederal.com.
What We Can Offer You:
DSFederal attracts the best people in the business with our competitive benefits package that includes medical, dental and vision coverage, 401k plan with employer contribution, paid holidays, vacation, and more. If you enjoy being a part of a high performing, professional services, and technology focused organization, we encourage you to apply!
Our Mission: Improve human life through transformative solutions.
Our Core Values: Integrity, People First, Disciplined Business Practices, Customer Centric, Value Creation.
EEO Statement: DS Federal is an Equal Opportunity / Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, or protected veteran status.
E-Verify Statement
DS Federal utilizes the E-Verify program for every potential new hire. This makes it possible for us to confirm that every employee who works for DS Federal is eligible to work in the United States. To learn more about E-Verify you can call 1-800-255-7688 or visit their website by clicking the logo below. E-Verify® is a registered trademark of the United States Department of Homeland Security.
#LI-LP1",#N/A,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$25 to $100 million (USD)
United Digestive,#N/A,"Atlanta, GA",Data Engineer,"GENERAL SUMMARY OF DUTIES: United Digestive’s Data Engineer will be responsible for developing and maintaining data pipelines, data warehouses, and data models to support the company's Business Intelligence reporting initiatives.
REPORTS TO: Director of Business Intelligence & Analytics
RESPONSIBILITIES:
Duties may include but are not limited to:
Implement, maintain, and improve data integration processes to collect, process, and manage large datasets
Collaborate with stakeholders to understand business requirements and design data solutions that meet their needs
Implement and maintain data quality checks and validations to ensure data accuracy and completeness
Optimize data processing and storage to ensure high availability, scalability, and performance
Develop and maintain automated data workflows and scheduling tools
Code ETL process using Azure Data Factory workflows and other technologies as appropriate
Troubleshoot and resolve data-related issues and incidents
Stay up to date with the latest technologies and trends in data engineering, specifically the Azure suite, and recommend improvements and optimizations to the data architecture and infrastructure
Ability to develop solutions and manage existing Azure DevOps pipelines
Maintains a high level of confidentiality with sensitive information
Performs any other duties and/or special tasks as assigned by United Digestive leadership
REQUIRED EDUCATION, SKILLS & EXPERIENCE:
Bachelor’s degree in Computer Science, Software Engineering, Information Technology, or in a related field required; healthcare-related Data Analytics experience preferred; 2+ years of data pipeline management required. Proficiency in SQL and experience with relational databases (e.g., Oracle, SQL Server, PostgreSQL); experience with data modeling, data warehousing, and ETL processes, proficiency in various programming language (e.g., Python, SQL, Java), experience with cloud platforms (e.g., AWS, Azure, GCP) and big data technologies (e.g., Databricks, Hadoop, Spark, Kafka, Cassandra). Must have knowledge of data visualization and BI tools (e.g., Tableau, Power BI) and how to support these tools in an efficient manner.
ADDITIONAL SKILLS AND EXPERIENCE:
Data Engineer must have:
Excellent verbal and written communication skills.
Excellent organizational skills and attention to detail.
Ability to multi-task and prioritize job needs
Ability to deal with patients, visitors, co-workers, and physicians with courtesy and respect
Work under pressure; assess, respond to, and communicate issues in a timely manner.
Self-motivation and be a proactive individual with a professional can-do attitude
PHYSICAL/MENTAL/ENVIRONMENTAL DEMANDS
Requires sitting and standing associated with a normal office environment. Travel as business needs dictate.
DRUG FREE WORKPLACE
United Digestive is a drug free workplace. All offers of employment are contingent upon passing a pre-employment drug screening.
EQUAL OPPORTUNITY EMPLOYER
United Digestive is an Equal Opportunity Employer and does not discriminate on the basis of race, religion, gender, color, or national origin in its employment practices.
Location: 1355 Peachtree Street NE STE 1600, Atlanta, GA 30309
Job Type: Full-time",#N/A,Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Purpose Financial
4.2",4.2,Remote,Data Engineer,"Address : 322 Rhett St, Greenville, South Carolina, United States - 29601
Purpose Financial, Inc. is an innovative consumer financial services company that offers a diverse suite of credit products, promoting financial inclusion and meeting consumers wherever they are. Through its brands, the company is committed to helping customers achieve their version of financial stability in the moment and in the future. Since 1997, Purpose Financial has been a pioneer in the consumer credit and financial services market offering money solutions in over 1,000 storefronts locations and online lending. Providing services in over 20 states, Purpose Financial employs over 2,700 team members.
At Purpose Financial, our people are our greatest asset. We are inclusive and encourage different points of view. We all belong here. We respect our colleagues. We value our customers, and we support our communities.
What do we do? Our sole purpose is to help each customer achieve their version of financial stability in the moment and in the future.
We have a history! Consumers know that when they choose Purpose, they’re backed by a trusted, reliable financial services company with 25 years in the business, 2,700+ employees, 1,000+ store locations, and operating in 24 states.
Benefits and Perks. We believe our employee benefits should make life better. Purpose Financial offers a comprehensive benefits package to everyone on our team.
Health & Welfare Benefits. When you take care of yourself, you’re a better you – which is better for everyone!
Health Savings & Flexible Spending Accounts. Pre-tax money for your family’s out-of-pocket expenses.
401k with Matching Contributions. Think bigger! You can build a better future.
Wellness & Employee Assistance Programs. Take care of yourself in every way that matters.
Generous Vacation Policy & Paid Time Off. Time off to take care of yourself, your family – and your bucket list!
Volunteer Days. We want you to care. Always. Take 2 days off each year to volunteer and make a difference in your community.
Tuition Reimbursement. Pursue your goals without fretting over all the costs.
Employee Discounts. Save on home and auto insurance, cell phone service, entertainment, shopping, pet insurance and even travel.
Performance Incentives & Awards. We want you to feel valued for all your contributions.
Hybrid Workforce Support. We believe in getting it done – in whatever way works best for you and your team.
Want to meet our executive team? Visit our home page at www.havepurpose.com.
Position Summary
The job function is responsible for building a scalable data infrastructure. The role will be part of the effort to enable Purpose Financial to democratize data by developing self-serving and curated data models for our single source of truth. The job function includes understanding of business requirements, defining data requirements, identifying enterprise data sources, planning, designing, and building data acquisition from sources, defining data model, designing data ingestion pipelines, data normalization, data transformation, and consumption using platforms such as Snowflake and AWS.
Job Responsibility
Take ownership of building/optimizing data pipelines to increase efficiency and performance
Develop and maintain data ingestion and integration processes that focus on data quality and monitoring
Use AWS and Snowflake tools such as Glue, Lambda, Snowpark, and Snowpipe to build data ingestion pipelines from various sources
Incorporate standards and best practices into data pipeline solutions
Participate in troubleshooting and problem-solving design discussions
Provide logical and physical data design, and database modeling
Solve complex data issues around data integration, unusable data elements, unstructured data sets, and other data processing incidents
Continuously design the future state of our data & reporting process by collaborating with IT and business partners.
Partner with enterprise teams, data scientist, architects to define development standards and solutions.
Manage cloud infrastructure as code (IaC), using Terraform.
Work closely with other members of the data team to better design and build metrics that enhance our analytical capabilities
Manage workflows and data build processes that help the BI team build key business metrics and design dashboard visualizations
Assess the effectiveness and accuracy of new data sources, data points, and data gathering techniques
Troubleshoot problems with data (missing, corrupted, invalid) and build data recovery strategy
Apply best practices for AWS & Snowflake architecture, ELT and data models
Be a leader for our data journey, act as SME for data and database related topics, and assist with development of data lakes and warehouses
Education Required
Bachelor’s degree (or foreign equivalent degree) in any Computer, Engineering, or other technical field.
Experience Required
5+ years experience in data warehousing
3+ years of experience in data architecture and design, with a good understanding of various data systems and structures (i.e. SQL, NoSQL, Key-Value, Streams)
3+ years of experience with building data pipelines and using data orchestration tools and implementing end to end Monitoring & Alerting solutions
3+ years of experience working with AWS or public cloud provider.
Strong understanding of SQL, data structures, data integrity, and schema design best practices
Programming experience required, preferably Python.
A strong advocate of data governance and data quality
Experience with database internals, database design, SQL and database programming
Experience in financial services (Banking and consumer lending preferred)
Experience of authoring CI/CD pipelines, automation elements related to infrastructure composition, deployment orchestration, and monitoring.
Familiarity and experience with Agile/SCRUM based development methodology.
Knowledge Required
Excellent written and verbal communications skills as would be needed to communicate in person, by phone, and through email; adaptability and flexibility to changing environment; and comfortable working in a dynamic, high volume, fast-paced environment. Ability to read, write, evaluate, and apply information. Ability to interact professionally and exhibit appropriate social skills. Ability to understand and ensure compliance with policies, procedures, and laws governing our industry/business and products. Ability to develop and maintain business relationships.
Physical Requirements
Sitting for long periods of time; standing occasionally; walking; bending; squatting; kneeling; pushing/pulling; reaching; twisting; frequent lifting of less than 10 lbs., occasional lifting of up to 20 lbs.; driving and having access during the workday to an insured and reliable transportation; typing; data entry; grasping; transferring items between hands and/or to another person or receptacle; use of office equipment to include computers; ability to travel to, be physically present at, and complete the physical requirements of the position at any assigned location.
Competencies
Integrity
Customer Advocacy
Interpersonal Skills
Results/Accountability
Inclusiveness
Technical Proficiency/Leadership
Big Data Trends
Data Management
Adaptability
Travel - None
Attire - Business Casual
Other - Must be eligible to work in the USA and able to pass a background check.
Job Type: Full-time
Compensation package:
Yearly pay
Experience level:
5 years
Schedule:
Day shift
Work Location: Remote",#N/A,Unknown,Company - Private,Financial Services,Financial Transaction Processing,#N/A,Unknown / Non-Applicable
"NexTek, LLC",#N/A,"Shoreview, MN",Sr. Data Engineer,"Nextek, LLC is a software development company in the financial services industry that creates web applications and solutions for financial services professionals. We are seeking a motivated Senior Data Engineer to join the team.
What we provide:
A laptop and other equipment/software necessary for work
Health Insurance, Dental Insurance, Vision Insurance
Paid time off (PTO), Volunteer time off (VTO)
Hybrid remote work options
401K
Life Insurance
Flexible schedules
Monthly Social Activities
Casual dress code
Professional development
Here is what we are looking for:
Experience in database administration, security, backup, and recovery
Preferred Knowledge:4+ years of experience in data/software engineering
Experience in the financial services industry
Experience with a variety of relational databases (particularly MySQL & MSSQL)
Experience with collecting and presenting analytics
Here is what you will be doing:
Display proficiency in relevant technical skills.
Maintain a deep understanding of the data structure.
Provide evaluations and recommendations for our data environment based on potential issues, tradeoffs, and risks.
Scope and implement solutions.
Make good estimates and set realistic expectations for delivery.
Ask for guidance when necessary.
Maintain and monitor the health of databases and associated solutions.
Make use of the same environments setup by the Software Engineering team
Thoroughly test code for bugs and security issues before deploying to production environments
Maintain and monitor actively deployed solutions
Follow and help define best practices
Peer reviews
Maintain awareness of industry trends
Share experience and insights to help your team grow
Increase understanding of impact on other teams
Increase understanding of the customer
All other duties as assigned
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Shoreview, MN: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Everybody Votes Campaign,#N/A,United States,Data Engineer,"ABOUT EVERYBODY VOTES CAMPAIGN
Everybody Votes Campaign (EVC) is a national non-partisan, not-for-profit hiring staff for a large-scale coordinated civic engagement campaign active through the 2024 election cycle. The campaign aims to create a more representative democracy by registering millions of underrepresented voters across the country. This effort focuses on voter registration in a targeted fashion by conducting at-scale, effective, efficient, metrics-driven registration work. Through this work, we seek to fundamentally change the make-up of the electorate and to increase the political power of traditionally underrepresented communities in our democracy.
We directly fund organizations who execute voter registration and run quality control operations. We are dedicated to being active participants with the organizations to ensure their programs are effective and promote an investment in the future of emerging communities.


ABOUT THE OPPORTUNITY
EVC seeks a Data Engineer to implement, maintain, and optimize voter registration data pipelines. The Data Engineer will be responsible for large scale reporting automation with a particular focus on standardizing complicated voter registration data across multiple databases, working closely on a multidisciplinary team of engineers, analysts, project managers, and field practitioners. This individual will be at the forefront of the campaign’s effort to develop advanced voter registration dashboards and pipeline solutions, working closely with partner organizations. The Data Engineer will report to the Director of Data Products.

WHAT YOU WILL DO IN YOUR ROLE

Centralize data from disparate sources across multiple databases and use innovative hygiene solutions to clean up traditionally complicated voter registration data
Optimize and automate the campaign’s voter registration data pipeline, integrate new external data sources, and help our data team automate and streamline manual reporting processes
Set best practices for data standardization and refinement
Participate in maintaining a well-documented, consistent codebase
Work as a team: pair-program, review code, co-design and plan, develop a shared vision for and an understanding of the work, document progress in JIRA and Confluence
This is a great opportunity for someone who:

Enjoys coming up with creative solutions to big questions through collaboration, and is able to use immediate challenges as windows into future opportunities.
Notices and fixes errors that others might overlook. Acknowledges mistakes and turns them into learning opportunities. Has a track record of leaving things better than they found them – simplified pipelines, strong documentation, code sharing discipline.
Plans ahead and finds alternative paths, when needed, to get to the finish line. Bounces back from setbacks and rejections. Holds a high bar when things are hectic.
Brings civic engagement experience working for or with groups that serve communities of color.
Stays ahead of the curve in an ever-changing technology environment.
Identifies decisions, policies, or practices that have disparate impacts based on identity. Is driven to make changes in systems and practices to operationalize equity.
CORE COMPETENCIES

Growth mindset: demonstrated ability to take and receive feedback with professionalism and grace from peers and staff as well as supervisors
Relentlessly goal-oriented: enjoys working toward and achieving ambitious goals; willing to go over, under, around, or through any obstacle that gets in the way of meeting goals with a proven track record of creating and executing/managing comprehensive strategic goals
Cultural competency: able to build relationships and collaborate with colleagues, partners, and stakeholders across multiple lines of identity difference
Keeps Calm in Stressful Situations: demonstrated capacity and willingness to work long hours during peak season, rolling up their sleeves and getting the work done
REQUIREMENTS FOR THIS ROLE

Proficiency and experience using APIs to push and pull data
Strong knowledge of SQL and management of relational databases (in particular, Redshift and PostgreSQL)
Experience working with messy data, and creating and maintaining data pipelines
Strong proficiency in programming (preference for Python)
Experience writing and editing clear, clean code and a willingness to use version control systems like git and Github
Interest in being a member of a diverse, multidisciplinary team that communicates technical concepts to non-technical audiences
Helpful but not required:

Previous experience working in the field of voter registration or voting rights
Familiar with cloud infrastructure (e.g. AWS, GCP)
Understanding of Terminal and the command line interface
Experience creating reports using data visualization & business intelligence tools (e.g. Tableau, PowerBI, Periscope, Google Data Studio)
BENEFITS AND CULTURE
We offer flexible remote forward work, and a generous benefits package; including 100% cost coverage of employee health benefits, 401K with an automatic employer contribution regardless of employee contribution level, virtual therapy, stipend for ergonomic office set ups and generous vacation and leave policies.


All employees must be eligible to work lawfully within the United States upon the commencement of employment. The organization does not sponsor visa applications for prospective or current staff.


Salary Range: $88,000-$102,500
Our work is centered on creating a deeply inclusive and significantly more representative electorate. In order to be successful in this role, the candidate must have the cultural competence to successfully work with a diverse group of staff, partners and stakeholders. We especially strongly encourage applicants with close ties to Black, Latinx, Indigenous, non-English-speaking, disability, and LGBTQ+ communities to apply. We are proudly an Equal Opportunity Employer.","$95,250 /yr (est.)",1 to 50 Employees,Nonprofit Organization,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Seamless.AI
3.4",3.4,"Columbus, OH",Senior Data Engineer - Remote US,"Role: Senior Data Engineer
The Opportunity
We are seeking a Senior Data Engineer with a strong background in data ingestion and processing for large data sets. The ideal candidate is maniacal about data accuracy and has a proven track record of building robust and scalable data pipelines. This role requires an experienced professional who can work collaboratively with cross-functional teams to design and implement solutions that meet business requirements and adhere to data security and privacy policies.
If you are a senior data engineer with a passion for data accuracy and a proven track record of building robust and scalable data pipelines, we encourage you to apply for this role. We offer competitive compensation, a flexible work environment, and opportunities for growth and advancement within the organization.
About Seamless
Seamless delivers the world's best sales leads. Through our product, we help sales teams maximize revenue, increase sales, and easily acquire their total addressable market using artificial intelligence; by development of a robust real-time contact and company search engine as well as a suite of technically-advanced tools to support sales and lead generation. We have been recognized as one of Ohio's fastest growing companies and have been recently ranked No. 7 in LinkedIn's Top 50 Startups of 2022, featured in Forbes as #1 Software company in Ohio in 2022, and on G2's ""Top 100 Highest Satisfaction Products for 2022"" list!
The Seamless Team
We have an amazing culture and work environment that anyone would want to be a part of. We encourage a culture of positivity. We thrive off of continuous feedback and do whatever it takes to help our team and customers be successful. You will grow as an individual, professionally, and be able to see and feel the impact you are making to the growth of Seamless every day.
Role Responsibilities
Design and develop data ingestion and processing pipelines that ensure data accuracy and completeness.
Work closely with cross-functional teams to understand business requirements and design solutions that meet those requirements.
Lead efforts to identify and implement data quality control measures to ensure consistency and reliability of data.
Monitor data pipelines to identify issues and proactively resolve them.
Evaluate and recommend new technologies and techniques to improve data pipeline performance and scalability.
Ensure compliance with data security and privacy policies and procedures.
Mentor junior engineers and provide technical leadership in data engineering best practices.

Candidate Requirements
Bachelor's or Master's degree in Computer Science, Software Engineering, or a related field.
3+ years of experience in data engineering, with a focus on data ingestion and accuracy.
Strong experience with large-scale data processing frameworks such as Hadoop, Spark, or Flink.
Proficiency in programming languages such as Python, Java, or Scala.
Experience with SQL and NoSQL databases.
Familiarity with cloud-based data processing and storage platforms such as AWS, Google Cloud, or Azure.
Strong experience in designing and implementing data quality control measures.
Excellent problem-solving and analytical skills.
Strong communication and collaboration skills.
Experience mentoring and leading teams of data engineers.
Seamless.AI has been delivering the world's best sales leads since 2015. Our product is the first real time, B2B search engine helping sales teams maximize revenue, increase sales, and easily acquire their total addressable market using artificial intelligence. We have been recognized as one of Ohio's fastest growing companies and won 2020 Best Places to Work and LinkedIn's Top 50 Tech Startups in 2020 and 2022. We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. Visa Sponsorship is not included in our hiring package. Applicants will need to be authorized to work in the U.S.","$93,575 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable
"Financial Information Technologies LLC
3.7",3.7,"Tampa, FL",Senior Data Engineer,"Join Fintech as a Senior Data Engineer!
Fintech is the leading business solutions provider for the beverage alcohol industry, empowering alcohol suppliers, distributors, and retailers with smart solutions that simplify beverage alcohol management. Our unique, thriving company culture promotes collaboration and growth at every level, and our comprehensive employee benefits have earned Fintech the title of a Tampa Bay Times Top 100 Workplaces for 2020 and 2021.
Fintech’s Senior Data Engineer brings a depth of relational database modeling and an understanding of transactional processing across a myriad of database types. They can analyze and assess new data sets to understand nuances of content in the context of purpose with an ability to conceptualize cleansing, harmonization, and modeling efforts. Working under the direction of the principal process architect the senior data engineer will lead a small team of experienced data wranglers to tackle a myriad of ad-hoc custom projects as well as service the development needs within our warehouse and app abstraction layers.
Essential Functions:
Collaborates with ELT/process automation, data insights, and data science teams
Builds data models in accordance with prescribed methodologies
Serves as knowledgeable backstop for level III ticket resolution
Guides and instructs junior developers and engineers on how to implement directives in accordance with project needs within adopted framework
Gains a familiarity with and contributes to the core meta-data driven data processing engine
Advising on data model consumption in analytics layers
Contributes to knowledge base
Qualifications:
8 + years of experience with SQL in multiple database flavors (SQL Server, Oracle, Snowflake, Postgres, Greenplum)
5 + years of experience with data ingest transformations and harmonization
5 + years of experience with database object creation and modeling
Analytical thinker that can adapt and problem solve in a fast-paced environment
Team oriented
Must be able to consume, understand, and implement a complicated but flexible processing back-end in a short time frame
Our Benefits:
Employer Matched 401K (Up to 10% of Employee Salary)
Company Paid Medical Insurance Option (Employee and Dependent Children)
Company Paid Dental Insurance Option (Employee only)
Company Paid Vision Insurance Option (Employee only)
Company Paid Long and Short-Term Disability
Company Paid Life and AD&D Insurance
Employee Recognition Program
18 PTO Days a Year
Six Paid Holidays
Business Casual Dress Code
Check out www.fintech.com for more information!
We E-Verify.
Fintech is an Equal Opportunity Employer that does not discriminate on the basis of actual or perceived race, color, creed, religion, national origin, ancestry, citizenship status, age, sex or gender (including pregnancy, childbirth and pregnancy-related conditions), gender identity or expression (including transgender status), sexual orientation, marital status, military service and veteran status, physical or mental disability, genetic information, or any other characteristic protected by applicable federal, state, or local laws and ordinances. Fintech’s management team is dedicated to this policy with respect to recruitment, hiring, placement, promotion, transfer, training, compensation, benefits, employee activities, access to facilities and programs and general treatment during employment.
Fintech is a Drug-Free Workplace.","$109,213 /yr (est.)",51 to 200 Employees,Company - Private,Financial Services,Financial Transaction Processing,1991,$25 to $100 million (USD)
"Farm Credit East
3.9",3.9,"Enfield, CT",Data Engineer,"Be part of a team focused on the success of our customers, the success of our communities, and the success of each other. Farm Credit East is the leading provider of loans and farm advisory services to farm, forest product, fishing, and other agricultural business owners across the northeast. We are One Team Working Together with a focus on our five pillars: Outstanding Customer and Employee Experience, Quality Growth, Operational Excellence, Commitment to our Communities, and Protecting Customer Information.

Position Summary
The Data Engineer is responsible for cleaning, managing, and sharing data that guides business decisions. Using ETL tools you will gather data from a variety of sources, checking for anomalies, automating processes, and generally making it easier for business stakeholders to generate valuable insights. This position will collaborate with internal and external organization to capture requirements, design, create, document, manage, and fulfill requests for on-going and/or ad-hoc reports, dashboards, and scorecards.

Duties and Responsibilities
Work with product stakeholders to implement, maintain, and enhance data models and solutions used to define and measure quality of data domains.
Design data models to meet requirements
Perform ETL (Extract, Transform, and Load) on data to meet stakeholder specifications.
Design and develop data access methods, datasets, views etc.
Develops data modeling and is responsible for data acquisition, access analysis, archive, recovery, load design and implementation.
Coordinates new data developments to ensure consistency with existing warehouse structure.
Collaborates with internal customers to capture requirements, design, create, document, manage and fulfill requests for on-going and/or ad-hoc reports, dashboards, and scorecards.
Assists with the development, implementation, and maintenance of front-end presentation (dashboards), automated report solutions and other BI solutions to support tactical and strategic reporting needs of the organization.
Assists in identification of data integrity problems and recommends solutions.
Work collaboratively with key stakeholders both internally and externally, including but not limited to Senior Management, Business Unit Leaders, Knowledge Exchange, and Farm Credit Financial Partners (FPI).

Job Qualifications/Requirements
Bachelor’s Degree in Computer Science, Business, Finance, or other related field from an accredited University.
Experience with MSFT SQL Server
Microsoft Azure (Data Bricks, Data Factory, Logic Apps, Functions, etc.)
2 plus years of experience in Finance related informatics, performance measurement, or analysis with strong relational database SQL skills.
1 + years of experience using Microsoft Azure product to perform ETL
Familiar with Databricks Unity catalog

Farm Credit East is an Equal Opportunity Employer. As an Equal Opportunity Employer, we do not discriminate on the basis of race, color, religion, national origin, sex, sexual orientation, gender identity or expression, age, marital status, parental status, political affiliation, disability status, protected veteran status, genetic information or any other status protected by federal, state or local law. It is our goal to make employment decisions that further the principle of equal employment opportunity by utilizing objective standards based upon an individual's qualifications for a specific job opening. In compliance with the Americans with Disabilities Act (“ADA”), if you have a disability and would like a reasonable accommodation in order to apply for a position with Farm Credit East, please call 1-800-562-2235 or e-mail FarmCreditCareers@farmcrediteast.com .","$86,685 /yr (est.)",201 to 500 Employees,Company - Private,Financial Services,Banking & Lending,1916,$100 to $500 million (USD)
"Longevity Holdings Inc.
3.9",3.9,"Minneapolis, MN",Associate Data Engineer (Temporary),"As a Associate Data Engineer, you will treat data as an asset to design, build, and execute high performance and data centric solutions by using the comprehensive big data capabilities for the company's data platform environment. In this role, you will build and optimize data products to bring data and analytics products and solutions to businesses.
Essential Job Responsibilities:
· Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions
· Leading data collection efforts and performing trend analysis to identify common performance challenges that require further attention
· Partner closely with our data scientists to ensure the right data is made available in a timely manner to deliver compelling and insightful solutions
· Building out scalable data pipelines and choosing the right tools for the right job. Manage, optimize, and monitor data pipelines
· Incorporate core data management competencies including data governance, data security, and data quality
Required Skills:
· Bachelors in a quantitative field
· 1+ years of data engineering or equivalent experience
· Proficient use of tools, techniques, and manipulation including Cloud platforms, programming languages, and an understanding of software engineering practices
· Demonstrated knowledge of relational data sets, structures, and SQL
· Familiarity with big data platforms such as Apache Spark, Hadoop, Kafka, etc.
· Experience leading data organization, dashboarding, & visualization efforts
· Inquisitive, proactive, and interested in learning new tools and techniques
· Excellent communication skills
Longevity Holdings Inc prohibits discrimination and harassment and will take affirmative action to employ and
advance in employment qualified individuals based on their status as protected veterans or individuals with
disabilities, race, color, religion, sex, national origin, sexual orientation, and gender identity.
Our privacy notice is available at https://longevity.inc/employment-privacy-notice
Job Type: Full-time
Pay: $20.00 - $30.00 per hour
Schedule:
Monday to Friday
Application Question(s):
Will you now or in the future require sponsorship for employment visa status (e.g., H-1B visa status)?
Work Location: Hybrid remote in Minneapolis, MN 55402",$25.00 /hr (est.),51 to 200 Employees,Company - Private,Management & Consulting,Research & Development,1998,$25 to $100 million (USD)
"Opensignal
4.5",4.5,"Boston, MA",Senior Data Engineer,"Department: Product/Technology
Location: Boston / US East Coast or Canada

Purpose of Role

We’re looking for a Senior Data Engineer to join our Marketing Performance Group in
transforming our real-world raw data into valuable and credible industry-leading metrics that
provide insights to our analysts and our customers.

What you will be doing
The creation and implementation of a framework to assist in building complex statistical
models. Working closely with our data scientists and our data engineers to create and
evolve products that measure market dynamics in the Telecommunication space that drive
our customers short-term marketing campaign tactics and their longer-term customer
acquisition and retention strategies. This role reports to our Engineering Manager.

We expect our Lead Data Engineer to do:
Own and improve our data pipeline. Assemble large, complex data sets that
meet business requirements, with engineering best practices in mind.
Champion building scalable and resilient data infrastructure, as well as tools to
extract and transform data used by stakeholders and customers.
Be security conscious and sensitive to privacy concerns and legislation related
to the data within the platform.
Have a continuous improvement mindset when it comes to both the platform
and the process.
Work efficiently, automate manual processes where possible, and take a test-
driven approach to engineering.
Take a keen interest in improving the platform’s scalability while understanding
the cost.
Be a good team player, with an agile approach and a can-do attitude.
Keep yourself current and make sure we follow best practices and engineering
standards.
Be an advocate for the platform and its health. Take ownership of your work
from conception through to support.
Be detail orientated and understand the importance of the credibility of our
metrics. Document and communicate with stakeholders in a language
understood by all.
Can work in a fast-paced environment with an ability to shift priorities and focus
on changing requirements and market demands.
Able to coach and mentor Data Engineers in best practices.
Cross-collaborate with the wider team to drive and maintain high standards in
our data pipeline builds.
Comfortable and effective at working in a remote capacity, collaborating with
team members across different locations through digital channels.

What we need from you:
As a Senior Data Engineer, we would expect you to have previous experience in
manipulating, processing, storing, and extracting value from big data.
Advanced with hands-on experience in architecting, crafting, documenting, and
developing highly scalable distributed data processing systems.
Advanced with big data tools, specifically Apache Spark.
Advanced with relational SQL databases. Prior experience with MSSQL,
Postgres, AWS Athena (Presto).
Advanced with SQL query authoring including DBT.
Experience with data pipeline / workflow tools i.e. Apache Airflow.
Experience with AWS cloud services like EC2, S3, managed Kubernetes, AWS
ECS, and Aurora.
Experience with object-oriented/object function scripting languages: Python and
Scala.
Experience in implementing complex clustering and classification models on
large datasets to support new product development.
Experience in writing tests, especially in BDD style and working with Git.
Strong analytic skills in working with unstructured datasets.
Experience in root cause analysis of data when asked to answer specific
business questions.
Experience building and optimizing & ""big data"" data pipelines.
Experience supporting and working with cross-functional teams.
Strong self-organizational skills
Experience being part of a cross-functional team, using agile methodologies.
Bachelor’s degree

For US applicants only

At this time, the company will not sponsor a new applicant for employment sponsorship for this position.
This employer participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S.

About Us
Opensignal is the leading global provider of independent insight and data into network
experience and market performance. Our user-centric approach allows communication
providers to constantly improve their network and maximize commercial performance.
Leading analysts, investors and financial institutions place a high value on our independent
analysis and we are regular contributors to their reports.
Real network experience is our focus and ultimately that’s what influences customer choice.
Our mission is to advance connectivity for all and here at Opensignal, the team is leading
the industry in enabling operators to link their network experience and market performance
in a way that has never before been possible.
With offices in London, Boston and Victoria, British Columbia, we are truly global, with
employees working across four continents and representing over 25 nationalities. We are
an equal opportunity employer dedicated to building an inclusive and diverse workforce.

Benefits:
We believe we are stronger when we not only celebrate our many differences, values, and
voices but include them in everyday practice. Having a diverse and inclusive culture is
essential, which is why we offer a flexible approach to work-life balance, operating in a
remote-hybrid way. We’ll help you get set up with the essentials you need to work from
home or the office. We also offer an attractive range of additional benefits, including:
Competitive compensation packages including a long-term equity program.
Comprehensive group benefits package and company-sponsored retirement
savings plan (details depend on your country of work).
Professional development opportunities: education reimbursement, learning
allowance, company-sponsored workshops, and more!
Generous holiday allowance, sick leave, parental leave, flexibility including Flex
Fridays, and the opportunity to work from abroad.
Charity matching and time off for community volunteering and DE&I
program/committees.
Regular virtual and in-person events and socials.","$148,405 /yr (est.)",51 to 200 Employees,Company - Private,Management & Consulting,Business Consulting,2010,Unknown / Non-Applicable
"Intone Networks
4.5",4.5,"Dallas, TX",Data Engineer,"Walmart Dallas, TX (Must be onsite from Day - Hybrid Schedule onsite a couple times per month) JOB DESCRIPTION: Looking for a Data Engineer who is very strong with Spark, Scala, Hive, GCP , Bit query. (They are commercializing data. Lots of pipeline work. Nice to have are - Druid, Azure, on GCP currently","$97,101 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$5 to $25 million (USD)
"Point Predictive, Inc.",#N/A,"San Diego, CA",Senior Data Engineer,"Senior Data Engineer, San Diego, CA based OR willing to relocate to San Diego in 60 Days.
Company
PointPredictive is a fast-growing technology start-up that leverages a patented combination of artificial and natural intelligence [Ai+Ni] to provide risk assessments in the auto lending, mortgage, and retail space. The platform has been proven to reduce lender loan losses by 40-60% with review rates of 5-10% of their applications, resulting in higher productivity of lender risk management departments, significantly lower losses to their bottom lines, and improved customer experience. The company was founded in 2013 by a seasoned team of technology entrepreneurs with over 20 years of experience in the startup space (including several acquisitions) and has financial backing from top tier investors.
Role:
The company is looking for an outstanding Senior Data Engineer to focus on its Data Asset, scale the Database and Data Asset for high performance and reliability. You will also serve as an engineering resource for data related questions, issues, and bugs. Core skills include Python, Snowflake, database systems and SQL, and Amazon Web Services (AWS).
Responsibilities:
· Developing new extract-transform-load (ETL) processes and pipelines. Must be able to manage large volumes of data flowing in from a variety of formats and into a variety of location.
· Identifying, designing and implementing internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes. Automation experience highly desired.
Write complex SQL; be fluent in relational based systems; have strong analytical and problem-solving skills.
Ability to represent complex algorithms in software; bring strong understanding of database technologies, management systems, data structures, and algorithms; a deep understanding in database architecture testing methodology.
Develop and execute test plan, debugging, and testing scripts and tools.
Building real-time streaming data pipelines; building and deploying data pipelines, data streams, and extract-transform-load (ETL) processes.
Manage Data Governance and Data Cleansing, as well as supporting production issues and customer requests.
Provide engineering support to customer issues and bugs. Research and implement fixes.
About you:
You have 5+ years of relevant software development industry experience building and operating scalable, fault-tolerant, distributed systems.
Database and software development experience with Python, SQL, Redshift, and experience with Amazon Web Services, Snowflake along with pub/sub streaming technologies like Kafka, Kinesis, Spark Streaming, etc.
Experience with container services.
Fluid with Amazon Web Services.
Experience with concurrency, multithreading, and the deployment of distributed system architectures.
Experience leading and shipping large scope technical projects in collaboration with multiple experienced engineers.
You have excellent communication skills and the ability to work well within a team and across engineering teams.
You are a strong problem solver and have solid production debugging skills.
You Thrive in a fast-paced environment and see yourself as a partner with the business with the shared goal of moving the business forward.
You have a high level of responsibility, ownership, and accountability.
Job Type: Full-time
Competitive pay, bonus, equity, and benefits:
Benefits:
401(k)
Health insurance
Dental insurance
Flexible spending account
Life insurance
Paid time off
Vision insurance
Schedule:
Monday to Friday
Supplemental pay types:
Bonus pay
Education:
Bachelor's or Master’s (Preferred)
Work Location: San Diego (Del Mar)
Job Type: Full-time
Pay: $105,000.00 - $160,000.00 per year
Benefits:
401(k)
Dental insurance
Flexible spending account
Health insurance
Life insurance
Paid time off
Vision insurance
Compensation package:
Bonus pay
Stock options
Yearly pay
Experience level:
5 years
Schedule:
Monday to Friday
Application Question(s):
Are you willing to be in the office 5 days a week to influence culture and capabilities ?
Are you willing to move to San Diego within 60 days if not within the commuting region ?
Work Location: Hybrid remote in San Diego, CA 92130","$132,500 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Express Global Solutions LLC
4.3",4.3,Remote,Data Engineer-Databricks Consultant,"5+ years' data engineering experience working with big data.
4+ years' experience developing in Databricks.
Job Types: Full-time, Contract
Pay: $80,631.44 - $97,104.52 per year
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote","$88,868 /yr (est.)",51 to 200 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Clinical Ink
4.4",4.4,Remote,Data Engineer,"Company Information
Clinical ink is the global life science company that brings data, technology, and patient science together to unlock clinical discovery. Our deep therapeutic-area expertise, coupled with Direct Data Capture, eCOA, eConsent, telehealth, neurocognitive testing, and digital biomarkers advancement, drive the industry standard for data precision and usher in a new generation of clinical trials. With offices in Philadelphia, PA, Winston Salem, NC, and Iowa City, IA, Clinical ink is rewriting the clinical development experience.

Job Description
Clinical ink is seeking a Data Engineer to join our Data Team based remotely across the United States! The Data Engineer will work to develop solutions used in applications for clinical trials. The ideal candidate will be a minimum of two years of experience as a software engineer and prior experience working with a variety of tools and frameworks. The Data Engineer's responsibilities include:
Develop data engineering solutions used in applications for clinical trial data collection that both make data available for further use and generate value out of data
Contribute to the methodology by which advanced analytics projects are delivered to clients and codify the tooling needed to support them
Build and support tools that allow data analysts and data scientists to work in complex projects
Implement quality, availability, and integrity of code, solutions, and respective systems and follow best practices related to data integrity, security, scalability, etc.
Participate in code inspections, reviews, and other activities to ensure quality
Qualifications
Bachelors in Mathematics, Statistics, Computer Engineering, Computer Science, or related field of study
2-5 years of experience in software engineering, working on multi-discipline teams
Experience with a variety of tools and frameworks such as Snowflake, Airflow, Spark, Kafka, RedShift, Sage Maker, Kubernetes, etc., AWS ecosystem (Lambda, Glue, S3, E2C, etc.), programming tools and querying languages (i.e., Python, C++, SQL, Scala, Java, etc.)
At least 2+ years of experience with Python
Data modelling and database development experience required
Data visualization experience preferred in Tableau and/or AWS QuickSight
Nice to have experience with issue tracking tools such as JIRA and Confluence
Ability to think creatively and take initiative; ability to learn new technical topics and develop new technical skills quickly
Willingness to learn and explore bleeding-edge/cutting-edge technologies
Additional Information
Clinical ink is an equal opportunity employer and does not discriminate against otherwise qualified applicants on the basis of race, color, creed, religion, ancestry, age, sex, marital status, national origin, disability or handicap, or veteran status.
www.clinicalink.com",#N/A,201 to 500 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2007,Unknown / Non-Applicable
"DocuSign
3.7",3.7,"San Francisco, CA",Data Engineer,"Company Overview

DocuSign helps organizations connect and automate how they agree. Our flagship product, eSignature, is the world’s #1 way to sign electronically on practically any device, from virtually anywhere, at any time. Today, more than a million customers and a billion users in over 180 countries use DocuSign to accelerate the process of doing business and simplify people’s lives.


What you'll do

DocuSign is seeking a talented and results-oriented Data Engineer to focus on delivering trusted data to the business. The Data Engineer delivers data for analytics using our Enterprise Data Warehouse, enabling the global DocuSign analytics community via curated, governed and cleansed data. As a member of the Global Data and Analytics team, the Data Engineer leverages a variety of technologies to accomplish this goal, ranging tools like Airflow, Matillion, dbt, Snowflake and Fivetran to languages like SQL and Python. The successful candidate will develop solutions with innovative cloud technologies, work on a variety of fast-paced assignments, and partner with world-class technical and business teams to maximize the value of data.

This position is an individual contributor role reporting to the Manager, Data Analytics.

Responsibility
Build data pipelines using Fivetran, dbt/Matillion, Snowflake and Airflow
Develop and maintain data documentation including ERD, data dictionaries, data lineage, and metadata
Ensure data quality and integrity by implementing appropriate data validation and cleansing techniques
Monitor and troubleshoot data pipelines to identify and resolve issues in a timely manner
Build POCs to validate new concepts and new technologies
Collaborate with business, engineering, and data science teams to understand their data needs and design efficient solutions to support their requirements


Job Designation

Hybrid:
Employee divides their time between in-office and remote work. Access to an office location is required. (Frequency: Minimum 2 days per week; may vary by team but will be weekly in-office expectation)


What you bring

Basic
Bachelor’s Degree in Computer Science, Data Analytics, Information Systems or similar, or equivalent work experience
5+ years of relevant experience
2+ years of dimensional and relational data modeling experience
Experience with modern data integration and transformation tools such as Fivetran, Dbt, and Matillion
Experience with workflow orchestration tools such as Airflow
Experience with MPP databases like Snowflake, Redshift and BigQuery
Experience with cloud platforms like AWS, Azure and GCP
Experience with versioning tools like git
Experience working with tools like Jira and Confluence
Experience with SQL and Python
Experience with document and data debugging

Preferred
Good knowledge of database and data warehouse concepts such as facts and dimensions to design and develop data models that support enterprise reporting and analytics needs
Ability to work independently with minimal supervision, as well as in a team environment
Excellent communication skills
Eye for detail, good data intuition, and a passion for data quality
Comfortable working in a rapidly changing environment with ambiguous requirements
Organizational and time management skills, with the ability to prioritize tasks and meet deadlines


Wage Transparency

Based on applicable legislation, the below details pay ranges in the following locations:

California: $118,300 - $182,775 base salary

Washington: $111,600 - $162,625 base salary

This role is also eligible for bonus, equity and
benefits.


Life at DocuSign

Working here
DocuSign is committed to building trust and making the world more agreeable for our employees, customers and the communities in which we live and work. You can count on us to listen, be honest, and try our best to do what’s right, every day. At DocuSign, everything is equal.

We each have a responsibility to ensure every team member has an equal opportunity to succeed, to be heard, to exchange ideas openly, to build lasting relationships, and to do the work of their life. Best of all, you will be able to feel deep pride in the work you do, because your contribution helps us make the world better than we found it. And for that, you’ll be loved by us, our customers, and the world in which we live.

Accommodation
DocuSign provides reasonable accommodations for qualified individuals with disabilities in job application procedures. If you need such an accommodation, including if you need accommodation to properly utilize our online system, you may contact us at accommodations@docusign.com.

If you experience any technical difficulties or issues during the application process, or with our interview tools, please reach out to us at taops@docusign.com for assistance.

Our global benefits
Paid time off
Take time to unwind with earned days off, plus paid company holidays based on your region.
Paid parental leave
Take up to six months off with your child after birth, adoption or foster care placement.
Full health benefits
Options for 100% employer-paid health plans from day one of employment.
Retirement plans
Select retirement and pension programs with potential for employer contributions.
Learning & development
Grow your career with coaching, online courses and education reimbursements.
Compassionate care leave
Paid time off following the loss of a loved one and other life-changing events.","$137,113 /yr (est.)",5001 to 10000 Employees,Company - Public,Information Technology,Information Technology Support Services,2003,$1 to $5 billion (USD)
"SEFCU
3.4",3.4,"Albany, NY",AWS Cloud Data Engineer,"If you are ready to join a company that truly cares about its employees, our members, and our community then you have come to the right place!
Summary of Role:
Broadview is looking for an outstanding Cloud Data Engineer to join the Data team. This is your opportunity to be a core part of the team that has direct impact on the organization’s day-to-day and strategic decision-making processes. As a Cloud Data engineer, you will be responsible for helping to accelerate the organization’s migration to the cloud, moving data from source systems to the AWS environment, managing data into a conformed data model, developing business KPIs and integrating them with front-end BI dashboards. You will get the exciting opportunity to work on large data sets in a cutting-edge data warehouse cloud environment. You will work closely with the business and technical teams to solve many non-standard and unique business problems and use creative problem solving to deliver actionable output.
Our team is serious about great design and redefining best practices with a cloud-based approach to scalability and automation. A successful candidate will be a self-starter, comfortable with ambiguity, with strong attention to detail, an ability to learn and work in a fast-paced environment, and an ability to work effectively with cross-functional teams
Essential Job Functions/Responsibilities:
Design, develop, test, and maintain cloud-based data solutions and infrastructure
Collaborate with cross-functional teams, including business analysts, data engineers, analysts and Cloud infrastructure team to ensure data solutions meet business requirements
Develop data ingestion pipeline to load data from various data sources
Develop and maintain scalable ETL pipelines to extract, transform, and load data
Implement data governance policies and ensure compliance with data security regulations of Broadview
Build and maintain data warehouses and data lakes on AWS
Develop insights into existing data warehouses and optimization approaches
Automate data processes using scripts and workflows to improve efficiency and reduce errors
Optimize data storage and retrieval processes to ensure high performance and scalability
Troubleshoot data-related issues and provide solutions to resolve them
Continuously monitor and improve data quality and data governance processes
Stay up-to-date with emerging technologies in cloud computing, data engineering, and data architecture and recommend best practices to the team.
Minimum Job Qualifications:
Bachelor's degree in Computer Science, Information Technology, or related field
Minimum of 3 years of experience in cloud computing and data engineering
3+ years of related work experience in a similar role. Preferred experience in banking, capital markets, insurance or asset management sectors
Experience with cloud-based data solutions like AWS S3, DMS, Redshift, Snowflake, Athena, EMS, API gateway
Hands-on experience with ETL tools like Apache Airflow or AWS Glue
Hands-on experience working with Quicksight
Knowledge of data modeling and database design principles
Familiarity with data governance and data security policies on AWS
Excellent problem-solving and analytical skills
Strong communication and collaboration skills to work with cross-functional teams
Practical experience in a data architecture/integration engagement across the end to end product life cycle – strategy, roadmap, requirements, design, development, testing, deployment, production support
Starting Compensation: $97,371 - $126,582 plus a competitive benefits package
Bilingual individuals who are fluent in a second language in addition to English are highly encouraged to apply.
We are an equal opportunity employer. We do not discriminate on the basis of race, creed, color, national origin, religion, sex, age, veteran status, disability, genetic information, gender identity, or any other protected class.
Broadview FCU is committed to ensuring individuals with disabilities and/or those who have special needs participate in the workforce and are afforded equal opportunity to apply and compete for jobs. If you would like to contact us regarding the accessibility of our Website or need assistance completing the application process, please contact us at
bv-talentacquisition@sefcu.com
.","$111,977 /yr (est.)",201 to 500 Employees,Nonprofit Organization,Financial Services,Banking & Lending,1953,Unknown / Non-Applicable
Exo Therapeutics,#N/A,"Cambridge, MA","Co-op, Data Engineer","Exo Therapeutics (https://exo-therapeutics.com) is a small molecule drug discovery and development company with a pioneering technology to address intractable pharmaceutical targets. By leveraging the company’s ExoSight™ platform, Exo is developing a deep pipeline of potent drug candidates that bind exosites, distal and unique binding pockets that have the potential to reprogram enzyme activity for precise and robust therapeutic effect. Through this specific and selective approach to challenging targets, the company’s team of world-class researchers is unlocking breakthrough therapeutics in oncology, inflammation and a broad range of other diseases.
EXO Therapeutics
Co-op, Data Engineer
Cambridge, Massachusetts, United States
About the company
Exo Therapeutics (https://exo-therapeutics.com/) is a privately held small molecule drug discovery and development company with a pioneering technology to address intractable pharmaceutical targets. By leveraging the company’s ExoSightTM platform, Exo is developing a deep pipeline of potent drug candidates that bind exosites, distal and unique binding pockets that have the potential to reprogram enzyme activity for precise and robust therapeutic effect. Through this specific and selective approach to challenging targets, the company’s team of world-class researchers is unlocking breakthrough therapeutics in oncology, inflammation and a broad range of other diseases.

Overview
Exo Therapeutics is seeking a highly motivated data engineer co-op to join our bioinformatics team. You will work in mining and organizing data sources, building the data analytical workflows and applications to enable oncology/immunology target discovery efforts.
The data engineer co-op will work under the supervision of the Director of Bioinformatics. The ideal candidate should have solid scientific, engineering, and statistical background with strong curiosity of applying knowledge and expertise in addressing problems in the biotech drug R&D setting.

Responsibilities
Explore and curate public data resources to continue building an exosite focused knowledgebase to empower novel target identification and evaluation.
Work with bioinformatics and chemoinformatics peers on internal datasets processing and formatting, data models definition.
Design and establish the computational framework to build/integrate the knowledgebase and the computational pipeline to support new target nomination and prioritization.
Help maintain and upgrade the current data processing pipelines wherever necessary.
Work with the manager to establish the data ingestion, data analysis and application development SOP
Required Qualifications
MS/PhD candidate in a relevant field, such as bioinformatics, data science, data engineering
Experience of working with large scale biomedical datasets and databases, such as uniprot, PDB database, Depmap, TCGA
Experience with building and maintaining data processing and data analytical workflows
Experience with setting up and using database or data warehouse, such as SQL, Redshift and snowflake is a plus
Passionate about applying your skills and knowledge in solving problems in the drug R&D setting
Job Duration
Jun – Dec, 2023

All fully qualified applicants who are authorized to work in the US at the time of application will receive consideration for employment without regard to race, religion, gender, sexual orientation, national origin, ancestry, disability, veteran status, as protected under law.","$131,330 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Leadstack Inc
4.3",4.3,Remote,Lead Data Engineer with Sql/Python,"LeadStack Inc. is an award winning, one of the nation's fastest growing, certified minority owned (MBE) staffing services provider of contingent workforce. As a recognized industry leader in contingent workforce solutions and Certified as a Great Place to Work, we're proud to partner with some of the most admired Fortune 500 brands in the world.
Title: Lead Data Engineer with Sql/Python
Location: Remote
Duration: 6+months
Direct Client
Immediate interview
Top Skills: Azure, SQL/Python Developer.
Job Description:
Design and develop data system integrations and data models for reporting and alerting.
Essential Job Functions:
Drive digital innovation by leveraging innovative new technologies and approaches to renovate, extend, and transform the existing core data assets, including SQL-based, NoSQL-based, and Cloud-based data platforms
Define high-level migration plans to address the gaps between the current and future state
Present opportunities with cost/benefit analysis to leadership to shape sound architectural decisions
Lead the analysis of the technology environment to detect critical deficiencies and recommend solutions for improvement
Interpreting data, analyzing results using statistical techniques
Developing and implementing data analyses, data collection systems and other strategies that optimize statistical efficiency and quality
Acquiring data from primary or secondary data sources and maintaining databases
Promote the reuse of data assets, including the management of the data catalog for reference
Key Responsibilities:
Innovate, develop, and drive the development and communication of data strategy and roadmaps across the technology organization to support project portfolio and business strategy
Drive the development and communication of enterprise standards for data domains and data solutions, focusing on simplified integration and streamlined operational and analytical uses
Drive digital innovation by leveraging innovative new technologies and approaches to renovate, extend, and transform the existing core data assets, including SQL-based, NoSQL-based, and Cloud-based data platforms
Define high-level migration plans to address the gaps between the current and future state, typically in sync with the budgeting or other capital planning processes Lead the analysis of the technology environment to detect critical deficiencies and recommend solutions for improvement
Mentor team members in data principles, patterns, processes and practices
Promote the reuse of data assets, including the management of the data catalog for referenceDraft and review architectural diagrams, interface specifications and other design documents
To know more about current opportunities at LeadStack , please visit us on https://leadstackinc.com/careers/
Job Type: Contract
Salary: $70.00 - $80.00 per hour
Experience level:
11+ years
Schedule:
8 hour shift
Experience:
Azure: 5 years (Preferred)
Data warehouse: 1 year (Preferred)
SQL developer: 6 years (Preferred)
Work Location: Remote",$75.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2016,Unknown / Non-Applicable
"Mass General Brigham
3.8",3.8,"Somerville, MA",Sr. Data Engineer (Data Lakes),"Sr. Data Engineer (Data Lakes)
- (3244480)

About Us:
At Mass General Brigham Digital, we pride ourselves on our ability to create maximum strategic, clinical, and operational value from established and emergent technologies for our patients, care teams, researchers, and employees. Digital health will not only enhance the equity and efficiency of healthcare delivery, but it will also help make medicine more personalized and precise.
We recognize that increasing value and continually improving quality while maintaining an inclusive focus are essential to organizational excellence, and we invite you to join us on this journey. The work we do in Digital is a strategic imperative, and there is a strong and growing understanding of how together we will transform Mass General Brigham in innovative and impactful ways.
General Summary/ Overview:
At Mass General Brigham Digital, we pride ourselves on our ability to create maximum strategic, clinical, and operational value from established and emergent technologies for our patients, care teams, researchers, and employees. Digital health will not only enhance the equity and efficiency of healthcare delivery, but it will also help make medicine more personalized and precise.
We recognize that increasing value and continually improving quality while maintaining an inclusive focus are essential to organizational excellence, and we invite you to join us on this journey. The work we do in Digital is a strategic imperative, and there is a strong and growing understanding of how together we will transform Mass General Brigham in innovative and impactful ways.
Summary:
Reporting to the Engineering Manager, Data Lake, the Senior Data Engineer (Azure Data Lake) will work towards analyzing, designing, developing, and building ADF data pipelines, ELT/ETL frameworks, and Azure data lake platforms, primarily focusing on Epic (EHR) data and other healthcare data; and will thrive as a member of an experienced, high performing and highly motivated team. Role will be responsible for participating in building out our existing EDW and our new Data Lake, expanding our data ecosystem and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. Requires advanced experience with data engineering and building Azure Cloud Data Lake, Azure Big Data Analytics technologies and architecture, Enterprise Analytics Solutions, and optimizing 'big data' data pipelines, architectures, and data sets. Expert level of experience with Design and Architecture of Azure big data frameworks/tools: Azure Data Lake, Azure Data Factory, Azure Data Bricks, Azure ML, SQL Data Warehouse. Advanced Experience with Hadoop based technologies (e.g., hdfs, Spark) and Programming experience in Python, SQL, Spark.
Principal Duties and Responsibilities:
Design, Develop, construct, test and maintain Data Lake architectures and large-scale data processing systems.
Support big data ecosystem related Tool selection and POC analysis.
Gather and process raw data at scale that meet functional / non-functional business requirements (including writing scripts, REST API calls, SQL Queries, etc.).
Develop data set processes for data modeling, mining and production.
Integrate new data management technologies ( Informatica DQ..) and software engineering tools into existing structures.
The candidate will be responsible for participating in building out our Data Lake platform, expanding and optimizing our data ecosystem and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams.
The ideal candidate is an experienced data pipeline builder who enjoys optimizing data systems and building them from the ground up.
The Data Engineer will support our Software Developers, Database Architects, Data Analysts and Data Scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects.
They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products.
Create and maintain optimal data pipeline architecture, assemble large, complex data sets that meet functional / non-functional business requirements on cloud based data platforms (e.g. Azure) and relational data systems (SQL Server, SSIS).
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, etc.
Build the data infrastructure required for optimal extraction, transformation, and loading of data from traditional/legacy data sources.
Work with stakeholders including the Management team, Product owners, and Architecture teams to assist with data-related technical issues and support their data infrastructure needs.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Use/s the Mass General Brigham values to govern decisions, actions, and behaviors. These values guide how we get our work done: Patients, Affordability, Accountability & Service Commitment, Decisiveness, Innovation & Thoughtful Risk; and how we treat each other: Diversity & Inclusion, Integrity & Respect, Learning, Continuous Improvement & Personal Growth, Teamwork & Collaboration.
Working Conditions:
This is a remote position.
Diversity Statement
As a not-for-profit organization, Mass General Brigham is committed to supporting patient care, research, teaching, and service to the community. We place great value on being a diverse, equitable and inclusive organization as we aim to reflect the diversity of the patients we serve. At Mass General Brigham, we believe in equal access to quality care, employment and advancement opportunities encompassing the full spectrum of human diversity: race, gender, sexual orientation, ability, religion, ethnicity, national origin and all the other forms of human presence and expression that make us better able to provide innovative and cutting-edge healthcare and research.

5+ Years of experience data engineering and building Azure Cloud Data Lake technologies and architecture, Enterprise Analytics Solutions, and optimizing 'big data' data pipelines, architectures and data sets.
5-7 Years of Experience with Hadoop based technologies (e.g. hdfs, Spark). Spark Experience desirable
5+ years of Programming experience in Python, SQL, PySpark.
Healthcare experience, most notably in Clinical data, Epic, Clarity, Caboodle, Payer data and reference data is a plus but not mandatory.
Experience with Design and Architecture of Azure big data frameworks/tools: Azure Data Lake, Azure Data Factory, Snowflake, Azure Data Bricks, Powershell.
Experience with Design and Architecture of relational SQL and NoSQL databases, including MS SQL Server, Cosmos DB.
Experience with Design and Architecture of data security and Azure security, VM, Vnet.
Experience with building processes supporting data transformation, data structures, metadata, dependency and workload management.
Experience leading and working with cross-functional teams in a dynamic environment.
Experience building Big data pipeline with Spark and/or Data Bricks is a plus.
Leading development of Data Lake Architectures from scratch.
Experience with Azure DevOps/CI-CD, Continuous integration and deployment.
Experience with Real time analytics on Spark, Kafka, Event Hub is a plus.
Experience in petabyte scale data environments and integration of data from multiple diverse sources.
Skills/Abilities/Competencies:
Advanced hands-on SQL, Spark, Python, pySpark (2+ of these) knowledge and experience working with relational databases for data querying and retrieval.
Strong SQL skills on multiple platform (preferred MPP systems).
Data Modeling tools (e.g. Erwin, Visio).
Strong interpersonal and communication skills, both written and verbal.
Strong Scrum/Agile development experience.
Excellent organizational skills and attention to detail, manage multiple tasks and projects, meet deadlines, follow through, and manage to schedule.
Strong innovation capabilities and the ability to think creatively.
Strong collaboration and team building skills within, across and outside of an organization.
Maintain and promote a positive team environment.
Maintains stable performance under pressure, demonstrating sensitivity to diverse organizational culture.
Ability to effectively cope with change, remain flexible and adaptable within a fast-paced environment with rapidly changing requirements, and ability to negotiate situations when the big picture is not clearly defined.

EEO Statement

Mass General Brigham is an Equal Opportunity Employer. By embracing diverse skills, perspectives, and ideas, we choose to lead. All qualified applicants will receive consideration for employment without regard to race, color, religious creed, national origin, sex, age, gender identity, disability, sexual orientation, military service, genetic information, and/or other status protected under the law. We will ensure that all individuals with a disability are provided a reasonable accommodation to participate in the job application or interview process, perform essential job functions, and receive other benefits and privileges of employment.

Primary Location MA-Somerville-MGB Assembly Row
Work Locations MGB Assembly Row 399 Revolution Drive Somerville 02145
Job Business and Systems Analyst
Organization Mass General Brigham
Schedule Full-time
Standard Hours 40
Shift Day Job
Employee Status Regular
Recruiting Department MGB Digital
Job Posting May 12, 2023","$118,726 /yr (est.)",1001 to 5000 Employees,Nonprofit Organization,Healthcare,Health Care Services & Hospitals,1994,$10+ billion (USD)
"Abile Group, Inc.
5.0",5.0,"Saint Louis, MO",Data Cloud Engineer - Master,"Overview:
Abile Group has an exciting and challenging opportunity for a Data Cloud Engineer-Master on a 10 year contract providing User Facing and Data Center Services supporting an Intelligence Community customer. All the personnel on the team will work together to support innovative design, engineering, procurement, implementation, operations, sustainment and disposal of user facing and data center information technology (IT) services on multiple networks and security domains, at multiple locations worldwide, to support the IC mission.

The right candidate will possess the below skills and qualifications and be ready to handle all responsibilities independently and professionally.
Responsibilities:
Provides technical/management leadership on major tasks or technology assignments.
Establishes goals and plans that meet project objectives. Has domain and expert technical knowledge.
Directs and controls activities for a client, having overall responsibility for financial management, methods, and staffing to ensure that technical requirements are met.
Interactions involve client negotiations and interfacing with senior management.
Decision making and domain knowledge may have a critical impact on overall project implementation.
May supervise others.
Qualifications:
Clearance Required: TS/SCI

Degree and Years of Experience: BS/BA and 10 -15 years of relevant experience

Required Skills:
Experience in the various aspects of hybrid cloud activities.
Supports procurement and deployment of Platform Services to enable application portability across the private and public cloud environments offered by NGA.
Readies NGA's Hybrid Cloud Environment for system migration to IC ITE and oversee the future expansion of NGA Hybrid Cloud to additional public clouds.
In concert with DCS Government, supports standardization of DCS operations in a NGA Hybrid Cloud Management environment.
Transforms Government cloud requirements into appropriate technological alternatives and provides expertise in hybrid virtualization and cloud environments.
Experience developing systems, products, and/or processes based on a total systems perspective.
Consults, plans, analyzes designs, develops tests, assures quality, configures, installs, implements, integrates, maintains, and manages systems.
Has and maintains a diverse set of skills across multiple technical disciplines with recognized expertise in multiple disciplines and possess advanced knowledge of multiple mature and emerging technologies.
Works across organizational boundaries, both internally and externally and helps to drive the relationship between technical solutions and business needs of customers. Analyzes, defines and documents customer needs and required functionality.
Designs, develops and tests theoretical and/or physical models and develops the system design, considering operational impacts, performance, testing, manufacturing, cost and schedule, training, maintenance, and support.
Performs system level design trade analysis, reviews and approves system specifications and description documents, determines how a system is to be built, tested, and implemented, plans the system development execution and ensures adherence to appropriate standards, policies, principles, and practices.
Analyzes system capacity and performance to support problem resolution and system enhancements and monitors systems tests.
Responds to inquiries from a variety of sources for the purpose of providing technical assistance, consultation, advice and support, and regularly provides advice and recommends actions and solutions involving complex issues.

About Abile Group, Inc.:
Abile Group, Inc. was formed in July 2004 to partner with the Intelligence Community and their Contractors in the areas of Enterprise Analytics & Performance Management, IT & Systems Engineering and Program & Project Management. We have significant experience with the Federal Government and are an EDWOSB dedicated to our employees and clients. We are looking for high performing employees who enjoy providing advice and guidance along with solutions development and implementation support, crafted by combining industry best practices with the clients’ subject matter experience and Abile’s breadth of expertise.
EEO Statement:
Abile Group, Inc. is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability. Anyone requiring reasonable accommodations should email careers@abilegroup.com with requested details. A member of the HR team will respond to your request within 2 business days.

Please review our current job openings and apply for the positions you believe may be a fit. If you are not an immediate fit, we will also keep your resume in our database for future opportunities.",#N/A,51 to 200 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Barclays
4.0",4.0,"Whippany, NJ",Data Engineer,"Data Engineer
Whippany, NJ
As a Barclays Data Engineer, you will be contributing directly to the execution of the business strategy and play a key role in development of future state data science platform. You will also work collaboratively with a cross- functional team of data scientists and database developers, business intelligence designers, architects, business analysts and infrastructure engineers.
Barclays is one of the world's largest and most respected financial institutions, with 329 years of success, quality, and innovation behind us. We've helped millions of individuals and businesses thrive, creating financial and digital solutions that the world now takes for granted. An important and growing presence in the USA, we offer careers providing endless opportunity.
We are currently in the early stages of implementing a hybrid working environment, which means that many colleagues spend part of their working hours at home and part in the office, depending on the nature of the role they are in. We’re flexible on how this works and it may continue to change and evolve. Depending on your team, typically this means that colleagues spend a minimum of between 20% to 60% of their time in the office, which could be over a week, a month or a quarter. However, some colleagues may choose to spend more time in the office over a typical period than their role type requires. We also have a flexible working process where, subject to business needs, all colleagues globally are able to request work patterns to reflect their personal circumstances
Please discuss the detail of the working pattern options for the role with the hiring manager.
What will you be doing?
Designing scalable and secure engineering solutions that will be leveraged by Banking colleagues and customers
Working collaboratively with cross-functional team of data scientists & database developers, business Intelligence designers, architects, business analysts and infrastructure engineers
Being responsible for full life cycle development and design of new data science platform with Python and AWS based applications and components
Working as a team player in a global development group, and participating in requirements and data analysis, design as well as development
Communicating and collaborating between the infrastructure, development, and business groups
Having excellent analytical skills, being self-motivated and capable of working in a dynamic environment that demands multi-tasking
Having the ability to generate ideas and efficiently mock-up proposals and demos
What we’re looking for:
B.S. degree in computer science or related field with emphasis on technology
Five years of experience in Python
At least two years of AWS experience
Two plus years of SQL experience
Skills that will help you in the role:
Working experience in financial industry, especially IB applications is a plus
Prior experience with AWS Services such as Lambda, Glue, Athena, ECS, Cloud Formation
Working Experience in developing REST APIs using Python
Experience in building and deploying services using any container orchestration tools such as Kubernetes, ECS, Docker Swarm, OpenShift
Where will you be working?
At Barclays, we are proud to be redefining the future of finance and here at Whippany we are defining the future of the workplace and the future of the way we work and live. We are creating a unique community, one of four strategic tech-enabled hubs that will redefine opportunity for everyone who works here. Whatever you do at Whippany, you’ll have every chance to build a world-class career in this world-class environment.

#LI-Hybrid
#data","$96,821 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Banking & Lending,1690,$10+ billion (USD)
"Mastery Logistics Systems, Inc
3.9",3.9,"Omaha, NE",Senior Data Engineer (Kafka),"About the Role
In the world of transportation, data is constantly moving, and Kafka is the roadway that keeps that traffic running smoothly to its destination. As a technical expert, you must be comfortable working across teams on multiple, high impact projects. You will be a valued part of a team that is constantly maturing Kafka use and event-driven architecture. Members of this team are responsible for the overall use and implementation of Kafka components including the Confluent platform, observability, governance, best practices, and solution development. An understanding of Kafka principles and enterprise integration patterns is required.
In order to be successful:
You are a self-directed person who can identify priorities.
You are a detail-oriented person who takes pride in keeping data correct and always having a backup plan.
You are a problem-solver who might write a script or find a tool to get things done when there isn't an established solution.
You want to learn and grow in the event-driven world.
You love Kafka! When you hear terms like ""event-driven"" or ""real-time streaming"" you're ready ready to dive in!
Responsibilities
Lead a team of Kafka engineers in an operational capacity
Develop and implement solutions using Kafka.
Administer and improve use of Kafka across the organization including Kafka Connect, ksqlDB, Streams, and custom implementations.
Work with multiple teams to ensure best use of Kafka and data-safe event streaming.
Understand and apply event-driven architecture patterns and Kafka best practices. Enable development teams to do the same.
Assist developers in choosing correct patterns, event modeling, and ensuring data integrity.
Continuous learning to be a Confluent/Kafka subject matter expert.
Work with Kafka and Confluent API's (e.g. metadata, metrics, admin) to provide pro-active insights and automation.
Work with SRE's to ensure Kafka-related metrics are exported to New Relic.
Perform regular reviews of performance data to ensure efficiency and resiliency.
Contribute regularly to event-driven patterns, best practices, and guidance.
Review feature release and change logs for Kafka, Confluent, and other related components to ensure best use of these systems across the organization.
Work with lead to ensure all teams are aware of technology changes and impact.
Acquire a deep understanding of source and sink connector technical details for a variety of platforms including PostgreSQL, MS SQL Server, Snowflake, and others as required.
Requirements
Be able to describe the primary components of Kafka and their function (brokers, zookeeper, topics).
At least two years of experience supporting applications in a production environment.
You will be expected to read and navigate code in multiple languages. Multi-language fluency and writing is not required.
Experience in a microservice architecture
Experience with event driven architecture
Proficiency in at least one programming language and one scripting language.
Proficiency with Docker containers.
Ability to participate in and contribute to code management in Github including actively collaborating in peer-reviews, feature branches, and resolving conflicts and commits.
Excellent written and verbal communication skills.
Strong sense of responsibility with a bias towards action.
Comfortable self-directing and prioritizing your own work.
Microservices experience is a plus.
Distributed tracing experience a plus.
An understanding of any cloud (Azure preferred) infrastructure and components is a plus, but is not required.
Create reference solutions.","$96,493 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Crystal Management
4.8",4.8,Remote,Big Data Platform Engineer,"About
Since 2005, Crystal Management provides information technology (IT) infrastructure, systems integration, cybersecurity, facility design and transition, and professional services to customers in the defense, civilian federal agencies, homeland security, intelligence, and commercial sectors. We understand the mission demands innovative approaches, technology, and people. With talented professionals deployed worldwide, Crystal Management delivers IT enterprise solutions, systems engineering, and management consulting services for the largest transformation and restationing programs in defense history. Crystal Management is a service-disabled veteran-owned small business.

Position Summary
DCSA is taking steps to strengthen its position as a key provider of cloud-based and big data services to various Consumers. Transformation of its large-scale Big Data infrastructure is a key element of the overall information technology strategy. The Big Data Platform is comprised of an integrated set of platform technologies that provide industry-leading data persistence and analysis capabilities. As a member of the infrastructure team, the Senior Big Data Platform Engineer will play a significant role in helping DCSA transform its Big Data ecosystem that supports applications including Data Analytics Services and other key initiatives.
This telework position is fully remote with occasional travel to the client worksite.
Responsibilities
Deploy, configure, upgrade, and sustain the development, testing, and production of BDP environments.
Work with the applications team to deploy and upgrade applications running on the Big Data Platform.
Provide subject matter expertise to the organization and will perform hands-on engineering and configuration tasks in complex, interdependent environments, and will lead triage, diagnosis, and remediation of platform-related issues.
Work with our key partners and industry peers to understand and influence industry technical direction in order that big data requirements are accommodated.

Education/Certification Requirements
High School diploma or equivalent education
DoD 8570 compliant IAT Level 2 baseline certification (e.g., CCNA-Security, CySA+, GICSP, GSEC, Security+ CE, CND, or SSCP)

Required Qualifications
9+ years (or commensurate experience) of demonstrated expertise in technology selection, architecture design, engineering and configuration, deployment, operational support, and issue resolution.
Full stack technical expertise from OS and configuration management through to Big Data ecosystem components such as:
Data processing and streaming analytics using Kafka and Storm
Platform management plan using RDAs, Containers, Consul, and Airflow
Data storage and analytics using Hadoop with YARN, Accumulo, and Apache Spark
Manage user accounts and authentication using Citadel, Kerberos, LDAP, and SAML
Proxy services using NGINX
Deployment automation and configuration management using Puppet and Ansible
Expertise with integration and administration of PostgreSQL databases
Experience operating in an AWS Cloud environment, including administration of EC2 resources, and understanding of key networking concepts such as VPC, subnet, and security groups.
Understanding of storage concepts such as EBS volumes and S3.
Strong understanding of security vulnerability detection and remediation using ACAS and DISA STIGs.
Strong understanding of various encryption technologies.
Strong experience in Linux-based scripting and systems administration.
Demonstrated ability to develop sizing and capacity planning for large-scale big data platforms and to periodically evaluate and optimize performance and platform throughput.
Excellent communications skills, including the ability to handle detailed technical communications and distill complex technical details into executive-level oral and written communications.

Preferred Qualifications
Cloudera Certified Data Engineer (CCDE) certification
Hortonworks Certified Associate (HCA) certification
AWS Certified Big Data - Specialty certification
Experience with Atlassian tools such as Jira and Confluence
Experience working in an Agile Framework environment

Clearance Requirement
Active Secret clearance

COVID-19 Safety Protocols: To protect the health and safety of its employees and to comply with customer requirements, employees in certain positions may be required to be fully vaccinated against COVID-19 or subject to facility entry safety protocols (e.g., testing, masking, physical distancing), subject to the status of the federal contractor mandate and customer site requirements.

Crystal Management, LLC provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws.

This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.
#NowHiring
#CoronaVirusHiring",#N/A,1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2005,$5 to $25 million (USD)
"TIAA
4.0",4.0,"Iselin, NJ",Senior Cloud Data Engineer,"The Senior Data Platform Engineer, Cloud role designs datastore systems that are appropriate for applications, customer needs and consistent with the overall design of the organization's information systems architecture. Under limited supervision, this job is responsible for the solution engineering and design, provisioning, delivery, service management, continuous automations of the organization's datastore systems.

Key Responsibilities and Duties
Design, develop and deliver cloud datastore solutions and develop automation pipelines to migrate data sets from On-prem to Cloud platforms. Practice Infrastructure as code to develop automation routines and integration flows to manage state of the datastore platform systems

Provision secures from start datastores and enable them with required security controls including encryption, masking, certificate/keys rotation etc.

Collaborates with developers, analysts, various system administrators to identify business requirements in designing efficient datastore solutions and interfaces.

Identifies and documents all system constraints, implications, and consequences of various proposed system changes.

Reviews technical documentation to guide system users and to assist with the ongoing operation, maintenance, and development of the system. Evaluates the efficiency and effectiveness of application operations and troubleshooting problems.

Provide expert level IT technical lead services, including the direction, evaluation, selection, configuration, implementation, and integration of new and existing technologies and tools in a cloud platform.

Responsible for development of cloud integrations and data migrations to support operations of Cloud infrastructure, provisioning, monitoring with (IaaS) and (PaaS) models

Deploy, automate, maintain, and manage AWS cloud-based production system, to ensure the availability, performance, scalability, and security of productions systems.

Manage the governance framework for DB-Services specific to private, hybrid and public cloud platform adhering to standards and integration with existing tools.

Ability to anticipate technology changes within a rapidly evolving environment.
Educational Requirements
Bachelor's Degree Preferred
Work Experience
3+ Years Required; 5+ Years Preferred
Physical Requirements
Physical Requirements: Sedentary Work

Career Level
7IC

Required Skills:
3 or more years of experience in SQL, ETL and ELT Tools.
Experience working with Data Virtualization Platforms like Starburst, Presto, Denodo, Dremio.
Preferred Skills:
Experience with AWS or GCP, PySpark, CI/CD Pipelines using ElectricFlow.
Base Pay Range: $88,600/yr. - $147,600/yr.
Actual base salary may vary based upon, but not limited to, relevant experience, time in role, base salary of internal peers, prior performance, business sector, and geographic location. In addition to base salary, the competitive compensation package may include, depending on the role, participation in an incentive program linked to performance (for example, annual discretionary incentive programs, non-annual sales incentive plans, or other non-annual incentive plans).
_____________________________________________________________________________________________________
Company Overview
TIAA is the leading provider of financial services in the academic, research, medical, cultural and government fields. We offer a wide range of financial solutions, including investing, banking, advice and education, and retirement services.
Benefits and Total Rewards
The organization is committed to making financial well-being possible for its clients, and is equally committed to the well-being of our associates. That’s why we offer a comprehensive Total Rewards package designed to make a positive difference in the lives of our associates and their loved ones. Our benefits include a superior retirement program and highly competitive health, wellness and work life offerings that can help you achieve and maintain your best possible physical, emotional and financial well-being. To learn more about your benefits, please review our
Benefits Summary
.
Equal Opportunity
We are an Equal Opportunity/Affirmative Action Employer. We consider all qualified applicants for employment regardless of age, race, color, national origin, sex, religion, veteran status, disability, sexual orientation, gender identity, or any other protected status.
Read more about the Equal Opportunity Law
here
.
Accessibility Support
TIAA offers support for those who need assistance with our online application process to provide an equal employment opportunity to all job seekers, including individuals with disabilities.
If you are a U.S. applicant and desire a reasonable accommodation to complete a job application please use one of the below options to contact our accessibility support team:
Phone: (800) 842-2755
Email:
accessibility.support@tiaa.org
Privacy Notices
For Applicants of TIAA, Nuveen and Affiliates residing in US (other than California), click
here
.
For Applicants of TIAA, Nuveen and Affiliates residing in California, please click
here
.
For Applicants of Nuveen residing in Europe and APAC, please click
here
.
For Applicants of Greenwood residing in Brazil (English), click
here
.
For Applicants of Greenwood residing in Brazil (Portuguese), click
here
.
For Applicants of Westchester residing in Brazil (English), click
here
.
For Applicants of Westchester residing in Brazil (Portuguese), click
here
.","$118,100 /yr (est.)",10000+ Employees,Company - Private,Financial Services,Investment & Asset Management,1918,$100 to $500 million (USD)
"Boston Dynamics AI Institute
4.7",4.7,"Cambridge, MA",Data Engineer,"Our Mission
Our mission is to solve the most important and fundamental challenges in AI and Robotics to enable future generations of intelligent machines that will help us all live better lives.

Data Engineers will work cross-functionally, creating new technology to improve software development for robots. If you have a passion for developing technology for robots and use it to advance their capabilities and usefulness, you will want to join us! We are onsite in our new Cambridge, MA office where we are building a collaborative and exciting new organization.
Responsibilities
Work collaboratively with research scientists and software engineers on software development for a range of different robotic platforms
Develop and maintain our data warehouses and data pipelines in cloud and on-premise infrastructureBuild event and batch driven ingestion systems for machine learning and R&D as needed
Develop and administer databases, knowledge bases, and distributed data stores
Create and use systems to clean, integrate, or fuse datasets to produce data products
Establish and monitor data integrity and value through visualization, profiling, and statistical tools
Perform updates, migrations, and administration tasks for data systems
Develop and implement a data governance, data retention strategyUse Python and SQL to develop, maintain and scale our data stores
Requirements
BS/MS in computer science, robotics, or a related field
5+ years of experience in a data engineering or similar role
Demonstrated experience with a variety of relational database and data warehousing technology such as AWS Redshift, Athena, RDS, BigQuery
Demonstrated experience with big data processing systems and distributed computing technology such as Databricks, Spark, Sagemaker, Kafka, etc
Strong experience with ETL design and implementations in the context of large, multimodal, and distributed datasets
Bonus (Not Required)
3+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)
2+ year experience working on real-time data and streaming applications
2+ years of experience with NoSQL implementation (Mongo, Cassandra)2+ years of experience with Airflow
2+ years of data warehousing experience (Redshift or Snowflake)
3+ years of experience with UNIX/Linux including basic commands and shell scripting
We provide equal employment opportunities to all employees and applicants for employment and prohibit discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.","$112,287 /yr (est.)",201 to 500 Employees,Subsidiary or Business Segment,Information Technology,Computer Hardware Development,1992,$5 to $25 million (USD)
"The Washington Post
4.3",4.3,"Washington, DC","Data Engineer, Election Platforms (all-levels)","Job Description
The Washington Post is hiring a data engineer for our Election Platforms team, to work on our data pipelines, tools, and infrastructure ahead of the 2024 presidential election.
The Election Platforms team focuses on leveraging data, partnering with stakeholders, and pioneering serviceable tools to facilitate The Post’s election coverage. This role is part of Elections Engineering, a cross-functional group of full-stack engineers and data scientists, with varied technical and academic backgrounds, responsible for delivering live election results across The Post’s platforms.
The role will be focused on contributing to our technical stack and developing new technology to facilitate robust and lively election coverage. In particular, we’re looking for candidates who would be comfortable working on deploying high-impact code in production and have an interest in data and newsroom tooling. We also work on other data engineering projects related to elections, such as voter registration data and campaign finance, and occasionally write
open
source
software
. Most of our work is in Python and TypeScript/React/Node.
We typically work at a higher level of abstraction than a single story, and aim to create general purpose tools that are reusable and easy to deploy. Our ideal candidates are creative problem-solvers, who are comfortable writing production level code to solve real-world newsroom problems. This position is remote-friendly for work in the US.
Motivation
You have a desire to contribute to The Post’s journalism through technology.
You are passionate about refining and improving team process, culture, documentation and communication while tackling complex data science and engineering challenges.
You want to work at a deeper level of abstraction than a single story or beat.
You are excited about learning new technologies and applying them to the practice of journalism.
Responsibilities
Work on the software components of our live election architecture including deployment, observability, participating in code reviews, and more.
Work closely with our engineers and data scientists to develop and maintain pipelines and administrative apps/dashboards for election data.
Work on tooling to make data preparation easier and more efficient for our live election model.
Work with newsroom stakeholders to assess needs, build useful and reusable tooling, and contribute to storytelling.
Foster effective team collaboration and communication by improving and documenting internal tools, processes and guidelines.
Assist in attracting and hiring team members who want to work at the intersection of journalism and technology by actively participating in recruiting – and contributing feedback on our overall hiring process.
Qualifications
A minimum of one year of experience building and maintaining backend and data engineering applications.
Proficiency across some of the languages and frameworks used in our engineering stack such as Python, TypeScript/React/Node, and AWS services.
A desire to work on cutting edge, cross-functional tools to help with features, data science, and machine learning/big data work in a team setting.
Experience with election data is not necessary, but is a plus.
Experience developing and deploying newsroom tools and applications is not necessary, but is a plus.
Levels
If you have less than three years, but at least one year, we’d love to evaluate you for a data engineer role.
If you have more than three years but less than about seven years, we’d love to evaluate you for a senior data engineer role.
If you have more than about seven years, we’d love to talk to you about a principal data engineer role.
Wherever you are in your life or career, The Washington Post offers comprehensive and inclusive benefits for every step of your journey:
Competitive medical, dental and vision coverage
Company-paid pension and 401(k) match
Three weeks of vacation and up to three weeks of paid sick leave
Nine paid holidays and two personal days
20 weeks paid parental leave for any new parent
Robust mental health resources
Backup care and caregiver concierge services
Gender affirming services
Pet insurance
Free Post digital subscription
Leadership and career development programs
Benefits may vary based on the job, full-time or part-time schedule, location, and collectively bargained status.
The Post strives to provide its readers with high-quality, trustworthy news and information while constantly innovating. That mission is best served by a diverse, multi-generational workforce with varied life experiences and perspectives. All cultures and backgrounds are welcomed.
The innovation doesn’t end in the Newsroom – dozens of teams power The Washington Post. We are now hiring the next innovator – how will you Impact Tomorrow?
#washpostlife","$83,249 /yr (est.)",1001 to 5000 Employees,Company - Private,Media & Communication,Publishing,1877,Unknown / Non-Applicable
"Ikigai Labs, Inc.
4.9",4.9,"Cambridge, MA","Software Engineer, Data Engineering","Ikigai Labs is a fast growing startup founded out of MIT to empower data operators. We are building an easy to use AI augmented data processing and analytics platform on the cloud. Our users depend on us to automate, maintain, and enhance day-to-day mission critical operations. We are a team of talented, hardworking and fun-loving engineers, data scientists, and data analysts working towards the goal of building the next generation of data tools.
Job Description
JOB TITLE: Software Engineer, Data Engineer [Full-time]
LOCATION: Cambridge, MA
SUMMARY:
Ikigai Labs is seeking a dynamic and passionate engineer with strong software fundamentals to join a high-performing data platform development team. We are looking for a team player who is a quick learner, performs in a rapid development cycle, has a drive to surpass expectations, and an eagerness to share their work and knowledge.
We encourage applicants from all backgrounds and communities. We are committed to having a team that is made up of diverse skills, experiences, and abilities.
Technologies
Languages: Python3, SQL
Databases: Postgres, Elasticsearch, DynamoDB, RDS
Cloud: Kubernetes, Helm, EKS, Terraform, AWS
Data Engineering: Apache Arrow, Dremio, Ray
Misc.: Apache Superset, Plotly Dash, Metabase, Jupyterhub, Stripe, Fivetran
The Position
Design and develop scalable data integration (ETL/ELT) processes
Design and develop an on-demand predictive modeling platform with gRPC
Utilize Kubernetes to orchestrate the deployment, scaling and management of Docker containers
Utilize and learn various AWS services to solve cloud-native problems
Implement a testing platform which performs sanity check, load test, scale test, heartbeat test, and performance test
Provide periodic support to our customer success team
Qualifications
0-3 years of experience with a bachelor's degree in Computer Science, Math, or Engineering; or a master's degree
Experience with Python, AWS services, and/or ETL/ELT pipeline experiences
Experience with Kubernetes and/or EKS (optional)
Understanding of the fundamentals of design patterns and testing best practices
The ability to learn quickly in a fast-paced environment
Excellent organizational, time management, and communication skills
The desire to work in an AGILE environment with a focus on pair programming
Willingness to discuss obstacles, find creative solutions, and take initiative
The ability to receive and give both constructive and encouragement feedback","$102,419 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Yes Energy, LLC
4.6",4.6,"Boulder, CO","Data Operations Engineer II (Hybrid) Boulder - CO, Chicago - IL, Dedham - MA or Houston - TX","Data Operations Engineer II

Join the Market Leader in Electric Power Trading Solutions
The electrical grid is the largest and most complicated machine ever built. Yes Energy’s industry leading electric power trading analytics software provides real time visibility into the massive amount of data that is generated by the North American electrical grid every day. Our unique and innovative view of the data informs real time trading decisions that keep utility prices low and the grid up and running. It’s both challenging work and work with a purpose.
Be a part of our successful, growing business.
We are currently working in a hybrid environment and are seeking to fill one full time Data Operations Engineer II position immediately in Boulder - CO (HQ), Chicago - IL, Dedham - MA or Houston - TX.
About the team
At Yes Energy, our Market Data Operations (MDO) team plays a crucial role in our business. We are the control room for our customers, responsible for ensuring access to reliable and timely data every minute of every day.
We take pride in our responsibility to maintain the accuracy and reliability of our data. We understand that our clients rely on us to provide them with the information they need to make informed decisions, and we take that responsibility very seriously. Like a control room operator who is constantly monitoring the grid and making adjustments to ensure stability, our team is constantly monitoring our data pipelines and making adjustments to ensure data accuracy and reliability.
Our team is passionate about what we do, and we are dedicated to helping clients navigate the complex and dynamic North American Energy Markets. We work together in a collaborative environment, and we look to continuously improve our processes to ensure that we are providing the highest quality data possible to our clients.
About you
You have a passion for working with inherently messy data
You believe that a deep understanding of the data leads to better solutions
You have a competitive attitude, taking ownership and accountability of the work you produce
You have strong problem-solving skills and a curious mindset
You have experience maintaining and designing data pipelines
Like to design, develop, analyze and troubleshoot PL/SQL code

What you will do
Maintain our real-time data pipelines and ensure so that we can provide reliable and accurate information to our clients
Support clients by answering complex data questions, providing timely and effective solutions, so that we can empower our clients to make informed decisions
Ensure highest possible quality and integrity of Yes Energy data; recommend and implement ways to improve data reliability, efficiency, and quality
Participate in weekly on-call rotations to help resolve critical data pipeline failures for our clients

Requirements
4+ years of SQL or equivalent experience
4+ years of Oracle PL/SQL or equivalent experience
Bonus points for
Experience with ETL and complex data pipelines
Energy industry experience or experience in equities/commodities trading
Experience with web scraping, including HTML parsing, HTTP protocols and network logs
Experience with Python and Bash Scripting
Familiarity with Agile development methodologies
Position Details
Full-time
Reports to Data Operation team Lead
Minimal travel may be required (up to 10 days per year)
Keywords:
Oracle, SQL, PL/SQL, REST API, Time Series Data, ETL, CLI tools.
About Yes Energy
Overview
Yes Energy delivers real-time market data and electric power trading decision solutions. Over 1,000 market participants use Yes Energy solutions daily. The business is a leader in all aspects of information content collection and management, as well as in developing and delivering data and market analytics solutions. Since its inception in 2008 Yes Energy has become a trusted and respected supplier of innovative and reliable solutions focused on the needs of power market analysts, traders and trade managers. Yes Energy has a team of amazing professionals located in Boulder, CO (HQ), Dedham, MA and Chicago, IL.
Culture
At Yes Energy we care about saying “Yes” to customers. We like to listen and learn, and develop our solutions in line with our customers’ needs. We think about customers as business partners and when we help them to be more successful … We are more successful too.
Around the office our culture is driven by some pretty fundamental values that we’re proud of:
We love innovation and solving tough challenges;
We are “high standards people” who combine passion and pride with hard work and rewards of all kinds- in an ethic that is consistent across the company;
We’re team-focused with a flat hierarchy- we work in small teams on well-defined projects that directly impact the success of the business;
We play to the strengths and experience of each person, while each of us also works along a continuum of roles adjacent to our focus area. This presents a challenge of maintaining a broad set of skills as well as an opportunity to learn and contribute in many ways;
We are constantly growing. Professional development happens every day and every year.
Compensation and Benefits
Salary Range: $90,000 - $120,000 plus bonus.
We offer strongly competitive salaries and real bonuses that are achievable and that you can impact. Our benefits package is also very competitive and it includes medical insurance, 401K Plan with matching, flexible vacation and flexible work schedules. Investment in both formal and informal professional development is encouraged and funded by Yes Energy.
At Yes Energy we are dedicated to building a diverse, inclusive and authentic workplace, so if you’re excited about this role but your past experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyways. You may be just the right candidate for this or other roles.
Yes Energy provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. In addition to federal law requirements, Yes Energy complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.","$105,000 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Software Development,2008,Unknown / Non-Applicable
"1SEO Digital Agency
4.2",4.2,"Bristol, PA",Data Integration Engineer,"Based just outside Philadelphia, PA, 1SEO Digital Agency proudly stands as a full-service Digital Marketing Agency and a Google Premier Partner, placing us in the top 1% of all agencies in the US. We prioritize our clients' growth, protection, and inspiration by being solely client-centric. Our team operates on a hybrid schedule, with the majority of us working four days in our Bristol, PA office and one day remotely each week.

We are looking for a skilled Data Engineer to help us solve the problem of multiple software systems with redundant and inconsistent data. The ideal candidate will have experience in integrating disparate data sources and implementing data pipelines to automate data flow between systems. They will work closely with our internal teams to identify pain points and design solutions to streamline our data processes and establish a single source of truth.

As our Data Integration Engineer, you will be responsible for designing, building, and maintaining our data infrastructure, including data warehouses, ETL pipelines, and data integration processes. You will be working with a variety of third-party SaaS applications, as well as helping to integrate with our own SaaS applications (in the future).
Responsibilities:
Design, build, and maintain data infrastructure, including data warehouses, ETL pipelines, and data integration solutions to connect multiple software systems
Collaborate with other teams to ensure data quality, accuracy, and accessibility
Work with a variety of third-party SDKs and APIs to integrate data and streamline workflow processes
Develop and implement data and cybersecurity protocols and procedures
Participate in the design and development of our future SaaS application
Troubleshoot and resolve issues with data integration processes
Document data integration processes and maintain data dictionaries
Requirements:
Bachelor's or Master's degree in Computer Science or Information Systems
Strong knowledge of database architecture, data modeling, and SQL
Experience with ETL tools and data integration techniques
Experience integrating cloud platforms, such as Google WorkSpace, Salesforce.com, NetSuite, QuickBooks, and various SaaS applications
Familiarity with software development best practices, including version control, testing, and continuous integration
Ability to communicate effectively with non-technical stakeholders and users
What You Can Expect From 1SEO Digital Agency:
Open-floor office environment with NO cubicles whatsoever. Basketball, Foosball, Billiards, and ping-pong are in the employee lounge.
A fully-stocked kitchen provided by ownership with catered lunches multiple times per week. There is no shortage of snacks & you could almost eat breakfast, lunch & dinner here every day.
Access to the gym in our building with NO membership fee. Work out before or after work, or during your lunch break.
The office is open from 8 am to 6 pm Monday-Friday. We offer a flexible work schedule for BOTH early and late risers

After 90 days of Full-time employment, we offer our full-time employees:
50% funded healthcare benefits (Medical, Dental & Vision) for the employee. Dependents can be added to the plan AND we offer Supplemental healthcare insurance at a reduced cost
Earn up to 3 weeks of PTO with an additional week given at year 3 AND another week at year 5.
You can join the 401K after your 1st year of employment, with up to a 4% match.
Generous incentive program for each anniversary you celebrate.","$102,793 /yr (est.)",51 to 200 Employees,Company - Private,Media & Communication,Advertising & Public Relations,2009,Unknown / Non-Applicable
"Apolis
3.9",3.9,"Saint Louis, MO",Data Engineer (Python Focused),"Role: Data Engineer (Python Focused) (W2 ONLY)
Location : St. Louis, MO 63102
Duration: 18 Months
The primary responsibilities of this role will be to work as a part of the data and analytics team of the client. The ideal candidate would have experience using ETL and other data engineering tools in a cloud environment. Additionally, this candidate would have coding and scripting experience creating code to move and manipulate data using tools such in Python such as Spark or Pandas. Microsoft Azure experience is a plus but relevant experience with other cloud-based environments will be considered.
This team uses Microsoft Azure to build and productionize data sets and processes to the product owners and product users and this role will be expected to contribute to those objectives. As a member of this team, you will work in conjunction with other data engineers, platform engineers, solution developers and others to execute and deliver working solutions to the end users.
Technical Skills
High focus on Python
More Programmer/Coder experience, less Cloud experience needed.
Must have experience with data analytics/engineering and cloud technologies and an in-depth working knowledge and understanding of data engineering, design, and integration.
Intermediate or higher level of proficiency with Python Must have the proven ability to learn quickly and apply cloud data engineering principles and skills to new tools and use cases.
Ability to define and take technical ownership of detailed requirements and collaborate effectively with various team members such as Business Analysts and Project Managers and end users.
Excellent communication (verbal and written) and problem-solving skills are essential for this position.
Initiative, Proactive Cooperation, and curiosity are mandatory behavioral skills required for this position.
Experience with several of the following technologies is preferred:
Experience with Microsoft Azure Platform. In particular, the data processing toolsets including Azure Data Factory Pipelines, Azure Databricks, Azure Data Lake Storage.
Experience using Notebooks (Such as Jupyter, DataBricks) also desired.
Experience using code languages such as Python and C#
Experience using query languages such as SQL, MDX and DAX
Experience using scripting languages such as PowerShell, M-Query (Power Query), and Windows batch commands.
Understanding of Agile and DevOps principles for software development and ownership.
Familiar with compliance and controls for software development and deployment.
Education Required
A bachelor's in computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field [or equivalent work experience] is not required but is preferred.
Job Type: Contract
Salary: $60.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Vision insurance
Compensation package:
Overtime pay
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
St. Louis, MO: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Preferred)
ETL: 5 years (Preferred)
Cloud infrastructure: 4 years (Preferred)
Work Location: One location",$65.00 /hr (est.),501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,1996,$25 to $100 million (USD)
"TIAA
4.0",4.0,"Iselin, NJ",Senior Cloud Data Engineer,"The Senior Data Platform Engineer, Cloud role designs datastore systems that are appropriate for applications, customer needs and consistent with the overall design of the organization's information systems architecture. Under limited supervision, this job is responsible for the solution engineering and design, provisioning, delivery, service management, continuous automations of the organization's datastore systems.

Key Responsibilities and Duties
Design, develop and deliver cloud datastore solutions and develop automation pipelines to migrate data sets from On-prem to Cloud platforms. Practice Infrastructure as code to develop automation routines and integration flows to manage state of the datastore platform systems

Provision secures from start datastores and enable them with required security controls including encryption, masking, certificate/keys rotation etc.

Collaborates with developers, analysts, various system administrators to identify business requirements in designing efficient datastore solutions and interfaces.

Identifies and documents all system constraints, implications, and consequences of various proposed system changes.

Reviews technical documentation to guide system users and to assist with the ongoing operation, maintenance, and development of the system. Evaluates the efficiency and effectiveness of application operations and troubleshooting problems.

Provide expert level IT technical lead services, including the direction, evaluation, selection, configuration, implementation, and integration of new and existing technologies and tools in a cloud platform.

Responsible for development of cloud integrations and data migrations to support operations of Cloud infrastructure, provisioning, monitoring with (IaaS) and (PaaS) models

Deploy, automate, maintain, and manage AWS cloud-based production system, to ensure the availability, performance, scalability, and security of productions systems.

Manage the governance framework for DB-Services specific to private, hybrid and public cloud platform adhering to standards and integration with existing tools.

Ability to anticipate technology changes within a rapidly evolving environment.
Educational Requirements
Bachelor's Degree Preferred
Work Experience
3+ Years Required; 5+ Years Preferred
Physical Requirements
Physical Requirements: Sedentary Work

Career Level
7IC

Required Skills:
3 or more years of experience in SQL, ETL and ELT Tools.
Experience working with Data Virtualization Platforms like Starburst, Presto, Denodo, Dremio.
Preferred Skills:
Experience with AWS or GCP, PySpark, CI/CD Pipelines using ElectricFlow.
Base Pay Range: $88,600/yr. - $147,600/yr.
Actual base salary may vary based upon, but not limited to, relevant experience, time in role, base salary of internal peers, prior performance, business sector, and geographic location. In addition to base salary, the competitive compensation package may include, depending on the role, participation in an incentive program linked to performance (for example, annual discretionary incentive programs, non-annual sales incentive plans, or other non-annual incentive plans).
_____________________________________________________________________________________________________
Company Overview
TIAA is the leading provider of financial services in the academic, research, medical, cultural and government fields. We offer a wide range of financial solutions, including investing, banking, advice and education, and retirement services.
Benefits and Total Rewards
The organization is committed to making financial well-being possible for its clients, and is equally committed to the well-being of our associates. That’s why we offer a comprehensive Total Rewards package designed to make a positive difference in the lives of our associates and their loved ones. Our benefits include a superior retirement program and highly competitive health, wellness and work life offerings that can help you achieve and maintain your best possible physical, emotional and financial well-being. To learn more about your benefits, please review our
Benefits Summary
.
Equal Opportunity
We are an Equal Opportunity/Affirmative Action Employer. We consider all qualified applicants for employment regardless of age, race, color, national origin, sex, religion, veteran status, disability, sexual orientation, gender identity, or any other protected status.
Read more about the Equal Opportunity Law
here
.
Accessibility Support
TIAA offers support for those who need assistance with our online application process to provide an equal employment opportunity to all job seekers, including individuals with disabilities.
If you are a U.S. applicant and desire a reasonable accommodation to complete a job application please use one of the below options to contact our accessibility support team:
Phone: (800) 842-2755
Email:
accessibility.support@tiaa.org
Privacy Notices
For Applicants of TIAA, Nuveen and Affiliates residing in US (other than California), click
here
.
For Applicants of TIAA, Nuveen and Affiliates residing in California, please click
here
.
For Applicants of Nuveen residing in Europe and APAC, please click
here
.
For Applicants of Greenwood residing in Brazil (English), click
here
.
For Applicants of Greenwood residing in Brazil (Portuguese), click
here
.
For Applicants of Westchester residing in Brazil (English), click
here
.
For Applicants of Westchester residing in Brazil (Portuguese), click
here
.","$118,100 /yr (est.)",10000+ Employees,Company - Private,Financial Services,Investment & Asset Management,1918,$100 to $500 million (USD)
"Leadstack Inc
4.3",4.3,Remote,Lead Data Engineer with Sql/Python,"LeadStack Inc. is an award winning, one of the nation's fastest growing, certified minority owned (MBE) staffing services provider of contingent workforce. As a recognized industry leader in contingent workforce solutions and Certified as a Great Place to Work, we're proud to partner with some of the most admired Fortune 500 brands in the world.
Title: Lead Data Engineer with Sql/Python
Location: Remote
Duration: 6+months
Direct Client
Immediate interview
Top Skills: Azure, SQL/Python Developer.
Job Description:
Design and develop data system integrations and data models for reporting and alerting.
Essential Job Functions:
Drive digital innovation by leveraging innovative new technologies and approaches to renovate, extend, and transform the existing core data assets, including SQL-based, NoSQL-based, and Cloud-based data platforms
Define high-level migration plans to address the gaps between the current and future state
Present opportunities with cost/benefit analysis to leadership to shape sound architectural decisions
Lead the analysis of the technology environment to detect critical deficiencies and recommend solutions for improvement
Interpreting data, analyzing results using statistical techniques
Developing and implementing data analyses, data collection systems and other strategies that optimize statistical efficiency and quality
Acquiring data from primary or secondary data sources and maintaining databases
Promote the reuse of data assets, including the management of the data catalog for reference
Key Responsibilities:
Innovate, develop, and drive the development and communication of data strategy and roadmaps across the technology organization to support project portfolio and business strategy
Drive the development and communication of enterprise standards for data domains and data solutions, focusing on simplified integration and streamlined operational and analytical uses
Drive digital innovation by leveraging innovative new technologies and approaches to renovate, extend, and transform the existing core data assets, including SQL-based, NoSQL-based, and Cloud-based data platforms
Define high-level migration plans to address the gaps between the current and future state, typically in sync with the budgeting or other capital planning processes Lead the analysis of the technology environment to detect critical deficiencies and recommend solutions for improvement
Mentor team members in data principles, patterns, processes and practices
Promote the reuse of data assets, including the management of the data catalog for referenceDraft and review architectural diagrams, interface specifications and other design documents
To know more about current opportunities at LeadStack , please visit us on https://leadstackinc.com/careers/
Job Type: Contract
Salary: $70.00 - $80.00 per hour
Experience level:
11+ years
Schedule:
8 hour shift
Experience:
Azure: 5 years (Preferred)
Data warehouse: 1 year (Preferred)
SQL developer: 6 years (Preferred)
Work Location: Remote",$75.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2016,Unknown / Non-Applicable
"Pfizer
4.1",4.1,"Tampa, FL","Senior Associate, Regulatory Quality Assurance Data Engineer","Pfizer has established a chief digital office which will lead the transformation of Pfizer into a digital powerhouse that will generate patient superior patient experiences that will result in better health outcomes.
As a member of the Digital Health Medicines and AI, Corporate Functions Data Solutions & Engineering team, you will help to realize Pfizer Digital strategy on the cloud by designing and deploying analytic solutions, work with high performing teams, and building lasting relationships with our business colleagues. This digital program will provide you with solid foundational understanding of the data types that drives business insights through data science and enterprise BI reporting solutions.
This role requires you to provide solution development and operations support for our Regulatory Quality Assurance (RQA) Domains (Audit Planning, Audits Made Easy, RQA Metrics, etc.). It will require you to maintain, enhance and provide operations support of our RQA solutions while leveraging our Data Hub, Dataiku, Snowflake, and data visualization product (Spotfire).
As the Senior Associate RQA Data Engineer, you will help in the pursuit of the Pfizer Digital strategy on the cloud by designing data products that drive business insights. You will be hands on and have opportunities to lead a group of vendors resources. You will be expected to partner with Digital Colleagues, (Enterprise Architecture, Client Partners, and Data Scientists, etc.) and Business Unit stakeholders to develop and sustain the RQA analytics strategy and data architecture. Data engineers also partner with solution development teams to ensure use case delivery goals are met while adhering to data architecture principles, guidelines, and standards.
Role Responsibilities
Reporting to the Senior Manager, Finance Sales Procurement Solution Delivery & Engineering, and as the Senior Associate RQA Data Engineer you will be responsible for data modeling, building/enhancing data interfaces, web applications, and visualizations that deliver insights that drive impactful business outcomes.
High level responsibilities may include (but are not limited to):
Leading the gathering, analysis and documentation of business and technical requirements
Create test plans, test scripts, and perform data validation
Enhance the design of the existing RQA Cloud Data Lake, Dataiku workflows, interface APIs, Web Apps, and our Spotfire dashboards.
Design automated solutions for building, testing, monitoring, and deploying ETL tools.
Develop internal APIs and DSS data solutions for thousands of end-users to power Audit applications and promote connectivity.
Opportunities to lead development teams that are driven to build impactful data products and visualizations
Perform root cause analysis and resolve Level 3 production and data issues
Coordinate with backend engineering team to analyze data to improve the efficiency, and speed of the application as well maintain elevated levels of data quality and data consistency.
Tune SQL queries, reports and ETL pipelines
Build and maintain data dictionary and process documentation
Present solutions to leadership, management, architects, and developers.
Work in conjunction with our cloud engineering staff, and partner with project managers, and analysts to deliver insights to the business
Professional Experience and Educational Requirements
Required:
Applicant must have a Bachelor’s degree with three years of relevant experience; OR Master’s degree with one year of relevant experience; OR Associate's degree with six years of relevant experience; OR eight years of relevant experience with a high school diploma or equivalent
At least 2 years of experience in data engineering, and/or reporting & analytics
Fundamental understanding of Data warehousing, data modeling, and data transformation
Exposure to web application development platforms such as Angular/Spring Boot; this is a must have.
Fundamental understanding of Cloud data warehouse solutions (Snowflake, Redshift, Spark, etc).
Working experience with one or more general purpose programming languages, including but not limited to: SQL, Java, Scala, Python, or JavaScript
Nice to Have:
Prior experience with data preparation and ETL: Dataiku, Informatica, Talend, Alteryx, etc is a major plus
Understanding of compliance and audit processes for GxP, SOX etc. is a plus
Cloud computing, machine learning, text analysis, NLP & Web development experience is a plus
Knowledge of Graph Databases (Neo4j, Titan, etc.) and query syntax is a plus
Experience with Semantic technologies and approaches is a plus
Full stack development using infrastructure cloud services (AWS preferred) and cloud-native tools and design patterns (Containers, Serverless, Docker, etc) is a plus and Open-Source technologies is a plus
Experience with sourcing and modeling data from application APIs
Prior experience with AWS Cloud stack (ECC, S3, Redshift) or Google Cloud Platform
Experience designing complex and inter - dependent data models for analytic, Machine learning use cases
Experience with Software engineering best-practices, including but not limited to version control (Git, TFS, Subversion, etc.), CI/CD (Jenkins, Maven, Gradle, etc.), automated unit testing, Dev Ops
Professional and Leadership Characteristics
Creative: Able to bring forth innovative ideas to improve our existing practices and takes calculated risks to innovate new capabilities within Pfizer Digital Business Analytics, with a focus on data products and analytics solutions
Analytical Thinker: Understands how to synthesize facts and information from varied data sources, both new and pre-existing, into discernable insights and perspectives; takes a problem-solving approach by connecting analytical thinking with an understanding of business drivers and how BA can provide value to the organization
Adaptable: Demonstrates flexibility in the face of shifting targets, thrives in new situations
Pioneering: Pushes self and others to think about new innovation and digital frontiers and ways to conquer them
Ambiguity Tolerant: Successfully navigates ambiguity to keep the organization on target and deliver against established timelines
Strong Data and Information Manager: Understands and uses analytical skills/tools to produce data in a clean, organized way to drive objective insights
Exceptional Communicator: Can understand, translate, and distill the complex, technical findings of the team into commentary that facilitates effective decision making by senior leaders; can readily align interpersonal style with the individual needs of customers
Highly Collaborative: Manages projects with and through others; shares responsibility and credit; develops self and others through teamwork; comfortable providing guidance and sharing expertise with others to help them develop their skills and perform at their best; helps others take appropriate risks; communicates frequently with team members earning respect and trust of the team
Proactive Self-Starter: Takes an active role in one’s own professional development; stays abreast of analytical trends, and cutting-edge applications of data

Work Location Assignment: Flexible
The annual base salary for this position ranges from $72,700.00 to $121,200.00. In addition, this position offers an annual bonus with a target of 7.5% of the base salary. Benefits offered include a retirement savings plan, paid vacation, holiday and personal days, paid caregiver/parental and medical leave, and health benefits to include medical, prescription drug, dental and vision coverage in accordance with the terms and conditions of the applicable plans. Salary range does not apply to the Tampa, FL location.
Relocation assistance may be available based on business needs and/or eligibility.
Sunshine Act
Pfizer reports payments and other transfers of value to health care providers as required by federal and state transparency laws and implementing regulations. These laws and regulations require Pfizer to provide government agencies with information such as a health care provider’s name, address and the type of payments or other value received, generally for public disclosure. Subject to further legal review and statutory or regulatory clarification, which Pfizer intends to pursue, reimbursement of recruiting expenses for licensed physicians may constitute a reportable transfer of value under the federal transparency law commonly known as the Sunshine Act. Therefore, if you are a licensed physician who incurs recruiting expenses as a result of interviewing with Pfizer that we pay or reimburse, your name, address and the amount of payments made currently will be reported to the government. If you have questions regarding this matter, please do not hesitate to contact your Talent Acquisition representative.
EEO & Employment Eligibility
Pfizer is committed to equal opportunity in the terms and conditions of employment for all employees and job applicants without regard to race, color, religion, sex, sexual orientation, age, gender identity or gender expression, national origin, disability or veteran status. Pfizer also complies with all applicable national, state and local laws governing nondiscrimination in employment as well as work authorization and employment eligibility verification requirements of the Immigration and Nationality Act and IRCA. Pfizer is an E-Verify employer.
Information & Business Tech
#LI-PFE","$96,950 /yr (est.)",10000+ Employees,Company - Public,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,1849,$10+ billion (USD)
"Mastery Logistics Systems, Inc
3.9",3.9,"Omaha, NE",Senior Data Engineer (Kafka),"About the Role
In the world of transportation, data is constantly moving, and Kafka is the roadway that keeps that traffic running smoothly to its destination. As a technical expert, you must be comfortable working across teams on multiple, high impact projects. You will be a valued part of a team that is constantly maturing Kafka use and event-driven architecture. Members of this team are responsible for the overall use and implementation of Kafka components including the Confluent platform, observability, governance, best practices, and solution development. An understanding of Kafka principles and enterprise integration patterns is required.
In order to be successful:
You are a self-directed person who can identify priorities.
You are a detail-oriented person who takes pride in keeping data correct and always having a backup plan.
You are a problem-solver who might write a script or find a tool to get things done when there isn't an established solution.
You want to learn and grow in the event-driven world.
You love Kafka! When you hear terms like ""event-driven"" or ""real-time streaming"" you're ready ready to dive in!
Responsibilities
Lead a team of Kafka engineers in an operational capacity
Develop and implement solutions using Kafka.
Administer and improve use of Kafka across the organization including Kafka Connect, ksqlDB, Streams, and custom implementations.
Work with multiple teams to ensure best use of Kafka and data-safe event streaming.
Understand and apply event-driven architecture patterns and Kafka best practices. Enable development teams to do the same.
Assist developers in choosing correct patterns, event modeling, and ensuring data integrity.
Continuous learning to be a Confluent/Kafka subject matter expert.
Work with Kafka and Confluent API's (e.g. metadata, metrics, admin) to provide pro-active insights and automation.
Work with SRE's to ensure Kafka-related metrics are exported to New Relic.
Perform regular reviews of performance data to ensure efficiency and resiliency.
Contribute regularly to event-driven patterns, best practices, and guidance.
Review feature release and change logs for Kafka, Confluent, and other related components to ensure best use of these systems across the organization.
Work with lead to ensure all teams are aware of technology changes and impact.
Acquire a deep understanding of source and sink connector technical details for a variety of platforms including PostgreSQL, MS SQL Server, Snowflake, and others as required.
Requirements
Be able to describe the primary components of Kafka and their function (brokers, zookeeper, topics).
At least two years of experience supporting applications in a production environment.
You will be expected to read and navigate code in multiple languages. Multi-language fluency and writing is not required.
Experience in a microservice architecture
Experience with event driven architecture
Proficiency in at least one programming language and one scripting language.
Proficiency with Docker containers.
Ability to participate in and contribute to code management in Github including actively collaborating in peer-reviews, feature branches, and resolving conflicts and commits.
Excellent written and verbal communication skills.
Strong sense of responsibility with a bias towards action.
Comfortable self-directing and prioritizing your own work.
Microservices experience is a plus.
Distributed tracing experience a plus.
An understanding of any cloud (Azure preferred) infrastructure and components is a plus, but is not required.
Create reference solutions.","$96,493 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"YSI
3.7",3.7,Remote,Senior Data Engineer,"Position Title: Senior Data Engineer/ Oracle Apex Developer
Job Id: 202301002
Location: Herndon, VA (Remote)
Yakshna Solutions, Inc., (YSI) is a CMMI Level 3 assessed, ISO 9001, 20000:1, 27001 certified, woman-owned small business enterprises, headquartered in Herndon, Virginia, USA. YSI provides professional IT solutions and services to business corporations and government organizations. YSI is committed to serve its business communities as a leading IT vendor providing innovative, quality, and cost-effective IT business solutions and services.
Our benefits are very competitive that include 401(k), health, dental, and vision insurance, Life insurance, short-term and long-term disability insurance, paid time off, training, and professional development assistance.
YSI is seeking a highly qualified Senior Data Engineer. The selected candidate will be able to communicate effectively (written/verbal), possess strong interpersonal skills, be self-motivated, and be innovative in a fast-paced environment.
Responsibilities:
The Data Engineer will be the senior technical expert on work associated with data management, data quality and data structures, coordinating with the ADA as needed to ensure development and data structures are synchronized.
The Data Engineer will design the approach for data tasks and will contribute to completion of data tasks and oversee execution, providing advice and guidance as necessary to junior staff.
Required Qualifications and Skills:
Bachelors or master’s in relevant filed.
Good Data Engineering/Management experience
· Should be familiar with the the Civil Works missions for Hydropower, Recreation, Environmental Stewardship, and Water Supply.
· Extensive technical knowledge of programming in a software stack that includes an Oracle Relational database, SQL, PL/SQL, Oracle Spatial, JavaScript, Oracle REST Data Services (ORDS), and Oracle APEX to make the necessary revisions to the relevant systems. In addition to these general skills and experience, also possess the following:
· Knowledge and experience in developing and maintaining a relational database operated in Amazon Web Services (AWS cloud).
· Knowledge and experience of data entry and options to increase automation and efficiency.
· Knowledge and experience with documenting data systems and processes and creating reports for increased efficiencies.
· Knowledge and experience in optimizing the performance of existing Oracle based applications, including procedures, functions, etc.
· Knowledge and experience with creating Representational State Transfer (RESTful) services utilizing common data dissemination formats.
· Knowledge and experience with making websites 508 compliant.
· Knowledge and experience with authentication and authorization procedures
· Knowledge and experience with maintaining geospatial data in oracle relational database.
· Knowledge and experience with geospatial web services (OGC & ESRI REST)
· Knowledge and experience with cloud native development methodologies
Job Types: Full-time, Contract
Pay: $110,000.00 - $120,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Professional development assistance
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
11+ years
8 years
9 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Herndon, VA 20170: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Preferred)
Experience:
Oracle Apex: 5 years (Preferred)
AWS CLOUD: 5 years (Preferred)
Erwin: 8 years (Preferred)
Data modeling: 8 years (Preferred)
Metadata: 5 years (Preferred)
Work Location: Remote","$115,000 /yr (est.)",10000+ Employees,Company - Public,Manufacturing,Machinery Manufacturing,2011,$1 to $5 billion (USD)
"Syngenta Group
4.0",4.0,"Downers Grove, IL",Senior Master Data Engineer,"Company Description

Syngenta Group is one of the world’s leading sustainable agriculture innovation companies, with roots going back more than 250 years. Our 53,000 people across more than 100 countries strive every day to transform agriculture through tailor-made solutions for the benefit of farmers, society and our planet –making us the world’s most local agricultural technology and innovation partner.
Syngenta Group is committed to operating at the highest standards of ethics and integrity. This is a commitment that we are making to investors, customers, society and employees. Syngenta Group is also committed to maintaining a workplace environment free from discrimination and harassment.

Job Description

The Senior Master Data Engineer will be responsible for ensuring Syngenta has the right identity data capabilities to support the current and future Syngenta production and commercial needs.
We have the responsibility to think beyond our past needs and help unlock future opportunities with one of our most valuable data assets. Robust, accurate and trusted identity data will unlock opportunities to improve customer experience, simplify vendor interactions and allow us to explore new ways of marketing our products and services.
The investment in the MDG platform has been the first phase or our journey to support our current and future business needs. The Senior Master Data Engineer will be responsible to build on this first phase and help to define our vision, strategy, operating model, and roadmap for the future of Party data capabilities. This may include supplementing the MDG platform with additional technology and services
Responsibilities
Contributes to creating a breakthrough transformation that shapes the Party data (including Identity and Reference Data) capability in line with Syngenta's strategic vision
Help define & deliver the strategy and roadmap for Identity data and reference data (including operating model, data products, technology, data quality & process analytics/health) ensuring solutions and technologies are maintainable and scalable
Enroll and align with stakeholders to experiment and leverage the Identity data capabilities within relevant domains (Employee, Sales/Customers, Legal Entities, Intercompany, Vendors). Including simplification, automation, rationalization, and harmonization.
Create and advocate Identity data offers (data as a product) that add value and solve business problems that improve integration and adoption.
Provide technical leadership in leveraging and experimenting with appropriate technologies including existing platforms as well as new opportunities (e.g. Microservices, API’s, AI, ML, etc.) to create Identity data products and services to meet business needs.
Seamlessly embed themselves in a cross-functional teams as a subject matter expert and participate in Identity data design authority.
Contribute to the creation and implementation of a reference data capability the compliments Identity master data.
The preferred candidate will be near an established Syngenta location The right candidate will be considered for a remote setting IN THE UNITED STATES.
We are unable to provide Visa Sponsorship for this position at this time.

Qualifications

Required Skills / Experience
Bachelor’s degree with 8 or more years of relevant experience
MUST HAVE hands on experience with a popular MDM tool
Extensive experience in customer master data management
Must have thought leadership and the ability to influence the business with best practices
Experience in reference data principles and practices.
Understand relevant technologies (Master Data Management tools, Microservices, etc)
Stakeholder management / influencing: able to engage with different functions, leadership levels and cultures

Additional Information

What we Offer
A culture that celebrates diversity & inclusion, promotes professional development, and strives for a work-life balance that supports the team members
We offer flexible work options to support your work and personal needs
Full Benefit Package (Medical, Dental & Vision) that starts your first day
401k plan with company match, Profit Sharing & Retirement Savings Contribution
Paid Vacation, 9 Paid Holidays, Maternity and Paternity Leave, Education Assistance, Wellness Programs, Corporate Discounts, among other benefits
Syngenta is an Equal Opportunity Employer and does not discriminate in recruitment, hiring, training, promotion or any other employment practices for reasons of race, color, religion, gender, national origin, age, sexual orientation, marital or veteran status, disability, or any other legally protected status
Family and Medical Leave Act (FMLA)
(http://www.dol.gov/whd/regs/compliance/posters/fmla.htm)
Equal Employment Opportunity Commission's (EEOC)
(http://webapps.dol.gov/elaws/firststep/poster_direct.htm)
Employee Polygraph Protection Act (EPPA)
(http://www.dol.gov/whd/regs/compliance/posters/eppa.htm)

#LI-SB2","$100,820 /yr (est.)",10000+ Employees,Company - Private,Agriculture,Crop Production,2000,$100 to $500 million (USD)
"Texas Capital Bank
3.0",3.0,"Richardson, TX",Senior Data Engineer,"Overview:
A Sr. Data Engineer is proficient in the development of all aspects of data processing including data warehouse architecture/modeling and ETL processing. The position focuses on development and delivery of analytical solutions using various tools including AWS Glue, Lambda, Snowflake and AWS RDS. A Sr. Data Engineer must be able to work autonomously with little guidance or instruction to deliver business value.
Responsibilities:
Partner with business stakeholders to gather requirements and translate them into technical specifications and process documentation for IT counterparts (onshore and offshore)
Highly proficient in the architecture and development of an event driven data warehouse; streaming, batch, data modeling, and storage
Advanced database knowledge; creating/optimizing SQL queries, stored procedures, functions, partitioning data, indexing, and reading execution plans
Skilled experience in writing and troubleshooting Python/PySpark scripts to generate extracts, cleanse, conform and deliver data for consumption
Expert level of understanding and implementing ETL architecture; data profiling, process flow, metric logging, and error handling
Support continuous improvement by investigating and presenting alternatives to processes and technologies to an architectural review board
Develop and ensure adherence to published system architectural decisions and development standards
Lead and foster junior data engineers in their careers to produce higher quality solutions at a faster velocity through optimization training and code review
Multi-task across several ongoing projects and daily duties of varying priorities as required
Interact with global technical teams to communicate business requirements and collaboratively build data solutions
Qualifications:
Education
Bachelor’s degree in computer science or MIS related area required or equivalent experience (industry experience substitutable)
Job experience
4-8 years of total experience in data engineering/Cloud development activity.
1+ years of experience in Banking and financial domain

Technical Requirement
Must be extremely proficient in Data Warehouse ETL Design/Architecture, dimensional/relational data modelling.
Experience in atleast one ETL development project, writing/analyzing complex stored procedures.
Should have entry level/intermediate experience in Python/PySpark – working knowledge on spark/pandas dataframe, spark multi-threading, exception handling, familiarity with different boto3 libraries, data transformation and ingestion methods, ability to write UDF.
Snowflake – Familiarity with stages and external tables, commands in snowflake like copy, unload data to/from S3, working knowledge of variant data type, flattening nested structure thru SQL, familiarity with marketplace integrations, role-based masking, pipes, data cloning, logs, user and role management is nice to have.
Familiarity with Coalesce is an added advantage for this job
Collibra integration experience for Data Quality and Governance in ETL pipeline development is nice to have.
AWS – Should have hands-on experience with S3, Glue (jobs, triggers, workflow, catalog, connectors, crawlers), CloudWatch, RDS and secrets manager.
AWS - VPC, IAM, Lambda, SNS, SQS, MWAA and Athena is nice to have.
Should have hands-on experience with version controlling tools like github, working knowledge on configuring, setting up CI/CD pipelines using yaml, pip files.
Streaming Services – Familiarity with Confluent Kafka or spark streaming or Kinesis (or equivalent) is nice to have.
Data Vault 2.0 (hubs satellite links) experience will be a
Highly proficient in Publisher, PowerPoint, SharePoint, Visio, Confluence and Azure DevOps
Working knowledge of best practices in value-driven development (requirements management, prototyping, hypothesis-driven development, usability testing)
Good communicator with problem solving mindset and focus on process improvement
Strong time management skills and a proven track record of meeting various deadlines
Strong executive presentation skills with expertise in PowerPoint and presentation best practices.
Consistently demonstrates clear and concise written and verbal communication skills
Good interpersonal skills, ability to interact with Senior Management
Highly self-motivated with a strong sense of initiative
Excellent multitasking skills and task management strategies
Ability to work well in a team environment, meet deadlines, demonstrate good time management, and multi-task in a fast-paced project environment.","$106,721 /yr (est.)",1001 to 5000 Employees,Company - Public,Financial Services,Banking & Lending,1998,Unknown / Non-Applicable
"CareFirst BlueCross BlueShield
3.5",3.5,"Reston, VA","Senior Data Engineer, (Hybrid)","Resp & Qualifications
PURPOSE:
The Senior Data Engineer is responsible for orchestrating, deploying, maintaining and scaling cloud OR on-premise infrastructure targeting big data and platform data management (Relational and NoSQL, distributed and converged) with emphasis on reliability, automation and performance. This role will focus on developing solutions and helping transform the company's platforms deliver data-driven, meaningful insights and value to company and clients.

ESSENTIAL FUNCTIONS:
Develops and maintains auditing systems (e.g., data warehouses, data lakes) using Ab Initio. Prepares and manipulates data using multiple technologies. Creates data collection frameworks for structured and unstructured data.
Interprets data, analyzes results using statistical techniques, and provides ongoing reports. Executes quantitative analyses that translate data into actionable insights. Provides analytical and data-driven decision-making support for key projects. Designs, manages, and conducts quality control procedures for data sets using data from multiple systems.
Supports Production and PLT environments; addresses issues in a timely fashion.
Improves data delivery engineering job knowledge by attending educational workshops; reviewing professional publications; establishing personal networks; benchmarking state-of-the-art practices; participating in professional societies.
Applies data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources.
Applies and implements best practices for data auditing, scalability, reliability and application performance.

SUPERVISORY RESPONSIBILITY:
Position does not have direct reports but is expected to assist in guiding and mentoring less experienced staff. May lead a team of matrixed resources.

QUALIFICATIONS:

Education Level: Bachelor's Degree, Details: Computer Science, Information Technology or Engineering or related field OR In lieu of a Bachelor's degree, an additional 4 years of relevant work experience is required in addition to the required work experience.

Experience: 5 years Experience with database design and developing modeling tools. Experience developing and updating ETL/ELT scripts. Hands-on experience with application development, relational database layout, development, data modeling.
The incumbent must have expereince in Ab Initio, Unix shell scripting, SQL, Axway, Cognos. Exposure to Big Data technologies (Cloudera) and AWS is a big plus. Must be willing to support off hours (weekends) implementation to Production and Plan Test environments and address issues as needed.
Preference will be given to candidates having hands-on experience with FEPOC and FEPDO systems such as HEDIS, FEDVIP Dental and Vision reports, Truven, IBM Watson Health etc.
Knowledge, Skills and Abilities (KSAs)
Knowledge and understanding of at least one programming language (i.e., Ab Initio, SQL, NoSQL, Python).
Knowledge and understanding of database design and implementation concepts.
Knowledge and understanding of data exchange formats.
Knowledge and understanding of data movement concepts.
Strong technical and analytical and problem solving skills to troubleshoot to solve a variety of problems.
Requires strong organizational and communication skills, written and verbal, with the ability to handle multiple priorities.
Must be able to effectively work in a fast-paced environment with frequently changing priorities, deadlines, and workloads that can be variable for long periods of time. Must be able to meet established deadlines and handle multiple customer service demands from internal and external customers, within set expectations for service excellence. Must be able to effectively communicate and provide positive customer service to every internal and external customer, including customers who may be demanding or otherwise challenging.

Department
Department: (EIS Technical Delivery)
Equal Employment Opportunity
CareFirst BlueCross BlueShield is an Equal Opportunity (EEO) employer. It is the policy of the Company to provide equal employment opportunities to all qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information.
Hire Range Disclaimer
Actual salary will be based on relevant job experience and work history.
Where To Apply
PeopleSoft/Self Service/Recruiting
Federal Disc/Physical Demand
Note: The incumbent is required to immediately disclose any debarment, exclusion, or other event that makes him/her ineligible to perform work directly or indirectly on Federal health care programs.
PHYSICAL DEMANDS:
The associate is primarily seated while performing the duties of the position. Occasional walking or standing is required. The hands are regularly used to write, type, key and handle or feel small controls and objects. The associate must frequently talk and hear. Weights up to 25 pounds are occasionally lifted.
Sponsorship in US
Must be eligible to work in the U.S. without Sponsorship
#LI-KT1","$128,662 /yr (est.)",5001 to 10000 Employees,Company - Private,Insurance,Insurance Carriers,1942,$100 to $500 million (USD)
"Abile Group, Inc.
5.0",5.0,"Saint Louis, MO",Data Cloud Engineer - Master,"Overview:
Abile Group has an exciting and challenging opportunity for a Data Cloud Engineer-Master on a 10 year contract providing User Facing and Data Center Services supporting an Intelligence Community customer. All the personnel on the team will work together to support innovative design, engineering, procurement, implementation, operations, sustainment and disposal of user facing and data center information technology (IT) services on multiple networks and security domains, at multiple locations worldwide, to support the IC mission.

The right candidate will possess the below skills and qualifications and be ready to handle all responsibilities independently and professionally.
Responsibilities:
Provides technical/management leadership on major tasks or technology assignments.
Establishes goals and plans that meet project objectives. Has domain and expert technical knowledge.
Directs and controls activities for a client, having overall responsibility for financial management, methods, and staffing to ensure that technical requirements are met.
Interactions involve client negotiations and interfacing with senior management.
Decision making and domain knowledge may have a critical impact on overall project implementation.
May supervise others.
Qualifications:
Clearance Required: TS/SCI

Degree and Years of Experience: BS/BA and 10 -15 years of relevant experience

Required Skills:
Experience in the various aspects of hybrid cloud activities.
Supports procurement and deployment of Platform Services to enable application portability across the private and public cloud environments offered by NGA.
Readies NGA's Hybrid Cloud Environment for system migration to IC ITE and oversee the future expansion of NGA Hybrid Cloud to additional public clouds.
In concert with DCS Government, supports standardization of DCS operations in a NGA Hybrid Cloud Management environment.
Transforms Government cloud requirements into appropriate technological alternatives and provides expertise in hybrid virtualization and cloud environments.
Experience developing systems, products, and/or processes based on a total systems perspective.
Consults, plans, analyzes designs, develops tests, assures quality, configures, installs, implements, integrates, maintains, and manages systems.
Has and maintains a diverse set of skills across multiple technical disciplines with recognized expertise in multiple disciplines and possess advanced knowledge of multiple mature and emerging technologies.
Works across organizational boundaries, both internally and externally and helps to drive the relationship between technical solutions and business needs of customers. Analyzes, defines and documents customer needs and required functionality.
Designs, develops and tests theoretical and/or physical models and develops the system design, considering operational impacts, performance, testing, manufacturing, cost and schedule, training, maintenance, and support.
Performs system level design trade analysis, reviews and approves system specifications and description documents, determines how a system is to be built, tested, and implemented, plans the system development execution and ensures adherence to appropriate standards, policies, principles, and practices.
Analyzes system capacity and performance to support problem resolution and system enhancements and monitors systems tests.
Responds to inquiries from a variety of sources for the purpose of providing technical assistance, consultation, advice and support, and regularly provides advice and recommends actions and solutions involving complex issues.

About Abile Group, Inc.:
Abile Group, Inc. was formed in July 2004 to partner with the Intelligence Community and their Contractors in the areas of Enterprise Analytics & Performance Management, IT & Systems Engineering and Program & Project Management. We have significant experience with the Federal Government and are an EDWOSB dedicated to our employees and clients. We are looking for high performing employees who enjoy providing advice and guidance along with solutions development and implementation support, crafted by combining industry best practices with the clients’ subject matter experience and Abile’s breadth of expertise.
EEO Statement:
Abile Group, Inc. is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability. Anyone requiring reasonable accommodations should email careers@abilegroup.com with requested details. A member of the HR team will respond to your request within 2 business days.

Please review our current job openings and apply for the positions you believe may be a fit. If you are not an immediate fit, we will also keep your resume in our database for future opportunities.",#N/A,51 to 200 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"NexTek, LLC",#N/A,"Shoreview, MN",Sr. Data Engineer,"Nextek, LLC is a software development company in the financial services industry that creates web applications and solutions for financial services professionals. We are seeking a motivated Senior Data Engineer to join the team.
What we provide:
A laptop and other equipment/software necessary for work
Health Insurance, Dental Insurance, Vision Insurance
Paid time off (PTO), Volunteer time off (VTO)
Hybrid remote work options
401K
Life Insurance
Flexible schedules
Monthly Social Activities
Casual dress code
Professional development
Here is what we are looking for:
Experience in database administration, security, backup, and recovery
Preferred Knowledge:4+ years of experience in data/software engineering
Experience in the financial services industry
Experience with a variety of relational databases (particularly MySQL & MSSQL)
Experience with collecting and presenting analytics
Here is what you will be doing:
Display proficiency in relevant technical skills.
Maintain a deep understanding of the data structure.
Provide evaluations and recommendations for our data environment based on potential issues, tradeoffs, and risks.
Scope and implement solutions.
Make good estimates and set realistic expectations for delivery.
Ask for guidance when necessary.
Maintain and monitor the health of databases and associated solutions.
Make use of the same environments setup by the Software Engineering team
Thoroughly test code for bugs and security issues before deploying to production environments
Maintain and monitor actively deployed solutions
Follow and help define best practices
Peer reviews
Maintain awareness of industry trends
Share experience and insights to help your team grow
Increase understanding of impact on other teams
Increase understanding of the customer
All other duties as assigned
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Shoreview, MN: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Acubed
4.0",4.0,"Sunnyvale, CA",Staff ML Engineer (Perception / Data),"WAYFINDER
Wayfinder is building scalable, certifiable autonomy systems to power the next generation of commercial aircraft. Our team of experts is driving the maturation of machine learning and other core technologies for autonomous flight; we are creating a reference architecture that includes hardware, software, and a data-driven development process to allow aircraft to perceive and react to their environment. Autonomous flight is transforming the transportation industry, and our team is at the heart of this revolution.
The Opportunity
As a Staff ML Engineer, you will lead the technical execution of our data and ML pipelines with an overall focus into improving the overall end to end perception pipeline (labeling, training and deployment).
This is a hands-on role, so you will be responsible for implementing things yourself, while also acting as a tech lead for the ML and Data engineering teams.
Responsibilities
Be an architect for our overall perception pipeline (training, testing, monitoring and labeling)
Explore, prototype and validate new algorithms/solutions
Be a mentor for other team members within the ML and Data teams
Champion highest quality of engineering excellence
Be able to move from research to productize ML models (fast iteration)
Negotiate initiatives and deliverables with stakeholders
Have deep understanding of the business and operational impact for different technology tradeoffs
Capable of influencing and building consensus in technical debates
Requirements
BS, MS, or higher degree, in CS/CE/EE, or equivalent industry experience
Extensive experience with ML frameworks such as Tensorflow, Caffe, and PyTorch
Strong programming skills in Python and C++
Growing expertise with state-of-the-art perception related ML models
Excellent mathematical reasoning skills, especially with probability
8+ years of experience in computer vision and machine learning
Expertise in setting architectures that are scalable, fault-tolerant and extensible for changes.
Ability to design across multiple systems
Ability to wear several hats between coding, technical strategy, mentorship etc.
Proven record of productizing computer vision models
Strongly Preferred Qualifications
PhD in computer science or machine learning
Experience with MCAP, OpenCV
Experience with CUDA
Real-world experience applying machine learning techniques in autonomous systems such as robots, cars, and UAV
Benefits
Exceptional PPO medical, dental and vision benefits with 100% of premiums covered for employee and their family/dependents
Generous PTO of 5 weeks (6 weeks after two years) in addition to 11 national holidays and unlimited paid sick days
Professional development reimbursement or $15,750 for flight training
3 months paid parental leave from Day 1
Pay Transparency Notice: Depending on your work location and years of experience, the target annual salary for this position can range from $160,000 to $220,000 + target bonus + benefits (including medical, dental, vision, 401(k), and flight training).
Note that Acubed does not offer sponsorship of employment-based nonimmigrant visa petitions for this role.","$190,000 /yr (est.)",51 to 200 Employees,Company - Public,Aerospace & Defense,Aerospace & Defense,#N/A,Less than $1 million (USD)
"Mass General Brigham
3.8",3.8,"Somerville, MA",Sr. Data Engineer (Data Lakes),"Sr. Data Engineer (Data Lakes)
- (3244480)

About Us:
At Mass General Brigham Digital, we pride ourselves on our ability to create maximum strategic, clinical, and operational value from established and emergent technologies for our patients, care teams, researchers, and employees. Digital health will not only enhance the equity and efficiency of healthcare delivery, but it will also help make medicine more personalized and precise.
We recognize that increasing value and continually improving quality while maintaining an inclusive focus are essential to organizational excellence, and we invite you to join us on this journey. The work we do in Digital is a strategic imperative, and there is a strong and growing understanding of how together we will transform Mass General Brigham in innovative and impactful ways.
General Summary/ Overview:
At Mass General Brigham Digital, we pride ourselves on our ability to create maximum strategic, clinical, and operational value from established and emergent technologies for our patients, care teams, researchers, and employees. Digital health will not only enhance the equity and efficiency of healthcare delivery, but it will also help make medicine more personalized and precise.
We recognize that increasing value and continually improving quality while maintaining an inclusive focus are essential to organizational excellence, and we invite you to join us on this journey. The work we do in Digital is a strategic imperative, and there is a strong and growing understanding of how together we will transform Mass General Brigham in innovative and impactful ways.
Summary:
Reporting to the Engineering Manager, Data Lake, the Senior Data Engineer (Azure Data Lake) will work towards analyzing, designing, developing, and building ADF data pipelines, ELT/ETL frameworks, and Azure data lake platforms, primarily focusing on Epic (EHR) data and other healthcare data; and will thrive as a member of an experienced, high performing and highly motivated team. Role will be responsible for participating in building out our existing EDW and our new Data Lake, expanding our data ecosystem and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. Requires advanced experience with data engineering and building Azure Cloud Data Lake, Azure Big Data Analytics technologies and architecture, Enterprise Analytics Solutions, and optimizing 'big data' data pipelines, architectures, and data sets. Expert level of experience with Design and Architecture of Azure big data frameworks/tools: Azure Data Lake, Azure Data Factory, Azure Data Bricks, Azure ML, SQL Data Warehouse. Advanced Experience with Hadoop based technologies (e.g., hdfs, Spark) and Programming experience in Python, SQL, Spark.
Principal Duties and Responsibilities:
Design, Develop, construct, test and maintain Data Lake architectures and large-scale data processing systems.
Support big data ecosystem related Tool selection and POC analysis.
Gather and process raw data at scale that meet functional / non-functional business requirements (including writing scripts, REST API calls, SQL Queries, etc.).
Develop data set processes for data modeling, mining and production.
Integrate new data management technologies ( Informatica DQ..) and software engineering tools into existing structures.
The candidate will be responsible for participating in building out our Data Lake platform, expanding and optimizing our data ecosystem and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams.
The ideal candidate is an experienced data pipeline builder who enjoys optimizing data systems and building them from the ground up.
The Data Engineer will support our Software Developers, Database Architects, Data Analysts and Data Scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects.
They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products.
Create and maintain optimal data pipeline architecture, assemble large, complex data sets that meet functional / non-functional business requirements on cloud based data platforms (e.g. Azure) and relational data systems (SQL Server, SSIS).
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, etc.
Build the data infrastructure required for optimal extraction, transformation, and loading of data from traditional/legacy data sources.
Work with stakeholders including the Management team, Product owners, and Architecture teams to assist with data-related technical issues and support their data infrastructure needs.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Use/s the Mass General Brigham values to govern decisions, actions, and behaviors. These values guide how we get our work done: Patients, Affordability, Accountability & Service Commitment, Decisiveness, Innovation & Thoughtful Risk; and how we treat each other: Diversity & Inclusion, Integrity & Respect, Learning, Continuous Improvement & Personal Growth, Teamwork & Collaboration.
Working Conditions:
This is a remote position.
Diversity Statement
As a not-for-profit organization, Mass General Brigham is committed to supporting patient care, research, teaching, and service to the community. We place great value on being a diverse, equitable and inclusive organization as we aim to reflect the diversity of the patients we serve. At Mass General Brigham, we believe in equal access to quality care, employment and advancement opportunities encompassing the full spectrum of human diversity: race, gender, sexual orientation, ability, religion, ethnicity, national origin and all the other forms of human presence and expression that make us better able to provide innovative and cutting-edge healthcare and research.

5+ Years of experience data engineering and building Azure Cloud Data Lake technologies and architecture, Enterprise Analytics Solutions, and optimizing 'big data' data pipelines, architectures and data sets.
5-7 Years of Experience with Hadoop based technologies (e.g. hdfs, Spark). Spark Experience desirable
5+ years of Programming experience in Python, SQL, PySpark.
Healthcare experience, most notably in Clinical data, Epic, Clarity, Caboodle, Payer data and reference data is a plus but not mandatory.
Experience with Design and Architecture of Azure big data frameworks/tools: Azure Data Lake, Azure Data Factory, Snowflake, Azure Data Bricks, Powershell.
Experience with Design and Architecture of relational SQL and NoSQL databases, including MS SQL Server, Cosmos DB.
Experience with Design and Architecture of data security and Azure security, VM, Vnet.
Experience with building processes supporting data transformation, data structures, metadata, dependency and workload management.
Experience leading and working with cross-functional teams in a dynamic environment.
Experience building Big data pipeline with Spark and/or Data Bricks is a plus.
Leading development of Data Lake Architectures from scratch.
Experience with Azure DevOps/CI-CD, Continuous integration and deployment.
Experience with Real time analytics on Spark, Kafka, Event Hub is a plus.
Experience in petabyte scale data environments and integration of data from multiple diverse sources.
Skills/Abilities/Competencies:
Advanced hands-on SQL, Spark, Python, pySpark (2+ of these) knowledge and experience working with relational databases for data querying and retrieval.
Strong SQL skills on multiple platform (preferred MPP systems).
Data Modeling tools (e.g. Erwin, Visio).
Strong interpersonal and communication skills, both written and verbal.
Strong Scrum/Agile development experience.
Excellent organizational skills and attention to detail, manage multiple tasks and projects, meet deadlines, follow through, and manage to schedule.
Strong innovation capabilities and the ability to think creatively.
Strong collaboration and team building skills within, across and outside of an organization.
Maintain and promote a positive team environment.
Maintains stable performance under pressure, demonstrating sensitivity to diverse organizational culture.
Ability to effectively cope with change, remain flexible and adaptable within a fast-paced environment with rapidly changing requirements, and ability to negotiate situations when the big picture is not clearly defined.

EEO Statement

Mass General Brigham is an Equal Opportunity Employer. By embracing diverse skills, perspectives, and ideas, we choose to lead. All qualified applicants will receive consideration for employment without regard to race, color, religious creed, national origin, sex, age, gender identity, disability, sexual orientation, military service, genetic information, and/or other status protected under the law. We will ensure that all individuals with a disability are provided a reasonable accommodation to participate in the job application or interview process, perform essential job functions, and receive other benefits and privileges of employment.

Primary Location MA-Somerville-MGB Assembly Row
Work Locations MGB Assembly Row 399 Revolution Drive Somerville 02145
Job Business and Systems Analyst
Organization Mass General Brigham
Schedule Full-time
Standard Hours 40
Shift Day Job
Employee Status Regular
Recruiting Department MGB Digital
Job Posting May 12, 2023","$118,726 /yr (est.)",1001 to 5000 Employees,Nonprofit Organization,Healthcare,Health Care Services & Hospitals,1994,$10+ billion (USD)
"Amway Inc.
3.8",3.8,"Ada, MI",Associate Data Engineer I/II,"Job title: Associate Data Engineer I/II
Department / Division: Global Supply Chain Analytics / Supply Chain
Location: Ada, MI
Salary Range (13): $61,093 - $79,062
Salary Range: (14): $71,540 - $92,581

This role is a hybrid in-office/remote role. You must be available to report to the Ada, MI office on Tuesdays & Wednesdays.

What do we need:
We are looking for an Associate Data Engineer with a desire to work in AWS or Google Cloud Platform to step into this newly created role to build and scale next generation data platforms for our Supply Chain organization. We need a high energy data wrangler who is ready to support our global team.

What’s special about this team:
The Supply Chain Analytics and Demand Planning Analytics business group provides the organization with high quality, innovative data solutions in a focused, fast and fun atmosphere. The team is seeking a tech savvy Data Engineer to work on building, operating, and scaling next generation data platforms and tools that will power data-driven analytics capabilities throughout the entire organization. This newly created role will report to the Manager of Supply Chain Analytics.

What’s special about this role:
As an Associate Data Engineer, you will be responsible for expanding and optimizing our data access and data pipeline analytics processes, as well as optimizing data flow and collection for cross functional teams. You will be a data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.

You will support our global community of data scientists and analysts on enterprise initiatives and will ensure optimal data delivery is consistent throughout ongoing projects. If you are excited by the prospect of optimizing or even re-designing our company’s analytics processes to support our next generation of advanced analytics capabilities, which in-turn translates to implementing strategic enhancements to maintain our competitive edge in the industry, this may be the opportunity for you.

Required qualifications:
0-1 years of experience preferred in Data Engineering, Business Analytics processes
Preferred Experience with AWS and Google Cloud Platform “big data” technologies
Familiarity or proficiency with technologies like Hadoop (and related ecosystem), Spark, Kafka, EC2, EMR, RDS, and Redshift in supporting data transformation, data structures, metadata, dependency and workload management
BA/BS degree in Computer Science, Finance, Supply Chain or related field

Skills to be successful in the role:
Self-directed and comfortable supporting the data needs of multiple teams, systems, and products within the Amway’s data eco-system
Understanding of distributed systems driving large-scale data processing and analytics with a successful history of manipulating, processing and extracting value from large disconnected datasets
Advanced working database knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases such as Microsoft and Oracle databases to advance analytics capabilities

This role is “Not” eligible for sponsorship.","$70,078 /yr (est.)",10000+ Employees,Company - Private,Manufacturing,Consumer Product Manufacturing,1959,$5 to $10 billion (USD)
"Shutterfly
3.3",3.3,"Tempe, AZ",Senior Data Engineer,"At Shutterfly, we make life’s experiences unforgettable. We believe there is extraordinary power in the self-expression. That’s why our family of brands helps customers create products and capture moments that reflect who they uniquely are.

As part of the Data Engineering team you will tackle the scalability, performance and distributed computing challenges needed to collect, process and store data for Shutterfly. You will enhance the Data Warehouse at Shutterfly on AWS, supporting the breadth and depth of the company’s analytic needs.
What You'll Do as a Sr. Data Engineer II
Build design, develop, test, deploy, maintain and enhance full-stack data engineering solutions
Provide technical leadership to the Data Warehouse and Shutterfly teams who use Shutterfly’s Enterprise Data Warehouse
With your technical expertise own and manage project priorities, deadlines and deliverables
Evangelize the benefits of existing solutions and new technologies to drive the use and push the technology of the Data Warehouse forward
Work closely with Data Operations to improve CI/CD pipelines am improve the operation and performance of the Data Warehouse
The Skills You'll Bring
Expert knowledge Python, Spark, SQL scripting and Java
7+ years of hands-on experience with building data ingress frameworks and experience with dealing with a wide variety of source data formats
Hands-on experience in building data applications, including design, implementation, debugging, and support.
Supporting a diverse and inclusive workforce is important to Shutterfly not only because it directly reflects our value of Embracing our Differences, but also because it’s the right thing to do for our business and for our people. Learn more about our commitment to Diversity, Equity and Inclusion at Shutterfly DE&I.","$135,953 /yr (est.)",10000+ Employees,Company - Private,Information Technology,Internet & Web Services,1999,$1 to $5 billion (USD)
"Octo
4.2",4.2,"Chantilly, VA",Data Engineer,"You…
As a Data Engineer, you will be joining the team that is deploying and delivering a cloud-based, multi-domain Common Data Fabric (CDF), which provides data sharing services to the entire DoD Intelligence Community (IC). The CDF connects all IC data providers and consumers. It uses fully automated policy-based access controls to create a machine-to-machine data brokerage service, which is enabling the transition away from legacy point-to-point solutions across the IC enterprise.
Us…
We were founded as a fresh alternative in the Government Consulting Community and are dedicated to the belief that results are a product of analytical thinking, agile design principles and that solutions are built in collaboration with, not for, our customers. This mantra drives us to succeed and act as true partners in advancing our client’s missions.
Program Mission…
The CDF program is an evolution for the way DoD programs, services, and combat support agencies access data by providing data consumers (e.g., systems, app developers, etc.) with a “one-stop shop” for obtaining ISR data. The CDF significantly increases the DI2E’s ability to meet the ISR needs of joint and combined task force commanders by providing enterprise data at scale. The CDF serves as the scalable, modular, open architecture that enables interoperability for the collection, processing, exploitation, dissemination, and archiving of all forms and formats of intelligence data. Through the CDF, programs can easily share data and access new sources using their existing architecture. The CDF is a network and end-user agnostic capability that enables enterprise intelligence data sharing from sensor tasking to product dissemination.
Responsibilities...
Primary responsibility is to work with data providers within the IC and DoD Enterprise to identify and ingest data sets into the CDF data broker. In this role you will:
Develop, optimize, and maintain data ingest flows using Apache Nifi and Python.
Develop within the components in the cloud platform, such as Apache Kafka, NiFi, and HBase.
Communicate with data owners to set up and ensure CDF streaming and batching components are working (including configuration parameters).
Document SOP related to streaming configuration, batch configuration or API management depending on role requirement.
Document details of each data ingest activity to ensure they can be understood by the rest of the team
What we’d like to see…
A minimum of 3 years of experience with programming and software development including analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems
DoD 8570 IAT Level II Certification (e.g. Security+) or the ability to obtain the certification within 90 days
Demonstrable CentOS command line knowledge
Working knowledge of web services environments, languages, and formats such as RESTful APIs, SOAP, FTP/SFTP, HTML, JavaScript, XML, and JSON
Understanding of foundational ETL concepts
Experience implementing data ignorations with in the IC DoD Enterprise.
Desired Skills:
Experience or expertise using, managing, and/or testing API Gateway tools and Rest APIs (desired)
2+ Experience in Python Development
Experience or expertise configuring an LDAP client to connect to IPA (desired)
Advanced organizational skills with the ability to handle multiple assignments
Strong written and oral communication skills
Years of Experience: Junior Level (0-4 years),Mid Level (5-8 years), Senior Level (9+)
Education: Bachelor's degree in systems engineering, computer engineering, or a related technical field (preferred)
Location: Chantilly, VA
Clearance: Active TS/SCI w/ ability to obtain CI Poly (preferred)","$99,041 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,$100 to $500 million (USD)
"Boston Dynamics AI Institute
4.7",4.7,"Cambridge, MA",Data Engineer,"Our Mission
Our mission is to solve the most important and fundamental challenges in AI and Robotics to enable future generations of intelligent machines that will help us all live better lives.

Data Engineers will work cross-functionally, creating new technology to improve software development for robots. If you have a passion for developing technology for robots and use it to advance their capabilities and usefulness, you will want to join us! We are onsite in our new Cambridge, MA office where we are building a collaborative and exciting new organization.
Responsibilities
Work collaboratively with research scientists and software engineers on software development for a range of different robotic platforms
Develop and maintain our data warehouses and data pipelines in cloud and on-premise infrastructureBuild event and batch driven ingestion systems for machine learning and R&D as needed
Develop and administer databases, knowledge bases, and distributed data stores
Create and use systems to clean, integrate, or fuse datasets to produce data products
Establish and monitor data integrity and value through visualization, profiling, and statistical tools
Perform updates, migrations, and administration tasks for data systems
Develop and implement a data governance, data retention strategyUse Python and SQL to develop, maintain and scale our data stores
Requirements
BS/MS in computer science, robotics, or a related field
5+ years of experience in a data engineering or similar role
Demonstrated experience with a variety of relational database and data warehousing technology such as AWS Redshift, Athena, RDS, BigQuery
Demonstrated experience with big data processing systems and distributed computing technology such as Databricks, Spark, Sagemaker, Kafka, etc
Strong experience with ETL design and implementations in the context of large, multimodal, and distributed datasets
Bonus (Not Required)
3+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)
2+ year experience working on real-time data and streaming applications
2+ years of experience with NoSQL implementation (Mongo, Cassandra)2+ years of experience with Airflow
2+ years of data warehousing experience (Redshift or Snowflake)
3+ years of experience with UNIX/Linux including basic commands and shell scripting
We provide equal employment opportunities to all employees and applicants for employment and prohibit discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.","$112,287 /yr (est.)",201 to 500 Employees,Subsidiary or Business Segment,Information Technology,Computer Hardware Development,1992,$5 to $25 million (USD)
"Barclays
4.0",4.0,"Whippany, NJ",Data Engineer,"Data Engineer
Whippany, NJ
As a Barclays Data Engineer, you will be contributing directly to the execution of the business strategy and play a key role in development of future state data science platform. You will also work collaboratively with a cross- functional team of data scientists and database developers, business intelligence designers, architects, business analysts and infrastructure engineers.
Barclays is one of the world's largest and most respected financial institutions, with 329 years of success, quality, and innovation behind us. We've helped millions of individuals and businesses thrive, creating financial and digital solutions that the world now takes for granted. An important and growing presence in the USA, we offer careers providing endless opportunity.
We are currently in the early stages of implementing a hybrid working environment, which means that many colleagues spend part of their working hours at home and part in the office, depending on the nature of the role they are in. We’re flexible on how this works and it may continue to change and evolve. Depending on your team, typically this means that colleagues spend a minimum of between 20% to 60% of their time in the office, which could be over a week, a month or a quarter. However, some colleagues may choose to spend more time in the office over a typical period than their role type requires. We also have a flexible working process where, subject to business needs, all colleagues globally are able to request work patterns to reflect their personal circumstances
Please discuss the detail of the working pattern options for the role with the hiring manager.
What will you be doing?
Designing scalable and secure engineering solutions that will be leveraged by Banking colleagues and customers
Working collaboratively with cross-functional team of data scientists & database developers, business Intelligence designers, architects, business analysts and infrastructure engineers
Being responsible for full life cycle development and design of new data science platform with Python and AWS based applications and components
Working as a team player in a global development group, and participating in requirements and data analysis, design as well as development
Communicating and collaborating between the infrastructure, development, and business groups
Having excellent analytical skills, being self-motivated and capable of working in a dynamic environment that demands multi-tasking
Having the ability to generate ideas and efficiently mock-up proposals and demos
What we’re looking for:
B.S. degree in computer science or related field with emphasis on technology
Five years of experience in Python
At least two years of AWS experience
Two plus years of SQL experience
Skills that will help you in the role:
Working experience in financial industry, especially IB applications is a plus
Prior experience with AWS Services such as Lambda, Glue, Athena, ECS, Cloud Formation
Working Experience in developing REST APIs using Python
Experience in building and deploying services using any container orchestration tools such as Kubernetes, ECS, Docker Swarm, OpenShift
Where will you be working?
At Barclays, we are proud to be redefining the future of finance and here at Whippany we are defining the future of the workplace and the future of the way we work and live. We are creating a unique community, one of four strategic tech-enabled hubs that will redefine opportunity for everyone who works here. Whatever you do at Whippany, you’ll have every chance to build a world-class career in this world-class environment.

#LI-Hybrid
#data","$96,821 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Banking & Lending,1690,$10+ billion (USD)
"JPMorgan Chase Bank, N.A.
3.8",3.8,"Aurora, CO",Data Center Operations Engineer II (Front End Day Shift),"Launch your career in Technology Operations and put your creative problem solving into action, delivering solutions that shape the future of global business. You'll work directly with clients to build strong customer relationships and problem-solve technical issues to make businesses more productive. Alongside a motivated team of fellow analysts, supervisors, and stakeholders, you'll develop innovative solutions to troubleshoot and resolve issues while accurately diagnosing problems and providing effective user support. Finally, your strong technology background will ensure that the security and standards of our commitment to excellence are met. And because professional development is a key component of our culture, you'll receive coaching, mentoring - and a host of other development opportunities - alongside your invaluable on-the-job experience.

This role requires a wide variety of strengths and capabilities, including:
Ability to identify problems and clearly communicate strategic solutions to clients
Desire to develop a working knowledge of change management, corporate IT audit processes, IT risk management, technical problem resolution, operations systems, and data sources knowledge
Strong initiative and desire to learn
Ability to effectively collaborate with team members and clients to achieve common goals
Good knowledge of Windows/MAC OS with the ability to carry out root cause analysis
Working knowledge of Microsoft Office products
Strong analytical and problem resolution skills
JPMorgan Chase & Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.
We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.
The health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the ""WELL Health-Safety Rating"" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.
As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm's current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm's vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.

We offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, discretionary incentive compensation which may be awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.
Equal Opportunity Employer/Disability/Veterans
Base Pay/Salary
Aurora,CO $31.98 - $52.16 / hour",$42.07 /hr (est.),10000+ Employees,Company - Public,Financial Services,Banking & Lending,1799,$10+ billion (USD)
"Brillio
3.8",3.8,"Austin, TX",AWS Data Engineer – R01525162,"AWS Data Engineer - R01525162
About Brillio:
Brillio is the partner of choice for many Fortune 1000 companies seeking to turn disruption into a competitive advantage through innovative digital adoption. Backed by Bain Capital, Brillio is one of the fastest growing digital technology service providers. We help clients harness the transformative potential of the four superpowers of technology - cloud computing, internet of things (IoT), artificial intelligence (AI), and mobility. Born digital in 2014, we apply Customer Experience Solutions, Data Analytics and AI, Digital Infrastructure and Security, and Platform and Product Engineering expertise to help clients quickly innovate for growth, create digital products, build service platforms, and drive smarter, data-driven performance. With delivery locations across United States, Romania, Canada, Mexico, and India, our growing global workforce of over 6,000 Brillians blends the latest technology and design thinking with digital fluency to solve complex business problems and drive competitive differentiation for our clients. Brillio was awarded ‘Great Place to Work’ in 2021 and 2022

Consultant
Primary Skills

Specialization

Job requirements
Job Description: We are looking for an experienced AWS Data Engineer to join our team and help us build and maintain our data infrastructure on AWS. As an AWS Data Engineer, you will work closely with data Architect, Business and Data analysts, BI Developer, Customer Business and engineering team, and other stakeholders to design, implement, and manage data pipelines and systems that support our business needs.

Key Job Responsibilities:
Design, implement, and maintain data pipelines and data processing systems, ETL Infrastructure on AWS using technologies like Apache Spark, AWS Glue, AWS Lambda, and AWS S3 and strong Python and Pandas hands on experience.
Collaborate with data Architect and analysts/BI Developer to understand their data requirements and design data models that meet their needs.
Work with DevOps engineers to ensure that data pipelines are reliable, scalable, and secure.
Monitor and troubleshoot data pipelines to ensure that data is flowing correctly and on time.
Develop automation scripts to streamline deployment, testing, and maintenance of data pipelines and systems.
Document technical designs, standard operating procedures, and best practices for data engineering on AWS.
Keep up to date with emerging technologies and best practices in data engineering and recommend new tools and technologies as appropriate.
Excellent communication and collaboration skills.
Need strong commitment to project, Problem solving, Team Player

Preferred qualifications:
Bachelor’s or master’s degree in computer science, Information Technology, or a related field.
3+ years of experience in data engineering with a focus on AWS technologies.
Strong knowledge of AWS services such as S3, EC2, Glue, Lambda, and Athena.
Experience designing and building data pipelines using Apache Spark.
Experience working with SQL and NoSQL databases.
Familiarity with ETL processes and tools such as Talend or Informatica.
Strong scripting skills in Python or Java.
#LI-CH1
Know what it’s like to work and grow at Brillio: Click here","$88,650 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,2014,$100 to $500 million (USD)
Exo Therapeutics,#N/A,"Cambridge, MA","Co-op, Data Engineer","Exo Therapeutics (https://exo-therapeutics.com) is a small molecule drug discovery and development company with a pioneering technology to address intractable pharmaceutical targets. By leveraging the company’s ExoSight™ platform, Exo is developing a deep pipeline of potent drug candidates that bind exosites, distal and unique binding pockets that have the potential to reprogram enzyme activity for precise and robust therapeutic effect. Through this specific and selective approach to challenging targets, the company’s team of world-class researchers is unlocking breakthrough therapeutics in oncology, inflammation and a broad range of other diseases.
EXO Therapeutics
Co-op, Data Engineer
Cambridge, Massachusetts, United States
About the company
Exo Therapeutics (https://exo-therapeutics.com/) is a privately held small molecule drug discovery and development company with a pioneering technology to address intractable pharmaceutical targets. By leveraging the company’s ExoSightTM platform, Exo is developing a deep pipeline of potent drug candidates that bind exosites, distal and unique binding pockets that have the potential to reprogram enzyme activity for precise and robust therapeutic effect. Through this specific and selective approach to challenging targets, the company’s team of world-class researchers is unlocking breakthrough therapeutics in oncology, inflammation and a broad range of other diseases.

Overview
Exo Therapeutics is seeking a highly motivated data engineer co-op to join our bioinformatics team. You will work in mining and organizing data sources, building the data analytical workflows and applications to enable oncology/immunology target discovery efforts.
The data engineer co-op will work under the supervision of the Director of Bioinformatics. The ideal candidate should have solid scientific, engineering, and statistical background with strong curiosity of applying knowledge and expertise in addressing problems in the biotech drug R&D setting.

Responsibilities
Explore and curate public data resources to continue building an exosite focused knowledgebase to empower novel target identification and evaluation.
Work with bioinformatics and chemoinformatics peers on internal datasets processing and formatting, data models definition.
Design and establish the computational framework to build/integrate the knowledgebase and the computational pipeline to support new target nomination and prioritization.
Help maintain and upgrade the current data processing pipelines wherever necessary.
Work with the manager to establish the data ingestion, data analysis and application development SOP
Required Qualifications
MS/PhD candidate in a relevant field, such as bioinformatics, data science, data engineering
Experience of working with large scale biomedical datasets and databases, such as uniprot, PDB database, Depmap, TCGA
Experience with building and maintaining data processing and data analytical workflows
Experience with setting up and using database or data warehouse, such as SQL, Redshift and snowflake is a plus
Passionate about applying your skills and knowledge in solving problems in the drug R&D setting
Job Duration
Jun – Dec, 2023

All fully qualified applicants who are authorized to work in the US at the time of application will receive consideration for employment without regard to race, religion, gender, sexual orientation, national origin, ancestry, disability, veteran status, as protected under law.","$131,330 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Delta
4.3",4.3,"Atlanta, GA",Senior Data Engineer Modeler,"United States, Georgia, Atlanta
Information Technology
11-May-2023
Ref #: 20782
LinkedIn Tag: #LI-JM2
How you'll help us Keep Climbing (overview & key responsibilities)
Location: Atlanta GA - NOT Remote

Delta IT is on a journey of transformation. We are changing the way we do business from top to bottom. As leaders with vision within Delta, we strive to build important and innovative solutions and are looking for team members to help us realize our vision.

Delta employees are problem solvers, doers, innovators.

We are proactive.

We are collaborative.

We deliver impact to our customers.

Join us on our transformation journey in becoming a best-in-class IT organization at the world's best airline!

The Senior Data Engineer, Modeler to join our Enterprise Data team. This position is responsible for building, modifying, modernizing the conceptual, logical, and physical data models used in Enterprise Data databases. The position also requires establishing and maintaining effective partnerships with the Business and IT stakeholders.

We are looking for someone with strong analytical and organizational skills to transform data into insights, distill requirements, and develop processes. The candidate should have excellent communication skills, business maturity, and feel comfortable in a fast-paced environment.

Responsibilities
Understand the data needs of the company for ingestion, migration, storage, and access
Work with business teams to gather requirements for the database design and model
Collaborate with the Enterprise Data team and business teams to define requirements, then design and build database models
Design and build conceptual, logical, and physical data models in accordance with companys data standards
Apply relational and dimensional models for raw ingestion and curated/semantic layers
Create Physical Data Structures (DDLs) and corresponding metadata
Creating and maintaining data reference architecture architectures and integration patterns
Updating knowledge by tracking and understanding emerging large data and modeling practices and standards
Benefits and Perks to Help You Keep Climbing
Our culture is rooted in a shared dedication to living our values Care, Integrity, Resilience, Servant Leadership, and Teamwork every day, in everything we do. At Delta, our people are our success. At the heart of what we offer is our focus on Sharing Success with Delta employees. Exploring a career at Delta gives you a chance to see the world while earning great compensation and benefits to help you keep climbing along the way:
Competitive salary, industry-leading prot sharing program, and performance incentives.
401(k) with generous company contributions up to 9%
Paid time off including vacation, holidays, paid personal time, maternity and parental leave.
Comprehensive health Benefits including medical, dental, vision, short/long term disability and life Benefits.
Family care assistance through fertility support, surrogacy and adoption assistance, lactation support, subsidized back-up care, and programs that help with loved ones in all stages.
Holistic Wellbeing programs to support physical, emotional, social, and financial health, including access to an employee assistance program offering support for you and anyone in your household, free financial coaching, and extensive resources supporting mental health.
Domestic and International space-available flight privileges for employees and eligible family members
Career development programs to achieve your long-term career goals.
World-wide partnerships to engage in community service and innovative goals created to focus on sustainability and reducing our carbon footprint
Business Resource Groups created to connect employees with common interests to promote inclusion, provide perspective and help implement strategies
Recognition rewards and awards through the platform Unstoppable Together
Access to over 500 discounts, specialty savings and voluntary benefits through Deltaperks such as car and hotel rentals and auto, home, and pet insurance, legal services, and childcare
What you need to succeed (minimum qualifications)
7 or more years of experience in Information Technology or related technical capacity
Expert in concepts and principles of data modeling
Knowledge of entity relationship, dimensional modeling, big data, enterprise data, and physical data models
Knowledge of relational databases and data architecture computer systems, including SQL Familiarity
Ability to design, build, and develop a new product, technology, or service from feasibility through to production
Familiarity with data modeling software such as SAP PowerDesigner, Microsoft Visio, E/R Studio or Erwin Data Modeler
Must have hands-on experience with cloud platforms; AWS preferred
Knowledge of big data platforms such as Teradata, Oracle DB, DB2, AWS Aurora, AWS Athena, etc.
Experience using Python and/or PowerShell scripting for data processing
Strong attention to detail
Excellent communicationwith both technical and business stakeholders
Ability to work in a fast-paced environment
Ability to work both independently and as part of a team
Understanding of the business and the ability to assess and address risk without negatively impacting the business
Consistently prioritizes safety and security of self, others, and personal data.
Embraces diverse people, thinking, and styles.
Possesses a high school diploma, GED, or high school equivalency.
Is at least 18 years of age and has authorization to work in the United States.
What will give you a competitive edge (preferred qualifications)
Bachelors or masters degree in Information Technology, Computer Science, Mathematics, Engineering, Information Systems, or equivalent
In-depth understanding of ETL and data ingestion processes used in large scale data warehouses
Knowledge of data architecture principles for on-prem and cloud solutions
Flexibility to adapt and plan for changing business objectives
Solid Understanding of Agile/Scrum development methodologies
Experience translating business outcome requirements into data model requirements
Ability to adapt communication style for technical and business audiences
< Go back","$124,024 /yr (est.)",10000+ Employees,Company - Public,Transportation & Logistics,"Airlines, Airports & Air Transportation",1928,$10+ billion (USD)
"Longevity Holdings Inc.
3.9",3.9,"Minneapolis, MN",Associate Data Engineer (Temporary),"As a Associate Data Engineer, you will treat data as an asset to design, build, and execute high performance and data centric solutions by using the comprehensive big data capabilities for the company's data platform environment. In this role, you will build and optimize data products to bring data and analytics products and solutions to businesses.
Essential Job Responsibilities:
· Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions
· Leading data collection efforts and performing trend analysis to identify common performance challenges that require further attention
· Partner closely with our data scientists to ensure the right data is made available in a timely manner to deliver compelling and insightful solutions
· Building out scalable data pipelines and choosing the right tools for the right job. Manage, optimize, and monitor data pipelines
· Incorporate core data management competencies including data governance, data security, and data quality
Required Skills:
· Bachelors in a quantitative field
· 1+ years of data engineering or equivalent experience
· Proficient use of tools, techniques, and manipulation including Cloud platforms, programming languages, and an understanding of software engineering practices
· Demonstrated knowledge of relational data sets, structures, and SQL
· Familiarity with big data platforms such as Apache Spark, Hadoop, Kafka, etc.
· Experience leading data organization, dashboarding, & visualization efforts
· Inquisitive, proactive, and interested in learning new tools and techniques
· Excellent communication skills
Longevity Holdings Inc prohibits discrimination and harassment and will take affirmative action to employ and
advance in employment qualified individuals based on their status as protected veterans or individuals with
disabilities, race, color, religion, sex, national origin, sexual orientation, and gender identity.
Our privacy notice is available at https://longevity.inc/employment-privacy-notice
Job Type: Full-time
Pay: $20.00 - $30.00 per hour
Schedule:
Monday to Friday
Application Question(s):
Will you now or in the future require sponsorship for employment visa status (e.g., H-1B visa status)?
Work Location: Hybrid remote in Minneapolis, MN 55402",$25.00 /hr (est.),51 to 200 Employees,Company - Private,Management & Consulting,Research & Development,1998,$25 to $100 million (USD)
"Yes Energy, LLC
4.6",4.6,"Boulder, CO","Data Operations Engineer II (Hybrid) Boulder - CO, Chicago - IL, Dedham - MA or Houston - TX","Data Operations Engineer II

Join the Market Leader in Electric Power Trading Solutions
The electrical grid is the largest and most complicated machine ever built. Yes Energy’s industry leading electric power trading analytics software provides real time visibility into the massive amount of data that is generated by the North American electrical grid every day. Our unique and innovative view of the data informs real time trading decisions that keep utility prices low and the grid up and running. It’s both challenging work and work with a purpose.
Be a part of our successful, growing business.
We are currently working in a hybrid environment and are seeking to fill one full time Data Operations Engineer II position immediately in Boulder - CO (HQ), Chicago - IL, Dedham - MA or Houston - TX.
About the team
At Yes Energy, our Market Data Operations (MDO) team plays a crucial role in our business. We are the control room for our customers, responsible for ensuring access to reliable and timely data every minute of every day.
We take pride in our responsibility to maintain the accuracy and reliability of our data. We understand that our clients rely on us to provide them with the information they need to make informed decisions, and we take that responsibility very seriously. Like a control room operator who is constantly monitoring the grid and making adjustments to ensure stability, our team is constantly monitoring our data pipelines and making adjustments to ensure data accuracy and reliability.
Our team is passionate about what we do, and we are dedicated to helping clients navigate the complex and dynamic North American Energy Markets. We work together in a collaborative environment, and we look to continuously improve our processes to ensure that we are providing the highest quality data possible to our clients.
About you
You have a passion for working with inherently messy data
You believe that a deep understanding of the data leads to better solutions
You have a competitive attitude, taking ownership and accountability of the work you produce
You have strong problem-solving skills and a curious mindset
You have experience maintaining and designing data pipelines
Like to design, develop, analyze and troubleshoot PL/SQL code

What you will do
Maintain our real-time data pipelines and ensure so that we can provide reliable and accurate information to our clients
Support clients by answering complex data questions, providing timely and effective solutions, so that we can empower our clients to make informed decisions
Ensure highest possible quality and integrity of Yes Energy data; recommend and implement ways to improve data reliability, efficiency, and quality
Participate in weekly on-call rotations to help resolve critical data pipeline failures for our clients

Requirements
4+ years of SQL or equivalent experience
4+ years of Oracle PL/SQL or equivalent experience
Bonus points for
Experience with ETL and complex data pipelines
Energy industry experience or experience in equities/commodities trading
Experience with web scraping, including HTML parsing, HTTP protocols and network logs
Experience with Python and Bash Scripting
Familiarity with Agile development methodologies
Position Details
Full-time
Reports to Data Operation team Lead
Minimal travel may be required (up to 10 days per year)
Keywords:
Oracle, SQL, PL/SQL, REST API, Time Series Data, ETL, CLI tools.
About Yes Energy
Overview
Yes Energy delivers real-time market data and electric power trading decision solutions. Over 1,000 market participants use Yes Energy solutions daily. The business is a leader in all aspects of information content collection and management, as well as in developing and delivering data and market analytics solutions. Since its inception in 2008 Yes Energy has become a trusted and respected supplier of innovative and reliable solutions focused on the needs of power market analysts, traders and trade managers. Yes Energy has a team of amazing professionals located in Boulder, CO (HQ), Dedham, MA and Chicago, IL.
Culture
At Yes Energy we care about saying “Yes” to customers. We like to listen and learn, and develop our solutions in line with our customers’ needs. We think about customers as business partners and when we help them to be more successful … We are more successful too.
Around the office our culture is driven by some pretty fundamental values that we’re proud of:
We love innovation and solving tough challenges;
We are “high standards people” who combine passion and pride with hard work and rewards of all kinds- in an ethic that is consistent across the company;
We’re team-focused with a flat hierarchy- we work in small teams on well-defined projects that directly impact the success of the business;
We play to the strengths and experience of each person, while each of us also works along a continuum of roles adjacent to our focus area. This presents a challenge of maintaining a broad set of skills as well as an opportunity to learn and contribute in many ways;
We are constantly growing. Professional development happens every day and every year.
Compensation and Benefits
Salary Range: $90,000 - $120,000 plus bonus.
We offer strongly competitive salaries and real bonuses that are achievable and that you can impact. Our benefits package is also very competitive and it includes medical insurance, 401K Plan with matching, flexible vacation and flexible work schedules. Investment in both formal and informal professional development is encouraged and funded by Yes Energy.
At Yes Energy we are dedicated to building a diverse, inclusive and authentic workplace, so if you’re excited about this role but your past experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyways. You may be just the right candidate for this or other roles.
Yes Energy provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. In addition to federal law requirements, Yes Energy complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.","$105,000 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Software Development,2008,Unknown / Non-Applicable
"ZAGG, Inc.
3.6",3.6,"Midvale, UT",Senior Data Engineer,"Position Summary
ZAGG is looking for an individual who can drive business change through insightful development of systems and applications. This position will work closely with business owners to architect, develop, and implement solutions across ERP, CRM, EDW, and other applications. A successful candidate will have excellent communication and technical skills to help implement business requirements into working solutions.
Salary: $120K to $135K
Responsibilities:
Administration – 30%
Work closely with multiple departments locally and overseas contributing and solving complex issues that will directly affect business operations and outcomes
Gather/evaluate requirements for business processes and technology enhancements while uncovering areas for improvement
Manage and prioritize projects and resources to ensure business goals are met and maximum value is created
Evangelize applications through effective process design and user training
Create and maintain documentation for operational and security audits

Development – 60%
Architect and manage solutions across multiple applications ensuring efficient integrations that provide redundancy, visibility, and extensibility
Utilize technology to improve the quality of life by automating and enhancing the ability of users/departments
Enhance cloud based data silos supporting Microsoft Dynamics CRM technologies
Conduct application testing and provide database management support
Create and maintain integrations between core applications, services, databases, etc.
Consolidate multiple data silos into a single EDW used for companywide Reporting and Analytics
Model and build data structures to support multi-dimensional data discovery

Other duties as required – 10%

Qualifications:
7+ years of similar experience in an engineering role utilizing an EDW system
Bachelor’s degree in Computer Science or related area
Exceptional analytical and conceptual thinking skills with a detail oriented and inquisitive personality
Proven experience gathering and interpreting business requirements and converting them to technical blue prints
Knowledgeable in enterprise technology stacks (Servers, Database, Network, EDI, etc)
Strong experience in cloud architecture and development (Azure, AWS)
Strong experience in supporting data structures supporting Microsoft Dynamics Technologies including MSSQL and Cloud CDS
Strong experience with integration tools and web service protocols such as SSIS, Fivetran, Synapse, Jitterbit, Smart Connect, Scribe, SOAP, REST, etc.
Experience in a development/scripting language such as python, powershell, etc.
Strong SQL Skills along with an understanding of data warehouse methodologies
Strong experience with data warehousing platforms (Azure DW, Redshift, Matrix, Snowflake)
Strong experience with visualization tools (PowerBI, Tableau, SSRS, etc)
Strong knowledge of SDLC and Agile/Scrum Framework
Strong understanding of operations in a consumer goods industry – sales forecasting, inventory, etc. is a plus
Be able to communicate with customers and co-workers in an effective, timely and professional manner
Strong interpersonal and meeting facilitation skills with technical project management experience being a plus
Must have a collaborative style and be able to cultivate and maintain an open environment where ideas are shared, questioned and tested","$127,500 /yr (est.)",501 to 1000 Employees,Company - Public,Manufacturing,Consumer Product Manufacturing,2005,$500 million to $1 billion (USD)
"Direct Line
3.8",3.8,"Ashburn, VA",Data Center Infrastructure Engineer,"SUMMARY:
Direct Line (“DL”) is a high growth global technology services company with primary focus in providing design, integration, installation, maintenance and managed services to well-known data center operators and technology companies. Direct Line deploys decades of experience and knowledge through key partnerships with hyperscale technology companies and multi-tenant data center operators that give its clients a competitive marketplace advantage. Direct Line is committed to continually improving our industry through certified training of cutting-edge technicians that deliver superior results with a passion for detail. Direct Line is headquartered in Fremont, California with additional locations in Virginia, Tennessee, North Carolina, New Mexico, the Pacific Northwest, Asia-Pacific, and Europe.
POSITION: Data Center Infrastructure Engineer
LOCATION: MUST BE LOCATED WITHIN A COMMUTABLE DISTANCE TO SANTA CLARA, CA OR ASHBURN, VA / MUST BE WILLING TO TRAVEL (75-80% travel)
Job description
We are now looking for a Data Center Infrastructure Engineer!
We are looking to grow our company and grow with the hardest working people in the world. Academic and commercial groups around the world are powering a revolution in artificial intelligence using deep learning techniques running on GPUs, enabling breakthroughs in the most complex problems from autonomous driving to medial image processing to natural language processing. Come work on an innovative company's AI technologies!
What You’ll Be Doing:
You will lead all aspects of and personally implement complex architectures in one of several data centers.
Solutions will include network, storage, and compute resources to meet customer requirements, SLAs and high levels of uptime. As a key member of the engineering team you will develop, implement, and lead rack-level elevation designs to ensure velocity and scale while efficiently utilizing space, power, and cooling.
Review, evaluate and improve the design and implementation of structured cable solutions to support network topologies
Establish continuous improvements in the design, implementation, deployment and operation of large-scale cloud-based solutions in power-dense air- and water-cooled environments
Develop and maintain processes and procedures associated with the management and deployment of data center infrastructure including asset management and RMAs
Support and expand data center monitoring applications, with a strong focus on CI/CD automation
You will ensure standards supporting operating procedures and engineering issues for problem incident management are followed, including all safety requirements
Handle network, electrical and mechanical operations at data centers focusing on availability, service delivery, and internal customer relationship management
Analyze and resolve critical engineering issues, often under tight timeframe pressures; Off hours and on-call hours are to be encouraged
What We Need to See:
You love solving hard problems and can work independently or as part of a team under tight timelines
You are passionate about providing outstanding support to customers
Bachelor’s degree in Math, Computer Science, or Engineering subject area. Equivalent background in Military Technical School also acceptable or equivalent experience in datacenter engineering operations.
6+ years’ experience as datacenter operations engineers with critical systems and telecommunications Infrastructure Standards, network certification is very desirable
Deep knowledge of data center operations including network, power, rack layouts, cabling, Raised Floor Systems, HOT/COLD aisle containment. Operational experience with compute, storage and GPU servers in both air- and water-cooled environments
Install, config, and maintain all NW and 3rd party HW
Experience with ERMA, Break-fix, etc.
Reading and understanding P2P cabling, labeling and cable mgt/dressing etc.
Ways to stand out of the crowd:
An obvious passion for getting things done in a fast-paced technology environment
Deep understanding of data center power and cooling infrastructure, of network and cabling infrastructure.
Experience with NetBox, CMMS, SNOW and Inventory Management tools.
You're a self-starter with an attitude for growth, continuous learning, and constantly looking to improve the team.
Attention to detail with superb interpersonal skills and the ability to effectively manage multiple priorities.
A positive attitude with a strategic outlook.
Constantly look to improve the team and build strong business relationships
Direct Line is a proud equal opportunity employer. We are a drug free, EEO employer committed to a diverse workforce. We will consider all qualified candidates regardless of race, color, national origin, sex, age, marital status, personal appearance, sexual orientation, gender identity, family responsibilities, disability, political affiliation, or veteran status.
Job Type: Full-time
Pay: $45.00 - $50.00 per hour
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible spending account
Health insurance
Life insurance
Paid time off
Referral program
Vision insurance
Schedule:
Monday to Friday
Experience:
CI/CD: 3 years (Required)
Linux: 3 years (Preferred)
Server Support: 3 years (Required)
Break/Fix / ERMA: 3 years (Preferred)
Data center Engineering: 3 years (Required)
Work Location: On the road",$47.50 /hr (est.),501 to 1000 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,1997,Unknown / Non-Applicable
"Chewy
3.5",3.5,"Richardson, TX",Data Engineer II,"Our Opportunity:
Chewy’s Data Analytics team has an exciting opportunity for a Data Engineer III to join the pack. Leveraging your strong expertise and background in data engineering and data analysis, you will be a part of a team responsible for operational and tactical reporting generating insights to grow Customer Service operations and planning. This includes building high quality data pipelines that drives analytic solutions and creating data products for analytics and data scientist team members to improve their productivity. Our organization is a fast-paced environment with new challenges and new opportunities each day. You will be responsible for building and implementing data products and technologies which will handle the growing business needs and play a key role in redefining what it means to be a world-class customer service organization.
What You'll Do:
Design, develop, optimize, and maintain data architecture and pipelines using design and programming patterns that follow best-in-class practices and principles.
Manage, maintain, and improve our SSOT tables and data marts, which drive critical business decisions every day.
Work closely with analytics teams and business partners, serving as a trusted partner who can advise, consult, and communicate data solutions.
Mentor and coach other data practitioners on data standards and practices.
Lead the evaluation, implementation and deployment of emerging tools and process for data engineering to improve overall productivity for the organization.
Partner with leaders, vendors, and other data practitioners across Chewy to develop technical architectures for strategic enterprise projects and initiatives.
Document technical details of work and follow agile sprint methodology, using tools like Jira, Confluence etc.

What You'll Need:
Bachelor of Science or Master’s degree in Computer Science, Engineering, Information Systems, Mathematics or related field
3+ years of enterprise experience as a data engineer and/or software engineer
3+ years applying and implementing database and data modeling techniques
3+ years working with enterprise data warehouse (ex. Snowflake, Vertica) and cloud environments (ex. AWS)
3+ years of experience building data integrations and pipelines from data lake, APIs, relational databases, and third-party systems
Strong software development skills in SQL
Self-motivated with strong problem-solving and self-learning skills.
Bonus:
Strong working knowledge of Python programming
Excellent communication and collaboration skills with ability to influence and guide stakeholders
Experience building dimensional models in data warehouses
Experience with data streaming tools and technologies like Kafka, Kinesis, or similar technologies
AWS Developer Certifications
E-commerce, Retail or startup experience
Experience in BI tools such as Tableau, Plotly, Power BI, etc.
Chewy is committed to equal opportunity. We value and embrace diversity and inclusion of all Team Members. If you have a disability under the Americans with Disabilities Act or similar law, and you need an accommodation during the application process or to perform these job requirements, or if you need a religious accommodation, please contact CAAR@chewy.com.

If you have a question regarding your application, please contact HR@chewy.com.

To access Chewy's Customer Privacy Policy, please click here. To access Chewy's California CPRA Job Applicant Privacy Policy, please click here.",#N/A,10000+ Employees,Company - Public,Retail & Wholesale,Pet & Pet Supplies Stores,2011,$5 to $25 million (USD)
"Johnson Controls
3.6",3.6,"Milwaukee, WI",PLM Data Engineer (Remote),"Job Details
What you will do
As the PLM Data Engineer, you are a leader within the Engineering IT team. Working under the Engineering Services IT leader, you will partner with peers in the Engineering Services IT organization as well as business and IT leaders globally. You will provide thought leadership on PLM data along with associated technologies and drive successful delivery of IT investments and services. You will develop, manage, and maintain relationships with business and IT leaders across the organization and participate in the development of IT strategies in support of business plans.
In this role you will support the data quality and the data health of the organization. This includes supporting the launch and operationalization of a PLM Data Factory enabling a large multi-year Windchill PLM deployment. You will support the development engineering data standards and data transformation as part of the Engineering Data Factory.
You will participate in the successful planning and execution of all IT related aspects of assigned projects and services as well as contributing to the business and project management aspects. You will coordinate and lead teams consisting of, but not limited to, Global Infrastructure, Enterprise Architecture, Application Delivery, Solution Development, Testing, Performance, Business, and IT Subject-Matter Experts, and including direct and indirect resources.
How you will do it
Serve as a liaison and project leader in partnership between the Business Unit (BU) IT department, Corporate IT department, BU Engineering Services department, Product Business teams, and cross-Product Business functions. Effectively collaborate with teams that span IT and the business.
Express a clear understanding of business structures, hierarchies, products, and services supported. Demonstrate knowledge outside of the technology arena with the Product Business supported.
Partner with other IT leaders, communicate and discuss technology trends, specific changes, and technology roadmaps.
Understand short- and long-term Engineering Services business plans and objectives, IT investments and assets. Participate in development of an IT strategy that enables Engineering Services business strategies. Lead the creation and continuous improvement of a subset of a business capability and IT enablement roadmap. Revise roadmap on at least an annual basis and support investment proposal development.
Support capture, aggregation, and prioritization of a portfolio of IT investments to enable needs as defined on the Engineering Services capability roadmaps. Champion Engineering Services priorities through IT investment portfolio governance process to ensure alignment of projects with business objectives, leverage of assets and practices, and synthesis with broader corporate-wide priorities. Champion understanding, support for, and value of Engineering Services IT investment portfolios.
Lead the ideation and high-level planning of IT projects including specification of high-level business requirements, and formulation of business process and technology solution options. Support creation and vetting of business cases, and high-level project planning (scope, schedule, & budget).
Project responsibilities include analysis, documentation, system design, development of project plans, time and cost estimates, and Capital Appropriation Requests.
Incorporate process standardization and simplification improvements in high-level designs to enable Global Products-wide leverage and streamline the IT applications landscape over time.
Gather requirements, perform analysis, and make recommendations for design of data models and software specifications; create project schedules; provide training; and coordinate implementation and support of developed applications.
Spearhead trade-off analyses and assessments (project timing, technology approaches, requirements prioritization, etc.), support solution design and testing activities, assess overall quality of solution and fit with business capability needs, ensure go-live readiness of Business and IT communities, and measure business results after solution stabilization.
Evaluate emerging technologies or methodologies and develop knowledge in these areas to apply to company projects.
Work closely with and drive accountability and results from internal and external service providers and support teams.
May provide third level support for applications, including working with technical teams, solving issues, routing to IT management, and providing documentation for other team members to use to respond to support issues.
Make decisions and achieve results while being a role model for company values.
Set challenging SMART personal goals that improve both personal skills and team deliverables.
What we look for
Required
Bachelor's Degree in Information Technology, Computer Science, Engineering, or a closely related subject area.
Minimum of 3 years' experience successfully delivering sophisticated Business Technology projects and achieving quantified business outcomes for a related business/industry.
Minimum 3 years' experience in technical data architect role.
Minimum 5 years' experience in product development domain.
Must have a thorough understanding of a broad cross-section of modern Information Technology concepts.
Experience developing solutions with large sophisticated global organizations.
Experience with implementation and roll-out of scalable out-of-the-box solutions as well as custom application development.
A foundational understanding of Agile concepts.
Demonstrated abilit to produce clear, concise, and accurate documentation detailing business processes and requirements.
Position requires knowledge of product development principles, processes, and techniques.
Position requires thorough understanding of Engineering tools including PLM, Informatica tools and PTC Windchill. Working knowledge of PTC Windchill required.
Ability to work collaboratively with a global community including Business Unit and Corporate IT, horizontal and vertical business functions, and end users.
Must have strong ability to communicate effectively with technology, business, and leadership audiences.
Travel varies from 25% to as much as 50% (at peak) including both domestic and international. Typical travel is 1-2 weeks.
Work with global teams, requiring flexibility of working hours.
Proficient English required.
Preferred:
PMI or SAFe certification
BRMi certification.
#DICE",#N/A,10000+ Employees,Company - Public,Manufacturing,Machinery Manufacturing,1885,$10+ billion (USD)
"Parker Wellbore
3.9",3.9,"Houston, TX","IT Data Engineer - (Houston, TX)","Company Description Parker Wellbore helps energy companies accomplish their drilling and production goals efficiently, reliably, and safely. Our global team supports oil and gas operators with innovative land and offshore drilling services, premium rental tools and well services, and advanced operations and management support. Founded in 1934, Parker Wellbore helps customers manage their costs and mitigate their risks, to achieve their operational goals in a safe and efficient manner. With experience in both harsh-environment regions and complex drilling situations, you can trust Parker Wellbore to get the job done.
Job Description

The IT Data Engineer will work closely with the business and IT team to build high-quality data pipelines supporting both our corporate and product divisions and will be key in supporting our current and future reporting and analytics solutions. This role requires a deep understanding of data architecture, data engineering, data analysis, reporting, and a basic understanding of data science techniques and workflows. This role will work with software developers, architects, and data analysts on expanding and optimizing our data and data pipeline architecture, as well as streamlining data flow and collection in support of our cross-functional teams.
Essential functions
Work with business to gather requirements and translate business needs to technical specifications
Builds and maintains Azure data platform including enterprise data lake and data warehouse in alignment with our strategic objectives and organizational goals
Create and maintain optimal data pipeline architecture
Collaborate with Cloud Solution Architects in implementing complex end-to-end Enterprise solutions on Microsoft Azure platform.
Develop policies and implement mechanisms for data ingestion into Azure data platform
Configure, validate, and implement various Azure tools such as but not limited to Databricks, Data Factory, Data Lake, Synapse Analytics, and Data Catalog as appropriate
Works with the Enterprise Information management manager, and enterprise architects to define data architecture and high-level solution design.
Actively collaborates with the business intelligence team, business teams, and project teams to understand data requirements, integration needs, constraints, and performance requirements
Works with Parker’s technology team to understand mathematical models and optimize data solutions accordingly.
Developing and maintaining Data Lake and data warehouse schematics, layouts, architectures, and relational/non-relational databases for data access and Advanced Analytics.

Qualifications

Necessary qualifications, skills and abilities
Bachelor’s Degree in Computer Science, or 3-5 years of equivalent work experience
Analytic Problem-Solving: Approaching high-level challenges with a clear eye on what is important; employing the right approach/methods to make the maximum use of time and human resources.
Effective Communication
Explore new territories to find creative and unusual ways to solve problems.
Data Analysis Knowledge: Understanding how data is collected, analyzed and utilized, Data Ingestion and Orchestration from on premise to Azure, Data Ingestion from Azure Blob Storage to Azure SQL DW
3-5 years of experience in Azure Data Factory, Azure Data Lake Analytics (USQL), Data processing in Azure, Azure DevOps CICD Pipelines, Azure SQL DW, Azure Blob Storage or Azure Data Lake Store, Azure Logic App/Functions, Azure Event Hub/IoT
Data Modeling, architecture and storage experience
3-5 years of experience in Power BI – DAX, SSAS tabular, PySpark/scripting, PowerShell, Stream Analytics
3-5 years of experience in Microsoft traditional data warehousing/BI (SQL Server, SSIS, SSAS, SSRS)
Exposure to Azure Services and big data processing solutions

Position competencies
Initiating & Driving Change • Acts as a catalyst for and takes responsibility for leading, directing, and managing organizational change • Develops new insights into situations and applies innovative solutions • Creates work environment that encourages creative thinking and innovation • Drives step changes in how the company operates • Understands how to change and addresses not only systems and processes, but also cultural aspects of change • Is good at bringing the creative ideas of others to market • Develops a change strategy that includes milestones and timelines • Accurately assesses the potential barriers and resources necessary for change initiatives • Understands and supports the need for change • Envisions and articulates the intended result of the change process • Provides direction and focus during the change process • Helps to generate support of the changes throughout the organization • Identifies and enlists allies who support the change process • Provides resources, removes barriers, and acts as an advocate for those initiating change
Result Focused • Establishes clear, specific performance goals, expectations, and priorities • Can be counted on to exceed goals successfully • Is constantly and consistently one of the top performers • Very bottom-line oriented • Steadfastly pushes self and others for results • Navigates quickly and effectively to resolve problems and obstacles • Persists to complete tasks / responsibilities, even in the face of difficulties • Develops a sense of urgency in others to complete tasks • Operates with personal ownership and looks for ways to improve performance all the time • Challenges him- or herself and others to raise the bar on performance • Focuses people on critical activities that yield a high impact • Holds self and others accountable for delivering high-quality results on time and within budget (e.g., models high work standards and demands the same from others)
Team Work • Blends people into teams when they are needed • Creates strong morale and spirit in his/her team • Shares wins and successes • Fosters open dialogue • Lets people finish and be responsible for their work • Seeks consensus among diverse viewpoints as a means of building group commitment • Defines success in terms of the whole team • Creates a feeling of belonging in the team • Values the contributions of all team members • Creates an environment that encourages open communication amongst team members • Creates an environment that encourages collective problem solving amongst team members
Customer Focus • Is dedicated to meeting the expectations and requirements of internal and external customers • Gets first-hand customer information and uses it to understand customers' business issues and needs for improvements in products and services • Acts with customer in mind • Establishes and maintains effective relationships with customers and gains their trust and respect • Genuinely enjoys working with customers to build long-term partnerships • Creates a sense of customer focus throughout their team/ department/ business unit
Physical demands and work environment
Ability to gather, analyze, and interpret data.
Ability to perform under stress, under pressure, and/or in emergency situations.
Ability to multitask, work in a fast-paced environment, meet deadlines, reason logically, and make sound decisions.
Ability to comprehend, remember, and follow verbal and written directions and comply with Company policies, procedures and standard.
Use repetitive wrist, hand or finger movements at a computer.
Ability to work as a team, communicate and interact with others in a professional manner, and consider alternative and diverse perspectives.

Additional Information

All your information will be kept confidential according to EEO guidelines.
Parker Wellbore provides equal opportunity for all people and will not discriminate on the basis of race, color, religion, sex, gender, sexual orientation, pregnancy, age, marital status, national origin, citizenship status, disability, genetic information, military service, veteran’s status or any other characteristic protected by applicable law.
If an applicant has a disability, the applicant may request accommodations when needed to enable that person to perform their essential job functions or to allow that person to participate in employment.","$55,584 /yr (est.)",1001 to 5000 Employees,Company - Public,"Energy, Mining & Utilities",Energy & Utilities,1934,Unknown / Non-Applicable
"Hy-Vee, Inc.
3.4",3.4,"Grimes, IA",Software Engineer II - Data,"At Hy-Vee our people are our strength. We promise “a helpful smile in every aisle” and those smiles can only come from a workforce that is fully engaged and committed to supporting our customers and each other.
Job Description:
Job Title: Software Engineer II
Department: Information Technology
FLSA : Exempt
General Function:
A developing professional participating in development, implementation, and support of applications with limited guidance.
Core Competencies
Partnerships
Growth mindset
Results oriented
Customer focused
Professionalism
Reporting Relations:
Accountable and Reports to: TBD
Positions that Report to you: TBD
Primary Duties and Responsibilities
Understand and apply foundational organization and industry engineering principles and core competencies; start to identify opportunities to improve.
Implements tasks of moderate scope with mostly defined specifications sometimes aided by direction or guidance from more experienced engineers. Demonstrates consistency, dependability, and confidence in work delivered.
Understands team's product, domain, and vision and how it fits into the overall business.
Understands team practices and processes and beings to discuss improvements with the team.
Willingly enters areas of ambiguity and unfamiliarity, often assisted by more experienced engineers when needed.
Manages risk by trying to unblock themselves first before seeking help. Can sometimes spot potential problems before they become problems. Starts to evaluate possible solutions by factoring in implications of each option.
Begin to identify tech debt, start to identify opportunities to improve, and sometimes make recommendations on how to implement.
Knowledge, Skills, Abilities, and Worker Characteristics:
Desire to grow as an individual through continuously learning new techniques.
Experience working within an environment with a continuous delivery mindset. Comfortable contributing in and to this kind of environment following existing patterns. Starting to identify opportunities to improve the process.
A few years experience developing and creating applications.
Aware of the importance of security.
Experience and Education
Bachelor degree preferred, or relevant experience.
Supervisory Responsibilities (Direct Reports)
None
Physical Requirements
Visual requirements include: ability to see detail at near range with or without correction.
Must be physically able to perform sedentary work: operating a computer, occasionally lifting or carrying objects of no more than 10 pounds, and occasionally standing or walking.
Must be able to perform the following physical activities: meeting with customers, kneeling, reaching, handling, grasping, feeling, talking, hearing, and repetitive motions.
Working Conditions
The duties for this position are performed in a general or remote office setting. There is weekly pressure to meet deadlines and handle multiple tasks in a day.
Equipment Used to Perform Job
Laptop and desktop computer, telephone, copier, Fax, printer, PC with Microsoft Office programs and other software relevant to specific position.
Financial Responsibility
None
Contacts
Has frequent contact with office personnel in other departments related to the position as well as occasional contact with users and customers.
Confidentiality
Has access to confidential information.
Are you ready to smile, apply today.","$73,262 /yr (est.)",10000+ Employees,Company - Private,Retail & Wholesale,Grocery Stores,1930,$5 to $10 billion (USD)
"DSFederal Inc
4.0",4.0,Remote,Data Engineer*,"Description:
We are seeking an experienced Data Engineer to design, build, and maintain our government client’s data infrastructure. The Data Engineer will work closely with cross-functional teams to develop scalable data solutions that support our client’s business needs.
Requirements:
Design, develop, and maintain data pipelines and data storage systems.
Collaborate with cross-functional teams to design and implement data-driven solutions.
Optimize and tune data systems for performance and scalability.
Implement and maintain data quality and validation processes.
Troubleshoot and debug data-related issues.
Participate in code reviews and contribute to best practices for data engineering.
Technical Skills Required:
Proficiency in one or more programming languages (Python, Java, etc.)
Experience with data modeling, database design, data marts, and data warehousing concepts
Knowledge of ETL tools and techniques
Experience with cloud-based data platforms such as AWS or Azure
Strong problem-solving and analytical skills
Desired Skills:
Familiarity with data visualization tools such as Tableau or Power BI.
Knowledge of machine learning concepts and frameworks.
Experience Required:
3+ years of experience in data engineering or related field
Education Required:
Bachelor's in engineering

Who We Are:
DSFederal is a leader in health IT and data analytics that delivers innovative solutions to improve healthcare delivery and address critical challenges, including cancer surveillance, HIV/AIDS, maternal and child health, and COVID-19 emergency response. Our highly experienced professionals serve over 50 different federal clients on over 80 projects. Our people are united by one mission – to improve human life through transformative solutions. www.dsfederal.com.
What We Can Offer You:
DSFederal attracts the best people in the business with our competitive benefits package that includes medical, dental and vision coverage, 401k plan with employer contribution, paid holidays, vacation, and more. If you enjoy being a part of a high performing, professional services, and technology focused organization, we encourage you to apply!
Our Mission: Improve human life through transformative solutions.
Our Core Values: Integrity, People First, Disciplined Business Practices, Customer Centric, Value Creation.
EEO Statement: DS Federal is an Equal Opportunity / Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, or protected veteran status.
E-Verify Statement
DS Federal utilizes the E-Verify program for every potential new hire. This makes it possible for us to confirm that every employee who works for DS Federal is eligible to work in the United States. To learn more about E-Verify you can call 1-800-255-7688 or visit their website by clicking the logo below. E-Verify® is a registered trademark of the United States Department of Homeland Security.
#LI-LP1",#N/A,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$25 to $100 million (USD)
"Apolis
3.9",3.9,"Saint Louis, MO",Data Engineer (Python Focused),"Role: Data Engineer (Python Focused) (W2 ONLY)
Location : St. Louis, MO 63102
Duration: 18 Months
The primary responsibilities of this role will be to work as a part of the data and analytics team of the client. The ideal candidate would have experience using ETL and other data engineering tools in a cloud environment. Additionally, this candidate would have coding and scripting experience creating code to move and manipulate data using tools such in Python such as Spark or Pandas. Microsoft Azure experience is a plus but relevant experience with other cloud-based environments will be considered.
This team uses Microsoft Azure to build and productionize data sets and processes to the product owners and product users and this role will be expected to contribute to those objectives. As a member of this team, you will work in conjunction with other data engineers, platform engineers, solution developers and others to execute and deliver working solutions to the end users.
Technical Skills
High focus on Python
More Programmer/Coder experience, less Cloud experience needed.
Must have experience with data analytics/engineering and cloud technologies and an in-depth working knowledge and understanding of data engineering, design, and integration.
Intermediate or higher level of proficiency with Python Must have the proven ability to learn quickly and apply cloud data engineering principles and skills to new tools and use cases.
Ability to define and take technical ownership of detailed requirements and collaborate effectively with various team members such as Business Analysts and Project Managers and end users.
Excellent communication (verbal and written) and problem-solving skills are essential for this position.
Initiative, Proactive Cooperation, and curiosity are mandatory behavioral skills required for this position.
Experience with several of the following technologies is preferred:
Experience with Microsoft Azure Platform. In particular, the data processing toolsets including Azure Data Factory Pipelines, Azure Databricks, Azure Data Lake Storage.
Experience using Notebooks (Such as Jupyter, DataBricks) also desired.
Experience using code languages such as Python and C#
Experience using query languages such as SQL, MDX and DAX
Experience using scripting languages such as PowerShell, M-Query (Power Query), and Windows batch commands.
Understanding of Agile and DevOps principles for software development and ownership.
Familiar with compliance and controls for software development and deployment.
Education Required
A bachelor's in computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field [or equivalent work experience] is not required but is preferred.
Job Type: Contract
Salary: $60.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Vision insurance
Compensation package:
Overtime pay
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
St. Louis, MO: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Preferred)
ETL: 5 years (Preferred)
Cloud infrastructure: 4 years (Preferred)
Work Location: One location",$65.00 /hr (est.),501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,1996,$25 to $100 million (USD)
"Texas Capital Bank
3.0",3.0,"Richardson, TX",Senior Data Engineer,"Overview:
A Sr. Data Engineer is proficient in the development of all aspects of data processing including data warehouse architecture/modeling and ETL processing. The position focuses on development and delivery of analytical solutions using various tools including AWS Glue, Lambda, Snowflake and AWS RDS. A Sr. Data Engineer must be able to work autonomously with little guidance or instruction to deliver business value.
Responsibilities:
Partner with business stakeholders to gather requirements and translate them into technical specifications and process documentation for IT counterparts (onshore and offshore)
Highly proficient in the architecture and development of an event driven data warehouse; streaming, batch, data modeling, and storage
Advanced database knowledge; creating/optimizing SQL queries, stored procedures, functions, partitioning data, indexing, and reading execution plans
Skilled experience in writing and troubleshooting Python/PySpark scripts to generate extracts, cleanse, conform and deliver data for consumption
Expert level of understanding and implementing ETL architecture; data profiling, process flow, metric logging, and error handling
Support continuous improvement by investigating and presenting alternatives to processes and technologies to an architectural review board
Develop and ensure adherence to published system architectural decisions and development standards
Lead and foster junior data engineers in their careers to produce higher quality solutions at a faster velocity through optimization training and code review
Multi-task across several ongoing projects and daily duties of varying priorities as required
Interact with global technical teams to communicate business requirements and collaboratively build data solutions
Qualifications:
Education
Bachelor’s degree in computer science or MIS related area required or equivalent experience (industry experience substitutable)
Job experience
4-8 years of total experience in data engineering/Cloud development activity.
1+ years of experience in Banking and financial domain

Technical Requirement
Must be extremely proficient in Data Warehouse ETL Design/Architecture, dimensional/relational data modelling.
Experience in atleast one ETL development project, writing/analyzing complex stored procedures.
Should have entry level/intermediate experience in Python/PySpark – working knowledge on spark/pandas dataframe, spark multi-threading, exception handling, familiarity with different boto3 libraries, data transformation and ingestion methods, ability to write UDF.
Snowflake – Familiarity with stages and external tables, commands in snowflake like copy, unload data to/from S3, working knowledge of variant data type, flattening nested structure thru SQL, familiarity with marketplace integrations, role-based masking, pipes, data cloning, logs, user and role management is nice to have.
Familiarity with Coalesce is an added advantage for this job
Collibra integration experience for Data Quality and Governance in ETL pipeline development is nice to have.
AWS – Should have hands-on experience with S3, Glue (jobs, triggers, workflow, catalog, connectors, crawlers), CloudWatch, RDS and secrets manager.
AWS - VPC, IAM, Lambda, SNS, SQS, MWAA and Athena is nice to have.
Should have hands-on experience with version controlling tools like github, working knowledge on configuring, setting up CI/CD pipelines using yaml, pip files.
Streaming Services – Familiarity with Confluent Kafka or spark streaming or Kinesis (or equivalent) is nice to have.
Data Vault 2.0 (hubs satellite links) experience will be a
Highly proficient in Publisher, PowerPoint, SharePoint, Visio, Confluence and Azure DevOps
Working knowledge of best practices in value-driven development (requirements management, prototyping, hypothesis-driven development, usability testing)
Good communicator with problem solving mindset and focus on process improvement
Strong time management skills and a proven track record of meeting various deadlines
Strong executive presentation skills with expertise in PowerPoint and presentation best practices.
Consistently demonstrates clear and concise written and verbal communication skills
Good interpersonal skills, ability to interact with Senior Management
Highly self-motivated with a strong sense of initiative
Excellent multitasking skills and task management strategies
Ability to work well in a team environment, meet deadlines, demonstrate good time management, and multi-task in a fast-paced project environment.","$106,721 /yr (est.)",1001 to 5000 Employees,Company - Public,Financial Services,Banking & Lending,1998,Unknown / Non-Applicable
"YSI
3.7",3.7,Remote,Senior Data Engineer,"Position Title: Senior Data Engineer/ Oracle Apex Developer
Job Id: 202301002
Location: Herndon, VA (Remote)
Yakshna Solutions, Inc., (YSI) is a CMMI Level 3 assessed, ISO 9001, 20000:1, 27001 certified, woman-owned small business enterprises, headquartered in Herndon, Virginia, USA. YSI provides professional IT solutions and services to business corporations and government organizations. YSI is committed to serve its business communities as a leading IT vendor providing innovative, quality, and cost-effective IT business solutions and services.
Our benefits are very competitive that include 401(k), health, dental, and vision insurance, Life insurance, short-term and long-term disability insurance, paid time off, training, and professional development assistance.
YSI is seeking a highly qualified Senior Data Engineer. The selected candidate will be able to communicate effectively (written/verbal), possess strong interpersonal skills, be self-motivated, and be innovative in a fast-paced environment.
Responsibilities:
The Data Engineer will be the senior technical expert on work associated with data management, data quality and data structures, coordinating with the ADA as needed to ensure development and data structures are synchronized.
The Data Engineer will design the approach for data tasks and will contribute to completion of data tasks and oversee execution, providing advice and guidance as necessary to junior staff.
Required Qualifications and Skills:
Bachelors or master’s in relevant filed.
Good Data Engineering/Management experience
· Should be familiar with the the Civil Works missions for Hydropower, Recreation, Environmental Stewardship, and Water Supply.
· Extensive technical knowledge of programming in a software stack that includes an Oracle Relational database, SQL, PL/SQL, Oracle Spatial, JavaScript, Oracle REST Data Services (ORDS), and Oracle APEX to make the necessary revisions to the relevant systems. In addition to these general skills and experience, also possess the following:
· Knowledge and experience in developing and maintaining a relational database operated in Amazon Web Services (AWS cloud).
· Knowledge and experience of data entry and options to increase automation and efficiency.
· Knowledge and experience with documenting data systems and processes and creating reports for increased efficiencies.
· Knowledge and experience in optimizing the performance of existing Oracle based applications, including procedures, functions, etc.
· Knowledge and experience with creating Representational State Transfer (RESTful) services utilizing common data dissemination formats.
· Knowledge and experience with making websites 508 compliant.
· Knowledge and experience with authentication and authorization procedures
· Knowledge and experience with maintaining geospatial data in oracle relational database.
· Knowledge and experience with geospatial web services (OGC & ESRI REST)
· Knowledge and experience with cloud native development methodologies
Job Types: Full-time, Contract
Pay: $110,000.00 - $120,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Professional development assistance
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
11+ years
8 years
9 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Herndon, VA 20170: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Preferred)
Experience:
Oracle Apex: 5 years (Preferred)
AWS CLOUD: 5 years (Preferred)
Erwin: 8 years (Preferred)
Data modeling: 8 years (Preferred)
Metadata: 5 years (Preferred)
Work Location: Remote","$115,000 /yr (est.)",10000+ Employees,Company - Public,Manufacturing,Machinery Manufacturing,2011,$1 to $5 billion (USD)
"Solect
3.4",3.4,"San Francisco, CA",Senior Data Engineer,"Job Locations US-CA-San Francisco | US-TX-Houston
Posted Date 1 month ago(4/7/2023 5:15 PM)
# of Openings
1
Category
Enterprise Technology
Job ID
2023-2250
Overview
Company Overview
Pattern Energy is a leading renewable energy company that develops, constructs, owns, and operates high-quality wind and solar generation, transmission, and energy storage facilities. Our mission is to transition the world to renewable energy through the sustainable development and responsible operation of facilities with respect for the environment, communities, and cultures where we have a presence.

Our approach begins and ends with establishing trust, accountability, and transparency. Our company values of creative spirit, pride of ownership, follow-through, and a team-first attitude drive us to pursue our mission every day. Our culture supports our values by fostering innovative and critical thinking and a deep belief in living up to our promises.

Headquartered in the United States, Pattern has a global portfolio of more than 35 power facilities and transmission assets, serving various customers that provide low-cost clean energy to millions of consumers.
Responsibilities
Job Purpose
Pattern Energy is embarking on a major business transformation to scale processes and systems to enable the significant growth of our business. One critical aspect of this transformation is building an enterprise data architecture capable of supporting strong enterprise analytical capabilities and interconnected data across functions for improved and real-time decision making. This new role of Senior Data Engineer will be instrumental in building this future.

The Senior Data Engineer will help guide Pattern’s journey by building novel and modern tools, pipelines, and data systems. Through lived experience this role will act in a key strategic resource in our transition from siloed, disparate data assets to a grand, unified, data estate. The senior data engineer will lead development of data systems, namely data lakehouse pipeline, and analytics environments to support data lifecycles across the business. These tools will help drive our efforts toward democratization of data, reducing duplication of effort, and adding value to the data stream.

Key Accountabilities
Work with the Data team to understand and interpret use cases around the business and develop tools/systems to support data lifecycle from the point of production to the point of consumption.
Implement modern technology and concepts to enable Pattern teams’ design of enterprise-grade systems capable of achieving data goals.
Develop pipelines and analytics tools, including automations wherever possible
Templatize these solutions to help our adoption of infrastructure-as-code
Qualifications
Experience/Qualifications/Education Required
Previous experience in building enterprise data lifecycle and cloud solutions.
5-7+ years experience, specific to data management, analytics, or data reporting.
B.S. in a technical field with M.S. preferred.
Significant experience delivering Data solutions using the Azure cloud stack
Experience with database, data lake, lakehouse design concepts and the use of SQL.
Experience with transformations and reporting solutions, namely Power BI
Experience with environment management, release management, code versioning, deployment methodologies, and CI/CD tools
Additional Requirements

Demonstrated excellence and ability to learn new programming skills and languages.
Strong understanding of security and governance principles, including access policies
Proven record of excellent communication and cooperation with varied stakeholders.
Renewable energy industry experience is strongly preferred.
Demonstrated experience building scalable data models & ingestion pipelines from a variety of systems including Enterprise Applications (e.g. ERP, CRM), Operational Assets (e.g. SCADA), Big Data (e.g. Weather or product configuration simulators).
Technical Skills
Experience throughout the Microsoft Integrated Data Platform (IDP) and related stacks including, but not limited to:
Azure, AAD, Synapse, Purview, Data Lake, Delta Lake, and Lakehouse design
Azure Data Factory, Data Explorer, Logic Apps, Power Automate
Databricks; PySpark, and optimized resource consumption, therein
The expected starting pay range for this role is $95,000 - $129,000 USD. This range is an estimate and base pay may be above or below the ranges based on several factors including but not limited to location, work experience, certifications, and education. In addition to base pay, Pattern’s compensation program includes a bonus structure for full-time employees of all levels. We also provide a comprehensive benefits package which includes medical, dental, vision, short and long-term disability, life insurance, voluntary benefits, family care benefits, employee assistance program, paid time off and bonding leave, paid holidays, 401(k)/RRSP retirement savings plan with employer contribution, and employee referral bonuses.

Pattern Energy Group is an Equal Opportunity Employer.","$112,000 /yr (est.)",51 to 200 Employees,Company - Private,"Energy, Mining & Utilities",Energy & Utilities,2009,$25 to $100 million (USD)
"Leadstack Inc
4.3",4.3,Remote,Lead Data Engineer with Sql/Python,"LeadStack Inc. is an award winning, one of the nation's fastest growing, certified minority owned (MBE) staffing services provider of contingent workforce. As a recognized industry leader in contingent workforce solutions and Certified as a Great Place to Work, we're proud to partner with some of the most admired Fortune 500 brands in the world.
Title: Lead Data Engineer with Sql/Python
Location: Remote
Duration: 6+months
Direct Client
Immediate interview
Top Skills: Azure, SQL/Python Developer.
Job Description:
Design and develop data system integrations and data models for reporting and alerting.
Essential Job Functions:
Drive digital innovation by leveraging innovative new technologies and approaches to renovate, extend, and transform the existing core data assets, including SQL-based, NoSQL-based, and Cloud-based data platforms
Define high-level migration plans to address the gaps between the current and future state
Present opportunities with cost/benefit analysis to leadership to shape sound architectural decisions
Lead the analysis of the technology environment to detect critical deficiencies and recommend solutions for improvement
Interpreting data, analyzing results using statistical techniques
Developing and implementing data analyses, data collection systems and other strategies that optimize statistical efficiency and quality
Acquiring data from primary or secondary data sources and maintaining databases
Promote the reuse of data assets, including the management of the data catalog for reference
Key Responsibilities:
Innovate, develop, and drive the development and communication of data strategy and roadmaps across the technology organization to support project portfolio and business strategy
Drive the development and communication of enterprise standards for data domains and data solutions, focusing on simplified integration and streamlined operational and analytical uses
Drive digital innovation by leveraging innovative new technologies and approaches to renovate, extend, and transform the existing core data assets, including SQL-based, NoSQL-based, and Cloud-based data platforms
Define high-level migration plans to address the gaps between the current and future state, typically in sync with the budgeting or other capital planning processes Lead the analysis of the technology environment to detect critical deficiencies and recommend solutions for improvement
Mentor team members in data principles, patterns, processes and practices
Promote the reuse of data assets, including the management of the data catalog for referenceDraft and review architectural diagrams, interface specifications and other design documents
To know more about current opportunities at LeadStack , please visit us on https://leadstackinc.com/careers/
Job Type: Contract
Salary: $70.00 - $80.00 per hour
Experience level:
11+ years
Schedule:
8 hour shift
Experience:
Azure: 5 years (Preferred)
Data warehouse: 1 year (Preferred)
SQL developer: 6 years (Preferred)
Work Location: Remote",$75.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2016,Unknown / Non-Applicable
"Chewy
3.5",3.5,"Richardson, TX",Data Engineer II,"Our Opportunity:
Chewy’s Data Analytics team has an exciting opportunity for a Data Engineer III to join the pack. Leveraging your strong expertise and background in data engineering and data analysis, you will be a part of a team responsible for operational and tactical reporting generating insights to grow Customer Service operations and planning. This includes building high quality data pipelines that drives analytic solutions and creating data products for analytics and data scientist team members to improve their productivity. Our organization is a fast-paced environment with new challenges and new opportunities each day. You will be responsible for building and implementing data products and technologies which will handle the growing business needs and play a key role in redefining what it means to be a world-class customer service organization.
What You'll Do:
Design, develop, optimize, and maintain data architecture and pipelines using design and programming patterns that follow best-in-class practices and principles.
Manage, maintain, and improve our SSOT tables and data marts, which drive critical business decisions every day.
Work closely with analytics teams and business partners, serving as a trusted partner who can advise, consult, and communicate data solutions.
Mentor and coach other data practitioners on data standards and practices.
Lead the evaluation, implementation and deployment of emerging tools and process for data engineering to improve overall productivity for the organization.
Partner with leaders, vendors, and other data practitioners across Chewy to develop technical architectures for strategic enterprise projects and initiatives.
Document technical details of work and follow agile sprint methodology, using tools like Jira, Confluence etc.

What You'll Need:
Bachelor of Science or Master’s degree in Computer Science, Engineering, Information Systems, Mathematics or related field
3+ years of enterprise experience as a data engineer and/or software engineer
3+ years applying and implementing database and data modeling techniques
3+ years working with enterprise data warehouse (ex. Snowflake, Vertica) and cloud environments (ex. AWS)
3+ years of experience building data integrations and pipelines from data lake, APIs, relational databases, and third-party systems
Strong software development skills in SQL
Self-motivated with strong problem-solving and self-learning skills.
Bonus:
Strong working knowledge of Python programming
Excellent communication and collaboration skills with ability to influence and guide stakeholders
Experience building dimensional models in data warehouses
Experience with data streaming tools and technologies like Kafka, Kinesis, or similar technologies
AWS Developer Certifications
E-commerce, Retail or startup experience
Experience in BI tools such as Tableau, Plotly, Power BI, etc.
Chewy is committed to equal opportunity. We value and embrace diversity and inclusion of all Team Members. If you have a disability under the Americans with Disabilities Act or similar law, and you need an accommodation during the application process or to perform these job requirements, or if you need a religious accommodation, please contact CAAR@chewy.com.

If you have a question regarding your application, please contact HR@chewy.com.

To access Chewy's Customer Privacy Policy, please click here. To access Chewy's California CPRA Job Applicant Privacy Policy, please click here.",#N/A,10000+ Employees,Company - Public,Retail & Wholesale,Pet & Pet Supplies Stores,2011,$5 to $25 million (USD)
"NexTek, LLC",#N/A,"Shoreview, MN",Sr. Data Engineer,"Nextek, LLC is a software development company in the financial services industry that creates web applications and solutions for financial services professionals. We are seeking a motivated Senior Data Engineer to join the team.
What we provide:
A laptop and other equipment/software necessary for work
Health Insurance, Dental Insurance, Vision Insurance
Paid time off (PTO), Volunteer time off (VTO)
Hybrid remote work options
401K
Life Insurance
Flexible schedules
Monthly Social Activities
Casual dress code
Professional development
Here is what we are looking for:
Experience in database administration, security, backup, and recovery
Preferred Knowledge:4+ years of experience in data/software engineering
Experience in the financial services industry
Experience with a variety of relational databases (particularly MySQL & MSSQL)
Experience with collecting and presenting analytics
Here is what you will be doing:
Display proficiency in relevant technical skills.
Maintain a deep understanding of the data structure.
Provide evaluations and recommendations for our data environment based on potential issues, tradeoffs, and risks.
Scope and implement solutions.
Make good estimates and set realistic expectations for delivery.
Ask for guidance when necessary.
Maintain and monitor the health of databases and associated solutions.
Make use of the same environments setup by the Software Engineering team
Thoroughly test code for bugs and security issues before deploying to production environments
Maintain and monitor actively deployed solutions
Follow and help define best practices
Peer reviews
Maintain awareness of industry trends
Share experience and insights to help your team grow
Increase understanding of impact on other teams
Increase understanding of the customer
All other duties as assigned
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Shoreview, MN: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Johns Hopkins University
4.0",4.0,"Baltimore, MD",PARADIM Data Engineer,"The Platform for the Accelerated Realization, Analysis, and Discovery of Interface Materials (PARADIM), a National Science Foundation Materials Innovation Platform developing the next generation of electronic and quantum materials, is looking for a full-time PARADIM Data Engineer to join the platforms materials data science team.

The successful candidate must be capable of designing and implementing software solutions related to data acquisition, streaming, processing, and storage, and should have a keen desire to partner with scientists developing a versatile, real-time, streaming data infrastructure to enable discovery and development of novel interface materials that will drive the quantum computing revolution.

The ideal candidate will also have technical skills that complement a vision and creativity to find solutions to connecting Big Data from diverse experimental and computational laboratories as well as the rigor and experience to develop high-quality, production software and data pipelines to address these challenges. The ability to travel by car to the campuses of both Cornell University in Ithaca, NY, and the Johns Hopkins University in Baltimore, MD, is strongly preferred.

Specific Duties & Responsibilities

With the chief data officer and PARADIM leadership team, design and implement software solutions related to data acquisition, streaming, processing, and storage, both centrally and at end stations at both PARADIM Institutions (JHU and Cornell).
Work with PARADIM scientists to design and implement a versatile, real-time, streaming data infrastructure to enable discovery and development of novel interface materials that will drive the quantum computing revolution.
Implement software tools for data analysis and interpretation as needed by the PARADIM platform.
Participate in the design of back-end and front-end systems to implement PARADIMs data vision.
Research and implement new technologies that could be beneficial to PARADIM.
Evaluate, test and vet new technology in support of PARADIM efforts.
Work with vendors to procure prototypes and demo units.
Attend department and University-sponsored training to increase knowledge, improve skills, and learn new skills.
May substitute University training for supervisor approved commercial job-related course offerings.

This position may be primarily remote (90%). Occasional in-person trips to Cornell University and/or Johns Hopkins University will be needed for work directly involving software on new hardware. This requirement may best suit candidates located between Baltimore, MD and Ithaca, NY.
Minimum Qualifications

Bachelor's Degree
Five years related experience
Additional education may substitute for required experience and additional related experience may substitute for required education, to the extent permitted by the JHU equivalency formula

Preferred Qualifications

The ideal candidate will also have technical skills that complement a vision and creativity to find solutions to connecting Big Data from diverse experimental and computational laboratories as well as the rigor and experience to develop high-quality, production software and data pipelines to address these challenges. The ability to travel by car to the campuses of both Cornell University in Ithaca, NY, and the Johns Hopkins University in Baltimore, MD, is strongly preferred.

Proficiency in at least one major object-oriented language such as Java, C++, or C#
Proficiency in at least one major software versioning and tracking platform (e.g. git, github, gitlab, svn)
Experience with python and pydata packages such as jupyterlab, cython, numpy, tensorflow
Experience working with instrumental data
Contributions towards open-source software and commitment to open-source development
Experience with Apache Kafka or Confluent Platform
Level of Independent Decision Making
Works independently

Classified Title: Systems Engineer
Role/Level/Range: ATP/04/PE
Starting Salary Range: $71,230 - $97,880 - $124,510 annually (Commensurate with experience)
Employee group: Full Time
Schedule: Monday-Friday, 8:30 am - 5:00pm
Exempt Status: Exempt
Location: Homewood Campus (Hybrid)
Department name: Chemistry
Personnel area: School of Arts & Sciences

Total Rewards
The referenced salary range is based on Johns Hopkins University's good faith belief at the time of posting. Actual compensation may vary based on factors such as geographic location, work experience, market conditions, education/training and skill level. Johns Hopkins offers a total rewards package that supports our employees' health, life, career and retirement. More information can be found here: https://hr.jhu.edu/benefits-worklife/

Please refer to the job description above to see which forms of equivalency are permitted for this position. If permitted, equivalencies will follow these guidelines:

JHU Equivalency Formula: 30 undergraduate degree credits (semester hours) or 18 graduate degree credits may substitute for one year of experience. Additional related experience may substitute for required education on the same basis. For jobs where equivalency is permitted, up to two years of non-related college course work may be applied towards the total minimum education/experience required for the respective job.

**Applicants who do not meet the posted requirements but are completing their final academic semester/quarter will be considered eligible for employment and may be asked to provide additional information confirming their academic completion date.

The successful candidate(s) for this position will be subject to a pre-employment background check. Johns Hopkins is committed to hiring individuals with a justice-involved background, consistent with applicable policies and current practice. A prior criminal history does not automatically preclude candidates from employment at Johns Hopkins University. In accordance with applicable law, the university will review, on an individual basis, the date of a candidate's conviction, the nature of the conviction and how the conviction relates to an essential job-related qualification or function.

The Johns Hopkins University values diversity, equity and inclusion and advances these through our key strategic framework, the JHU Roadmap on Diversity and Inclusion .

Equal Opportunity Employer

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.

EEO is the Law

Learn more:
https://www.eeoc.gov/sites/default/files/migrated_files/employers/poster_screen_reader_optimized.pdf

Accommodation Information

If you are interested in applying for employment with The Johns Hopkins University and require special assistance or accommodation during any part of the pre-employment process, please contact the Talent Acquisition Office at jhurecruitment@jhu.edu . For TTY users, call via Maryland Relay or dial 711. For more information about workplace accommodations or accessibility at Johns Hopkins University, please visit accessibility.jhu.edu .

Johns Hopkins has mandated COVID-19 and influenza vaccines, as applicable. Exceptions to the COVID and flu vaccine requirements may be provided to individuals for religious beliefs or medical reasons. Requests for an exception must be submitted to the JHU vaccination registry. For additional information, applicants for SOM positions should visit https://www.hopkinsmedicine.org/coronavirus/covid-19-vaccine/ and all other JHU applicants should visit https://covidinfo.jhu.edu/health-safety/covid-vaccination-information/ .

The following additional provisions may apply, depending on campus. Your recruiter will advise accordingly.

The pre-employment physical for positions in clinical areas, laboratories, working with research subjects, or involving community contact requires documentation of immune status against Rubella (German measles), Rubeola (Measles), Mumps, Varicella (chickenpox), Hepatitis B and documentation of having received the Tdap (Tetanus, diphtheria, pertussis) vaccination. This may include documentation of having two (2) MMR vaccines; two (2) Varicella vaccines; or antibody status to these diseases from laboratory testing. Blood tests for immunities to these diseases are ordinarily included in the pre-employment physical exam except for those employees who provide results of blood tests or immunization documentation from their own health care providers. Any vaccinations required for these diseases will be given at no cost in our Occupational Health office.

Note: Job Postings are updated daily and remain online until filled.

To apply, visit https://jobs.jhu.edu/job/Baltimore-PARADIM-Data-Engineer-MD-21218/1027795600/

jeid-8befc9e383ba7d4394f407df6f0f70f2","$124,510 /yr (est.)",10000+ Employees,College / University,Education,Colleges & Universities,1876,$1 to $5 billion (USD)
"Land O'Lakes
3.9",3.9,"Arden Hills, MN",Data Operations Engineer,"Join Land O’Lakes, Inc., and help us bring food from farmer to fork. We’re a global, Fortune 250 company and a farmer-owned cooperative. While benefits can vary by location and role, some offer the following. Please speak with your local HR Representative regarding benefit eligibility.

Medical, dental, vision, life and AD&D; short- and long-term disability insurance
Retirement savings plan and profit-sharing plan, both with company match
Paid time off and paid holidays, plus paid benefits for maternity and parental leaves
Data Operations Engineer
This position is remote (virtual) and can be located anywhere in the United States.
The Data Ops Engineer has an understanding that data is the foundation of any information-driven operating environment and is critical to the success of business outcomes. The Data Ops Engineer will work with the Data Architecture and Data Engineering teams to learn/assist in the design of solutions that ensure consistent, reliable, efficient, and sustainable data are delivered using best practices, standards and processes. The result of these efforts is highly accurate and trusted data assets for use in the Enterprise Data Platform and across the organization. Additionally, the Data Ops Engineer will also learn/assist in the effort to support the tools used to build the data platform for DevOps processes. The Data Operations Engineer will work to enhance the platform and utilities to drive efficiencies in existing data pipelines as well as the process to engineer new ones. Areas of responsibility:
Automate administration tasks using Python and SQL against API’s and CLI’s to nurture and develop the data platform
Work closely with technical staff to learn and optimize their technical environment needs
Administer cloud native platforms including Snowflake, Databricks, Qlik, Event Hubs, Power BI and other Azure services
Build notification solutions to ensure cloud platforms are operational and alerts are sent when they are not using methods such as pub/sub
Install patches and upgrades
Support issue resolution and escalation
Facilitate reporting of business metrics of platform (adoption, alignment to business goals, alignment to product team needs, etc.)
Build custom DevOps capabilities where tools or platforms do not have them built-in
Engage with software vendors to understand new capabilities of their platforms and tools
Establish platform-wide standards
Measure data pipeline alignment to standards
Measure uptime of platforms
Participate in MVP and PoC activities to understand new technical capabilities of the technology platform
Perform root cause analysis when there are issues
Required Experience/Education:
College or Vocation Training in Computer Science, Programming, or similar technical areas.
Or 1+ years work experience in data processing, programming languages (SQL, Python, SAS, R, etc.), infrastructure, dev ops, etc.. This could experience could be in school or a work environment.
Required Competencies/Skills:
Basic technical knowledge and understanding of data and processing
Ability to collaborate across all levels and areas of the data platform team
Ability to participate in PoC efforts to research emerging technologies
Ability to work closely with technical and business people, as well as perform hands-on implementation
CI/CD
Knowledge of modern data platform tools (Snowflake, Redshift/AWS, Big Query/Google, etc)
Knowledge of Python, SQL, Linux, Docker, or similar tools
Preferred Experience/Education:
Post High School college or tech training
Preferred Competencies/Skills:
Infrastructure as code
Desire to work in a fast paced, changing environment
Travel: This position requires quarterly travel to the corporate office.
Salary: $65,360-$98,040
Land O'Lakes, Inc. is an Equal Opportunity Employer (EOE) M/F/Vets/Disabled. The company maintains a drug-free workforce, including pre- and post-employment substance abuse testing pursuant to a Drug and Alcohol Policy.
Neither Land O’Lakes, nor its search firms, will ever contact you and ask for confidential information over the phone or in email. If you receive a call or email like this, please do not provide the information being requested.","$81,700 /yr (est.)",5001 to 10000 Employees,Company - Private,Manufacturing,Food & Beverage Manufacturing,1921,$10+ billion (USD)
"Abile Group, Inc.
5.0",5.0,"Saint Louis, MO",Data Cloud Engineer - Master,"Overview:
Abile Group has an exciting and challenging opportunity for a Data Cloud Engineer-Master on a 10 year contract providing User Facing and Data Center Services supporting an Intelligence Community customer. All the personnel on the team will work together to support innovative design, engineering, procurement, implementation, operations, sustainment and disposal of user facing and data center information technology (IT) services on multiple networks and security domains, at multiple locations worldwide, to support the IC mission.

The right candidate will possess the below skills and qualifications and be ready to handle all responsibilities independently and professionally.
Responsibilities:
Provides technical/management leadership on major tasks or technology assignments.
Establishes goals and plans that meet project objectives. Has domain and expert technical knowledge.
Directs and controls activities for a client, having overall responsibility for financial management, methods, and staffing to ensure that technical requirements are met.
Interactions involve client negotiations and interfacing with senior management.
Decision making and domain knowledge may have a critical impact on overall project implementation.
May supervise others.
Qualifications:
Clearance Required: TS/SCI

Degree and Years of Experience: BS/BA and 10 -15 years of relevant experience

Required Skills:
Experience in the various aspects of hybrid cloud activities.
Supports procurement and deployment of Platform Services to enable application portability across the private and public cloud environments offered by NGA.
Readies NGA's Hybrid Cloud Environment for system migration to IC ITE and oversee the future expansion of NGA Hybrid Cloud to additional public clouds.
In concert with DCS Government, supports standardization of DCS operations in a NGA Hybrid Cloud Management environment.
Transforms Government cloud requirements into appropriate technological alternatives and provides expertise in hybrid virtualization and cloud environments.
Experience developing systems, products, and/or processes based on a total systems perspective.
Consults, plans, analyzes designs, develops tests, assures quality, configures, installs, implements, integrates, maintains, and manages systems.
Has and maintains a diverse set of skills across multiple technical disciplines with recognized expertise in multiple disciplines and possess advanced knowledge of multiple mature and emerging technologies.
Works across organizational boundaries, both internally and externally and helps to drive the relationship between technical solutions and business needs of customers. Analyzes, defines and documents customer needs and required functionality.
Designs, develops and tests theoretical and/or physical models and develops the system design, considering operational impacts, performance, testing, manufacturing, cost and schedule, training, maintenance, and support.
Performs system level design trade analysis, reviews and approves system specifications and description documents, determines how a system is to be built, tested, and implemented, plans the system development execution and ensures adherence to appropriate standards, policies, principles, and practices.
Analyzes system capacity and performance to support problem resolution and system enhancements and monitors systems tests.
Responds to inquiries from a variety of sources for the purpose of providing technical assistance, consultation, advice and support, and regularly provides advice and recommends actions and solutions involving complex issues.

About Abile Group, Inc.:
Abile Group, Inc. was formed in July 2004 to partner with the Intelligence Community and their Contractors in the areas of Enterprise Analytics & Performance Management, IT & Systems Engineering and Program & Project Management. We have significant experience with the Federal Government and are an EDWOSB dedicated to our employees and clients. We are looking for high performing employees who enjoy providing advice and guidance along with solutions development and implementation support, crafted by combining industry best practices with the clients’ subject matter experience and Abile’s breadth of expertise.
EEO Statement:
Abile Group, Inc. is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability. Anyone requiring reasonable accommodations should email careers@abilegroup.com with requested details. A member of the HR team will respond to your request within 2 business days.

Please review our current job openings and apply for the positions you believe may be a fit. If you are not an immediate fit, we will also keep your resume in our database for future opportunities.",#N/A,51 to 200 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Point Predictive, Inc.",#N/A,"San Diego, CA",Senior Data Engineer,"Senior Data Engineer, San Diego, CA based OR willing to relocate to San Diego in 60 Days.
Company
PointPredictive is a fast-growing technology start-up that leverages a patented combination of artificial and natural intelligence [Ai+Ni] to provide risk assessments in the auto lending, mortgage, and retail space. The platform has been proven to reduce lender loan losses by 40-60% with review rates of 5-10% of their applications, resulting in higher productivity of lender risk management departments, significantly lower losses to their bottom lines, and improved customer experience. The company was founded in 2013 by a seasoned team of technology entrepreneurs with over 20 years of experience in the startup space (including several acquisitions) and has financial backing from top tier investors.
Role:
The company is looking for an outstanding Senior Data Engineer to focus on its Data Asset, scale the Database and Data Asset for high performance and reliability. You will also serve as an engineering resource for data related questions, issues, and bugs. Core skills include Python, Snowflake, database systems and SQL, and Amazon Web Services (AWS).
Responsibilities:
· Developing new extract-transform-load (ETL) processes and pipelines. Must be able to manage large volumes of data flowing in from a variety of formats and into a variety of location.
· Identifying, designing and implementing internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes. Automation experience highly desired.
Write complex SQL; be fluent in relational based systems; have strong analytical and problem-solving skills.
Ability to represent complex algorithms in software; bring strong understanding of database technologies, management systems, data structures, and algorithms; a deep understanding in database architecture testing methodology.
Develop and execute test plan, debugging, and testing scripts and tools.
Building real-time streaming data pipelines; building and deploying data pipelines, data streams, and extract-transform-load (ETL) processes.
Manage Data Governance and Data Cleansing, as well as supporting production issues and customer requests.
Provide engineering support to customer issues and bugs. Research and implement fixes.
About you:
You have 5+ years of relevant software development industry experience building and operating scalable, fault-tolerant, distributed systems.
Database and software development experience with Python, SQL, Redshift, and experience with Amazon Web Services, Snowflake along with pub/sub streaming technologies like Kafka, Kinesis, Spark Streaming, etc.
Experience with container services.
Fluid with Amazon Web Services.
Experience with concurrency, multithreading, and the deployment of distributed system architectures.
Experience leading and shipping large scope technical projects in collaboration with multiple experienced engineers.
You have excellent communication skills and the ability to work well within a team and across engineering teams.
You are a strong problem solver and have solid production debugging skills.
You Thrive in a fast-paced environment and see yourself as a partner with the business with the shared goal of moving the business forward.
You have a high level of responsibility, ownership, and accountability.
Job Type: Full-time
Competitive pay, bonus, equity, and benefits:
Benefits:
401(k)
Health insurance
Dental insurance
Flexible spending account
Life insurance
Paid time off
Vision insurance
Schedule:
Monday to Friday
Supplemental pay types:
Bonus pay
Education:
Bachelor's or Master’s (Preferred)
Work Location: San Diego (Del Mar)
Job Type: Full-time
Pay: $105,000.00 - $160,000.00 per year
Benefits:
401(k)
Dental insurance
Flexible spending account
Health insurance
Life insurance
Paid time off
Vision insurance
Compensation package:
Bonus pay
Stock options
Yearly pay
Experience level:
5 years
Schedule:
Monday to Friday
Application Question(s):
Are you willing to be in the office 5 days a week to influence culture and capabilities ?
Are you willing to move to San Diego within 60 days if not within the commuting region ?
Work Location: Hybrid remote in San Diego, CA 92130","$132,500 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"TIAA
4.0",4.0,"Iselin, NJ",Senior Cloud Data Engineer,"The Senior Data Platform Engineer, Cloud role designs datastore systems that are appropriate for applications, customer needs and consistent with the overall design of the organization's information systems architecture. Under limited supervision, this job is responsible for the solution engineering and design, provisioning, delivery, service management, continuous automations of the organization's datastore systems.

Key Responsibilities and Duties
Design, develop and deliver cloud datastore solutions and develop automation pipelines to migrate data sets from On-prem to Cloud platforms. Practice Infrastructure as code to develop automation routines and integration flows to manage state of the datastore platform systems

Provision secures from start datastores and enable them with required security controls including encryption, masking, certificate/keys rotation etc.

Collaborates with developers, analysts, various system administrators to identify business requirements in designing efficient datastore solutions and interfaces.

Identifies and documents all system constraints, implications, and consequences of various proposed system changes.

Reviews technical documentation to guide system users and to assist with the ongoing operation, maintenance, and development of the system. Evaluates the efficiency and effectiveness of application operations and troubleshooting problems.

Provide expert level IT technical lead services, including the direction, evaluation, selection, configuration, implementation, and integration of new and existing technologies and tools in a cloud platform.

Responsible for development of cloud integrations and data migrations to support operations of Cloud infrastructure, provisioning, monitoring with (IaaS) and (PaaS) models

Deploy, automate, maintain, and manage AWS cloud-based production system, to ensure the availability, performance, scalability, and security of productions systems.

Manage the governance framework for DB-Services specific to private, hybrid and public cloud platform adhering to standards and integration with existing tools.

Ability to anticipate technology changes within a rapidly evolving environment.
Educational Requirements
Bachelor's Degree Preferred
Work Experience
3+ Years Required; 5+ Years Preferred
Physical Requirements
Physical Requirements: Sedentary Work

Career Level
7IC

Required Skills:
3 or more years of experience in SQL, ETL and ELT Tools.
Experience working with Data Virtualization Platforms like Starburst, Presto, Denodo, Dremio.
Preferred Skills:
Experience with AWS or GCP, PySpark, CI/CD Pipelines using ElectricFlow.
Base Pay Range: $88,600/yr. - $147,600/yr.
Actual base salary may vary based upon, but not limited to, relevant experience, time in role, base salary of internal peers, prior performance, business sector, and geographic location. In addition to base salary, the competitive compensation package may include, depending on the role, participation in an incentive program linked to performance (for example, annual discretionary incentive programs, non-annual sales incentive plans, or other non-annual incentive plans).
_____________________________________________________________________________________________________
Company Overview
TIAA is the leading provider of financial services in the academic, research, medical, cultural and government fields. We offer a wide range of financial solutions, including investing, banking, advice and education, and retirement services.
Benefits and Total Rewards
The organization is committed to making financial well-being possible for its clients, and is equally committed to the well-being of our associates. That’s why we offer a comprehensive Total Rewards package designed to make a positive difference in the lives of our associates and their loved ones. Our benefits include a superior retirement program and highly competitive health, wellness and work life offerings that can help you achieve and maintain your best possible physical, emotional and financial well-being. To learn more about your benefits, please review our
Benefits Summary
.
Equal Opportunity
We are an Equal Opportunity/Affirmative Action Employer. We consider all qualified applicants for employment regardless of age, race, color, national origin, sex, religion, veteran status, disability, sexual orientation, gender identity, or any other protected status.
Read more about the Equal Opportunity Law
here
.
Accessibility Support
TIAA offers support for those who need assistance with our online application process to provide an equal employment opportunity to all job seekers, including individuals with disabilities.
If you are a U.S. applicant and desire a reasonable accommodation to complete a job application please use one of the below options to contact our accessibility support team:
Phone: (800) 842-2755
Email:
accessibility.support@tiaa.org
Privacy Notices
For Applicants of TIAA, Nuveen and Affiliates residing in US (other than California), click
here
.
For Applicants of TIAA, Nuveen and Affiliates residing in California, please click
here
.
For Applicants of Nuveen residing in Europe and APAC, please click
here
.
For Applicants of Greenwood residing in Brazil (English), click
here
.
For Applicants of Greenwood residing in Brazil (Portuguese), click
here
.
For Applicants of Westchester residing in Brazil (English), click
here
.
For Applicants of Westchester residing in Brazil (Portuguese), click
here
.","$118,100 /yr (est.)",10000+ Employees,Company - Private,Financial Services,Investment & Asset Management,1918,$100 to $500 million (USD)
"Octo
4.2",4.2,"Chantilly, VA",Data Engineer,"You…
As a Data Engineer, you will be joining the team that is deploying and delivering a cloud-based, multi-domain Common Data Fabric (CDF), which provides data sharing services to the entire DoD Intelligence Community (IC). The CDF connects all IC data providers and consumers. It uses fully automated policy-based access controls to create a machine-to-machine data brokerage service, which is enabling the transition away from legacy point-to-point solutions across the IC enterprise.
Us…
We were founded as a fresh alternative in the Government Consulting Community and are dedicated to the belief that results are a product of analytical thinking, agile design principles and that solutions are built in collaboration with, not for, our customers. This mantra drives us to succeed and act as true partners in advancing our client’s missions.
Program Mission…
The CDF program is an evolution for the way DoD programs, services, and combat support agencies access data by providing data consumers (e.g., systems, app developers, etc.) with a “one-stop shop” for obtaining ISR data. The CDF significantly increases the DI2E’s ability to meet the ISR needs of joint and combined task force commanders by providing enterprise data at scale. The CDF serves as the scalable, modular, open architecture that enables interoperability for the collection, processing, exploitation, dissemination, and archiving of all forms and formats of intelligence data. Through the CDF, programs can easily share data and access new sources using their existing architecture. The CDF is a network and end-user agnostic capability that enables enterprise intelligence data sharing from sensor tasking to product dissemination.
Responsibilities...
Primary responsibility is to work with data providers within the IC and DoD Enterprise to identify and ingest data sets into the CDF data broker. In this role you will:
Develop, optimize, and maintain data ingest flows using Apache Nifi and Python.
Develop within the components in the cloud platform, such as Apache Kafka, NiFi, and HBase.
Communicate with data owners to set up and ensure CDF streaming and batching components are working (including configuration parameters).
Document SOP related to streaming configuration, batch configuration or API management depending on role requirement.
Document details of each data ingest activity to ensure they can be understood by the rest of the team
What we’d like to see…
A minimum of 3 years of experience with programming and software development including analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems
DoD 8570 IAT Level II Certification (e.g. Security+) or the ability to obtain the certification within 90 days
Demonstrable CentOS command line knowledge
Working knowledge of web services environments, languages, and formats such as RESTful APIs, SOAP, FTP/SFTP, HTML, JavaScript, XML, and JSON
Understanding of foundational ETL concepts
Experience implementing data ignorations with in the IC DoD Enterprise.
Desired Skills:
Experience or expertise using, managing, and/or testing API Gateway tools and Rest APIs (desired)
2+ Experience in Python Development
Experience or expertise configuring an LDAP client to connect to IPA (desired)
Advanced organizational skills with the ability to handle multiple assignments
Strong written and oral communication skills
Years of Experience: Junior Level (0-4 years),Mid Level (5-8 years), Senior Level (9+)
Education: Bachelor's degree in systems engineering, computer engineering, or a related technical field (preferred)
Location: Chantilly, VA
Clearance: Active TS/SCI w/ ability to obtain CI Poly (preferred)","$99,041 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,$100 to $500 million (USD)
"Press Ganey Associates, Inc.
3.4",3.4,Remote,Senior Data Engineer,"About Press Ganey:
Press Ganey , the leading Human Experience (HX) healthcare performance improvement company, offers an integrated suite of solutions that address safety, clinical excellence, patient experience and workforce engagement. The company works with more than 41,000 healthcare facilities in its mission to reduce patient suffering and enhance caregiver resilience to improve the overall safety, quality, and experience of care. Press Ganey is a PG Forsta company.
Position Description:
Our company is looking for a Senior Data Engineer who will be responsible for designing and building scalable and robust data systems. The ideal candidate will be an expert in data engineering technologies, have a strong understanding of data architecture, and be able to work with large and complex data sets. The Senior Data Engineer will also work closely with the data science and analytics teams to ensure data integrity and develop data pipelines.
Duties & Responsibilities:
· Design and build large scale data pipelines to process and analyze large volumes of data.
· Build and maintain efficient data infrastructure, ensuring data quality and consistency.
· Work with the data science and analytics teams to ensure data accuracy and completeness.
· Collaborate with other engineers to build scalable and maintainable data systems.
· Develop and implement data governance and security policies.
· Continuously evaluate and improve data processes to optimize system performance.
· Keep up to date with the latest data engineering technologies and trends.
Technical Skills:
· Expertise in SQL and NoSQL databases, data warehousing, and data modeling.
· Fluency in Python and SQL. Additional data or system languages (e.g. Java, Scala, Go, R) a plus.
· Experience using data pipeline frameworks such as Apache Beam or Apache Spark at scale.
· Experience using data orchestration / automation frameworks such as Airflow, Databricks and MLFlow.
· Hands on experience with one or more of the major cloud providers (GCP, AWS, Azure). Experience with infrastructure-as-code (e.g. Cloud Formation, Terraform) a plus.
· Experience with data visualization tools such as Tableau or Power BI.
Minimum Qualifications:
· Bachelor’s degree in Computer Science, Engineering, or a related field.
· 5+ years of experience in data engineering or related field.
· Excellent problem-solving skills and ability to work independently.
· Strong communication and collaboration skills.
Preferred Qualifications:
· Master’s degree in Computer Science, Engineering, or a related field.
· Experience in machine learning or data science.
All positions at Press Ganey require an applicant who has accepted an offer to undergo a background check. The specific checks are based on the nature of the position. Background checks may include some or all of the following: SSN/SIN validation, education verification, employment verification, and criminal check, search against global sanctions and government watch lists, fingerprint verification, credit check, and/or drug test. By applying for a position with Press Ganey, you understand that you will be required to undergo a background check should you be made an offer. You also understand that the offer is contingent upon successful completion of the background check and results consistent with Press Ganey's employment policies. You will be notified during the hiring process which checks are required for the position.
Press Ganey Associates LLC is an Equal Employment Opportunity/Affirmative Action employer and well committed to a diverse workforce. We do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, veteran status, and basis of disability or any other federal, state or local protected class.
Pay Transparency Non-Discrimination Notice – Press Ganey will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor's legal duty to furnish information.
The expected base salary for this position ranges from $96,000 - $150,000. It is not typical for offers to be made at or near the top of the range. Salary offers are based on a wide range of factors including relevant skills, training, experience, education, and, where applicable, licensure or certifications obtained. Market and organizational factors are also considered. In addition to base salary and a competitive benefits package, successful candidates are eligible to receive a discretionary bonus or commission tied to achieved results.
#LI-Remote","$123,000 /yr (est.)",1001 to 5000 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,1985,$500 million to $1 billion (USD)
"Great Dane
2.9",2.9,"Chicago, IL",Data Engineer,"Data Engineer - (230005R)
Description
With thousands of employees worldwide, teamwork and collaboration are valued here.

We look for employees who are driven, determined and ready to accelerate their future. By joining our team, you will earn competitive pay, benefits, insurance, 401k, pension and more while working in an environment with the highest safety standards in the industry.
The Position:
The Data Engineer is responsible for expanding and optimizing our current data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is experienced in both data pipeline creation and data transformation. The Data Engineer will support our software developers, system architects, data analysts and Business Analysts on all data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s current data architecture to support our next generation of products and data initiatives.
Key Responsibilities:
Create and maintain optimal data pipeline architecture.
Assemble complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep data separated and secure across all platforms used.
Work with data and business experts to strive for greater functionality in our data systems.
Other duties as assigned.
Qualifications
Requirements:
Education: Bachelor’s degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
Experience: 5+ years of experience in Data Engineer role
Skills: Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing data pipelines architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
A successful history of manipulating, processing, and extracting value from disconnected datasets.
Project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience with relational SQL databases, including MS SQL server, Oracle, DB2, and Maria.
Experience with Data Cloud platforms like SnowFlake and or Data Bricks.
Experience with data pipeline and workflow management tools: SQDR, Airflow, Fivetran, Airbyte, etc.
Experience with object-oriented/object function scripting languages: Python, Java
Travel: 20% at most
Physical Demands/Work Environment:
The physical demands and work environment characteristics described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
Physical demands: While performing duties of job, employee is occasionally required to stand; walk; sit; use hands to finger, handle or feel objects; reach with hands and arms; talk and hear. Specific vision abilities required by the job include close and distance vision.
Work environment: The noise level in the work environment is usually minimal to moderate.
Must be willing to work occasional nights and/or weekends as business commitments dictate.
Great Dane is an Equal Opportunity Employer
Primary Location
: US-IL-Chicago
Work Locations
: Chicago N. LaSalle St. 222 N. LaSalle St. Suite 920 Chicago 60601
Job
: Information Systems
Schedule
: Full-time
Shift
: 1st Shift","$104,652 /yr (est.)",5001 to 10000 Employees,Company - Private,Manufacturing,Transportation Equipment Manufacturing,1900,$500 million to $1 billion (USD)
"Watauga Group
3.6",3.6,"Atlanta, GA",Associate Data Engineer,"WHO WE ARE:
At Watauga Group, we leverage two decades of specialized media expertise and our love for the outdoors and entertainment to help outdoor recreation & attraction brands maximize their sales and elevate advertising ROI.Our unique blend of marketplace intelligence and deep insights into the media behaviors and preferences of outdoor participants and attraction visitors enables us to surgically target untapped consumer audiences and connect brands with the greatest number of potential customers at every step of their buying journeys.Watauga’s brand and performance advertising experts navigate today’s complex media landscape to create fully integrated strategies encompassing the most effective mix of digital and traditional media channels, platforms, data, and technologies. Our end-to-end media solutions include Broadcast TV & Radio, OTT & Streaming Audio, Out-of-Home, Digital Display, Paid Search, Paid Social, Sponsorships, and more.
Certified by the WBENC, Watauga is one of the largest women-owned media agencies in North America with offices in Orlando, Atlanta, and Birmingham.
WHY JOIN US:
Generous health benefits package including employer contribution to medical insurance, employer-paid life insurance and disability insurance
Employer match to 401(k) retirement plan
Flexible PTO
Flexible Hybrid Schedule
Paid Parental Leave
Health Savings Account
Tuition Reimbursement
Career progression
Bonus and incentive plans
ABOUT THE JOB:
PRIMARY DUTIES:
The duties and responsibilities of this position include but are not limited to those listed below. These duties and responsibilities may be modified at any time by Management. Modifications will be in writing and will be acknowledged by both parties.
Assist with maintenance of Python/SQL ETL pipelines
Assist with administration of PostgreSQL databases
Assist with design of database architecture to support business analysts
Assist with maintenance of Azure Cloud environment
Create and maintain internal and external dashboards for reporting & data visualization in Power BI
Transform, improve, and integrate data from multiple sources, into accessible, understandable, and usable datasets.
Assist in building and maintaining DataMart tables to optimize BI performance.
Maintain thorough documentation of dashboard data requirements.
Provide quality assurance of imported data.
Assist with pipeline and database development and maintenance.
Work with Digital Media team to ensure the proper data needs are delivered with focus on accuracy and attention to detail.
CORE COMPETENCIES:
Achievement and Results Orientation
Adaptability and Flexibility
Analytical and Strategic Thinking
Attention to Detail with Accuracy
Communication – Written, Oral, Presenting
Learning Support and Continuous Learning
Process Orientation
Problem Solving
Teamwork, Cooperation, and Working with Others
EDUCATION AND EXPERIENCE REQUIREMENTS:
Bachelor’s degree (Computer Science, Data Analytics, Accounting, or Finance a plus)
Strong data analysis skills
Strong data visualization skills
Proficient in Microsoft Excel
Working knowledge of MS Power BI Development, Deployment, and Integration
SQL and Python experience is a plus
Familiarity with CM360 or other tag management systems is a plus
1 year relevant work experience with digital marketing or related industry is a plus
PHYSICAL CRITERIA:
Able to lift and carry 20 pounds
Able to sit for prolonged periods of time at a computer
Industry
Marketing & Advertising
Employment Type
Full-time","$74,258 /yr (est.)",1 to 50 Employees,Company - Private,Media & Communication,Advertising & Public Relations,2004,$5 to $25 million (USD)
"Parkland Health and Hospital System
3.7",3.7,"Dallas, TX",Data Engineer - PCHP,"Interested in a career with both meaning and growth? Whether your abilities are in direct patient care or one of the many other areas of healthcare administration and support, everyone at Parkland works together to fulfill our mission: the health and well-being of individuals and communities entrusted to our care. By joining Parkland, you become part of a diverse healthcare legacy that’s served our community for more than 125 years. Put your skills to work with us, seek opportunities to learn and join a talented team where patient care is more than a job. It’s our passion.

Primary Purpose
The Parkland Community Health Plan’s (PCHP’s) Data Engineer is responsible for maintaining the data systems including business intelligence, ETL, and supporting backup strategies to provide PCHP with secure, dependable, and accurate data including data transfer, data integrity, and data storage responsibilities. The Data Engineer will collaborate with Database Administrators, server team, storage team, and other teams to plan maintenance activities and with PHCP’s analytics team for report or universe deployments. The Data Engineer will also be involved in dashboard and report development activities.

Minimum Specifications

Education
Bachelor’s degree in computer science, management information systems, information technology, statistics, mathematics, or related discipline.

Experience
Seven years of experience in maintaining business intelligence, data warehouse solutions, or ETL in a Run or Production environment.
Six years of experience troubleshooting ETL load related issues (SSIS or Data Solutions).
Six years of experience with ETL development and maintenance experience in a data warehouse environment.
Experience with systems engineering (hardware / software) capacity.
Experience with database or report portal tool administration is preferred.
Experience at a healthcare or managed care organization is preferred.

Certification/Registration/Licensure
System Administration or Reporting Tool Administrative Certification is preferred. (i.e., Epic Cogito or Clarity, SAP Business Objects, Tibco Composite, Microsoft Certified Solutions Engineer (MCSE), Oracle Certified Professional (OCP), etc.)
PMP or other project management certificate or training is preferred.

Skills or Special Abilities
Proficiency with ETL tool Build or Run activities.
Ability to create reports and/or build virtual data environments.
Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy.
Proficiency with Microsoft Office Excel, Word and Outlook is required; Access and PowerPoint are preferred.
Demonstrated critical thinking and troubleshooting skills accompanied by a high level of detail.
Demonstrated ability to plan and manage multiple processes and projects simultaneously.
High level of attention to detail.
Strong verbal and written communication skills.
Demonstrated ability to collaborate effectively and work as part of a team.
Independent worker and self-starter, having the ability to provide internal motivation and drive.
Proficiency with server or application patching, backups, scripting is preferred.
Understanding of SSIS and Apache NiFi is preferred.
Proficiency with Business Objects Administration is preferred.

Responsibilities
Implements and maintains high-value business intelligence environments.
Maintains the data systems including business intelligence, ETL, and supporting backup strategies.
Has a strong understanding of all the tools within the environment, regardless of vendor, and quickly and efficiently triages, troubleshoots, and restores services during outages or service degradation.
Responsible for being on-call for Business Intelligence and ETL cycles.
Works with the Database Analyst and storage teams to ensure proper backups are taken, test back-ups periodically, and ensures that the system can be restored in the time of a disaster.
Proactively identifies areas for improvement in our Business Intelligence environment.
Documents all routine processes and cross-trains other team members.
Improves function, speed, and accuracy of data distribution methods.
Develops automated reports and dashboards.

Job Accountabilities
Identifies ways to improve work processes and improve customer satisfaction. Makes recommendations to supervisor, implements, and monitors results as appropriate in support of the overall goals of PCHP.
Stays abreast of the latest developments, advancements, and trends in the field by attending seminars/workshops, reading professional journals, actively participating in professional organizations, and/or maintaining certification or licensure. Integrates knowledge gained into current work practices.
Maintains knowledge of applicable rules, regulations, policies, laws, and guidelines that impact the area. Develops effective internal controls designed to promote adherence with applicable laws, accreditation agency requirements, and customer requirements. Seeks advice and guidance as needed to ensure proper understanding.

Parkland Health and Hospital System prohibits discrimination based on age (40 or over), race, color, religion, sex (including pregnancy), sexual orientation, gender identity, gender expression, genetic information, disability, national origin, marital status, political belief, or veteran status. As part of our commitment to our patients and employees’ wellness, Parkland Health is a tobacco and smoke-free campus.","$97,618 /yr (est.)",5001 to 10000 Employees,Hospital,Healthcare,Health Care Services & Hospitals,1894,$500 million to $1 billion (USD)
"Mass General Brigham
3.8",3.8,"Somerville, MA",Sr. Data Engineer (Data Lakes),"Sr. Data Engineer (Data Lakes)
- (3244480)

About Us:
At Mass General Brigham Digital, we pride ourselves on our ability to create maximum strategic, clinical, and operational value from established and emergent technologies for our patients, care teams, researchers, and employees. Digital health will not only enhance the equity and efficiency of healthcare delivery, but it will also help make medicine more personalized and precise.
We recognize that increasing value and continually improving quality while maintaining an inclusive focus are essential to organizational excellence, and we invite you to join us on this journey. The work we do in Digital is a strategic imperative, and there is a strong and growing understanding of how together we will transform Mass General Brigham in innovative and impactful ways.
General Summary/ Overview:
At Mass General Brigham Digital, we pride ourselves on our ability to create maximum strategic, clinical, and operational value from established and emergent technologies for our patients, care teams, researchers, and employees. Digital health will not only enhance the equity and efficiency of healthcare delivery, but it will also help make medicine more personalized and precise.
We recognize that increasing value and continually improving quality while maintaining an inclusive focus are essential to organizational excellence, and we invite you to join us on this journey. The work we do in Digital is a strategic imperative, and there is a strong and growing understanding of how together we will transform Mass General Brigham in innovative and impactful ways.
Summary:
Reporting to the Engineering Manager, Data Lake, the Senior Data Engineer (Azure Data Lake) will work towards analyzing, designing, developing, and building ADF data pipelines, ELT/ETL frameworks, and Azure data lake platforms, primarily focusing on Epic (EHR) data and other healthcare data; and will thrive as a member of an experienced, high performing and highly motivated team. Role will be responsible for participating in building out our existing EDW and our new Data Lake, expanding our data ecosystem and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. Requires advanced experience with data engineering and building Azure Cloud Data Lake, Azure Big Data Analytics technologies and architecture, Enterprise Analytics Solutions, and optimizing 'big data' data pipelines, architectures, and data sets. Expert level of experience with Design and Architecture of Azure big data frameworks/tools: Azure Data Lake, Azure Data Factory, Azure Data Bricks, Azure ML, SQL Data Warehouse. Advanced Experience with Hadoop based technologies (e.g., hdfs, Spark) and Programming experience in Python, SQL, Spark.
Principal Duties and Responsibilities:
Design, Develop, construct, test and maintain Data Lake architectures and large-scale data processing systems.
Support big data ecosystem related Tool selection and POC analysis.
Gather and process raw data at scale that meet functional / non-functional business requirements (including writing scripts, REST API calls, SQL Queries, etc.).
Develop data set processes for data modeling, mining and production.
Integrate new data management technologies ( Informatica DQ..) and software engineering tools into existing structures.
The candidate will be responsible for participating in building out our Data Lake platform, expanding and optimizing our data ecosystem and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams.
The ideal candidate is an experienced data pipeline builder who enjoys optimizing data systems and building them from the ground up.
The Data Engineer will support our Software Developers, Database Architects, Data Analysts and Data Scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects.
They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products.
Create and maintain optimal data pipeline architecture, assemble large, complex data sets that meet functional / non-functional business requirements on cloud based data platforms (e.g. Azure) and relational data systems (SQL Server, SSIS).
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, etc.
Build the data infrastructure required for optimal extraction, transformation, and loading of data from traditional/legacy data sources.
Work with stakeholders including the Management team, Product owners, and Architecture teams to assist with data-related technical issues and support their data infrastructure needs.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Use/s the Mass General Brigham values to govern decisions, actions, and behaviors. These values guide how we get our work done: Patients, Affordability, Accountability & Service Commitment, Decisiveness, Innovation & Thoughtful Risk; and how we treat each other: Diversity & Inclusion, Integrity & Respect, Learning, Continuous Improvement & Personal Growth, Teamwork & Collaboration.
Working Conditions:
This is a remote position.
Diversity Statement
As a not-for-profit organization, Mass General Brigham is committed to supporting patient care, research, teaching, and service to the community. We place great value on being a diverse, equitable and inclusive organization as we aim to reflect the diversity of the patients we serve. At Mass General Brigham, we believe in equal access to quality care, employment and advancement opportunities encompassing the full spectrum of human diversity: race, gender, sexual orientation, ability, religion, ethnicity, national origin and all the other forms of human presence and expression that make us better able to provide innovative and cutting-edge healthcare and research.

5+ Years of experience data engineering and building Azure Cloud Data Lake technologies and architecture, Enterprise Analytics Solutions, and optimizing 'big data' data pipelines, architectures and data sets.
5-7 Years of Experience with Hadoop based technologies (e.g. hdfs, Spark). Spark Experience desirable
5+ years of Programming experience in Python, SQL, PySpark.
Healthcare experience, most notably in Clinical data, Epic, Clarity, Caboodle, Payer data and reference data is a plus but not mandatory.
Experience with Design and Architecture of Azure big data frameworks/tools: Azure Data Lake, Azure Data Factory, Snowflake, Azure Data Bricks, Powershell.
Experience with Design and Architecture of relational SQL and NoSQL databases, including MS SQL Server, Cosmos DB.
Experience with Design and Architecture of data security and Azure security, VM, Vnet.
Experience with building processes supporting data transformation, data structures, metadata, dependency and workload management.
Experience leading and working with cross-functional teams in a dynamic environment.
Experience building Big data pipeline with Spark and/or Data Bricks is a plus.
Leading development of Data Lake Architectures from scratch.
Experience with Azure DevOps/CI-CD, Continuous integration and deployment.
Experience with Real time analytics on Spark, Kafka, Event Hub is a plus.
Experience in petabyte scale data environments and integration of data from multiple diverse sources.
Skills/Abilities/Competencies:
Advanced hands-on SQL, Spark, Python, pySpark (2+ of these) knowledge and experience working with relational databases for data querying and retrieval.
Strong SQL skills on multiple platform (preferred MPP systems).
Data Modeling tools (e.g. Erwin, Visio).
Strong interpersonal and communication skills, both written and verbal.
Strong Scrum/Agile development experience.
Excellent organizational skills and attention to detail, manage multiple tasks and projects, meet deadlines, follow through, and manage to schedule.
Strong innovation capabilities and the ability to think creatively.
Strong collaboration and team building skills within, across and outside of an organization.
Maintain and promote a positive team environment.
Maintains stable performance under pressure, demonstrating sensitivity to diverse organizational culture.
Ability to effectively cope with change, remain flexible and adaptable within a fast-paced environment with rapidly changing requirements, and ability to negotiate situations when the big picture is not clearly defined.

EEO Statement

Mass General Brigham is an Equal Opportunity Employer. By embracing diverse skills, perspectives, and ideas, we choose to lead. All qualified applicants will receive consideration for employment without regard to race, color, religious creed, national origin, sex, age, gender identity, disability, sexual orientation, military service, genetic information, and/or other status protected under the law. We will ensure that all individuals with a disability are provided a reasonable accommodation to participate in the job application or interview process, perform essential job functions, and receive other benefits and privileges of employment.

Primary Location MA-Somerville-MGB Assembly Row
Work Locations MGB Assembly Row 399 Revolution Drive Somerville 02145
Job Business and Systems Analyst
Organization Mass General Brigham
Schedule Full-time
Standard Hours 40
Shift Day Job
Employee Status Regular
Recruiting Department MGB Digital
Job Posting May 12, 2023","$118,726 /yr (est.)",1001 to 5000 Employees,Nonprofit Organization,Healthcare,Health Care Services & Hospitals,1994,$10+ billion (USD)
"Vertex, Inc.
3.9",3.9,Remote,Sr. Data Engineer- Cloud (Remote),"Job Description:
This position is responsible for performing analysis, design, implementation, testing, maintenance, and support tasks for data-intensive software applications programming. Improves system quality by identifying issues and common patterns and developing standard operating procedures. Enhances applications by identifying opportunities for improvement, making recommendations, and designing and implementing systems. ESSENTIAL JOB FUNCTIONS AND RESPONSIBILITIES: · Prepare technical design specifications based on functional requirements and analysis documents. · Implement, test, maintain and support software, based on technical design specifications. · Improve system quality by identifying issues and common patterns, and developing standard operating procedures · Enhance applications by identifying opportunities for improvement, making recommendations, and designing and implementing systems · Maintain and improve existing codebases and peer review code changes · Liaise with colleagues to implement technical designs · Investigating and using new technologies where relevant · Provide written knowledge transfer material · Review functional requirements, analysis, and design documents and provide feedback. · Assist customer support with technical problems and questions. · Assist and mentor other development staff. · Perform special assignments. · Participate in architecture and code reviews. · Lead or participate in other projects or duties. · Occasional travel required. (Up to 5%) · Participate in other projects or duties. SUPERVISORY RESPONSIBILITIES: · N/A KNOWLEDGE, SKILLS, AND ABILITIES: * * * * * * * · Ability to network with key contacts outside own area of expertise. * * · Must possess strong interpersonal, organizational, presentation and facilitation skills. · Must be results oriented and customer focused. · Must possess good organizational skills. * * EDUCATION AND TRAINING: · Bachelor’s degree in computer science, Information Systems, or related field; or equivalent combination of education/experience. Master’s degree is a plus. · 7 years or more of extensive experience developing mission critical and low latency solutions. · At least 4 years of experience with developing and debugging distributed systems and data pipelines in the cloud. AWS is a must. · Extensive experience with SQL cloud databases like Snowflake (a must-have experience), and MS SQLServer. Experience with NoSQL databases like AWS DynamoDB and Azure Cosmos is a plus. · Good understanding of data modeling, ETL, data curation, and big data performance tuning. · Experience with Business Intelligence tools is a plus. · Experience working with AWS and/or Azure DevOps and extensive debugging experience. · Ability to code in one or more languages like Python, Java, Scala. · An understanding of unit testing, test driven development, functional testing, and performance · Knowledge of at least one shell scripting language. Other Qualifications The Winning Way behaviors that all Vertex employees need to meet the expectations of each other, our customers, and our partners. • Communicate with Clarity - Be clear, concise, and actionable. Be relentlessly constructive. Seek and provide meaningful feedback. • Act with Urgency - Adopt an agile mentality - frequent iterations, improved speed, resilience. 80/20 rule - better is the enemy of done. Don’t spend hours when minutes are enough. • Work with Purpose - Exhibit a ""We Can"" mindset. Results outweigh effort. Everyone understands how their role contributes. Set aside personal objectives for team results. • Drive to Decision - Cut the swirl with defined deadlines and decision points. Be clear on individual accountability and decision authority. Guided by a commitment to and accountability for customer outcomes. • Own the Outcome - Defined milestones, commitments and intended results. Assess your work in context, if you’re unsure, ask. Demonstrate unwavering support for decisions. COMMENTS: The above statements are intended to describe the general nature and level of work being performed by individuals in this position. Other functions may be assigned, and management retains the right to add or change the duties at any time.",#N/A,1001 to 5000 Employees,Company - Public,Information Technology,Software Development,1978,$100 to $500 million (USD)
"aventiv
3.5",3.5,"Carrollton, TX",Data Center Implementation Engineer,"Welcome to Aventiv! Please watch this brief video to find out if this is the place you want to be!
https://vimeo.com/391578629/5ba31cc5e9

Job Purpose:
Responsible for Engineering and installation of all hardware systems into the enterprise Data Center environments. Spares management, logistics management, vendor management, configuration management and hardware asset tracking will also be key deliverables of this assignment.
THIS IS A HYBRID POSITION WITH APPROXIMATELY 10% TRAVEL REQUIRED.

Essential Duties
Prepare all change controls related to hardware Engineering and installations
Work with various departments to compile requirements and update all Data Center Engineering.
Perform all Installations for IT hardware
Provide Spares management tracking of the Spares hubs kept to maintain component failures
Provide details to create hardware project budgets.
Perform or manage vendors as required to perform periodic preventative maintenance.
Provide all logistics support for all material deliveries to and between IT Data Center environments.
Perform other duties as assigned

Knowledge, Skills, and Abilities
Highly motivated
Good organizational skills
Great interpersonal and communication skills
Excellent analytical and reporting skills
Solid IT Data Center background
Proficient at using MS Visio, Excel, and Word
Knowledgeable and trouble ticketing (Heat) and Environmental monitoring Systems (Orion)

Minimum Qualifications
HS Diploma or GED
Minimum 5 years’ experience in Data Center Engineering and implementation
Experience with AC and DC electrical systems, Data Center Rack, flooring and cable management systems, Camera, environmental and infrastructure monitoring systems used within Data Centers, and Safety practices for Data Center implementations
10% travel required

Preferred Qualifications
Bachelor’s degree in Electrical Engineering, Mechanical, or Business Administration or equivalent field of study
Network Engineering experience a plus
System administration (Microsoft or Linux) experience a plus
Project Management experience or certification is plus

Physical Demands
Standing, sitting, walking, speaking, listening, bending, reaching, pushing, pulling, lifting, grasping and manipulating tools, typing, using peripheral computer tools.
May be required to lift up to 50 pounds.

Salary and Benefits:
At Aventiv, our salary and benefits are designed to fit you as a whole person. We offer a salary range based on experience and qualifications to ensure your unique contributions are met with our most competitive offer.
$83,500-$95,200 /year
Health Insurance
401(k)
Disability
Life Insurance
Paid Time Off
Voluntary Benefits

Aventiv Privacy Policy:
www.aventiv.com/privacy

Equal Employment Policy:
Aventiv is proud to be an equal opportunity employer. All decisions regarding recruiting, hiring, promotion, assignment, training, termination and other terms and conditions of employment will be made without regard to race, color, national origin, biological sex, sexual orientation, gender identity, gender expression, gender presentation, religion, age, pregnancy, disability, work-related injury, veteran status, genetic information, marital status, or any other factor that the law protects from employment discrimination. We do not discriminate based on genetic information in accordance with the Genetic Information Nondiscrimination Act.","$89,350 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,2004,$500 million to $1 billion (USD)
"Atos
3.7",3.7,"Troy, MI",DATA ENGINEER,"Publication Date:
May 10, 2023

Ref. No:
480012

Location:
Troy, MI, US, 48083

The future is our choice

At Atos, as the global leader in secure and decarbonized digital, our purpose is to help design the future of the information space. Together we bring the diversity of our people’s skills and backgrounds to make the right choices with our clients, for our company and for our own futures.

Position : Cloud Data Engineer

Location : Westlake Village, CA

Job Description

Design and Build scalable, secure, low latency, resilient and cost-effective solutions for enabling predictive and prescriptive analytics across the organization
Design/ Architect frameworks to Operationalize ML models through serverless architecture and support unsupervised continuous training models
Take over and scale our data models (Tableau, Dynamo DB, Kibana)
Experience in shipping low-latency massive scale systems to production
Communicate data-backed findings to a diverse constituency of internal and external stakeholders
Build frameworks for data ingestion pipeline both real time and batch using best practices in data modeling, ETL/ELT processes and hand off to data engineers
Participate in technical decisions and collaborate with talented peers.
Review code, implementations and give meaningful feedback that helps others build better solutions.
Helps drive technology direction and choices of technologies by making recommendations based on experience and research. MINIMUM
REQUIREMENTS & SPECIAL ATTRIBUTES

5 or more years of experience working directly with enterprise data solutions
Hands on experience working in a public cloud environment and on-prem infrastructure.
Specialty on Columnar Databases like Redshift Spectrum, Time Series data stores like Apache Pinot and the AWS cloud infrastructure
Experience with in-memory, serverless, streaming technologies and orchestration tools such as Docker, Spark, Kafka, Airflow, Kubernetes is needed
Excellent SQL skills and Python coding is a must
Current hands-on implementation experience required, possessing 5or more years of IT platform implementation experience.
AWS Certified Big Data - Specialty desirable
Experience designing and implementing AWS big data and analytics solutions in large digital and retail environments is desirable
Advanced knowledge and experience in online transactional (OLTP) processing and analytical processing (OLAP) databases, data lakes, and schemas.
Experience with AWS Cloud Data Lake Technologies and operational experience of Kinesis/Kafka, S3, Glue and Athena.
Experience with any of the message / file formats: Parquet, Avro, ORC
Design and development experience on subscribing to a Streaming Service, EMS, MQ, Java, XSD, File Adapter, and ESB based applications
Experience in distributed architectures such as Microservices, SOA, RESTful APIs and data integration architectures is a plus
Hands on experience migrating On-Prem Data solutions to cloud
Prior experience managing On-prem Enterprise Data Warehouse solutions like Netezza is a plus
Experience with a wide variety of modern data processing technologies, including
Big Data Stack (Spark, spectrum, Flume, Kafka, Kinesis etc.)
Data streaming (Kafka, SQS/SNS queuing, etc)
Expert in Columnar databases primarily, Redshift or like technologies lile Snowflake, Firebolt
Expert in Commonly used AWS services (S3, Lambda, Redshift, Glue, EC2, etc)
Expertise in Python, pySpark or similar programming languages is a must have
BI tools (Tableau, Domo, MicroStrategy) is a plus
Skilled in AWS Compute such as EC2, Lambda, Beanstalk, or ECS
Skilled in AWS Management and Governance suite of products such as CloudTrail, CloudWatch, or Systems Manager
Skilled in Amazon Web Services (AWS) offerings, development, and networking platforms
Skilled in AWS Analytics such as Athena, EMR, or Glue
Proficiency in Oracle, MYSQL and Microsoft SQL Server Databases is a plus
Understanding Continuous Integration / Continuous Delivery with experience in Jenkins
Here at Atos, diversity and inclusion are embedded in our DNA. Read more about our commitment to a fair work environment for all.

Atos is a recognized leader in its industry across Environment, Social and Governance (ESG) criteria. Find out more on our CSR commitment.

Choose your future. Choose Atos.

Nearest Major Market: Troy

Nearest Secondary Market: Detroit","$90,916 /yr (est.)",10000+ Employees,Company - Public,Information Technology,Information Technology Support Services,1997,$10+ billion (USD)
"HORNE LLP
3.5",3.5,"Ridgeland, MS",Data Engineer,"The Data Engineer will develop, optimize, and support Horne’s data integrations, APIs and ETL processes. The Data Engineer should know how to examine new data system requirements and implement processes. The ideal candidate will also have proven experience in cloud data management, with excellent analytical and problem-solving abilities.
Responsibilities:
Creates data pipelines, big data platforms and data integrations in databases, data warehouses and data lakes, and works with various cloud and on-premises technologies.

Gathers and analyzes data from databases and other source systems, runs machine learning algorithms and predictive models, creates data visualizations for business users.

Extracts data from business systems, cleanses, analyzes, and creates reports and dashboards to highlight trends and other business information for end users.

Create complex functions, scripts, stored procedures, and triggers to support application development.

Work on multiple, small to large projects as a team member, or independently on small projects.

Troubleshoot applications and provide corrective measures.

Monitor the system performance by performing regular tests, troubleshooting, and integrating new features.

Offer support by responding to system problems in a timely manner.

Requirements:
Bachelor’s degree in computer science, Computer Engineering, or relevant field.

Strong knowledge of relational databases, data warehouses.

Strong knowledge in developing complex SQL scripts.

Proficiency in Python and/or Scala, machine learning.

Experience in data visualization, and data analytics.

Knowledge of data integration and ETL tools Informatica IICS is a preferred skill.

Experience with Business Intelligence tools such as Tableau, Power BI is a preferred skill.

Experience with cloud computing environments, particularly Microsoft Azure, is a preferred skill.

Knowledge of big data computing on Spark framework.

Some knowledge of data modeling

Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities
The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information. 41 CFR 60-1.35(c)","$89,047 /yr (est.)",501 to 1000 Employees,Company - Private,Financial Services,Accounting & Tax,1962,$25 to $100 million (USD)
"Hollstadt & Associates
4.5",4.5,Minnesota,Senior Data Engineer,"Minnesota - Developer
Hollstadt Overview
Hollstadt Consulting is a management and technology consulting firm dedicated to placing professionals at engagements where they will excel. When you work with us, you'll work with a refreshingly real company led and staffed by seasoned experts who are also down-to-earth, good people. We're committed to treating you with respect and helping you achieve your career aspirations.

Since 1990, Hollstadt has been a trusted partner to more than 150 domestic and global companies and has successfully completed over 2,000 projects. Our continued growth has created challenging and rewarding opportunities for accomplished IT and Business Consultants. Hollstadt Consulting is an equal opportunity employer including disability/veteran.

Job Description

The Senior Data Engineer will oversee the department's data integration work, including developing data models, maintaining a data warehouse and analytics environment, and writing scripts for data integration and analysis. This role will work closely and collaboratively with members of other areas to define requirements, mine and analyze data, integrate data from a variety of sources, and deploy high quality data pipelines in support of the analytics needs.

Requirements
Bachelor’s Degree in Computer Science or Management Information Systems (MIS) or Business, Finance or Accounting with an emphasis in MIS
Minimum 5 years experience of developing and supporting enterprise level data warehouse systems
Strong knowledge of relational databases and SQL. Extract, Transform, and Load (ETL) data into a relational database
General data manipulation skills: read in data, process and clean it, transform and recode it, merge different data sets together, reformat data between wide and long, etc.
Demonstrated ability to learn new techniques and troubleshoot code without support, ex. find answers to common programming challenges
Strong knowledge of T-SQL language as evidenced by ability to write complex SQL queries, Microsoft SQL Management Studio, SQL Analysis Services and SQL Server Integration Services
Demonstrated ability to work independently and be a self-starter
Demonstrated ability to work effectively in teams, in both a lead and support role
Strong analytical and problem-solving skills with the ability to work independently with minimal supervision
Cloud knowledge or exposure
Experience working with cloud infrastructure services like Amazon Web Services and Google Cloud
Agile LeSS (seems like this would make the candidate stand out and they'd be very interested in interviewing)
Mentorship experience
Preferred
Experience working with Data Vault 2.0
Experience with advanced data visualization and mapping




Benefits + Perks


Comprehensive Benefit Plan
Hollstadt offers a competitive and comprehensive benefit package which includes Medical, Dental, Vision, Long Term/Short Term Disability, and Life Insurance. With three different medical plans to choose from, you can enroll in the coverage you need from single to family, or anywhere in between!

Remarketing Process
Hollstadt is based on retention and relationships. We get to know your strengths and career wishes throughout your assignment and then start remarket discussions 6-8 weeks prior to your end date. By being proactive, we are able to keep your down time between assignments as short as possible, unless you choose otherwise.

Professional Development
Hollstadt offers free bi-weekly training courses for our consultants as well as on-demand access to past sessions through our consultant portal. Trainings give our consultants the continuing education they need to excel on their projects.

401k + Matching
One popular benefit is our 401(k) match on the first 4% of your contributions. Hollstadt wants to help you reach your long-term financial goals and understands that planning for your future is critical. Consultants also have access to support from a Financial Advisor.

Bonus Opportunities
We appreciate and reward loyalty. Join Hollstadt, stay for 5 years, and we’ll give you a $5,000 Longevity Award bonus! Additionally, we know great talent knows other great talent. If you are on contract with Hollstadt and refer one of your connections who gets placed, we’ll pay you $1,000!

Ongoing Support & Networking
We have made a significant investment in building a support program for our consultant team - so you never have to feel like you are going it alone. We also have a Consultant Coach program which acts like a 'work buddy' to provide a safe ear for questions or concerns at your client site.",#N/A,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,1990,$25 to $100 million (USD)
"Veolia
3.8",3.8,"Paramus, NJ",Senior Data Engineer,"Company Description Veolia Group aims to be the benchmark company for ecological transformation. With nearly 220,000 employees worldwide, the Group designs and provides game-changing solutions that are both useful and practical for water, waste and energy management. Through its three complementary business activities, Veolia helps to develop access to resources, preserve available resources and replenish them. In 2021, the Veolia group provided 79 million inhabitants with drinking water and 61 million with sanitation, produced nearly 48 million megawatt hours and recovered 48 million tonnes of waste. Veolia Environnement (Paris Euronext: VIE) achieved consolidated revenue of 28,508 billion euros in 2021. www.veolia.com
Job Description
Develop and operate data management tools, monitoring data flows, data quality, data cleansing and data processing
Create and document logical data integration strategies for data flows between disparate systems and the enterprise data warehouse/data lakes
Collaborate with different stakeholders (engineers, data stewards) to collect required data from internal and external systems
Work in an Agile environment that focuses on collaboration and teamwor
Improve and extend existing data infrastructure services
Monitor production job schedule and correct job failures in a timely manner

Qualifications
MS degree in Computer Science or computer related field from an accredited institution.
5+ years hands proven experience as a Data Engineer or similar role.
5+ years of strong experience building, running and maintaining datalake(s) and warehouse(s) in a cloud environment.
More than 4 years of experience developing with Python.
4+ years performing with production environments in a DevOps culture managing code composed of multi-developer teams, following industry best practices.
4+ years SQL development experience.
Experience with data modeling
4+ years bash scripting experience.
Strong experience with Git, CI/CD (preferably GitLab) and Docker.
Experience deploying and running services in Cloud Big Data platforms such as BigQuery and Snowflake.
Strong experience with GCP services.
Experience designing and building data pipelines using tools like Apache Beam, CDAP (Data Fusion) or other ETLs.
Knowledge with CDC design patterns and their challenges.
Experience with DAG workflows orchestration such as Apache Airflow.
Experience with NoSQL databases is a plus (i.e Firestore, MongoDB).
Experience designing and developing APIs is a plus (i.e using FastAPI, Flask).

(Nice to have) Google Cloud Data Engineer
Abilities:
Being able to work in a large company with different stakeholders.
Embrace mentorship through design sessions, code reviews, and community building.
Take ownership and support solutions you develop.
Value collaboration with other members of the team.
Have a product mindset.
Good communication.

Additional Information

A subsidiary of Veolia group, Veolia North America (VNA) offers a full spectrum of water, waste and energy management services, including water and wastewater treatment, commercial and hazardous waste collection and disposal, energy consulting and resource recovery. VNA helps commercial, industrial, healthcare, higher education and municipality customers throughout North America. Headquartered in Boston, Mass., Veolia North America has more than 10,000 employees working at more than 350 locations across the continent. www.veolianorthamerica.com As an inclusive company, Veolia is committed to diversity and gives equal consideration to all applications, without discrimination. We are an Equal Opportunity Employer! All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. As an inclusive company, Veolia is committed to diversity and gives equal consideration to all applications, without discrimination. We are an Equal Opportunity Employer! All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status.
Disclaimer: The salary, other compensation, and benefits information is accurate as of the date of this posting. The Company reserves the right to modify this information at any time, subject to applicable law.","$119,169 /yr (est.)",10000+ Employees,Company - Public,"Energy, Mining & Utilities",Energy & Utilities,1853,$1 to $5 billion (USD)
"Mastery Logistics Systems, Inc
3.9",3.9,"Omaha, NE",Senior Data Engineer (Kafka),"About the Role
In the world of transportation, data is constantly moving, and Kafka is the roadway that keeps that traffic running smoothly to its destination. As a technical expert, you must be comfortable working across teams on multiple, high impact projects. You will be a valued part of a team that is constantly maturing Kafka use and event-driven architecture. Members of this team are responsible for the overall use and implementation of Kafka components including the Confluent platform, observability, governance, best practices, and solution development. An understanding of Kafka principles and enterprise integration patterns is required.
In order to be successful:
You are a self-directed person who can identify priorities.
You are a detail-oriented person who takes pride in keeping data correct and always having a backup plan.
You are a problem-solver who might write a script or find a tool to get things done when there isn't an established solution.
You want to learn and grow in the event-driven world.
You love Kafka! When you hear terms like ""event-driven"" or ""real-time streaming"" you're ready ready to dive in!
Responsibilities
Lead a team of Kafka engineers in an operational capacity
Develop and implement solutions using Kafka.
Administer and improve use of Kafka across the organization including Kafka Connect, ksqlDB, Streams, and custom implementations.
Work with multiple teams to ensure best use of Kafka and data-safe event streaming.
Understand and apply event-driven architecture patterns and Kafka best practices. Enable development teams to do the same.
Assist developers in choosing correct patterns, event modeling, and ensuring data integrity.
Continuous learning to be a Confluent/Kafka subject matter expert.
Work with Kafka and Confluent API's (e.g. metadata, metrics, admin) to provide pro-active insights and automation.
Work with SRE's to ensure Kafka-related metrics are exported to New Relic.
Perform regular reviews of performance data to ensure efficiency and resiliency.
Contribute regularly to event-driven patterns, best practices, and guidance.
Review feature release and change logs for Kafka, Confluent, and other related components to ensure best use of these systems across the organization.
Work with lead to ensure all teams are aware of technology changes and impact.
Acquire a deep understanding of source and sink connector technical details for a variety of platforms including PostgreSQL, MS SQL Server, Snowflake, and others as required.
Requirements
Be able to describe the primary components of Kafka and their function (brokers, zookeeper, topics).
At least two years of experience supporting applications in a production environment.
You will be expected to read and navigate code in multiple languages. Multi-language fluency and writing is not required.
Experience in a microservice architecture
Experience with event driven architecture
Proficiency in at least one programming language and one scripting language.
Proficiency with Docker containers.
Ability to participate in and contribute to code management in Github including actively collaborating in peer-reviews, feature branches, and resolving conflicts and commits.
Excellent written and verbal communication skills.
Strong sense of responsibility with a bias towards action.
Comfortable self-directing and prioritizing your own work.
Microservices experience is a plus.
Distributed tracing experience a plus.
An understanding of any cloud (Azure preferred) infrastructure and components is a plus, but is not required.
Create reference solutions.","$96,493 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Delaware North
3.6",3.6,"Buffalo, NY",Data Engineer,"The Opportunity
Delaware North Global Headquarters is hiring a Data Engineer to join our Information Technology team in Buffalo, New York. As a Data Engineer, you will work with programming languages, frameworks, databases, front-end tools, back-end tools, and applications connected via APIs to collect raw data and transform the data into canonical models. The Data Engineering team is tasked with harmonizing and enhancing the data to provide trusted datasets to our consumers. The work our data manager team does forms the foundation of company initiatives to help automate business processes and gather insights, to help the company make more informed decisions.
Minimum - Anticipated Maximum Salary: $70700 - $93700 / year
The advertised pay range represents what we believe at the time of this job posting, that we would be willing to pay for this position. Only in special circumstances, where a candidate has education, training, or experience that far exceeds the requirements for the position, would we consider paying higher than the stated range. Information on our comprehensive benefits package can be found at https://careers.delawarenorth.com/whatweoffer.

At Delaware North, we care about our team member’s personal and professional journeys. These are just some of the benefits we offer:
Health, dental, and vision insurance
401(k) with company match
Performance bonuses
Paid vacation days and holidays
Paid parental bonding leave
Tuition and/or professional certification reimbursement
Generous friends-and-family discounts at many of our hotels and resorts
Responsibilities
Utilize technology stacks such as Apigee, Python, Django, Apache Airflow, MongoDB, PostgreSQL, and Amazon Web Services such as Lambda, EBS, S3, SQS, ECS, RDS, EC2 and Redshift to build datasets.
Design and implement project-based solutions.
Implement data platform improvements and new features.
Assist the support team with the resolution of data platform bug fixes.
Interface with clients, vendors, and internal users of the data platform on understanding the data.
Author documentation for standard operating procedures and knowledge base articles.
Develop integration tests to validate solutions.
Qualifications
Bachelor’s degree or equivalent from an accredited college or university in Computer Science, Information Systems, or similar STEM field preferred.
Minimum of 2 years of experience developing data pipeline ETL processes.
Extensive experience following End to End Agile Development Lifecycle and writing in SQL and Query.
Data persistence methods such as NoSQL and RDBMS, data structures and formats such as JSON, XML, and parquet.
Cloud computing experience as it relates to event-based serverless architecture, AWS preferred.
Extensive experience with handling large data sets, high performance computing, building high performance solutions and data integration projects.
Technical specification and use case documentation, such as UML, Domain and Entity-Relationship Modeling, Business Process Notation.
Must be legally authorized to work in the US without sponsorship.
Who We Are
At Delaware North, you’ll love where you work, who you work with, and how your day unfolds. Whether it’s in sporting venues, casinos, airports, national parks, iconic hotels, or premier restaurants, there’s no telling where your career can ultimately take you. We empower you to do great work in a company with 100 years of success, stability and growth. If you have drive and enjoy the thrill of making things happen - share our vision and grow with us.
Delaware North Companies, Incorporated and its subsidiaries consider applicants for all positions without regard to race, color, religion, creed, gender, national origin, age, disability, marital or veteran status, sexual orientation, or any other legally protected status. Delaware North is an equal opportunity employer.","$82,200 /yr (est.)",10000+ Employees,Company - Private,Restaurants & Food Service,Catering & Food Service Contractors,1915,$1 to $5 billion (USD)
"Procore Technologies
4.4",4.4,Oregon,Staff Data Engineer,"Job Description

We’re looking for a Staff Data Engineer to join Procore’s Data Intelligence group. In this role, you’ll be a part of the data platform engineering team focused on building datamarts on the Procore Data Platform. The products and services that you build will support the data driven needs of many existing and forthcoming products of Procore.
As a Staff Data Engineer, you’ll partner with data engineers, platform engineers and product leaders to create and support Procore Risk Advisors Underwriting product portfolio. Use your analytical, data modeling, pipeline development skills to create and enhance the core data driven decision making at Procore. Procore Risk Advisors is a burgeoning and high value organization at Procore that is bringing technology driven experiences for quick and secure experiences in the Insurance industry. Backed by the wealth of data and industry leadership of Procore, PRA is breaking new ground in how the construction industry perceives Insurance.
This position reports into Senior Manager and will be based in remotely. We’re looking for someone to join us immediately.
What you’ll do:
Build the design and development of big data predictive analytics using object-oriented analysis, design and programming skills, and design patterns
Implement ETL workflows for data matching, data cleansing, data integration, and management
Maintain existing data pipelines and develop new data pipeline using big data technologies
Responsible for leading the effort of continuously improving reliability, scalability, and stability of the enterprise data platform
Contribute to and lead the continuous improvement of the software development framework and processes by collaborating with Quality Assurance engineers
Reproduce, troubleshoot and determine the root cause of production issues
Deliver observable, reliable, and secure software, embracing the “you build it, you run it” mentality, and focus on automation and GitOps
Participate in daily standups, team meetings, sprint planning, and demo/retrospectives while working cross-functionality with other teams to drive the innovation of our products
What we’re looking for:
BS degree in Computer Science, a similar technical field of study, or equivalent practical experience; MS or Ph.D. degree in Computer Science or a related field is preferred
8+ years of experience in a Data Engineering position
Strong expertise with 2+ years of experience building enterprise techniques for large scale distributed system design and data processing including:
Building streaming data pipelines using Kafka, Spark, or Flink
Building and maintain data warehouses in support of BI tools
Building data pipeline framework for data workflow to process large data sets and Real-Time & Batch Data Pipeline development
Experience in processing structured and unstructured data into a form suitable for analysis and reporting with integration with a variety of data metric providers ranging from advertising, web analytics, and consumer devices
Desire to be actively hands-on with code using Java and Python (70-90%) along with willingness and passion for mentoring junior engineers and performing code reviews
Possess strong knowledge or familiarity with Apache Beam or AWS managed services for data (Glue, Athena, Data Pipeline, Flink, Spark) and Snowflake
Develop data catalogs and data cleanliness to ensure clarity and correctness of key business metrics

Additional Information

Base Pay Range $169,280-$232,760. Eligible for Bonus Incentive Compensation. Eligible for Equity Compensation. Procore is committed to offering competitive, fair, and commensurate compensation, and has provided an estimated pay range for this role. Actual compensation will be based on a candidate’s job-related skills, experience, education or training, and location.
Perks & Benefits
At Procore, we invest in our employees and provide a full range of benefits and perks to help you grow and thrive. From generous paid time off and healthcare coverage to career enrichment and development programs, learn more details about what we offer and how we empower you to be your best.
About Us
Procore Technologies is building the software that builds the world. We provide cloud-based construction management software that helps clients more efficiently build skyscrapers, hospitals, retail centers, airports, housing complexes, and more. At Procore, we have worked hard to create and maintain a culture where you can own your work and are encouraged and given resources to try new ideas. Check us out on Glassdoor to see what others are saying about working at Procore.
We are an equal-opportunity employer and welcome builders of all backgrounds. We thrive in a diverse, dynamic, and inclusive environment. We do not tolerate discrimination against employees on the basis of age, color, disability, gender, gender identity or expression, marital status, national origin, political affiliation, race, religion, sexual orientation, veteran status, or any other classification protected by law.
If you'd like to stay in touch and be the first to hear about new roles at Procore, join our Talent Community.","$201,020 /yr (est.)",1001 to 5000 Employees,Company - Public,Information Technology,Enterprise Software & Network Solutions,2002,Unknown / Non-Applicable
"Syntelligent Analytic Solutions
3.9",3.9,"Washington, DC",Data Engineer,"Syntelligent Analytic Solutions, LLC provides uniquely qualified personnel with the expertise and tools needed to fulfill our customers’ management and technical requirements in the intelligence, defense, homeland security and commercial market space.
Our customers’ and Syntelligent’s success are built upon the core values of People First, Integrity & Accountability, Mission Driven, Community Focus and Team Oriented.
Syntelligent is seeking a Data Engineer with an active TS/SCI clearance for upcoming work with a federal client in Washington, DC.
Description
Syntelligent is seeking a data engineer to work in a variety of settings to develop and design data pipelines to support end-to-end solutions. Candidate will build systems that collect, manage, and convert raw data into usable information for data scientists and business analysts to interpret.
Duties
Develop and design data pipelines to support an end-to-end solution.
Develop and maintain artifacts i.e., schemas, data dictionaries, and transforms related to ETL processes.
Integrate data pipelines with AWS cloud services to extract meaningful insights.
Manage production data within multiple datasets ensuring fault tolerance and redundancy.
Design and develop robust and functional dataflows to support raw data and expected data.
Provide Tier 3 technical support for deployed applications and dataflows.
Collaborate with the rest of data engineering team to design and launch new features. Includes coordination and documentation of dataflows, capabilities, etc.
Skills
Amazon Web Services (AWS)
Database Administration
Data Engineering
ETL Architecture and Development
End-to-End Processes
API Development
Extract, Transform, and Load (ETL)
Data Pipeline
Tier 3 Technical Support
Preferred
Database administration and development experience will be a plus for consideration.
Experience with cloud message APIs and usage of push notifications.
Keen interest in learning and using the latest software tools, methods, and technologies to solve real world problem sets vital to national security.
Qualifications:
Active TS/SCI clearance ONLY
Bachelor’s degree
Three (3) to five (5) years of relevant experience.
Clearance level required: TS/SCI
Location:
For any unclassified development efforts, remote work is available. Position does require client site work at DHS HQs on Nebraska Ave NW, DC
Online applications, please.
The salary ranges for these positions will be set based on experience, geographic location and possibly contractual requirements and could fall outside of this range. Other rewards may include annual bonuses, Spot bonuses, and program-specific awards. In addition, Syntelligent provides a variety of benefits to all our Full-Time employees.
When we review candidates' information, we are looking for the best matches for the position based on the qualifications listed in the job posting. If your skills and experience appear to match an open position, a recruitment services professional or a hiring manager may contact you.
Syntelligent Analytic Solutions, LLC is an Equal Employment Opportunity and Affirmative Action employer. It is the policy of the company to provide equal employment opportunities to all qualified applicants without regard to race, color, religion, sex, sexual orientation and gender identity or expression, national origin or protected veteran status and will not be discriminated against on the basis of disability. If you are a qualified disabled veteran or individual with a disability and need reasonable accommodation to use or access our online system, please contact our Human Resources department at 540-736-4570, Extension #2","$100,358 /yr (est.)",1 to 50 Employees,Company - Public,Telecommunications,"Cable, Internet & Telephone Providers",#N/A,Unknown / Non-Applicable
"ZAGG, Inc.
3.6",3.6,"Midvale, UT",Senior Data Engineer,"Position Summary
ZAGG is looking for an individual who can drive business change through insightful development of systems and applications. This position will work closely with business owners to architect, develop, and implement solutions across ERP, CRM, EDW, and other applications. A successful candidate will have excellent communication and technical skills to help implement business requirements into working solutions.
Salary: $120K to $135K
Responsibilities:
Administration – 30%
Work closely with multiple departments locally and overseas contributing and solving complex issues that will directly affect business operations and outcomes
Gather/evaluate requirements for business processes and technology enhancements while uncovering areas for improvement
Manage and prioritize projects and resources to ensure business goals are met and maximum value is created
Evangelize applications through effective process design and user training
Create and maintain documentation for operational and security audits

Development – 60%
Architect and manage solutions across multiple applications ensuring efficient integrations that provide redundancy, visibility, and extensibility
Utilize technology to improve the quality of life by automating and enhancing the ability of users/departments
Enhance cloud based data silos supporting Microsoft Dynamics CRM technologies
Conduct application testing and provide database management support
Create and maintain integrations between core applications, services, databases, etc.
Consolidate multiple data silos into a single EDW used for companywide Reporting and Analytics
Model and build data structures to support multi-dimensional data discovery

Other duties as required – 10%

Qualifications:
7+ years of similar experience in an engineering role utilizing an EDW system
Bachelor’s degree in Computer Science or related area
Exceptional analytical and conceptual thinking skills with a detail oriented and inquisitive personality
Proven experience gathering and interpreting business requirements and converting them to technical blue prints
Knowledgeable in enterprise technology stacks (Servers, Database, Network, EDI, etc)
Strong experience in cloud architecture and development (Azure, AWS)
Strong experience in supporting data structures supporting Microsoft Dynamics Technologies including MSSQL and Cloud CDS
Strong experience with integration tools and web service protocols such as SSIS, Fivetran, Synapse, Jitterbit, Smart Connect, Scribe, SOAP, REST, etc.
Experience in a development/scripting language such as python, powershell, etc.
Strong SQL Skills along with an understanding of data warehouse methodologies
Strong experience with data warehousing platforms (Azure DW, Redshift, Matrix, Snowflake)
Strong experience with visualization tools (PowerBI, Tableau, SSRS, etc)
Strong knowledge of SDLC and Agile/Scrum Framework
Strong understanding of operations in a consumer goods industry – sales forecasting, inventory, etc. is a plus
Be able to communicate with customers and co-workers in an effective, timely and professional manner
Strong interpersonal and meeting facilitation skills with technical project management experience being a plus
Must have a collaborative style and be able to cultivate and maintain an open environment where ideas are shared, questioned and tested","$127,500 /yr (est.)",501 to 1000 Employees,Company - Public,Manufacturing,Consumer Product Manufacturing,2005,$500 million to $1 billion (USD)
"Titan America
3.8",3.8,"Roanoke, VA",Data Engineer,"Data Engineer
We are seeking a motivated and detail-oriented Data Engineer to join our team based in Roanoke, Virginia. As a Data Engineer, you will play a crucial role in designing, developing and maintaining scalable data pipelines and transformations for business-critical use cases. In addition, you may take on various Software Development and Data Science responsibilities as we frequently share responsibilities and knowledge across our growing team.
Responsibilities:
Design, develop and maintain scalable data pipelines for extracting, transforming, and loading data from various sources
Collaborate with cross-functional teams to identify data needs and determine the best data solutions
Develop and implement data models to support business requirements and ensure data quality
Ensure the security and privacy of sensitive data by implementing appropriate access controls
Monitor and optimize data pipeline performance to ensure timely and accurate data delivery
Document data pipeline processes, data dictionaries, and data storage solutions
Requirements:
Bachelor's degree in Computer Science, Computer Engineering, or a related technical field
Minimum of five years of professional experience working as a Data Engineer or Software Developer
Strong hands-on experience with data warehouse and transformation solutions, i.e. Domo, Snowflake or similar
Proficient in at least one scripting language such as Python, JavaScript, or R
Understanding of data modeling, data integration and data quality processes
Familiarity with cloud platforms such as AWS, Azure, or Google Cloud Platform
Strong analytical and problem solving skills
Full Stack Software Development experience in a professional setting is highly desired, but not required
This is an excellent opportunity for a driven and collaborative individual to make a significant impact in a dynamic and growing team. If you have a passion for data and a desire to work in a fast-paced and dynamic environment, we want to hear from you!
Success begins with hiring the right people to partner with us as we grow and develop our businesses. People are central to everything we do. It is through their efforts and talents that Titan has been successful for over 100 years.
Titan America is an Equal Employment Opportunity (EEO) / Affirmative Action employer and welcomes all qualified applicants. Applicants will receive fair and impartial consideration without regard to race, sex, color, religion, national origin, age, disability, veteran status, genetic data, or other legally protected status.
Job Type: Full-time
Pay: $75,000.00 - $85,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Referral program
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Roanoke, VA 24019: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Do you currently or will you in the future need sponsorship to work in the United States?
This position requires the successful candidate to be in the office in Roanoke, VA at least 3 days a week. Can you meet this requirement?
This position requires the successful candidates to be a full time, W2 employee. Can you meet this requirement?
Education:
Bachelor's (Preferred)
Experience:
Data Engineering: 5 years (Preferred)
Work Location: Hybrid remote in Roanoke, VA 24019","$80,000 /yr (est.)",1001 to 5000 Employees,Company - Public,"Energy, Mining & Utilities",Mining & Metals,1902,Unknown / Non-Applicable
"MyCare Medical Group
4.1",4.1,"Lutz, FL",Data Engineer,"Job Summary
The Data Engineer will design, develop, optimize, and maintain data architecture and pipelines that adhere to ELT principles and business goals.
Job Responsibilities:
Designs, develops, optimizes, and maintains data architecture and pipelines that adhere to ELT principles and business goals.
Solves complex data problems to deliver insights that helps business achieve its goals.
Creates data products for engineer, analyst, and data scientist team members to accelerate their productivity.
Engineers effective features for modelling in close collaboration with business analysts and stakeholders.
Leads the evaluation, implementation and deployment of emerging tools and process for analytics data engineering in order to improve productivity as a team and quality.
Partners with engineers, BI, and solutions architects to develop technical architectures for strategic enterprise projects and initiatives.
Fosters a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions.
Advises, consults, mentors, and coaches other data and analytic professionals on data standards and practices.
Develops and delivers communication and education plans on analytic data engineering capabilities, standards, and processes.
Job Qualifications:
Education and Work Experience: Bachelor’s degree in computer science, information systems, engineering, or a related field; Masters preferred.
3+ years of experience in data engineering role.
Expertise in ELT and data analysis, SQL and at least one programming language (e.g., SQL, Python).
Conceptual knowledge of data and analytics, such as dimensional modelling, reporting tools, data governance, and structured and unstructured data.
Skills: Strong understanding of agile methodologies and experience as a Data Engineer on a cross-functional agile team preferred.
Experience with designing and maintaining data warehouses and/or data lakes with technologies, such as Hadoop, Spark, or distributed databases, like Redshift and Snowflake, and experience with housing, accessing, and transforming data in a variety of relational databases.
Experience with cloud database technologies (e.g., Azure) and developing solutions on cloud computing services and infrastructure in the data and analytics space.
Experience in building data pipelines and deploying/maintaining them with best-in-class tools (e.g., Git and Jenkins).
Familiarity with the Linux operating system.
Basic familiarity with BI tools (e.g., Tableau, Power BI, Looker).
Experience working in data engineering or data architect role.
Candidate Qualities: Focuses on business agility and agile delivery with a “fail-fast, succeed early” mindset and emphasis on measurable outcomes.
Brings a high-energy and passionate outlook to the job and can influence those around them.
Able to build a sense of trust and rapport that creates a comfortable & effective workplace.
Passion for innovation and “can do” attitude.
BENEFITS
Comprehensive benefits package, including Health, Vision, Dental, and Life insurancesFSA and Life Assistance Program (EAP)
401(k) Retirement Plan
Health Advocacy, Travel Assistance, and My Secure Advantage
PTO Accrual and Holidays
#ZIP
Job Type: Full-time
Pay: $125,000.00 - $150,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Employee discount
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Referral program
Relocation assistance
Retirement plan
Tuition reimbursement
Vision insurance
Compensation package:
Performance bonus
Yearly bonus
Experience level:
3 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Lutz, FL 33558: Reliably commute or planning to relocate before starting work (Preferred)
Experience:
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person","$137,500 /yr (est.)",501 to 1000 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2018,Unknown / Non-Applicable
"Loyal
3.9",3.9,"Atlanta, GA",Senior Cloud Data Operations Engineer,"Loyal is an organization centered on experience and building a platform that allows consumers to make meaningful decisions when it comes to healthcare. We deeply understand providers, locations, services, appointments, business rules, and moreover, we understand patients - who they are, the preferred method of communication, upcoming appointments, lapsed appointments, outstanding bills, health risks, and more. With this intelligence, our platform fuels highly relevant and personalized experiences across all mediums (website, email, voice…) allowing patients to get healthy, stay healthy, and have a better relationship with the health care provider.
**This is a remote role**
Summary
The Senior Cloud Data Operations Engineer is responsible for delivering Loyal's cloud-based data warehouse. They interface with groups across the organization to ensure we are collecting and storing data in a way that makes it easy to create and consume. They identify best practices for big data management, select the right tooling, create a workable data architecture, and drive an implementation roadmap others can depend on. The Senior Cloud Data Operations Engineer also works closely with our security group to ensure data security and privacy. They also mentor others on the team, providing technical leadership to those they work with.
You will
Create data streams and consolidate data sources into a single cloud-based data warehouse
Architect and catalog data warehouse storage schema
Work with product teams to enable easy data aggregation
Work with data consumers (analysts, data scientists, and AI/ML engineers) to ensure data is easily accessible and consumable
Create and execute a roadmap that meets data storage and availability goals
Ensure data is secure through data analysis, access control, and encryption
You have
Bachelor's degree in computer science, information systems, applied math or related field, or equivalent work experience
Minimum 5 years of experience working with cloud data processing services (AWS, Azure, or GCP).
Minimum 3 year experience with Big Data technologies, such as Databricks, Snowflake, Spark, Hadoop, etc.
Strong experience in creating data architecture and storage schemas required
Significant working knowledge of SQL and other query languages and tools
Significant experience with building relational database systems strongly preferred
Experience with ETL tools a plus
Knowledge of Python preferred
Software development, data analytics, or data science experience is a plus.
Experience working within a software as a service (SaaS) company preferred.
Experience working within a high growth and/or ambiguous environment, with proven experience to be dynamic preferred.
#LI-REMOTE
We know that potential candidates are often less likely to apply to a position if they don't match 100% of the job qualifications. Don't let that be why you miss out on this opportunity! We encourage you to apply if you can demonstrate many of these skills and competencies.
Loyal to our employees
We are a remote-friendly company! We encourage you to apply from anywhere in the United States. We also believe in a work/life balance that fulfills you while you're here and supports you when you're not. We built our benefits package to prove that we're committed to you having everything you need (including a little fun). Here is what we offer full-time employees:
Flexible paid time off, sick and personal days
At least one holiday per month (sometimes, more!)
Full health, dental, and vision insurance - Loyal pays the premium for all employees!
One Time Home Office Setup Stipend For Remote & Hybrid Roles
Monthly Internet Stipend for Remote & Hybrid Roles
Long term & short term disability
401[k] plan
16 Weeks Paid Parental Leave
2 Volunteer days per year
Matching Gift Program
Participation Grant Program
Annual Travel/Team Events up to twice per year (post-COVID)
Our Commitment
We believe that the key to Loyal's success is you. Your unique background, life experience, knowledge, self-expression, and talent make you uniquely you. Who you are, what you have experienced, and how you think inspires us to be innovative and bold.
Loyal is an equal opportunity employer. We hire great people from a wide variety of backgrounds, not just because it's the right thing to do, but because it makes our company stronger. We welcome the unique contributions that you can bring in terms of your education, opinions, culture, ethnicity, race, ancestry, sex, gender identity and expression, national origin, citizenship, marital status, age, languages spoken, veteran status, color, religion, disability, sexual orientation, and beliefs.
We consider qualified applicants regardless of criminal histories, consistent with legal requirements.
Further, consistent with applicable federal and state law, Loyal provides reasonable accommodations when requested by qualified applicants or employees with disabilities, unless doing so would cause an undue hardship. Loyal's policy regarding requests for reasonable accommodation applies to all aspects of employment, including the application process. If you require a reasonable accommodation in connection with the application process, please contact the Talent Acquisition Department at talentacquisition@loyalhealth.com.
E-Verify
This employer participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S. If E-Verify cannot confirm that you are authorized to work, this employer is required to give you written instructions and an opportunity to contact Department of Homeland Security (DHS) or Social Security Administration (SSA) so you can begin to resolve the issue before the employer can take any action against you, including terminating your employment. Employers can only use E-Verify once you have accepted a job offer and completed the I-9 Form.
COVID-19 Vaccinations
Consistent with Loyal policy, candidates performing in-person work will be required to show proof of being fully vaccinated against COVID-19 upon commencing employment. Reasonable accommodations will be considered on a case-by-case basis for exemptions to this requirement in accordance with applicable law. If you require a reasonable accommodation to Loyal's vaccination requirement, please contact the People Department at people@loyalhealth.com.",#N/A,51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2015,$5 to $25 million (USD)
"shaped.ai Inc.
4.0",4.0,"New York, NY",Data Engineer,"Hello! We’re building the future of content discovery infrastructure with AI — come join us!

Shaped (YC W22) is an API for developers to seamlessly add personalized ranking and recommendation to their products. These frictionless discovery experiences help end-users find what they want faster, and, consequently, grow conversion and engagement business metrics. Behind the scenes, we’re building state-of-the-art ML infrastructure for training and deploying large AI models to power our personalization engine.
We’re a seed stage start-up backed by top-tier investors (e.g. Y-Combinator, Susa Ventures, Tribe Capital etc…) and executives (e.g. Google, Amazon, Uber, Dropbox etc…). Our team comes from Meta, Google, Apple and Uber. We’re a remote team but have a small office in Brooklyn, New York.
We’ve been lucky to have a huge amount of success since our initial launch earlier this year and now looking to expand our real-time data engineering capabilities. We are looking for a data engineer to design, build and optimize Shaped's real-time streaming data ingestion endpoints. You will be a founding engineer working on state-of-the-art infrastructure. As one of Shaped’s early employees you will help shape our product, culture and vision.
This role is for a full-time data engineer at $120k - $200k a year and 0.1 - 0.5% equity.

Responsibilities:
Manage the unification of our batch and real-time data processing pipelines.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Requirements:
Bachelor's in computer science, data science or mathematics related field. Master's degree or PhD will be advantageous.
5+ years of data engineering experience, ideally applied to machine learning use-cases.
Experience with AWS or common cloud technologies
Proficient with Python, SQL.
Extensive knowledge of both offline and online large scale data processing frameworks: Spark, Beam, Flink.
Extensive knowledge with stream technologies like Kakfa or Kinesis.
Excellent written and verbal communication skills.
We’re excited to work with you. Come build the future of AI and discovery with us!","$160,000 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Barracuda Networks Inc.
3.6",3.6,"Alpharetta, GA",Data Engineer,"Job ID: 23-579

Come Join Our Passionate Team! At Barracuda, we make the world a safer place. We believe every business deserves access to cloud-enabled, enterprise-grade security solutions that are easy to buy, deploy, and use. We protect email, networks, data and applications with innovative solutions that grow and adapt with our customers’ journey. More than 200,000 organizations worldwide trust Barracuda to protect them — in ways they may not even know they are at risk — so they can focus on taking their business to the next level.
We know a diverse workforce adds to our collective value and strength as an organization. Barracuda Networks is proud to be an Equal Opportunity Employer, committed to equal employment opportunity and equitable compensation regardless of race, gender, religion, sex, sexual orientation, national origin, or disability.
Envision yourself at Barracuda
We currently have an exciting new position open for a Data Engineer. This person will assist in driving technology innovation for engineering projects and work on architecture of AWS, Elastic and Kubernetes. They will also work closely with the Technology Architect to create and drive the Architecture Review Board.
What you’ll be working on
Determine internal / external customer data needs and identify system specifications
Analyze the needs of large systems and breaking them down into smaller manageable parts
Responsible for designing the journey of data from various sources to the database and everything in between
Plan and design the structure of data, brainstorm with team
Analyze best technology products and systems for internal teams
Help provide build vs buy decisions on data architecture
Communicate data requirements to engineers; explain data architecture to them and provide assistance throughout SDLC
Choose / maintain suitable software, hardware and integration methods
Help resolve technical problems as and when they arise
Review testing procedures to ensure data performant and error free
Ensure that database satisfy quality standards and procedures. Setup such standards.
Work with senior IT personnel to devise plans for future IT requirements of the organization
Produce progress reports on key infrastructures - Elastic and Data Pipelines
What you bring to the role
Bachelor Degree or Post Graduate in Computer Science, or relevant work experience
5+ years of experience in an Engineering / Operations heavy organization
1+ years of experience as a Data Analysis and Data Design responsible for data architecture of a SaaS application
Experience in understanding and designing data sharding in a distributed database
Some Hands-on experience in software development and system administration
Knowledge of strategic IT solutions
Familiarity with programming languages like JavaScript or Java or Python
Familiarity with Linux OS and Windows alike
Experience in cloud computing and cloud Technologies (AWS, GCP, Azure)
Excellent communication skills – you should be adept at listening to, understanding, and explaining key concepts to managerial and technical resources.
Good Anticipation and problem-solving skills
Ability to prioritize and manage time – responsible for multiple streams of work at the same time
Relationship building - Ability to form a good rapport with internal and external stakeholders
In-depth knowledge about quality standards and security best practices

Preferred Skills:
Expert level Understanding of Elastic eco system - Elasticsearch, Logstash, Kibana, Filebeats
Understanding of secure tunnel communication using VPN
Experience designing data at scale
AWS experience is preferred in Cloud Technologies
What you’ll get from us
A team where you can voice your opinion, make an impact, and where you and your experience are valued. Internal mobility – there are opportunities for cross training and the ability to attain your next career step within Barracuda.
Equity, in the form of non-qualified options
High-quality health benefits
Retirement Plan with employer match
Career-growth opportunities
Flexible Time Off and Paid Time Off benefits
Volunteer opportunities
#LI-Remote","$98,161 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,$100 to $500 million (USD)
Atika Technologies,#N/A,"Pittsburgh, PA",Data Engineer,"Role: Data Engineer/PySpark Developer, Pittsburgh, PA - Day One Onsite
Fulltime and Contract(W2)
LinkedIn Must
USC/GC/EADs
Three round of interview: First two on Zoom Video Call, End Client Round will be Onsite on the Client's Location (Pittsburgh, PA)
Job Description:
Python(PySpark), Jupiter hub, airflow, Java
Database knowledge - Oracle / SQL / Data Analytics
Regulatory reporting knowledge preferred
Good communication skills
Open / Ability to learn new technologies
Very good Analytical Skills
Experience in ETL is preferable
Job Types: Full-time, Contract
Salary: $120,000.00 - $130,000.00 per year
Benefits:
Dental insurance
Health insurance
Life insurance
Compensation package:
Bonus pay
Performance bonus
Yearly pay
Experience level:
10 years
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Pittsburgh, PA 15259: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Required)
Spark: 4 years (Required)
Regulatory reporting: 3 years (Required)
SQL: 4 years (Required)
IT: 10 years (Required)
Airflow: 4 years (Required)
Jupiter: 4 years (Required)
Work Location: One location","$125,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"DataArt
4.6",4.6,Remote,Data Engineer,"CLIENT
Our client is a pioneer in US schools’ education since 2000, it is leading the way in next-generation curriculum and formative assessment. They develop a number of solutions and interactive web products for teachers, students and their parents.

The company is technology-driven with a huge number of software engineers involved in product development. They have a very solid approach to technology, employ best practices and processes, with a focus on cutting-edge frameworks, languages and tools.

We invite to the company, not a project
TEAM
There are multiple autonomous and empowered teams who work with their own part of the product and choose their own way of doing things and they expect people to be proactive and good team players.
Responsibilities
Building well-tested and optimized ETL data pipelines for both full and delta extraction
Collaborating with data analysts and learning scientists to store, aggregate, and calculate captured students’ work
Contributing to leading industry data standards, such as Caliper Analytics or xAPI
Improving our deployment and testing automation data pipelines
Requirements
3+ years of professional software development or data engineering experience
Strong CS and data engineering fundamentals
Proven fluency in SQL and a development language such as Python
Experience with the following storages: Snowflake, AWS Storage Services (S3, RDS, Glacier, DynamoDB)
Experience with ETL/BI: Airflow, DBT, Matillion, Looker
Experience with Cloud Infrastructure: AWS Kinesis, Lambda, API Gateway, Terraform
Understanding of ETL/ELT pipelines and Data Warehousing design, tooling, and support
Understanding of different data formatting (JSON, CSV, XML) and data storage techniques (3NF, EAV Model, Star Schema, Data Lake)
BS in Computer Science, Data Science, or equivalent
Experience in education or ed-tech
Proven passion and talent for teaching fellow engineers and non-engineers
Proven passion for building and learning: open source contributions, pet projects, self-education, Stack Overflow
Strong communication skills in writing, conversation",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,1997,$100 to $500 million (USD)
"Iron Bow Technologies
3.5",3.5,"Washington, DC",Sr. Data Center Engineer,"Iron Bow Technologies is seeking a Sr. Data Center Engineer to support our Federal customer, The Department of Justice (DOJ), Criminal Division (CRM), in Washington, DC. The selected candidate will support our flexible and scalable solution with the DOJ CRM to extend cloud-native features to existing legacy and next-generation applications while also future proofing the infrastructure to easily accommodate future requirements, such as transitioning to a public cloud.
This position is hybrid and requires 2 days a week onsite at the customer location in NW Washington, DC and support at the customer data center as needed in Sterling, VA.
Essential Duties and Responsibilities:
Assess and provide VMware solutions to include vCenter, vSphere, SRM, ESXi
Primary storage solutions and data file storage management, NAS file storage solutions
Data Center blade servers and rack servers
Server virtualization
Experience:
10 years of related experience providing system administration support in a virtualized environment, including at least 6 years of VMWare solutioning, including vCenter, vSphere, SRM, and ESXi
Data Center operations experience, with SAN and NAS
Experience supporting primary storage solutions and data file storage management, NAS file storage solutions
Hands on experience with Data Center blade and rack servers
Ability to work staggered shift hours when needed to support critical coverage and/or time-sensitive needs
Be a member of our Team providing coordinated 24x7 IaaS on-call support
U.S. Citizen
Currently hold or obtain/maintain a Public Trust High clearance
Education:
BS in Computer Science, Information Science, or other relevant discipline is preferred – additional years of experience may be considered in lieu of the degree
Preferred Certifications:
VMware Certified Professional
Primary storage certifications such as HPE, Dell-EMC, NetApp, or Pure Storage
VCP-DCV
Travel Requirements:
No travel
PRINCIPALS ONLY. NO AGENCIES PLEASE. MUST BE A US CITIZEN ABLE TO OBTAIN A PUBLIC TRUST SECURITY CLEARANCE.
Iron Bow Technologies is a leading IT solution provider dedicated to successfully transforming technology investments into business capabilities for government, commercial and healthcare clients. Our depth of technical expertise, global reach and strategic partnerships with industry leaders as well as disruptive technology partners uniquely positions Iron Bow to target the right solution to ensure your successful business outcome. We partner with our clients from planning and implementation through ongoing maintenance and management to deliver solutions that are strong, flexible and on target .
Iron Bow Technologies is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.
#LI-TM1",#N/A,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,1983,$500 million to $1 billion (USD)
Proits Hub LLC,#N/A,Remote,Data Analytics Engineer,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote
Speak with the employer
+91 9417599933",$60.00 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"R1 RCM, Inc.
3.4",3.4,Remote,Data Engineer III,"The Software Engineer III serves as a Data Engineer focused on building next generation of healthcare products and services. As part of a collaborative, motivated Agile-based team, the Software Engineer III is responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.
Responsibilities
Create and maintain optimal data pipeline architecture to assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Work with stakeholders to optimize the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using ‘big data’ technologies.
Work with stakeholders including the Product, Data and Engineering teams to assist with data-related technical issues and support their data infrastructure needs.
Adhere to and ensure engineering best practices using modern SDLC that enables CI/CD and favors automation, auditability, automated testing, infrastructure, and policy as code.
Ability to collaborate effectively with other architects and engineers delivering end to end, high-performing, highly reliable, scalable, and operable solutions.
Required Qualifications
Bachelor’s degree in Computer Science, Engineering, or a related technical field; or equivalent experience.
3+ years of experience delivering data engineering applications and services.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets .
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Experience in any one of the big data technologies (Hadoop, MapReduce , Spark, Data Lake etc. )
Experience in any one of the stream-processing systems ( Kafka, Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages : Python, Java, Scala, etc.
Understanding of modern software development including version control, unit testing, and continuous integration and deployment.
Strong communication skills, with proven ability to present complex ideas and to document concisely.
Preferred Qualifications
Working knowledge of Cloud providers and services such as Amazon AWS or Microsoft Azure.
Experience with relational SQL and NoSQL databases, including MongoDB and ElasticSearch.
Knowledge of healthcare data standards such as HL7, FHIR, EDI X12.
Knowledge of the healthcare revenue cycle, EMRs, practice management systems.
Working in an evolving healthcare setting, we use our shared expertise to deliver innovative solutions. Our fast-growing team has opportunities to learn and grow through rewarding interactions, collaboration and the freedom to explore professional interests.

Our associates are given valuable opportunities to contribute, to innovate and create meaningful work that makes an impact in the communities we serve around the world. We also offer a culture of excellence that drives customer success and improves patient care. We believe in giving back to the community and offer a competitive benefits package including:
Comprehensive Medical, Dental, Vision & RX Coverage
Paid Time Off, Volunteer Time & Holidays
401K with Company Match
Company-Paid Life Insurance, Short-Term Disability & Long-Term Disability
Tuition Reimbursement
Parental Leave
R1 RCM Inc. (“the Company”) is dedicated to the fundamentals of equal employment opportunity. The Company’s employment practices , including those regarding recruitment, hiring, assignment, promotion, compensation, benefits, training, discipline, and termination shall not be based on any person’s age, color, national origin, citizenship status, physical or mental disability, medical condition, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status or any other characteristic protected by federal, state or local law. Furthermore, the Company is dedicated to providing a workplace free from harassment based on any of the foregoing protected categories.
If you have a disability and require a reasonable accommodation to complete any part of the job application process, please contact us at 312-496-7709 for assistance.
CA PRIVACY NOTICE: California resident job applicants can learn more about their privacy rights California Consent",#N/A,10000+ Employees,Company - Public,Healthcare,Health Care Services & Hospitals,2003,Unknown / Non-Applicable
"Petrolink International
2.9",2.9,"Houston, TX",Data Analysis Engineer,"Location:
Houston, Texas, U.S.A.
Job Type:
Full-time
Job Description:
Petrolink is a global company that specializes in user-driven technology for the Energy Industry.
We are looking for a Data Analysis Engineer to assist with creating and delivering algorithms primarily used by our engineering solutions.
This position is based in our U.S. office in Houston, Texas.
Duties & Responsibilities:
Research and provide logic for engineering solutions algorithms.
Collaborate with Subject Matter Experts (Drilling Engineers, Mud loggers, Pore Pressure Analysts, and other specialists from the Drilling Industry) to understand different challenges in the Drilling Industry and the practices in place to address/solve these challenges.
Code the algorithms using C# to run on Petrolink’s proprietary analytics engine.
Develop, test, and maintain algorithms.
Support escalations related to issues in algorithms.
Learn existing algorithms to provide support when required.
Prepare training material and sessions for stakeholders as part of the production release.
Skills & Qualifications:
Proficient in C#.
Experience with Python is an advantage.
Knowledge in drilling practices, and ability to speak to other drilling experts and capture requirements and logic.
Knowledge in drilling performance KPIs is an advantage.
Prior experience with WITSML is an advantage.
Function and communicate well in a team having diverse expertise.
Bi-lingual (English and Spanish) preferred.
At least 2 years’ relevant experience.
Benefits:
Health insurance
Dental insurance
401(k) with matching
Health savings account
Life insurance
Long-term disability
This job description is intended to explain the general nature and level of work being performed. Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.
Applicants must be legally authorized to work in the United States for any employer.
To apply for this position, applicants must meet the above criteria.
Interested applicants fulfilling the above criteria, please send your resume to careers.usa@petrolink.com.","$81,133 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"Tipico - North America
4.0",4.0,"Denver, CO",Data Engineer,"Company Description

Tipico is an energetic, passionate, bold tech company, and we love sports! Tipico has recently expanded to the US market, establishing offices in Hoboken, New Jersey and Denver, Colorado. We are driven by our core values: passion, progress and trust. Our goal is to build the best possible mobile sports betting product in the industry, in order to amplify the emotions of millions of sports fans every day. Let’s Make This Interesting ™
Please note: this role is located in our Denver, CO office; we work off a hybrid model and come into the office 2 days per week.

Job Description

The Data Engineer will report directly to the Data Engineering Manager and work together with rest of the data engineers, BI analysts, and product teams. The position holder ensures this is done by doing the following:
Create and maintain both batch (ETL/ELT) and real-time data pipelines and architecture
Ensure all data provided by Data Platform is of the highest quality and is delivered in a timely manner and in line with agreed SLAs
Assemble large and complex data sets that meet functional and non-functional business requirements
Identify, design, and implement internal process improvements, including automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies
Collaborate with stakeholders including data architects, Executive, Product, Data and IT team members from the beginning to the delivery of a project
Build a scalable, modern, and trusted data ecosystem, making sure it is responsive, available and in line with business requirements
Take part in sprints, workloads, and deliverables through agile methodologies
Manage quality assurance, verification of accuracy and consistency of data, specifically KPI measures sent to the business
Thrive working in a fast-paced and challenging environment
Open to continuous learning individually and with the team
Comfortable with being challenged / seeking challenge
Willing to work flexible hours if needed

Qualifications
3+ years in Data Platforms / Datawarehouse / BI experience
Must have experience in successfully implementing data-centric applications, such as data warehouses, operational data stores, and data integration projects
Strong analytical and problem-solving skills that balance with creative approaches
Ability to learn new and complex concepts quickly
Strong experience with large-scale production databases and SQL
Solid understanding of cloud data services: AWS services such as S3, Athena, EC2, RedShift, EMR, EKS, RDS and Lambda
Proficient with one or more big data & new data technologies such as DBT, NIFI, Airflow, Kafka
Experience with data modelling
Experience with ETL development (extractions, data load, aggregation, transformation)
Knowledge in working with time-series/analytics databases such as Elasticsearch
Understanding of containerization and orchestration technologies like Docker/Kubernetes
Must be able to take ownership of projects with a high level of attention to detail and priority
Team player with a hands-on approach and attitude
Must be comfortable with on call rotation
Nice to have:
Familiarity with Marketing tooling and/or CRM systems is preferred
Familiarity with using business intelligence tools such as Domo, Tableau or Looker
Knowledge about and / or experience with sports betting
Background in digital companies, online, gambling, gaming, e-commerce, marketing, or mobile apps
Familiarity with one scripting language, preferably Python

Additional Information

What's in it for you:
Work in an environment where you, your work and ideas matter and have an impact.
You will be a be part of the newest and up and coming USA online iGaming and sports betting companies in the market.
Work in a new and young business with high growth potential.
Build your own success story together with us.
Work with self-organized, self-responsible and entrepreneurial employees.
Start-up feeling, backed-up by a leading European sports betting house. We are a high-volume business and are taking off in the US!
Competitive salary, Medical, Vision, Dental Benefits, Unlimited PTO, and more
Salary for this position is $120k+ & annual performance bonus","$120,000 /yr (est.)",201 to 500 Employees,Company - Private,"Arts, Entertainment & Recreation",Gambling,2004,Unknown / Non-Applicable
Atalan Tech,#N/A,Remote,Data Engineer,"Atalan Tech is a start-up with a mission to mitigate the impact of clinician burnout and turnover in our nation’s health system. We are expanding our team and looking to hire the critical role of Data Engineer. The Data Engineer will work closely with the Chief Technology Officer, Chief Scientific Officer (UCLA Ph.D.), Chief Product Officer (Oxford Ph.D.), and Chief Executive Officer to deliver actionable insights from data in a variety of situations.
We are a small, close-knit team. This is an excellent opportunity for someone to join a mission-oriented, early-stage company at the ground floor and grow with us. There will be plenty of room for growth and creativity, and opportunities to tackle challenges that present one of the toughest problems in our society.
Requirements
Strong hands-on experience of designing, developing and managing data pipelines
Strong coding skills in Python (required) and SQL (required) and R (strongly preferred)
Experience building and maintaining data pipelines on AWS using EC2, EBS, Redshift, Postgres, S3, Data Pipeline, Kinesis, Lambda, etc.
Thorough understanding and experience with task management tools (Airflow, NiFi, etc.)
Knowledge of data management best practices for ETL, data governance, data warehousing, data modeling and data science
Basic knowledge of Linux (Redhat preferred) administration
Minimum B.S in Computer Science, software engineering or related field (or equivalent in related professional experience)
Strong communication skills
Strong problem-solving/critical thinking skills
Ability to work in a fast-paced, dynamic start-up environment
Some knowledge of healthcare, health tech, or healthcare data analytics (preferred)
Typical Tasks and Responsibilities include:
Working closely with CTO and CSO to build data pipelines for machine learning
Assisting the CPO in managing and supporting analysis of enterprise data to drive better products and solutions
Managing company’s data infrastructure on AWS (Postgres/RDS, Redshift, S3, etc.)
Synthesizing multiple data sources, such as internal medical records and HR data
Integrating with client systems including EHR (Epic, Cerner), HRIS, flat files, etc.
Working closely with other key members on other projects
Developing and designing data security standards and policies in line with industry best practices
About Us:
Atalan Tech’s mission is to improve the wellness of clinicians and their patients. We blend labor economics, machine learning, and clinical psychology with a team from Harvard, Yale, and UCLA. Atalan Tech is looking to contribute innovative and sustainable solutions to one of the most pressing problems in the healthcare industry. Building upon our proof-of-concept success, we have recently secured multiple engagements with nationwide healthcare organizations and have successfully raised institutional funding to expand our reach.
Contact Us:
If you’re interested in this position, please send us your CV/resume and cover letter.
Job Type: Full-time
Pay: $108,448.20 - $130,604.27 per year
Experience level:
2 years
Schedule:
Monday to Friday
Application Question(s):
Do you know the role is full time?
Can you start within the next month?
What are your salary expectations?
Have you held the title of a data engineer? If not, what experience do you have as a data engineer? Please be very specific.
Have you released Python code to a production environment in the last year?
Experience:
Data analytics: 2 years (Required)
Work Location: Remote","$119,526 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Quadrant Resource
4.1",4.1,Remote,Data Engineer,"Job Description:
4-6 years of experience as a Data Engineer in the Snowflake-Fivetran-dbt stack
• Possess a solid understanding of end to end Data ingestion, ETL, data analysis, Reporting processes
• Expert in ETL development using Fivetran and experienced in implementing data pipelines for Snowflake Cloud DWH
•Experienced in using dbt for data transformation
•Expertise on Tableau/Metabase for building reports/dashboards
•Expert in troubleshooting and resolving ETL issues, data load failures/problems and transformation translation problems
•Excellent communicator with the ability to work effectively with distributed teams
Job Type: Full-time
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Data Engineer: 9 years (Required)
Fivetran: 8 years (Required)
DBT: 7 years (Required)
Snowflake: 6 years (Required)
Work Location: Remote",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,2004,Unknown / Non-Applicable
Diamondpick,#N/A,"Alpharetta, GA",Java Software Engineer with Big-Data,"About us
There is someone out there who fits your requirements like a glove! We are an innovative Talent Solutions company with a vision to build talent supply chain models without compromise for the technology industry.
We are a new-age talent solutions company with a vision to build an enterprise recruitment model without compromise. We love to solve complex talent challenges and strive to be an enabler of business success for our clients using data, technology and our team’s vast experience in the technology market.
Senior Java Developer with Big data
Alpharetta, GA (Hybrid Work from Day1)
Job Description:
The ideal candidate must possess strong background on frontend and backend development technologies.
The candidate must possess excellent written and verbal communication skills with the ability to collaborate effectively with domain experts and technical experts in the team.
Responsibilities:
As a Java Senior Developer, you will
Maintain active relationships with Product Owner to understand business requirements, lead requirement gathering meetings and review designs with the product owner
Own his backlog items and coordinate with other team members to develop the features planned for each sprint
Perform technical design reviews and code reviews
Be Responsible for prototyping, developing, and troubleshooting software in the user interface or service layers
Perform peer reviews on source code to ensure reuse, scalability and the use of best practices
Participate in collaborative technical discussions that focus on software user experience, design, architecture, and development
Perform demonstrations for client stakeholders on project features and sub features, which utilizes the latest Front end and Backend development technologies
Requirements:
6+ years of experience in Java/JEE development
Skills in developing applications using multi-tier architecture
Knowledge of google/AWS cloud
Java/JEE, Spring, Spring boot, REST/SOAP web services, Hibernate, SQL, Tomcat, Application servers (WebSphere), SONAR, Agile, AJAX, Jenkins..etc
Skills in UML, application designing/architecture, Design Patterns..etc
Skills in Unit testing application using Junit or similar technologies
Good communication skills
Leadership skills
Provide overlap coverage with onsite/customer teams
Capability to support QA teams with test plans, root cause analysis and defect fixing
Strong experience in Responsive design, cross browser web applications
Strong knowledge of web service models
Strong knowledge in creating and working with APIs
Experience with Cloud services, specifically on Google cloud
Strong exposure in Agile, Scaled Agile based development models
Familiar with Interfaces such as REST web services, swagger profiles, JSON payloads.
Familiar with tools/utilities such as Bitbucket / Jira / Confluence
Job Types: Full-time, Contract, Temporary
Pay: From $60.00 per hour
Benefits:
Flexible schedule
Compensation package:
Hourly pay
Experience level:
8 years
Schedule:
8 hour shift
Day shift
Work Location: Hybrid remote in Alpharetta, GA 30005",$60.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Purpose Financial
4.2",4.2,Remote,Data Engineer,"Address : 322 Rhett St, Greenville, South Carolina, United States - 29601
Purpose Financial, Inc. is an innovative consumer financial services company that offers a diverse suite of credit products, promoting financial inclusion and meeting consumers wherever they are. Through its brands, the company is committed to helping customers achieve their version of financial stability in the moment and in the future. Since 1997, Purpose Financial has been a pioneer in the consumer credit and financial services market offering money solutions in over 1,000 storefronts locations and online lending. Providing services in over 20 states, Purpose Financial employs over 2,700 team members.
At Purpose Financial, our people are our greatest asset. We are inclusive and encourage different points of view. We all belong here. We respect our colleagues. We value our customers, and we support our communities.
What do we do? Our sole purpose is to help each customer achieve their version of financial stability in the moment and in the future.
We have a history! Consumers know that when they choose Purpose, they’re backed by a trusted, reliable financial services company with 25 years in the business, 2,700+ employees, 1,000+ store locations, and operating in 24 states.
Benefits and Perks. We believe our employee benefits should make life better. Purpose Financial offers a comprehensive benefits package to everyone on our team.
Health & Welfare Benefits. When you take care of yourself, you’re a better you – which is better for everyone!
Health Savings & Flexible Spending Accounts. Pre-tax money for your family’s out-of-pocket expenses.
401k with Matching Contributions. Think bigger! You can build a better future.
Wellness & Employee Assistance Programs. Take care of yourself in every way that matters.
Generous Vacation Policy & Paid Time Off. Time off to take care of yourself, your family – and your bucket list!
Volunteer Days. We want you to care. Always. Take 2 days off each year to volunteer and make a difference in your community.
Tuition Reimbursement. Pursue your goals without fretting over all the costs.
Employee Discounts. Save on home and auto insurance, cell phone service, entertainment, shopping, pet insurance and even travel.
Performance Incentives & Awards. We want you to feel valued for all your contributions.
Hybrid Workforce Support. We believe in getting it done – in whatever way works best for you and your team.
Want to meet our executive team? Visit our home page at www.havepurpose.com.
Position Summary
The job function is responsible for building a scalable data infrastructure. The role will be part of the effort to enable Purpose Financial to democratize data by developing self-serving and curated data models for our single source of truth. The job function includes understanding of business requirements, defining data requirements, identifying enterprise data sources, planning, designing, and building data acquisition from sources, defining data model, designing data ingestion pipelines, data normalization, data transformation, and consumption using platforms such as Snowflake and AWS.
Job Responsibility
Take ownership of building/optimizing data pipelines to increase efficiency and performance
Develop and maintain data ingestion and integration processes that focus on data quality and monitoring
Use AWS and Snowflake tools such as Glue, Lambda, Snowpark, and Snowpipe to build data ingestion pipelines from various sources
Incorporate standards and best practices into data pipeline solutions
Participate in troubleshooting and problem-solving design discussions
Provide logical and physical data design, and database modeling
Solve complex data issues around data integration, unusable data elements, unstructured data sets, and other data processing incidents
Continuously design the future state of our data & reporting process by collaborating with IT and business partners.
Partner with enterprise teams, data scientist, architects to define development standards and solutions.
Manage cloud infrastructure as code (IaC), using Terraform.
Work closely with other members of the data team to better design and build metrics that enhance our analytical capabilities
Manage workflows and data build processes that help the BI team build key business metrics and design dashboard visualizations
Assess the effectiveness and accuracy of new data sources, data points, and data gathering techniques
Troubleshoot problems with data (missing, corrupted, invalid) and build data recovery strategy
Apply best practices for AWS & Snowflake architecture, ELT and data models
Be a leader for our data journey, act as SME for data and database related topics, and assist with development of data lakes and warehouses
Education Required
Bachelor’s degree (or foreign equivalent degree) in any Computer, Engineering, or other technical field.
Experience Required
5+ years experience in data warehousing
3+ years of experience in data architecture and design, with a good understanding of various data systems and structures (i.e. SQL, NoSQL, Key-Value, Streams)
3+ years of experience with building data pipelines and using data orchestration tools and implementing end to end Monitoring & Alerting solutions
3+ years of experience working with AWS or public cloud provider.
Strong understanding of SQL, data structures, data integrity, and schema design best practices
Programming experience required, preferably Python.
A strong advocate of data governance and data quality
Experience with database internals, database design, SQL and database programming
Experience in financial services (Banking and consumer lending preferred)
Experience of authoring CI/CD pipelines, automation elements related to infrastructure composition, deployment orchestration, and monitoring.
Familiarity and experience with Agile/SCRUM based development methodology.
Knowledge Required
Excellent written and verbal communications skills as would be needed to communicate in person, by phone, and through email; adaptability and flexibility to changing environment; and comfortable working in a dynamic, high volume, fast-paced environment. Ability to read, write, evaluate, and apply information. Ability to interact professionally and exhibit appropriate social skills. Ability to understand and ensure compliance with policies, procedures, and laws governing our industry/business and products. Ability to develop and maintain business relationships.
Physical Requirements
Sitting for long periods of time; standing occasionally; walking; bending; squatting; kneeling; pushing/pulling; reaching; twisting; frequent lifting of less than 10 lbs., occasional lifting of up to 20 lbs.; driving and having access during the workday to an insured and reliable transportation; typing; data entry; grasping; transferring items between hands and/or to another person or receptacle; use of office equipment to include computers; ability to travel to, be physically present at, and complete the physical requirements of the position at any assigned location.
Competencies
Integrity
Customer Advocacy
Interpersonal Skills
Results/Accountability
Inclusiveness
Technical Proficiency/Leadership
Big Data Trends
Data Management
Adaptability
Travel - None
Attire - Business Casual
Other - Must be eligible to work in the USA and able to pass a background check.
Job Type: Full-time
Compensation package:
Yearly pay
Experience level:
5 years
Schedule:
Day shift
Work Location: Remote",#N/A,Unknown,Company - Private,Financial Services,Financial Transaction Processing,#N/A,Unknown / Non-Applicable
"Intersec
4.5",4.5,Remote,Senior Data Engineer,"Architect and implement highly scalable data pipeline to streamline migration of data from source data repositories to the centralized data platform
Implement agreed upon data models (e.g., data marts) to enable faster and efficient consumption of data by downstream consumers
Conduct performance tuning to optimize data migration techniques and ensure adherence to identified service-level agreements (SLAs)
Define and implement automated validation checks to ensure data integrity between source and target
Assist in onboarding consumers with existing technology to connect to the platform
Setup predictive alert mechanism for pipeline degradation for IMF pipelines.
Enhance data quality framework for data pipelines and derived datasets
Enhance Data Audit Balance and Control mechanism for data pipelines
Enhance pipelines to be certified copies of authoritative source
Populate metadata for the Enterprise with particular focus on the Data Sources being onboarded to EDP (Technical and Business Glossaries) related to IMF Modernization Programs
Validate correct lineage for datasets
Required:
*8+ years of Databricks, Data Modeling, and/or Metadata management experience
* Familiarity with architecting and developing real and near-real-time solutions
* Bachelor's Degree in Engineering, Mathematics, computer Science, Statistics or other quantitative discipline, or 4 years equivalent professional experience
* US Citizenship and ability to gain a public trust
Preferred:
* Familiarity with IRS systems and ETL pipelines
* Ability to work independently and manage multiple tasks
* IRS MBI Clearance with IRS Laptop and Informatica Installed
Job Type: Full-time
Salary: $70.00 - $75.00 per hour
Schedule:
Monday to Friday
Application Question(s):
Are you a US/GC card holder?
How many years of Databricks, Data Modeling, and/or Metadata management experience do you have?
Work Location: Remote",$72.50 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,Unknown / Non-Applicable
"Apolis
3.9",3.9,"Saint Louis, MO",Data Engineer (Python Focused),"Role: Data Engineer (Python Focused) (W2 ONLY)
Location : St. Louis, MO 63102
Duration: 18 Months
The primary responsibilities of this role will be to work as a part of the data and analytics team of the client. The ideal candidate would have experience using ETL and other data engineering tools in a cloud environment. Additionally, this candidate would have coding and scripting experience creating code to move and manipulate data using tools such in Python such as Spark or Pandas. Microsoft Azure experience is a plus but relevant experience with other cloud-based environments will be considered.
This team uses Microsoft Azure to build and productionize data sets and processes to the product owners and product users and this role will be expected to contribute to those objectives. As a member of this team, you will work in conjunction with other data engineers, platform engineers, solution developers and others to execute and deliver working solutions to the end users.
Technical Skills
High focus on Python
More Programmer/Coder experience, less Cloud experience needed.
Must have experience with data analytics/engineering and cloud technologies and an in-depth working knowledge and understanding of data engineering, design, and integration.
Intermediate or higher level of proficiency with Python Must have the proven ability to learn quickly and apply cloud data engineering principles and skills to new tools and use cases.
Ability to define and take technical ownership of detailed requirements and collaborate effectively with various team members such as Business Analysts and Project Managers and end users.
Excellent communication (verbal and written) and problem-solving skills are essential for this position.
Initiative, Proactive Cooperation, and curiosity are mandatory behavioral skills required for this position.
Experience with several of the following technologies is preferred:
Experience with Microsoft Azure Platform. In particular, the data processing toolsets including Azure Data Factory Pipelines, Azure Databricks, Azure Data Lake Storage.
Experience using Notebooks (Such as Jupyter, DataBricks) also desired.
Experience using code languages such as Python and C#
Experience using query languages such as SQL, MDX and DAX
Experience using scripting languages such as PowerShell, M-Query (Power Query), and Windows batch commands.
Understanding of Agile and DevOps principles for software development and ownership.
Familiar with compliance and controls for software development and deployment.
Education Required
A bachelor's in computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field [or equivalent work experience] is not required but is preferred.
Job Type: Contract
Salary: $60.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Vision insurance
Compensation package:
Overtime pay
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
St. Louis, MO: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Preferred)
ETL: 5 years (Preferred)
Cloud infrastructure: 4 years (Preferred)
Work Location: One location",$65.00 /hr (est.),501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,1996,$25 to $100 million (USD)
"Contact Government Services, LLC
4.7",4.7,"Washington, DC",Data Engineer,"CGS is seeking a passionate and driven Data Engineer to support a rapidly growing Data Analytics and Business Intelligence platform focused on providing solutions that empower our Federal customers with the tools and capabilities needed to turn data into actionable insights. The ideal candidate is a critical thinker and perpetual learner; excited to gain exposure and build skillsets across a range of technologies while solving some of our clients’ toughest challenges. As a Data Engineer, you will be integral to data operations for the development and integration of multiple data types across a range of data sets and sources.

You will be responsible for the day-to-day operations of systems that depend on data, ensuring data is properly processed and securely transferred to its appropriate location, in a timely manner. Processing data will include managing, manipulating, storing and parsing data in a data pipeline for variety of target sources. You will also support maintenance of applications and tools that reside on these systems such as upgrades, patches, configuration changes, etc. The work is performed in a multidisciplinary team environment using agile methodologies. The candidate we seek must be highly motivated and enthusiastic about implementing new technologies and learning about new data in a small team environment where deadlines are important.
Responsibilities:
Complete development efforts across data pipeline to store, manage, store, and provision to data consumers
Being an active and collaborating member of an Agile/Scrum team and following all Agile/Scrum best practices
Write code to ensure the performance and reliability of data extraction and processing
Support continuous process automation for data ingest
Achieve technical excellence by advocating for and adhering to lean-agile engineering principles and practices such as API-first design, simple design, continuous integration, version control, and automated testing
Work with program management and engineers to implement and document complex and evolving requirements
Help cultivate an environment that promotes customer service excellence, innovation, collaboration, and teamwork
Collaborate with others as part of a cross-functional team that includes user experience researchers and designers, product managers, engineers, and other functional specialists
Required Skills:
Must be a US Citizen
Must be able to obtain a Public Trust Clearance
7+ years of IT experience including experience in design, management, and solutioning of large, complex data sets and models.
Experience with developing data pipelines from many sources from structured and unstructured data sets in a variety of formats
Proficiency developing ETL processes, and performing test and validation steps
Proficiency to manipulate data (Python, R, SQL, SAS)
Strong knowledge of big data analysis and storage tools and technologies
Strong understanding of the agile principles and ability to apply them
Strong understanding of the CI/CD pipelines and ability to apply them
Experience with relational database, such as, PostgreSQL
Work comfortably in version control systems, such as, Git Repositories
Contact Government Services strives to simplify and enhance government bureaucracy through the optimization of human, technical, and financial resources. We combine cutting edge technology with world class personnel to deliver customized solutions that fit specific needs and solve the most challenging, dynamic problems. Our team emphasizes technical expertise and an empathy to public service to provide unique top tier support for our government engagements.
SOME BENEFITS OF THIS POSITION INCLUDE:
Health, Dental, & Vision insurance
Life Insurance
401k
Flexible Spending Account (Health, Dependent Care, and Commuter)
Paid Time Off & Observance of State/Federal Holidays
For more information about CGS please visit: https://www.cgsfederal.com
39U0b7kJu8","$103,213 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
WellTrust Medical Group,#N/A,"Charlotte, NC",Data Engineer,"The Data Engineer will assist the senior analyst in complex modeling and projects to ensure their completion within the allotted timeline. Helps track measures of success and develop reports that measure operational and customer metrics. Demonstrates ability to troubleshoot data consumption and data quality problems.

Responsibilities:
Build and maintain reports using My SQL Server
Identify optimal data sources, structure data validation, and cache optimization.
Performs business problem modeling and quantitative analytics including regression analysis, time-series, hierarchical cluster analysis, and data mining.
Helps track measures of success and develop reports that measure operational and customer metrics.
Demonstrates ability to troubleshoot data consumption and data quality problems.
Evaluates and recommend improvements to current models, metrics, and analytic processes to improve operational efficiency and insight.
Design, document, and communicate effective technical solutions meeting end-user requirements
Perform analyses with large amounts of complex data, and uses data visualization tools to communicate results.
Requirements:

Bachelor's degree or equivalent experience in a quantitative field (Mathematics, Statistics, Computer Science, Economics, Finance, Physics, Engineering)
3+ years of professional working experience as a Data Engineer or Analyst
1-3 years of data warehouse experience
Build and maintain reports using My SQL Server
Strong analytical and creative problem-solving skills
Design, document, and communicate effective technical solutions meeting end-user
SQL and Excel Macro development
Strong understanding of various data environments such as SQL, Oracle, Big Query, etc.
Experience with data visualization tools such as Looker Studio, Tableau, D3.js, etc.
Proficient in R, Python, MATLAB, Java, or another programming language.",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Radiant System,#N/A,"Pleasanton, CA",Data Engineer,"First Priority: Pleasanton, CA –
Second Priority for a superstar: Plano, TX
Experience in developing and deploying data pipelines, preferably in the Cloud
Experience working with Spark, Databricks, and other relevant technologies in Microsoft Azure cloud
Experience in productionizing and deploying Big Data platforms and applications
Expert in writing SQL statements
Job Type: Contract
Application Question(s):
Are you willing to work on W2?
Work Location: In person",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Fracsys Inc,#N/A,"Washington, DC",Data Engineer,"Fracsys Inc is hiring a Data Engineer position. The ideal candidate must have at least 12+ years of industry experience. He or she must be responsible for successful technical delivery of Data Warehousing and Business Intelligence projects using a formal project management methodology.
Responsibilities
Directs, motivates and develops key technical personnel
Provides technical direction across the full lifecycle system development process
Provides strategic planning and comprehensive senior level technical consulting to IT senior management and senior technical staff
Working independently within guidelines
Provides detailed inputs for plans and schedules, including goals, risks, and resource allocation
Conduct diagnostic, troubleshooting steps and present conclusions to a varied audience
comfortable slinging some code to solve a problem
Build Data Marts and Data warehouses to facilitate analytical capabilities across the varied sources of data
Design and implement data patterns on cloud using cloud Data warehouse and data virtualization tools
Evaluate new tools and technologies for target state on cloud
Learn and implement new ETL, BI tools and data warehousing technologies
Qualifications
Minimum twelve (12) years in a Technical Lead or Consulting role leading Information Management related initiatives (system integration, data warehouse build, data mart build, or similar)
MUST have proven leadership experience for team of at least 3 technical resources.
Experience creating Data Marts, Data warehouses on-premise and on cloud for real time and batch processing frameworks
Experience building and optimizing big-data data pipelines and data sets including Postgres, AWS Relational Data Service
extensive hands-on experience building and optimizing data structures for data analytics, data science and business intelligence
Experience building self-service data consumption patterns and knowledge of cloud-based data Lake platforms
Experience wrangling data (structured and unstructured), in-depth knowledge of database architecture
Experience utilizing or leading implementations leveraging ETL tools (Informatica / Talend), BI Reporting tools such as MicroStrategy, Microsoft Power BI, data modeling tools such as Erwin, Oracle, SQL server, NoSQL, JDBC, UNIX shell scripting, PERL and JAVA, XML/JSON files, SAS , Python, AWS cloud-native technologies, S3, Athena, Redshift
Familiarity with the following technologies: Hadoop, Kafka, Airflow, Hive, Presto, Athena, S3, Aurora, EMR, Spark
Ability to drive, contribute to, and communicate solutions to technical product challenges
Ability to roll-up your sleeves and work in process or resource gaps and fill it in yourself
Excellent written and oral communication skills in English
Education
Bachelor’s degree in computer science, Data Analytics, Information Systems, or a related degree or equivalent experience. Master’s degree is preferred.
Job Type: Full-time
Pay: From $96,336.72 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Experience level:
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Washington, DC: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person","$96,337 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"DSFederal Inc
4.0",4.0,Remote,Data Engineer*,"Description:
We are seeking an experienced Data Engineer to design, build, and maintain our government client’s data infrastructure. The Data Engineer will work closely with cross-functional teams to develop scalable data solutions that support our client’s business needs.
Requirements:
Design, develop, and maintain data pipelines and data storage systems.
Collaborate with cross-functional teams to design and implement data-driven solutions.
Optimize and tune data systems for performance and scalability.
Implement and maintain data quality and validation processes.
Troubleshoot and debug data-related issues.
Participate in code reviews and contribute to best practices for data engineering.
Technical Skills Required:
Proficiency in one or more programming languages (Python, Java, etc.)
Experience with data modeling, database design, data marts, and data warehousing concepts
Knowledge of ETL tools and techniques
Experience with cloud-based data platforms such as AWS or Azure
Strong problem-solving and analytical skills
Desired Skills:
Familiarity with data visualization tools such as Tableau or Power BI.
Knowledge of machine learning concepts and frameworks.
Experience Required:
3+ years of experience in data engineering or related field
Education Required:
Bachelor's in engineering

Who We Are:
DSFederal is a leader in health IT and data analytics that delivers innovative solutions to improve healthcare delivery and address critical challenges, including cancer surveillance, HIV/AIDS, maternal and child health, and COVID-19 emergency response. Our highly experienced professionals serve over 50 different federal clients on over 80 projects. Our people are united by one mission – to improve human life through transformative solutions. www.dsfederal.com.
What We Can Offer You:
DSFederal attracts the best people in the business with our competitive benefits package that includes medical, dental and vision coverage, 401k plan with employer contribution, paid holidays, vacation, and more. If you enjoy being a part of a high performing, professional services, and technology focused organization, we encourage you to apply!
Our Mission: Improve human life through transformative solutions.
Our Core Values: Integrity, People First, Disciplined Business Practices, Customer Centric, Value Creation.
EEO Statement: DS Federal is an Equal Opportunity / Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, or protected veteran status.
E-Verify Statement
DS Federal utilizes the E-Verify program for every potential new hire. This makes it possible for us to confirm that every employee who works for DS Federal is eligible to work in the United States. To learn more about E-Verify you can call 1-800-255-7688 or visit their website by clicking the logo below. E-Verify® is a registered trademark of the United States Department of Homeland Security.
#LI-LP1",#N/A,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$25 to $100 million (USD)
"Take Command Health
4.9",4.9,"Dallas, TX",Data Engineer,"Company: Take Command
Position: Data Engineer
Salary: $120,000-$130,000 plus bonus
Location: Dallas or Austin, TX
About Take Command
Take Command is a start-up on a mission to improve the healthcare system, starting with health insurance. Pragmatically speaking, we help employers reimburse employees for individual insurance instead of offering a traditional one-size-fits-all group plan. We believe this model can empower employees (when they have the right support) to be savvy healthcare consumers and have a transformative impact on the entire healthcare system.
Let’s be honest—health insurance is usually a confusing, frustrating, and even emotional experience for people. We want to fix that with a new model, great technology, and a superior user experience. We have made a great start, but we need your help to fully realize our long-term vision.
As a part of the dev team, you'll build the framework for both our clients and our team to succeed. You will be a pillar in our company as we grow and make our product the best product on the market!
In this role, you will:
Take ownership for delivering multiple features, their quality and assist team members in delivering their features
Recommend improvements to data quality, processes and code to make it easier to use, manage and maintain
Experience designing and developing data models and data migrations
Experience managing and maintaining ETL or ELT processes (Hevo, BigQuery, Looker, etc.)
Hands-on development experience with SQL, relational database design (ERDs)
Must be able to write SQL & contribute code
Be product-minded; focus not only on what needs to be built but why something needs to be built
You may be a great fit for this role if you have:
3 or more years of professional experience as a data analyst or data engineer
Cloud-Based Data Tools e.g. Hevo, BigQuery, Looker, etc.
Database technologies: SQL, RDBMS / Relational databases (Postgres, Amazon RDS / Aurora, etc.)
Nice to have experience: Python, Java, Typescript or another modern OOP language
Cloud infrastructure: AWS
Source control: Git
Working at Take Command
We’re excited to build a team and culture that reflects our values! We offer competitive pay, health benefits, and equity options to share with this position. We want you to be an owner along with us in revolutionizing healthcare.
A generously funded ICHRA for medical, dental, and vision premiums and medical expenses. You get to use our own product and we think that’s so exciting and rare!
Unlimited personal vacation in addition to regular company holidays.
401(k): 90-day eligibility for 2% match that vests immediately!
Access to LinkedIn Learning.
We have two beautiful offices conveniently located near The Hill (Dallas) and Holly Commons (Austin). The kitchen is well-stocked and we've designed the space to have lots of different areas to work!
Paid parental leave for new parents.
Flexible on where you work – we hope to see you around the office on average 3 days a week especially when white-boarding or building relationships with your colleagues, but you also have the ability to work from home or wherever you'd like when you need to get focus work done.
More About Us
We secured our Series A funding in 2021 and are thrilled to be able to expand our team. Despite being a small startup in a land of health insurance giants, we’re the recognized industry leader for what we do (health insurance reimbursements) and passionate about bringing it to market because we know we can help fix a broken system and improve our clients’ wellbeing and health outcomes.
We’ve been featured in The New York Times, The Wall Street Journal, The Dallas Morning News, and other national healthcare publications and are excited about our growth opportunities.
Take Command knows diversity and inclusion among our teammates is integral to our company’s success and growth. Our vision is to recruit, develop, and retain the best team from a diverse candidate pool.
This has mostly been about us, but we’d love to hear from you-we can’t wait to hear your story!
Take Command is an equal opportunity employer! We celebrate diversity and are committed to creating an inclusive environment for all employees.
Xrygn4NXMr","$125,000 /yr (est.)",1 to 50 Employees,Company - Private,Insurance,Insurance Agencies & Brokerages,2014,Unknown / Non-Applicable
"Integration Developer Network LLC
4.9",4.9,Remote,Data Engineer with PL/SQL Background,"Requirements:
Recent PL/SQL development work experience with Oracle 19.
Must have working experience with tools like Airflow, Git.
Must have python coding experience and REST API development experience.
Plus would be experience with microservices, docker, Kubernetes.
Plus would be experience with accounting and financial data
Experience in event driven and cloud based architectures and working knowledge of containerization and streaming technologies.
Detailed work experience on Oracle, SingleStore or IBM I DB2 database, SQL & PL/SQL, including ANSI SQL.
Experience in database analysis, development and maintenance of business applications specific to oil & gas back-office operations. Expertise in various ETL tools such as SQL*Loader, Equalum, Python scripts.
Familiarity with No-SQL databases and pipelines (Elasticsearch, MongoDB).
2+ years’ experience in the design, implementation and support of full life-cycle data engineering projects.
Bachelor's degree in Computer Science, Engineering or other equivalent degree, with 5+ years data engineering experience or Master's degree in Computer Science, Engineering or other equivalent degree with 2+ years data engineering preferred.
High school diploma or GED is required.
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Experience level:
9 years
Schedule:
8 hour shift
Experience:
Python: 8 years (Required)
PL/SQL: 5 years (Required)
ETL: 5 years (Required)
Work Location: Remote",$62.50 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2011,$1 to $5 million (USD)
Vibrant Planet,#N/A,California,Data Engineer,"About Vibrant Planet
We are a team of leaders in science, forestry, policy, and tech, building a cloud-based, data-driven platform to increase the pace and scale of forest restoration and reduce catastrophic wildfire, tree mortality, forest degradation, and deforestation. Our current software modernizes land management planning and monitoring with AI-driven data development, user friendly scenario building and decision support, and forest resilience trends and treatment outcome detection. The system quantifies potential and actual treatment benefits, helping to grow markets for ecosystem services (carbon, water, biodiversity, sustainable forest-derived products).
Our initial platform (launched in September 2021) is focused on temperate, “fire adapted” landscapes in California and the Western US. Our mission is global; our roadmap expands into other geographies and forest types accordingly.
Driven by our sense of urgency to protect vital forests and the services they provide, we use our expertise to build sophisticated, AI/ML-driven, user friendly products to democratize the use of data and accelerate the process of stabilizing and restoring our forests.
Vibrant Planet is backed by climate and ecosystem resilience solutions leaders, including Grantham Foundation, Earthshot, Elemental Excelerator, Ecosystem Integrity Fund, Chris Cox (CPO at FB), Neil Hunt (ex CPO of Netflix), Cisco, Valia Ventures, and Halogen Ventures.
For further information please visit: VibrantPlanet.net
Equal Opportunity Employer: Vibrant Planet is committed to diversity. We encourage applicants from all cultures, races, colors, religions, sexes, national or regional origins, ages, disability status, sexual orientation, gender identity, military, or other status protected by law to apply.
We are most interested in finding the best candidate for the job, and that candidate may come from a less traditional background, but have capacity to grow into and thrive in the position after some mentoring. We do not require that you have experience with every job description task. We will consider any equivalent combination of knowledge, skills, education, and experience to meet minimum qualifications. We encourage each candidate to think broadly about their unique background and skill set and how it may relate to the role. This is important to us. We aren’t just saying this, we mean it.
For further information please visit: https://vibrantplanet.net
About the Role
We are looking for a versatile, hands-on data focused engineer to help us scale and build our geospatial pipelines and tooling. You will be part of a small team of engineers and scientists, tackling some of the most important climate change related issues facing the world today. This is a remote role (and company) so being comfortable and effective working in a distributed team is crucial.
Key Responsibilities:
Design, develop, scale, and maintain data pipelines that ingest, transform, and store large volumes of geospatial data from multiple sources.
Optimize data processing and storage performance and cost efficiency by leveraging cloud-based technologies and services.
Collaborate with engineers, scientists and other stakeholders to share knowledge and build expertise.
Lead and participate in development life cycle activities like design, coding, testing, and production release.
Contribute to our evolving engineering culture, standards, tooling, and processes.
Mentor and support other engineers and deeply review code.
Technical Qualifications
Strong software engineering skill set.
2+ years experience in data engineering or a related field.
Experience with distributed data processing frameworks and tools (e.g., Airflow, Hadoop, Spark).
Proficiency with Python or an equivalent language. Ability to write clean, high-quality code and tests to keep our system fast, reliable, and monitorable.
Bonus Qualifications - Absolutely not required, but nice to have
Airflow
Pandas / Geopandas
Jupyter notebooks
Geospatial processing
Lidar data
Satellite imagery
Experience with ArcGIS/QGIS
Other Expectations:
Strong communication skills; discussing complex technical concepts to engineers and non-engineers is no problem to you.
Collaborative and supportive team player, with a desire to enrich our engineering culture.
Eagerness to learn, think creatively, and share knowledge with others.
Ability to write understandable, testable code with an eye towards maintainability.
Proactive and empathetic mindset - you love to roll up your sleeves to fix problems
Location - We are a remote first company with the majority of the team in the US west coast time zone. We also have a small and growing presence in New Zealand. Location can be flexible, but we are primarily targeting those two time zones and anything in between.",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Argo Data
2.7",2.7,"Richardson, TX",Senior Data Engineer,"ARGO is a leading provider of software and analytics solutions for both the financial services and healthcare industries. ARGO transforms business processes for financial service providers and healthcare organizations using proven business models and software innovation informed by real customer challenges, breakthrough technology, and rich analytics.
ARGO- Data Engineer II (Markham, ON; Memphis, TN; ...) (theapplicantmanager.com)
JOB SUMMARY
The Data Engineer II is responsible for designing and implementing data solutions to support business needs. The successful candidate will have a strong background in software engineering and database design, as well as experience with working with cloud-based tools and technologies.
EXPECTED WORK AND PERFORMANCE
This section is intended to provide a more detailed overview of what the expectations of the job are to include: work to be performed, expected performance results and how they will be measured, as well as connection to corporate strategy, values, and mission. Job duties and responsibilities include the following. Other duties may be assigned as needed.
Design, build, and maintain data pipelines for data extraction, transformation, and loading (ETL) from various sources.
Ensure data security and privacy protocols are implemented and enforced.
Monitor and optimize data storage and processing performance to ensure high performance and cost-effectiveness.
Collaborate with data scientists, analysts, and other stakeholders to understand their data needs and translate them into technical solutions.
Maintain documentation of data systems, processes, and policies.
Stay current with the latest data tools, technologies, and trends and recommend improvements.
Mentor and train junior data engineers as needed.
QUALIFICATIONS
Bachelor's degree in Computer Science, Software Engineering, or related field.
3-5 years of experience in software engineering and SQL database design.
Proficiency in programming languages such as Python.
Strong understanding of data warehousing, ETL processes, and data modeling.
Experience with cloud infrastructure technologies such as Microsoft Azure.
Excellent problem-solving and analytical skills.
Ability to work well in a team-oriented environment and communicate effectively with both technical and non-technical stakeholders.
PHYSICAL DEMANDS
The candidate will work at our corporate headquarters in Richardson, TX. While performing the duties of this job, the employee is regularly required to stand; walk; sit; use hands to finger, handle, or feel; reach with hands and arms; talk or hear. Specific vision abilities required by this job include close vision, distance vision, color vision, peripheral vision, depth perception, and ability to adjust focus.
ADDITIONAL REQUIREMENTS:
Applicants for U.S. or Canadian based positions with ARGO must be legally authorized to work in the United States or Canada. Verification of employment eligibility will be required at the time of hire. Visa sponsorship is not available at this time for ARGO positions.
Pre-employment background screening will be conducted.
Job Type: Full-time
Pay: $70,745.42 - $141,624.04 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Life insurance
Paid time off
Professional development assistance
Referral program
Relocation assistance
Retirement plan
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Performance bonus
Yearly bonus
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Richardson, TX 75081: Reliably commute or willing to relocate with an employer-provided relocation package (Required)
Experience:
Informatica: 3 years (Required)
SQL: 5 years (Required)
Data warehouse: 3 years (Required)
Python: 3 years (Required)
Azure: 3 years (Required)
Work Location: In person","$106,185 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Computer Hardware Development,1980,$25 to $100 million (USD)
"Seamless.AI
3.4",3.4,"Columbus, OH",Senior Data Engineer - Remote US,"Role: Senior Data Engineer
The Opportunity
We are seeking a Senior Data Engineer with a strong background in data ingestion and processing for large data sets. The ideal candidate is maniacal about data accuracy and has a proven track record of building robust and scalable data pipelines. This role requires an experienced professional who can work collaboratively with cross-functional teams to design and implement solutions that meet business requirements and adhere to data security and privacy policies.
If you are a senior data engineer with a passion for data accuracy and a proven track record of building robust and scalable data pipelines, we encourage you to apply for this role. We offer competitive compensation, a flexible work environment, and opportunities for growth and advancement within the organization.
About Seamless
Seamless delivers the world's best sales leads. Through our product, we help sales teams maximize revenue, increase sales, and easily acquire their total addressable market using artificial intelligence; by development of a robust real-time contact and company search engine as well as a suite of technically-advanced tools to support sales and lead generation. We have been recognized as one of Ohio's fastest growing companies and have been recently ranked No. 7 in LinkedIn's Top 50 Startups of 2022, featured in Forbes as #1 Software company in Ohio in 2022, and on G2's ""Top 100 Highest Satisfaction Products for 2022"" list!
The Seamless Team
We have an amazing culture and work environment that anyone would want to be a part of. We encourage a culture of positivity. We thrive off of continuous feedback and do whatever it takes to help our team and customers be successful. You will grow as an individual, professionally, and be able to see and feel the impact you are making to the growth of Seamless every day.
Role Responsibilities
Design and develop data ingestion and processing pipelines that ensure data accuracy and completeness.
Work closely with cross-functional teams to understand business requirements and design solutions that meet those requirements.
Lead efforts to identify and implement data quality control measures to ensure consistency and reliability of data.
Monitor data pipelines to identify issues and proactively resolve them.
Evaluate and recommend new technologies and techniques to improve data pipeline performance and scalability.
Ensure compliance with data security and privacy policies and procedures.
Mentor junior engineers and provide technical leadership in data engineering best practices.

Candidate Requirements
Bachelor's or Master's degree in Computer Science, Software Engineering, or a related field.
3+ years of experience in data engineering, with a focus on data ingestion and accuracy.
Strong experience with large-scale data processing frameworks such as Hadoop, Spark, or Flink.
Proficiency in programming languages such as Python, Java, or Scala.
Experience with SQL and NoSQL databases.
Familiarity with cloud-based data processing and storage platforms such as AWS, Google Cloud, or Azure.
Strong experience in designing and implementing data quality control measures.
Excellent problem-solving and analytical skills.
Strong communication and collaboration skills.
Experience mentoring and leading teams of data engineers.
Seamless.AI has been delivering the world's best sales leads since 2015. Our product is the first real time, B2B search engine helping sales teams maximize revenue, increase sales, and easily acquire their total addressable market using artificial intelligence. We have been recognized as one of Ohio's fastest growing companies and won 2020 Best Places to Work and LinkedIn's Top 50 Tech Startups in 2020 and 2022. We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. Visa Sponsorship is not included in our hiring package. Applicants will need to be authorized to work in the U.S.","$93,575 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable
"DSMH LLC
5.0",5.0,Remote,"Data Engineer (AWS, Python)","Title: Data Engineer
Duration: 12+ Months
Technical Skills
(Required)
Four years or more of experience in data engineering and/or software
development.
Three years or more of experience with development, operations, or
architecture in AWS using Python.
Experience with development and delivery of microservices using serverless
AWS services (S3, RDS, Aurora, DynamoDB, Lambda, SNS, SQS, Kinesis,
IAM)
Strong competency in testing, including unit testing to coverage standards and
integration testing.
(Desired)
Software development experience with object-oriented development and design
patterns
Experience with serverless design in AWS
AWS technical certifications (Developer Associate, Solution Architect Associate)
Familiarity with machine learning and data design to support machine learning
Experience with productionizing machine learning workloads
Experience with ElasticSearch/ELK, Kibana, Grafana, and/or Prometheus
Experience with OpenTelemetry
Soft Skills
(Required)
Verbal and written communication skills, problem solving skills, customer
service and interpersonal skills.
Basic ability to work independently and manage one’s time.
(Desired)
Ability to work collaboratively in a complex, rapidly changing, and culturally
diverse environment.
Ability to clearly communicate complex technical ideas.
Comfortable working in a dynamic environment where digital is still evolving as
a core offering.
Job Type: Contract
Pay: $50.00 - $55.00 per hour
Experience level:
6 years
Schedule:
Monday to Friday
Experience:
AWS: 5 years (Required)
Python: 5 years (Required)
Work Location: Remote",$52.50 /hr (est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,$1 to $5 million (USD)
"WEX Inc.
3.7",3.7,Remote,Data Engineer,"We are seeking a Data Engineer to play a critical role in the development of WEX's enterprise data & analytics capabilities. You will be part of an organization focusing on the development and delivery of data solutions and capabilities for WEX’s data platform. The successful candidate is motivated by thinking big data, technically proficient, and enjoys working in a fast paced environment.
The base pay range represents the anticipated low and high end of the pay range for this position. Actual pay rates will vary and will be based on various factors, such as your qualifications, skills, competencies, and proficiency for the role. Base pay is one component of WEX's total compensation package. Most sales positions are eligible for commission under the terms of an applicable plan. Non-sales roles are typically eligible for a quarterly or annual bonus based on their role and applicable plan. WEX's comprehensive and market competitive benefits are designed to support your personal and professional well-being. Benefits include health, dental and vision insurances, retirement savings plan, paid time off, health savings account, flexible spending accounts, life insurance, disability insurance, tuition reimbursement, and more. For more information, check out the ""About Us"" section.
Salary Pay Range: $80,500.00 - $108,000.00","$94,250 /yr (est.)",1001 to 5000 Employees,Company - Public,Financial Services,Investment & Asset Management,1983,$1 to $5 billion (USD)
"Titan America
3.8",3.8,"Roanoke, VA",Data Engineer,"Data Engineer
We are seeking a motivated and detail-oriented Data Engineer to join our team based in Roanoke, Virginia. As a Data Engineer, you will play a crucial role in designing, developing and maintaining scalable data pipelines and transformations for business-critical use cases. In addition, you may take on various Software Development and Data Science responsibilities as we frequently share responsibilities and knowledge across our growing team.
Responsibilities:
Design, develop and maintain scalable data pipelines for extracting, transforming, and loading data from various sources
Collaborate with cross-functional teams to identify data needs and determine the best data solutions
Develop and implement data models to support business requirements and ensure data quality
Ensure the security and privacy of sensitive data by implementing appropriate access controls
Monitor and optimize data pipeline performance to ensure timely and accurate data delivery
Document data pipeline processes, data dictionaries, and data storage solutions
Requirements:
Bachelor's degree in Computer Science, Computer Engineering, or a related technical field
Minimum of five years of professional experience working as a Data Engineer or Software Developer
Strong hands-on experience with data warehouse and transformation solutions, i.e. Domo, Snowflake or similar
Proficient in at least one scripting language such as Python, JavaScript, or R
Understanding of data modeling, data integration and data quality processes
Familiarity with cloud platforms such as AWS, Azure, or Google Cloud Platform
Strong analytical and problem solving skills
Full Stack Software Development experience in a professional setting is highly desired, but not required
This is an excellent opportunity for a driven and collaborative individual to make a significant impact in a dynamic and growing team. If you have a passion for data and a desire to work in a fast-paced and dynamic environment, we want to hear from you!
Success begins with hiring the right people to partner with us as we grow and develop our businesses. People are central to everything we do. It is through their efforts and talents that Titan has been successful for over 100 years.
Titan America is an Equal Employment Opportunity (EEO) / Affirmative Action employer and welcomes all qualified applicants. Applicants will receive fair and impartial consideration without regard to race, sex, color, religion, national origin, age, disability, veteran status, genetic data, or other legally protected status.
Job Type: Full-time
Pay: $75,000.00 - $85,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Referral program
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Roanoke, VA 24019: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Do you currently or will you in the future need sponsorship to work in the United States?
This position requires the successful candidate to be in the office in Roanoke, VA at least 3 days a week. Can you meet this requirement?
This position requires the successful candidates to be a full time, W2 employee. Can you meet this requirement?
Education:
Bachelor's (Preferred)
Experience:
Data Engineering: 5 years (Preferred)
Work Location: Hybrid remote in Roanoke, VA 24019","$80,000 /yr (est.)",1001 to 5000 Employees,Company - Public,"Energy, Mining & Utilities",Mining & Metals,1902,Unknown / Non-Applicable
Atika Technologies,#N/A,"Pittsburgh, PA",Data Engineer,"Role: Data Engineer/PySpark Developer, Pittsburgh, PA - Day One Onsite
Fulltime and Contract(W2)
LinkedIn Must
USC/GC/EADs
Three round of interview: First two on Zoom Video Call, End Client Round will be Onsite on the Client's Location (Pittsburgh, PA)
Job Description:
Python(PySpark), Jupiter hub, airflow, Java
Database knowledge - Oracle / SQL / Data Analytics
Regulatory reporting knowledge preferred
Good communication skills
Open / Ability to learn new technologies
Very good Analytical Skills
Experience in ETL is preferable
Job Types: Full-time, Contract
Salary: $120,000.00 - $130,000.00 per year
Benefits:
Dental insurance
Health insurance
Life insurance
Compensation package:
Bonus pay
Performance bonus
Yearly pay
Experience level:
10 years
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Pittsburgh, PA 15259: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Required)
Spark: 4 years (Required)
Regulatory reporting: 3 years (Required)
SQL: 4 years (Required)
IT: 10 years (Required)
Airflow: 4 years (Required)
Jupiter: 4 years (Required)
Work Location: One location","$125,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"MyCare Medical Group
4.1",4.1,"Lutz, FL",Data Engineer,"Job Summary
The Data Engineer will design, develop, optimize, and maintain data architecture and pipelines that adhere to ELT principles and business goals.
Job Responsibilities:
Designs, develops, optimizes, and maintains data architecture and pipelines that adhere to ELT principles and business goals.
Solves complex data problems to deliver insights that helps business achieve its goals.
Creates data products for engineer, analyst, and data scientist team members to accelerate their productivity.
Engineers effective features for modelling in close collaboration with business analysts and stakeholders.
Leads the evaluation, implementation and deployment of emerging tools and process for analytics data engineering in order to improve productivity as a team and quality.
Partners with engineers, BI, and solutions architects to develop technical architectures for strategic enterprise projects and initiatives.
Fosters a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions.
Advises, consults, mentors, and coaches other data and analytic professionals on data standards and practices.
Develops and delivers communication and education plans on analytic data engineering capabilities, standards, and processes.
Job Qualifications:
Education and Work Experience: Bachelor’s degree in computer science, information systems, engineering, or a related field; Masters preferred.
3+ years of experience in data engineering role.
Expertise in ELT and data analysis, SQL and at least one programming language (e.g., SQL, Python).
Conceptual knowledge of data and analytics, such as dimensional modelling, reporting tools, data governance, and structured and unstructured data.
Skills: Strong understanding of agile methodologies and experience as a Data Engineer on a cross-functional agile team preferred.
Experience with designing and maintaining data warehouses and/or data lakes with technologies, such as Hadoop, Spark, or distributed databases, like Redshift and Snowflake, and experience with housing, accessing, and transforming data in a variety of relational databases.
Experience with cloud database technologies (e.g., Azure) and developing solutions on cloud computing services and infrastructure in the data and analytics space.
Experience in building data pipelines and deploying/maintaining them with best-in-class tools (e.g., Git and Jenkins).
Familiarity with the Linux operating system.
Basic familiarity with BI tools (e.g., Tableau, Power BI, Looker).
Experience working in data engineering or data architect role.
Candidate Qualities: Focuses on business agility and agile delivery with a “fail-fast, succeed early” mindset and emphasis on measurable outcomes.
Brings a high-energy and passionate outlook to the job and can influence those around them.
Able to build a sense of trust and rapport that creates a comfortable & effective workplace.
Passion for innovation and “can do” attitude.
BENEFITS
Comprehensive benefits package, including Health, Vision, Dental, and Life insurancesFSA and Life Assistance Program (EAP)
401(k) Retirement Plan
Health Advocacy, Travel Assistance, and My Secure Advantage
PTO Accrual and Holidays
#ZIP
Job Type: Full-time
Pay: $125,000.00 - $150,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Employee discount
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Referral program
Relocation assistance
Retirement plan
Tuition reimbursement
Vision insurance
Compensation package:
Performance bonus
Yearly bonus
Experience level:
3 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Lutz, FL 33558: Reliably commute or planning to relocate before starting work (Preferred)
Experience:
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person","$137,500 /yr (est.)",501 to 1000 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2018,Unknown / Non-Applicable
WellTrust Medical Group,#N/A,"Charlotte, NC",Data Engineer,"The Data Engineer will assist the senior analyst in complex modeling and projects to ensure their completion within the allotted timeline. Helps track measures of success and develop reports that measure operational and customer metrics. Demonstrates ability to troubleshoot data consumption and data quality problems.

Responsibilities:
Build and maintain reports using My SQL Server
Identify optimal data sources, structure data validation, and cache optimization.
Performs business problem modeling and quantitative analytics including regression analysis, time-series, hierarchical cluster analysis, and data mining.
Helps track measures of success and develop reports that measure operational and customer metrics.
Demonstrates ability to troubleshoot data consumption and data quality problems.
Evaluates and recommend improvements to current models, metrics, and analytic processes to improve operational efficiency and insight.
Design, document, and communicate effective technical solutions meeting end-user requirements
Perform analyses with large amounts of complex data, and uses data visualization tools to communicate results.
Requirements:

Bachelor's degree or equivalent experience in a quantitative field (Mathematics, Statistics, Computer Science, Economics, Finance, Physics, Engineering)
3+ years of professional working experience as a Data Engineer or Analyst
1-3 years of data warehouse experience
Build and maintain reports using My SQL Server
Strong analytical and creative problem-solving skills
Design, document, and communicate effective technical solutions meeting end-user
SQL and Excel Macro development
Strong understanding of various data environments such as SQL, Oracle, Big Query, etc.
Experience with data visualization tools such as Looker Studio, Tableau, D3.js, etc.
Proficient in R, Python, MATLAB, Java, or another programming language.",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Take Command Health
4.9",4.9,"Dallas, TX",Data Engineer,"Company: Take Command
Position: Data Engineer
Salary: $120,000-$130,000 plus bonus
Location: Dallas or Austin, TX
About Take Command
Take Command is a start-up on a mission to improve the healthcare system, starting with health insurance. Pragmatically speaking, we help employers reimburse employees for individual insurance instead of offering a traditional one-size-fits-all group plan. We believe this model can empower employees (when they have the right support) to be savvy healthcare consumers and have a transformative impact on the entire healthcare system.
Let’s be honest—health insurance is usually a confusing, frustrating, and even emotional experience for people. We want to fix that with a new model, great technology, and a superior user experience. We have made a great start, but we need your help to fully realize our long-term vision.
As a part of the dev team, you'll build the framework for both our clients and our team to succeed. You will be a pillar in our company as we grow and make our product the best product on the market!
In this role, you will:
Take ownership for delivering multiple features, their quality and assist team members in delivering their features
Recommend improvements to data quality, processes and code to make it easier to use, manage and maintain
Experience designing and developing data models and data migrations
Experience managing and maintaining ETL or ELT processes (Hevo, BigQuery, Looker, etc.)
Hands-on development experience with SQL, relational database design (ERDs)
Must be able to write SQL & contribute code
Be product-minded; focus not only on what needs to be built but why something needs to be built
You may be a great fit for this role if you have:
3 or more years of professional experience as a data analyst or data engineer
Cloud-Based Data Tools e.g. Hevo, BigQuery, Looker, etc.
Database technologies: SQL, RDBMS / Relational databases (Postgres, Amazon RDS / Aurora, etc.)
Nice to have experience: Python, Java, Typescript or another modern OOP language
Cloud infrastructure: AWS
Source control: Git
Working at Take Command
We’re excited to build a team and culture that reflects our values! We offer competitive pay, health benefits, and equity options to share with this position. We want you to be an owner along with us in revolutionizing healthcare.
A generously funded ICHRA for medical, dental, and vision premiums and medical expenses. You get to use our own product and we think that’s so exciting and rare!
Unlimited personal vacation in addition to regular company holidays.
401(k): 90-day eligibility for 2% match that vests immediately!
Access to LinkedIn Learning.
We have two beautiful offices conveniently located near The Hill (Dallas) and Holly Commons (Austin). The kitchen is well-stocked and we've designed the space to have lots of different areas to work!
Paid parental leave for new parents.
Flexible on where you work – we hope to see you around the office on average 3 days a week especially when white-boarding or building relationships with your colleagues, but you also have the ability to work from home or wherever you'd like when you need to get focus work done.
More About Us
We secured our Series A funding in 2021 and are thrilled to be able to expand our team. Despite being a small startup in a land of health insurance giants, we’re the recognized industry leader for what we do (health insurance reimbursements) and passionate about bringing it to market because we know we can help fix a broken system and improve our clients’ wellbeing and health outcomes.
We’ve been featured in The New York Times, The Wall Street Journal, The Dallas Morning News, and other national healthcare publications and are excited about our growth opportunities.
Take Command knows diversity and inclusion among our teammates is integral to our company’s success and growth. Our vision is to recruit, develop, and retain the best team from a diverse candidate pool.
This has mostly been about us, but we’d love to hear from you-we can’t wait to hear your story!
Take Command is an equal opportunity employer! We celebrate diversity and are committed to creating an inclusive environment for all employees.
Xrygn4NXMr","$125,000 /yr (est.)",1 to 50 Employees,Company - Private,Insurance,Insurance Agencies & Brokerages,2014,Unknown / Non-Applicable
"Loyal
3.9",3.9,"Atlanta, GA",Senior Cloud Data Operations Engineer,"Loyal is an organization centered on experience and building a platform that allows consumers to make meaningful decisions when it comes to healthcare. We deeply understand providers, locations, services, appointments, business rules, and moreover, we understand patients - who they are, the preferred method of communication, upcoming appointments, lapsed appointments, outstanding bills, health risks, and more. With this intelligence, our platform fuels highly relevant and personalized experiences across all mediums (website, email, voice…) allowing patients to get healthy, stay healthy, and have a better relationship with the health care provider.
**This is a remote role**
Summary
The Senior Cloud Data Operations Engineer is responsible for delivering Loyal's cloud-based data warehouse. They interface with groups across the organization to ensure we are collecting and storing data in a way that makes it easy to create and consume. They identify best practices for big data management, select the right tooling, create a workable data architecture, and drive an implementation roadmap others can depend on. The Senior Cloud Data Operations Engineer also works closely with our security group to ensure data security and privacy. They also mentor others on the team, providing technical leadership to those they work with.
You will
Create data streams and consolidate data sources into a single cloud-based data warehouse
Architect and catalog data warehouse storage schema
Work with product teams to enable easy data aggregation
Work with data consumers (analysts, data scientists, and AI/ML engineers) to ensure data is easily accessible and consumable
Create and execute a roadmap that meets data storage and availability goals
Ensure data is secure through data analysis, access control, and encryption
You have
Bachelor's degree in computer science, information systems, applied math or related field, or equivalent work experience
Minimum 5 years of experience working with cloud data processing services (AWS, Azure, or GCP).
Minimum 3 year experience with Big Data technologies, such as Databricks, Snowflake, Spark, Hadoop, etc.
Strong experience in creating data architecture and storage schemas required
Significant working knowledge of SQL and other query languages and tools
Significant experience with building relational database systems strongly preferred
Experience with ETL tools a plus
Knowledge of Python preferred
Software development, data analytics, or data science experience is a plus.
Experience working within a software as a service (SaaS) company preferred.
Experience working within a high growth and/or ambiguous environment, with proven experience to be dynamic preferred.
#LI-REMOTE
We know that potential candidates are often less likely to apply to a position if they don't match 100% of the job qualifications. Don't let that be why you miss out on this opportunity! We encourage you to apply if you can demonstrate many of these skills and competencies.
Loyal to our employees
We are a remote-friendly company! We encourage you to apply from anywhere in the United States. We also believe in a work/life balance that fulfills you while you're here and supports you when you're not. We built our benefits package to prove that we're committed to you having everything you need (including a little fun). Here is what we offer full-time employees:
Flexible paid time off, sick and personal days
At least one holiday per month (sometimes, more!)
Full health, dental, and vision insurance - Loyal pays the premium for all employees!
One Time Home Office Setup Stipend For Remote & Hybrid Roles
Monthly Internet Stipend for Remote & Hybrid Roles
Long term & short term disability
401[k] plan
16 Weeks Paid Parental Leave
2 Volunteer days per year
Matching Gift Program
Participation Grant Program
Annual Travel/Team Events up to twice per year (post-COVID)
Our Commitment
We believe that the key to Loyal's success is you. Your unique background, life experience, knowledge, self-expression, and talent make you uniquely you. Who you are, what you have experienced, and how you think inspires us to be innovative and bold.
Loyal is an equal opportunity employer. We hire great people from a wide variety of backgrounds, not just because it's the right thing to do, but because it makes our company stronger. We welcome the unique contributions that you can bring in terms of your education, opinions, culture, ethnicity, race, ancestry, sex, gender identity and expression, national origin, citizenship, marital status, age, languages spoken, veteran status, color, religion, disability, sexual orientation, and beliefs.
We consider qualified applicants regardless of criminal histories, consistent with legal requirements.
Further, consistent with applicable federal and state law, Loyal provides reasonable accommodations when requested by qualified applicants or employees with disabilities, unless doing so would cause an undue hardship. Loyal's policy regarding requests for reasonable accommodation applies to all aspects of employment, including the application process. If you require a reasonable accommodation in connection with the application process, please contact the Talent Acquisition Department at talentacquisition@loyalhealth.com.
E-Verify
This employer participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S. If E-Verify cannot confirm that you are authorized to work, this employer is required to give you written instructions and an opportunity to contact Department of Homeland Security (DHS) or Social Security Administration (SSA) so you can begin to resolve the issue before the employer can take any action against you, including terminating your employment. Employers can only use E-Verify once you have accepted a job offer and completed the I-9 Form.
COVID-19 Vaccinations
Consistent with Loyal policy, candidates performing in-person work will be required to show proof of being fully vaccinated against COVID-19 upon commencing employment. Reasonable accommodations will be considered on a case-by-case basis for exemptions to this requirement in accordance with applicable law. If you require a reasonable accommodation to Loyal's vaccination requirement, please contact the People Department at people@loyalhealth.com.",#N/A,51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2015,$5 to $25 million (USD)
"Tipico - North America
4.0",4.0,"Denver, CO",Data Engineer,"Company Description

Tipico is an energetic, passionate, bold tech company, and we love sports! Tipico has recently expanded to the US market, establishing offices in Hoboken, New Jersey and Denver, Colorado. We are driven by our core values: passion, progress and trust. Our goal is to build the best possible mobile sports betting product in the industry, in order to amplify the emotions of millions of sports fans every day. Let’s Make This Interesting ™
Please note: this role is located in our Denver, CO office; we work off a hybrid model and come into the office 2 days per week.

Job Description

The Data Engineer will report directly to the Data Engineering Manager and work together with rest of the data engineers, BI analysts, and product teams. The position holder ensures this is done by doing the following:
Create and maintain both batch (ETL/ELT) and real-time data pipelines and architecture
Ensure all data provided by Data Platform is of the highest quality and is delivered in a timely manner and in line with agreed SLAs
Assemble large and complex data sets that meet functional and non-functional business requirements
Identify, design, and implement internal process improvements, including automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies
Collaborate with stakeholders including data architects, Executive, Product, Data and IT team members from the beginning to the delivery of a project
Build a scalable, modern, and trusted data ecosystem, making sure it is responsive, available and in line with business requirements
Take part in sprints, workloads, and deliverables through agile methodologies
Manage quality assurance, verification of accuracy and consistency of data, specifically KPI measures sent to the business
Thrive working in a fast-paced and challenging environment
Open to continuous learning individually and with the team
Comfortable with being challenged / seeking challenge
Willing to work flexible hours if needed

Qualifications
3+ years in Data Platforms / Datawarehouse / BI experience
Must have experience in successfully implementing data-centric applications, such as data warehouses, operational data stores, and data integration projects
Strong analytical and problem-solving skills that balance with creative approaches
Ability to learn new and complex concepts quickly
Strong experience with large-scale production databases and SQL
Solid understanding of cloud data services: AWS services such as S3, Athena, EC2, RedShift, EMR, EKS, RDS and Lambda
Proficient with one or more big data & new data technologies such as DBT, NIFI, Airflow, Kafka
Experience with data modelling
Experience with ETL development (extractions, data load, aggregation, transformation)
Knowledge in working with time-series/analytics databases such as Elasticsearch
Understanding of containerization and orchestration technologies like Docker/Kubernetes
Must be able to take ownership of projects with a high level of attention to detail and priority
Team player with a hands-on approach and attitude
Must be comfortable with on call rotation
Nice to have:
Familiarity with Marketing tooling and/or CRM systems is preferred
Familiarity with using business intelligence tools such as Domo, Tableau or Looker
Knowledge about and / or experience with sports betting
Background in digital companies, online, gambling, gaming, e-commerce, marketing, or mobile apps
Familiarity with one scripting language, preferably Python

Additional Information

What's in it for you:
Work in an environment where you, your work and ideas matter and have an impact.
You will be a be part of the newest and up and coming USA online iGaming and sports betting companies in the market.
Work in a new and young business with high growth potential.
Build your own success story together with us.
Work with self-organized, self-responsible and entrepreneurial employees.
Start-up feeling, backed-up by a leading European sports betting house. We are a high-volume business and are taking off in the US!
Competitive salary, Medical, Vision, Dental Benefits, Unlimited PTO, and more
Salary for this position is $120k+ & annual performance bonus","$120,000 /yr (est.)",201 to 500 Employees,Company - Private,"Arts, Entertainment & Recreation",Gambling,2004,Unknown / Non-Applicable
"Barracuda Networks Inc.
3.6",3.6,"Alpharetta, GA",Data Engineer,"Job ID: 23-579

Come Join Our Passionate Team! At Barracuda, we make the world a safer place. We believe every business deserves access to cloud-enabled, enterprise-grade security solutions that are easy to buy, deploy, and use. We protect email, networks, data and applications with innovative solutions that grow and adapt with our customers’ journey. More than 200,000 organizations worldwide trust Barracuda to protect them — in ways they may not even know they are at risk — so they can focus on taking their business to the next level.
We know a diverse workforce adds to our collective value and strength as an organization. Barracuda Networks is proud to be an Equal Opportunity Employer, committed to equal employment opportunity and equitable compensation regardless of race, gender, religion, sex, sexual orientation, national origin, or disability.
Envision yourself at Barracuda
We currently have an exciting new position open for a Data Engineer. This person will assist in driving technology innovation for engineering projects and work on architecture of AWS, Elastic and Kubernetes. They will also work closely with the Technology Architect to create and drive the Architecture Review Board.
What you’ll be working on
Determine internal / external customer data needs and identify system specifications
Analyze the needs of large systems and breaking them down into smaller manageable parts
Responsible for designing the journey of data from various sources to the database and everything in between
Plan and design the structure of data, brainstorm with team
Analyze best technology products and systems for internal teams
Help provide build vs buy decisions on data architecture
Communicate data requirements to engineers; explain data architecture to them and provide assistance throughout SDLC
Choose / maintain suitable software, hardware and integration methods
Help resolve technical problems as and when they arise
Review testing procedures to ensure data performant and error free
Ensure that database satisfy quality standards and procedures. Setup such standards.
Work with senior IT personnel to devise plans for future IT requirements of the organization
Produce progress reports on key infrastructures - Elastic and Data Pipelines
What you bring to the role
Bachelor Degree or Post Graduate in Computer Science, or relevant work experience
5+ years of experience in an Engineering / Operations heavy organization
1+ years of experience as a Data Analysis and Data Design responsible for data architecture of a SaaS application
Experience in understanding and designing data sharding in a distributed database
Some Hands-on experience in software development and system administration
Knowledge of strategic IT solutions
Familiarity with programming languages like JavaScript or Java or Python
Familiarity with Linux OS and Windows alike
Experience in cloud computing and cloud Technologies (AWS, GCP, Azure)
Excellent communication skills – you should be adept at listening to, understanding, and explaining key concepts to managerial and technical resources.
Good Anticipation and problem-solving skills
Ability to prioritize and manage time – responsible for multiple streams of work at the same time
Relationship building - Ability to form a good rapport with internal and external stakeholders
In-depth knowledge about quality standards and security best practices

Preferred Skills:
Expert level Understanding of Elastic eco system - Elasticsearch, Logstash, Kibana, Filebeats
Understanding of secure tunnel communication using VPN
Experience designing data at scale
AWS experience is preferred in Cloud Technologies
What you’ll get from us
A team where you can voice your opinion, make an impact, and where you and your experience are valued. Internal mobility – there are opportunities for cross training and the ability to attain your next career step within Barracuda.
Equity, in the form of non-qualified options
High-quality health benefits
Retirement Plan with employer match
Career-growth opportunities
Flexible Time Off and Paid Time Off benefits
Volunteer opportunities
#LI-Remote","$98,161 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,$100 to $500 million (USD)
"shaped.ai Inc.
4.0",4.0,"New York, NY",Data Engineer,"Hello! We’re building the future of content discovery infrastructure with AI — come join us!

Shaped (YC W22) is an API for developers to seamlessly add personalized ranking and recommendation to their products. These frictionless discovery experiences help end-users find what they want faster, and, consequently, grow conversion and engagement business metrics. Behind the scenes, we’re building state-of-the-art ML infrastructure for training and deploying large AI models to power our personalization engine.
We’re a seed stage start-up backed by top-tier investors (e.g. Y-Combinator, Susa Ventures, Tribe Capital etc…) and executives (e.g. Google, Amazon, Uber, Dropbox etc…). Our team comes from Meta, Google, Apple and Uber. We’re a remote team but have a small office in Brooklyn, New York.
We’ve been lucky to have a huge amount of success since our initial launch earlier this year and now looking to expand our real-time data engineering capabilities. We are looking for a data engineer to design, build and optimize Shaped's real-time streaming data ingestion endpoints. You will be a founding engineer working on state-of-the-art infrastructure. As one of Shaped’s early employees you will help shape our product, culture and vision.
This role is for a full-time data engineer at $120k - $200k a year and 0.1 - 0.5% equity.

Responsibilities:
Manage the unification of our batch and real-time data processing pipelines.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Requirements:
Bachelor's in computer science, data science or mathematics related field. Master's degree or PhD will be advantageous.
5+ years of data engineering experience, ideally applied to machine learning use-cases.
Experience with AWS or common cloud technologies
Proficient with Python, SQL.
Extensive knowledge of both offline and online large scale data processing frameworks: Spark, Beam, Flink.
Extensive knowledge with stream technologies like Kakfa or Kinesis.
Excellent written and verbal communication skills.
We’re excited to work with you. Come build the future of AI and discovery with us!","$160,000 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Intersec
4.5",4.5,Remote,Senior Data Engineer,"Architect and implement highly scalable data pipeline to streamline migration of data from source data repositories to the centralized data platform
Implement agreed upon data models (e.g., data marts) to enable faster and efficient consumption of data by downstream consumers
Conduct performance tuning to optimize data migration techniques and ensure adherence to identified service-level agreements (SLAs)
Define and implement automated validation checks to ensure data integrity between source and target
Assist in onboarding consumers with existing technology to connect to the platform
Setup predictive alert mechanism for pipeline degradation for IMF pipelines.
Enhance data quality framework for data pipelines and derived datasets
Enhance Data Audit Balance and Control mechanism for data pipelines
Enhance pipelines to be certified copies of authoritative source
Populate metadata for the Enterprise with particular focus on the Data Sources being onboarded to EDP (Technical and Business Glossaries) related to IMF Modernization Programs
Validate correct lineage for datasets
Required:
*8+ years of Databricks, Data Modeling, and/or Metadata management experience
* Familiarity with architecting and developing real and near-real-time solutions
* Bachelor's Degree in Engineering, Mathematics, computer Science, Statistics or other quantitative discipline, or 4 years equivalent professional experience
* US Citizenship and ability to gain a public trust
Preferred:
* Familiarity with IRS systems and ETL pipelines
* Ability to work independently and manage multiple tasks
* IRS MBI Clearance with IRS Laptop and Informatica Installed
Job Type: Full-time
Salary: $70.00 - $75.00 per hour
Schedule:
Monday to Friday
Application Question(s):
Are you a US/GC card holder?
How many years of Databricks, Data Modeling, and/or Metadata management experience do you have?
Work Location: Remote",$72.50 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,Unknown / Non-Applicable
Redzara.us,#N/A,"Atlanta, GA",Azure Data Engineer,"Responsible for developing new and modernizing existing applications to Azure Cloud Platform.
The ideal candidate must demonstrate in-depth knowledge and understanding of Database concepts.
Requirements:
7+ years of experience in Data Warehousing and Business Intelligence
5+ Years in BI development leveraging the Microsoft SQL Server. Needs to be an expert in T-SQL
5+ Years in SSIS and use of Stored Procedures.
1-2 Years in Azure Data Factory, SQL VMs, build and maintain ETL
Ingest and aggregate data from both internal and external data sources to build our data lake
Strong understanding data modeling (i.e., conceptual, logical, and physical model design, experience with Operation Data Stores, Enterprise Data warehouses and Data Marts).
Job Type: Contract
Pay: $43.49 - $57.00 per hour
Compensation package:
1099 contract
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Atlanta, GA 30334: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure Data Factory: 3 years (Required)
Azure: 3 years (Required)
Data warehouse: 1 year (Preferred)
Work Location: Hybrid remote in Atlanta, GA 30334",$50.25 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Apolis
3.9",3.9,"Saint Louis, MO",Data Engineer (Python Focused),"Role: Data Engineer (Python Focused) (W2 ONLY)
Location : St. Louis, MO 63102
Duration: 18 Months
The primary responsibilities of this role will be to work as a part of the data and analytics team of the client. The ideal candidate would have experience using ETL and other data engineering tools in a cloud environment. Additionally, this candidate would have coding and scripting experience creating code to move and manipulate data using tools such in Python such as Spark or Pandas. Microsoft Azure experience is a plus but relevant experience with other cloud-based environments will be considered.
This team uses Microsoft Azure to build and productionize data sets and processes to the product owners and product users and this role will be expected to contribute to those objectives. As a member of this team, you will work in conjunction with other data engineers, platform engineers, solution developers and others to execute and deliver working solutions to the end users.
Technical Skills
High focus on Python
More Programmer/Coder experience, less Cloud experience needed.
Must have experience with data analytics/engineering and cloud technologies and an in-depth working knowledge and understanding of data engineering, design, and integration.
Intermediate or higher level of proficiency with Python Must have the proven ability to learn quickly and apply cloud data engineering principles and skills to new tools and use cases.
Ability to define and take technical ownership of detailed requirements and collaborate effectively with various team members such as Business Analysts and Project Managers and end users.
Excellent communication (verbal and written) and problem-solving skills are essential for this position.
Initiative, Proactive Cooperation, and curiosity are mandatory behavioral skills required for this position.
Experience with several of the following technologies is preferred:
Experience with Microsoft Azure Platform. In particular, the data processing toolsets including Azure Data Factory Pipelines, Azure Databricks, Azure Data Lake Storage.
Experience using Notebooks (Such as Jupyter, DataBricks) also desired.
Experience using code languages such as Python and C#
Experience using query languages such as SQL, MDX and DAX
Experience using scripting languages such as PowerShell, M-Query (Power Query), and Windows batch commands.
Understanding of Agile and DevOps principles for software development and ownership.
Familiar with compliance and controls for software development and deployment.
Education Required
A bachelor's in computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field [or equivalent work experience] is not required but is preferred.
Job Type: Contract
Salary: $60.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Vision insurance
Compensation package:
Overtime pay
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
St. Louis, MO: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Preferred)
ETL: 5 years (Preferred)
Cloud infrastructure: 4 years (Preferred)
Work Location: One location",$65.00 /hr (est.),501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,1996,$25 to $100 million (USD)
Diamondpick,#N/A,"Alpharetta, GA",Java Software Engineer with Big-Data,"About us
There is someone out there who fits your requirements like a glove! We are an innovative Talent Solutions company with a vision to build talent supply chain models without compromise for the technology industry.
We are a new-age talent solutions company with a vision to build an enterprise recruitment model without compromise. We love to solve complex talent challenges and strive to be an enabler of business success for our clients using data, technology and our team’s vast experience in the technology market.
Senior Java Developer with Big data
Alpharetta, GA (Hybrid Work from Day1)
Job Description:
The ideal candidate must possess strong background on frontend and backend development technologies.
The candidate must possess excellent written and verbal communication skills with the ability to collaborate effectively with domain experts and technical experts in the team.
Responsibilities:
As a Java Senior Developer, you will
Maintain active relationships with Product Owner to understand business requirements, lead requirement gathering meetings and review designs with the product owner
Own his backlog items and coordinate with other team members to develop the features planned for each sprint
Perform technical design reviews and code reviews
Be Responsible for prototyping, developing, and troubleshooting software in the user interface or service layers
Perform peer reviews on source code to ensure reuse, scalability and the use of best practices
Participate in collaborative technical discussions that focus on software user experience, design, architecture, and development
Perform demonstrations for client stakeholders on project features and sub features, which utilizes the latest Front end and Backend development technologies
Requirements:
6+ years of experience in Java/JEE development
Skills in developing applications using multi-tier architecture
Knowledge of google/AWS cloud
Java/JEE, Spring, Spring boot, REST/SOAP web services, Hibernate, SQL, Tomcat, Application servers (WebSphere), SONAR, Agile, AJAX, Jenkins..etc
Skills in UML, application designing/architecture, Design Patterns..etc
Skills in Unit testing application using Junit or similar technologies
Good communication skills
Leadership skills
Provide overlap coverage with onsite/customer teams
Capability to support QA teams with test plans, root cause analysis and defect fixing
Strong experience in Responsive design, cross browser web applications
Strong knowledge of web service models
Strong knowledge in creating and working with APIs
Experience with Cloud services, specifically on Google cloud
Strong exposure in Agile, Scaled Agile based development models
Familiar with Interfaces such as REST web services, swagger profiles, JSON payloads.
Familiar with tools/utilities such as Bitbucket / Jira / Confluence
Job Types: Full-time, Contract, Temporary
Pay: From $60.00 per hour
Benefits:
Flexible schedule
Compensation package:
Hourly pay
Experience level:
8 years
Schedule:
8 hour shift
Day shift
Work Location: Hybrid remote in Alpharetta, GA 30005",$60.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"DataArt
4.6",4.6,Remote,Data Engineer,"CLIENT
Our client is a pioneer in US schools’ education since 2000, it is leading the way in next-generation curriculum and formative assessment. They develop a number of solutions and interactive web products for teachers, students and their parents.

The company is technology-driven with a huge number of software engineers involved in product development. They have a very solid approach to technology, employ best practices and processes, with a focus on cutting-edge frameworks, languages and tools.

We invite to the company, not a project
TEAM
There are multiple autonomous and empowered teams who work with their own part of the product and choose their own way of doing things and they expect people to be proactive and good team players.
Responsibilities
Building well-tested and optimized ETL data pipelines for both full and delta extraction
Collaborating with data analysts and learning scientists to store, aggregate, and calculate captured students’ work
Contributing to leading industry data standards, such as Caliper Analytics or xAPI
Improving our deployment and testing automation data pipelines
Requirements
3+ years of professional software development or data engineering experience
Strong CS and data engineering fundamentals
Proven fluency in SQL and a development language such as Python
Experience with the following storages: Snowflake, AWS Storage Services (S3, RDS, Glacier, DynamoDB)
Experience with ETL/BI: Airflow, DBT, Matillion, Looker
Experience with Cloud Infrastructure: AWS Kinesis, Lambda, API Gateway, Terraform
Understanding of ETL/ELT pipelines and Data Warehousing design, tooling, and support
Understanding of different data formatting (JSON, CSV, XML) and data storage techniques (3NF, EAV Model, Star Schema, Data Lake)
BS in Computer Science, Data Science, or equivalent
Experience in education or ed-tech
Proven passion and talent for teaching fellow engineers and non-engineers
Proven passion for building and learning: open source contributions, pet projects, self-education, Stack Overflow
Strong communication skills in writing, conversation",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,1997,$100 to $500 million (USD)
"Contact Government Services, LLC
4.7",4.7,"Washington, DC",Data Engineer,"CGS is seeking a passionate and driven Data Engineer to support a rapidly growing Data Analytics and Business Intelligence platform focused on providing solutions that empower our Federal customers with the tools and capabilities needed to turn data into actionable insights. The ideal candidate is a critical thinker and perpetual learner; excited to gain exposure and build skillsets across a range of technologies while solving some of our clients’ toughest challenges. As a Data Engineer, you will be integral to data operations for the development and integration of multiple data types across a range of data sets and sources.

You will be responsible for the day-to-day operations of systems that depend on data, ensuring data is properly processed and securely transferred to its appropriate location, in a timely manner. Processing data will include managing, manipulating, storing and parsing data in a data pipeline for variety of target sources. You will also support maintenance of applications and tools that reside on these systems such as upgrades, patches, configuration changes, etc. The work is performed in a multidisciplinary team environment using agile methodologies. The candidate we seek must be highly motivated and enthusiastic about implementing new technologies and learning about new data in a small team environment where deadlines are important.
Responsibilities:
Complete development efforts across data pipeline to store, manage, store, and provision to data consumers
Being an active and collaborating member of an Agile/Scrum team and following all Agile/Scrum best practices
Write code to ensure the performance and reliability of data extraction and processing
Support continuous process automation for data ingest
Achieve technical excellence by advocating for and adhering to lean-agile engineering principles and practices such as API-first design, simple design, continuous integration, version control, and automated testing
Work with program management and engineers to implement and document complex and evolving requirements
Help cultivate an environment that promotes customer service excellence, innovation, collaboration, and teamwork
Collaborate with others as part of a cross-functional team that includes user experience researchers and designers, product managers, engineers, and other functional specialists
Required Skills:
Must be a US Citizen
Must be able to obtain a Public Trust Clearance
7+ years of IT experience including experience in design, management, and solutioning of large, complex data sets and models.
Experience with developing data pipelines from many sources from structured and unstructured data sets in a variety of formats
Proficiency developing ETL processes, and performing test and validation steps
Proficiency to manipulate data (Python, R, SQL, SAS)
Strong knowledge of big data analysis and storage tools and technologies
Strong understanding of the agile principles and ability to apply them
Strong understanding of the CI/CD pipelines and ability to apply them
Experience with relational database, such as, PostgreSQL
Work comfortably in version control systems, such as, Git Repositories
Contact Government Services strives to simplify and enhance government bureaucracy through the optimization of human, technical, and financial resources. We combine cutting edge technology with world class personnel to deliver customized solutions that fit specific needs and solve the most challenging, dynamic problems. Our team emphasizes technical expertise and an empathy to public service to provide unique top tier support for our government engagements.
SOME BENEFITS OF THIS POSITION INCLUDE:
Health, Dental, & Vision insurance
Life Insurance
401k
Flexible Spending Account (Health, Dependent Care, and Commuter)
Paid Time Off & Observance of State/Federal Holidays
For more information about CGS please visit: https://www.cgsfederal.com
39U0b7kJu8","$103,213 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Fracsys Inc,#N/A,"Washington, DC",Data Engineer,"Fracsys Inc is hiring a Data Engineer position. The ideal candidate must have at least 12+ years of industry experience. He or she must be responsible for successful technical delivery of Data Warehousing and Business Intelligence projects using a formal project management methodology.
Responsibilities
Directs, motivates and develops key technical personnel
Provides technical direction across the full lifecycle system development process
Provides strategic planning and comprehensive senior level technical consulting to IT senior management and senior technical staff
Working independently within guidelines
Provides detailed inputs for plans and schedules, including goals, risks, and resource allocation
Conduct diagnostic, troubleshooting steps and present conclusions to a varied audience
comfortable slinging some code to solve a problem
Build Data Marts and Data warehouses to facilitate analytical capabilities across the varied sources of data
Design and implement data patterns on cloud using cloud Data warehouse and data virtualization tools
Evaluate new tools and technologies for target state on cloud
Learn and implement new ETL, BI tools and data warehousing technologies
Qualifications
Minimum twelve (12) years in a Technical Lead or Consulting role leading Information Management related initiatives (system integration, data warehouse build, data mart build, or similar)
MUST have proven leadership experience for team of at least 3 technical resources.
Experience creating Data Marts, Data warehouses on-premise and on cloud for real time and batch processing frameworks
Experience building and optimizing big-data data pipelines and data sets including Postgres, AWS Relational Data Service
extensive hands-on experience building and optimizing data structures for data analytics, data science and business intelligence
Experience building self-service data consumption patterns and knowledge of cloud-based data Lake platforms
Experience wrangling data (structured and unstructured), in-depth knowledge of database architecture
Experience utilizing or leading implementations leveraging ETL tools (Informatica / Talend), BI Reporting tools such as MicroStrategy, Microsoft Power BI, data modeling tools such as Erwin, Oracle, SQL server, NoSQL, JDBC, UNIX shell scripting, PERL and JAVA, XML/JSON files, SAS , Python, AWS cloud-native technologies, S3, Athena, Redshift
Familiarity with the following technologies: Hadoop, Kafka, Airflow, Hive, Presto, Athena, S3, Aurora, EMR, Spark
Ability to drive, contribute to, and communicate solutions to technical product challenges
Ability to roll-up your sleeves and work in process or resource gaps and fill it in yourself
Excellent written and oral communication skills in English
Education
Bachelor’s degree in computer science, Data Analytics, Information Systems, or a related degree or equivalent experience. Master’s degree is preferred.
Job Type: Full-time
Pay: From $96,336.72 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Experience level:
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Washington, DC: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person","$96,337 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Integration Developer Network LLC
4.9",4.9,Remote,Data Engineer with PL/SQL Background,"Requirements:
Recent PL/SQL development work experience with Oracle 19.
Must have working experience with tools like Airflow, Git.
Must have python coding experience and REST API development experience.
Plus would be experience with microservices, docker, Kubernetes.
Plus would be experience with accounting and financial data
Experience in event driven and cloud based architectures and working knowledge of containerization and streaming technologies.
Detailed work experience on Oracle, SingleStore or IBM I DB2 database, SQL & PL/SQL, including ANSI SQL.
Experience in database analysis, development and maintenance of business applications specific to oil & gas back-office operations. Expertise in various ETL tools such as SQL*Loader, Equalum, Python scripts.
Familiarity with No-SQL databases and pipelines (Elasticsearch, MongoDB).
2+ years’ experience in the design, implementation and support of full life-cycle data engineering projects.
Bachelor's degree in Computer Science, Engineering or other equivalent degree, with 5+ years data engineering experience or Master's degree in Computer Science, Engineering or other equivalent degree with 2+ years data engineering preferred.
High school diploma or GED is required.
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Experience level:
9 years
Schedule:
8 hour shift
Experience:
Python: 8 years (Required)
PL/SQL: 5 years (Required)
ETL: 5 years (Required)
Work Location: Remote",$62.50 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2011,$1 to $5 million (USD)
Atalan Tech,#N/A,Remote,Data Engineer,"Atalan Tech is a start-up with a mission to mitigate the impact of clinician burnout and turnover in our nation’s health system. We are expanding our team and looking to hire the critical role of Data Engineer. The Data Engineer will work closely with the Chief Technology Officer, Chief Scientific Officer (UCLA Ph.D.), Chief Product Officer (Oxford Ph.D.), and Chief Executive Officer to deliver actionable insights from data in a variety of situations.
We are a small, close-knit team. This is an excellent opportunity for someone to join a mission-oriented, early-stage company at the ground floor and grow with us. There will be plenty of room for growth and creativity, and opportunities to tackle challenges that present one of the toughest problems in our society.
Requirements
Strong hands-on experience of designing, developing and managing data pipelines
Strong coding skills in Python (required) and SQL (required) and R (strongly preferred)
Experience building and maintaining data pipelines on AWS using EC2, EBS, Redshift, Postgres, S3, Data Pipeline, Kinesis, Lambda, etc.
Thorough understanding and experience with task management tools (Airflow, NiFi, etc.)
Knowledge of data management best practices for ETL, data governance, data warehousing, data modeling and data science
Basic knowledge of Linux (Redhat preferred) administration
Minimum B.S in Computer Science, software engineering or related field (or equivalent in related professional experience)
Strong communication skills
Strong problem-solving/critical thinking skills
Ability to work in a fast-paced, dynamic start-up environment
Some knowledge of healthcare, health tech, or healthcare data analytics (preferred)
Typical Tasks and Responsibilities include:
Working closely with CTO and CSO to build data pipelines for machine learning
Assisting the CPO in managing and supporting analysis of enterprise data to drive better products and solutions
Managing company’s data infrastructure on AWS (Postgres/RDS, Redshift, S3, etc.)
Synthesizing multiple data sources, such as internal medical records and HR data
Integrating with client systems including EHR (Epic, Cerner), HRIS, flat files, etc.
Working closely with other key members on other projects
Developing and designing data security standards and policies in line with industry best practices
About Us:
Atalan Tech’s mission is to improve the wellness of clinicians and their patients. We blend labor economics, machine learning, and clinical psychology with a team from Harvard, Yale, and UCLA. Atalan Tech is looking to contribute innovative and sustainable solutions to one of the most pressing problems in the healthcare industry. Building upon our proof-of-concept success, we have recently secured multiple engagements with nationwide healthcare organizations and have successfully raised institutional funding to expand our reach.
Contact Us:
If you’re interested in this position, please send us your CV/resume and cover letter.
Job Type: Full-time
Pay: $108,448.20 - $130,604.27 per year
Experience level:
2 years
Schedule:
Monday to Friday
Application Question(s):
Do you know the role is full time?
Can you start within the next month?
What are your salary expectations?
Have you held the title of a data engineer? If not, what experience do you have as a data engineer? Please be very specific.
Have you released Python code to a production environment in the last year?
Experience:
Data analytics: 2 years (Required)
Work Location: Remote","$119,526 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"R1 RCM, Inc.
3.4",3.4,Remote,Data Engineer III,"The Software Engineer III serves as a Data Engineer focused on building next generation of healthcare products and services. As part of a collaborative, motivated Agile-based team, the Software Engineer III is responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.
Responsibilities
Create and maintain optimal data pipeline architecture to assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Work with stakeholders to optimize the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using ‘big data’ technologies.
Work with stakeholders including the Product, Data and Engineering teams to assist with data-related technical issues and support their data infrastructure needs.
Adhere to and ensure engineering best practices using modern SDLC that enables CI/CD and favors automation, auditability, automated testing, infrastructure, and policy as code.
Ability to collaborate effectively with other architects and engineers delivering end to end, high-performing, highly reliable, scalable, and operable solutions.
Required Qualifications
Bachelor’s degree in Computer Science, Engineering, or a related technical field; or equivalent experience.
3+ years of experience delivering data engineering applications and services.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets .
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Experience in any one of the big data technologies (Hadoop, MapReduce , Spark, Data Lake etc. )
Experience in any one of the stream-processing systems ( Kafka, Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages : Python, Java, Scala, etc.
Understanding of modern software development including version control, unit testing, and continuous integration and deployment.
Strong communication skills, with proven ability to present complex ideas and to document concisely.
Preferred Qualifications
Working knowledge of Cloud providers and services such as Amazon AWS or Microsoft Azure.
Experience with relational SQL and NoSQL databases, including MongoDB and ElasticSearch.
Knowledge of healthcare data standards such as HL7, FHIR, EDI X12.
Knowledge of the healthcare revenue cycle, EMRs, practice management systems.
Working in an evolving healthcare setting, we use our shared expertise to deliver innovative solutions. Our fast-growing team has opportunities to learn and grow through rewarding interactions, collaboration and the freedom to explore professional interests.

Our associates are given valuable opportunities to contribute, to innovate and create meaningful work that makes an impact in the communities we serve around the world. We also offer a culture of excellence that drives customer success and improves patient care. We believe in giving back to the community and offer a competitive benefits package including:
Comprehensive Medical, Dental, Vision & RX Coverage
Paid Time Off, Volunteer Time & Holidays
401K with Company Match
Company-Paid Life Insurance, Short-Term Disability & Long-Term Disability
Tuition Reimbursement
Parental Leave
R1 RCM Inc. (“the Company”) is dedicated to the fundamentals of equal employment opportunity. The Company’s employment practices , including those regarding recruitment, hiring, assignment, promotion, compensation, benefits, training, discipline, and termination shall not be based on any person’s age, color, national origin, citizenship status, physical or mental disability, medical condition, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status or any other characteristic protected by federal, state or local law. Furthermore, the Company is dedicated to providing a workplace free from harassment based on any of the foregoing protected categories.
If you have a disability and require a reasonable accommodation to complete any part of the job application process, please contact us at 312-496-7709 for assistance.
CA PRIVACY NOTICE: California resident job applicants can learn more about their privacy rights California Consent",#N/A,10000+ Employees,Company - Public,Healthcare,Health Care Services & Hospitals,2003,Unknown / Non-Applicable
Proits Hub LLC,#N/A,Remote,Data Analytics Engineer,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote
Speak with the employer
+91 9417599933",$60.00 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"PSEG
3.9",3.9,"Edison, NJ",AWS Data Engineer,"Requisition: 73755
PSEG Company: PSEG Services Corp.
Salary Range: $ 78,600 - $ 149,400
Incentive: PIP 10%
Work Location Category: Remote Local
PSEG operates under a Flexible Work Model where flexible work is offered when job requirements allow. In support of this model, roles have been categorized into one of four work location categories: onsite roles, hybrid roles that are a blend of onsite and remote work, remote local roles that are primarily home-based but require some level of purpose-driven in-person interaction and living within a commutable distance, and remote non-local roles that can be effectively performed remotely with the ability to work in approved states.
PSEG offers a unique experience to our more than 12,000 employees – we provide the resources and opportunities for career development that come with being a Fortune 500 company, as well as the attention, camaraderie and care for one another you might typically associate with a small business. Our focus on combatting climate change through clean energy technology, our new net zero climate vision for 2030 and enhanced commitment to diversity, equity and inclusion; and supporting the communities we serve make this a particularly exciting time to join PSEG.
Job Summary
The Technology Engineer is a direct report to the Senior Technology Engineer with understanding of business goals, business processes, technology expertise in one or more domains, technology solution design and implementation.
The Technology Engineer is involved in the full technology life cycle and responsible for designing, coding, configuring, testing, implementing and supporting application software. Technology Engineers work closely with Analysts and Product Managers to understand the business requirements that drive the analysis and physical design of technical solutions. Technology Engineers may be assigned to either implementation or support functions. Implementation involves creating new technology based solutions whereas support involves upgrades, maintenance or issue resolution to existing technology solutions.

Job Responsibilities
Primary Responsibilities:
Design, configuration, development, documentation and testing of technology solutions to meet business or technology requirements. Evaluation of existing technology solutions to determine fit for purpose for the new business or technology requirements. Recommendation of technology alternatives. Collaboration with other individuals to ensure proper integration of the new technology solution with the existing technology solutions.
Analysis of end user’s needs, business and technology requirements. Translation of these requirements into technology solution capabilities and design. Alternatively, using the user needs, business and technology knowledge to peer review designs, implementation plans, software code, configuration settings or other artifacts to ensure technology solution being created by others meets the user needs, business and technology requirements.
Provides support toward resolution of escalated support tickets. May perform the needed analysis to identify root cause of reported incidents, identify the short and long term remediation for the identified incidents
Specific responsibilities include:
Analyzes end-user needs and designs, configures, develops and tests technology solutions to satisfy demand. Performs build versus buy analysis for business demand.
Partners with business analysts to translate business needs to technology solution requirements.
Evaluates existing technology solutions and platforms and provides recommendations for improving technology performance by conducting gap analysis, identifying feasible alternative solutions, and assisting in the scope of needed modifications.
Collaborates with all stakeholders such as enterprise architects, software development, operations, cybersecurity and infrastructure to integrate applications and hardware.
Ensures that the design and technology solution implementation meets security and QA standards. Suggests fixes to issues by doing a thorough analysis of root cause and impact of any defect(s).
Provides support toward resolution of escalated support tickets. Applies operation break fixes and performs other proactive maintenance activities until permanent solutions can be implemented. Supports and participates in the solution deployment process for new functionality/ subsystems/modules, upgrades, updates and fixes to the production environment.
Makes solutions production-ready by following the standard change management processes, completing required forms, following procedures, completing version control documents, etc.

Job Specific Qualifications
Bachelor’s degree in Computer Science or a related field 4 years of professional technology solution engineering
Demonstrated technology solution ownership and adoption, projects or other work experiences.
Demonstrated experience in analysis of end-user needs to configure, develop and test low to medium complexity technology solutions to satisfy business demand.
Demonstrated track record of implementing technology solutions using structured methodologies such as agile (SCRUM, Kanban etc.) and waterfall.

Required Competencies:
AWS Data Cloud development experience
Experience developing and sustaining data load jobs using Extract, Transform, Load (ETL/ELT) methodologies and tools such as, Python, SQL, Shell Scripts and AWS Services.
Developing and sustaining a data ingestion pipeline to AWS leveraging services such as Athena, Glue, Lambda, S3, Relational Database Service (RDS) and Redshift.
Integration of AWS Data Lake with reporting tools such as PowerBI.
Experience using databases such as Oracle, MS SQL Server, and developing software code in one or more programming languages (Java, Python, etc.)
Experience with production support of mission critical technology solutions.
Experience documenting technical solutions and system process flow techniques.
Ability to work independently, multi-task effectively, and be flexible to accommodate change in priorities as necessary.
Strong analytical ability, communication skills, excellent problem solving skills, and ability to learn new technical concepts.
Ability to foster working relationships with Client departments, IT Management and Software Service Providers.
Desired Qualifications:
Graduate degree or MBA
AWS Developer/Architect certification
Minimum Years of Experience
4 years of experience
Education
Bachelors in Engineering or Computer Science
Bachelor in Information Technology
Certifications
None Noted
Disclaimer
Certain positions at the Company may require you to have access to Part 810-Controlled Information. Under the law, the Company is limited in who it can share this information with and in certain circumstances it is necessary to obtain specific authorization before the Company can share this information. Accordingly, if the position does require access to this information, you must complete a 10 CFR Part 810 Export Control Compliance Nationality Request Form, a copy of which will be provided to you by Talent Acquisition if an offer is made. If there is a need for specific authorization, due to the time it takes to obtain authorization from the government, we will likely not be able to further proceed with an offer
Candidates must foster an inclusive work environment and respect all aspects of diversity. Successful candidates must demonstrate and value differences in others' strengths, perspectives, approaches, and personal choices.
As an employee of PSE&G or PSEG LI, you should be aware that during storm restoration efforts, you may be required to perform functions outside of your routine duties and on a schedule that may be different from normal operations.
Certain positions at the Company may require you to have access to 10 CFR Part 810 controlled information. If the position does require access to this information, the Talent Acquisition representative will provide further details upon making an offer.
PSEG is an equal opportunity employer, dedicated to a policy of non-discrimination in employment, including the hiring process, based on any legally protected characteristic. Legally protected characteristics include race, color, religion, national origin, sex, age, marital status, sexual orientation, disability or veteran status or any other characteristic protected by federal, state, or local law in locations where PSEG employs individuals.
Business needs may cause PSEG to cancel or delay filling position at any time during the selection process.
This site (http://www.pseg.com) is strictly for candidates who are not currently PSEG employees. PSEG employees must apply for jobs internally through emPower which can be accessed through sharepoint.pseg.com by clicking on the emPower icon, then selecting careers.

PEOPLE WITH DISABILITIES:
PSEG is committed to providing reasonable accommodations to individuals with disabilities. If you have a disability and need assistance applying for a position, please call 973-430-3845 or email accommodations@pseg.com. If you need to request a reasonable accommodation to perform the essential functions of the job, email accommodations@pseg.com. Any information provided regarding a disability will be kept strictly confidential and will not be shared with anyone involved in making a hiring decision.

ADDITIONAL EEO/AA INFORMATION (Click link below)
Know your Rights: Workplace Discrimination is Illegal
Pay Transparency Nondiscrimination Provision","$114,000 /yr (est.)",10000+ Employees,Company - Public,"Energy, Mining & Utilities",Energy & Utilities,1903,$5 to $10 billion (USD)
Vibrant Planet,#N/A,California,Data Engineer,"About Vibrant Planet
We are a team of leaders in science, forestry, policy, and tech, building a cloud-based, data-driven platform to increase the pace and scale of forest restoration and reduce catastrophic wildfire, tree mortality, forest degradation, and deforestation. Our current software modernizes land management planning and monitoring with AI-driven data development, user friendly scenario building and decision support, and forest resilience trends and treatment outcome detection. The system quantifies potential and actual treatment benefits, helping to grow markets for ecosystem services (carbon, water, biodiversity, sustainable forest-derived products).
Our initial platform (launched in September 2021) is focused on temperate, “fire adapted” landscapes in California and the Western US. Our mission is global; our roadmap expands into other geographies and forest types accordingly.
Driven by our sense of urgency to protect vital forests and the services they provide, we use our expertise to build sophisticated, AI/ML-driven, user friendly products to democratize the use of data and accelerate the process of stabilizing and restoring our forests.
Vibrant Planet is backed by climate and ecosystem resilience solutions leaders, including Grantham Foundation, Earthshot, Elemental Excelerator, Ecosystem Integrity Fund, Chris Cox (CPO at FB), Neil Hunt (ex CPO of Netflix), Cisco, Valia Ventures, and Halogen Ventures.
For further information please visit: VibrantPlanet.net
Equal Opportunity Employer: Vibrant Planet is committed to diversity. We encourage applicants from all cultures, races, colors, religions, sexes, national or regional origins, ages, disability status, sexual orientation, gender identity, military, or other status protected by law to apply.
We are most interested in finding the best candidate for the job, and that candidate may come from a less traditional background, but have capacity to grow into and thrive in the position after some mentoring. We do not require that you have experience with every job description task. We will consider any equivalent combination of knowledge, skills, education, and experience to meet minimum qualifications. We encourage each candidate to think broadly about their unique background and skill set and how it may relate to the role. This is important to us. We aren’t just saying this, we mean it.
For further information please visit: https://vibrantplanet.net
About the Role
We are looking for a versatile, hands-on data focused engineer to help us scale and build our geospatial pipelines and tooling. You will be part of a small team of engineers and scientists, tackling some of the most important climate change related issues facing the world today. This is a remote role (and company) so being comfortable and effective working in a distributed team is crucial.
Key Responsibilities:
Design, develop, scale, and maintain data pipelines that ingest, transform, and store large volumes of geospatial data from multiple sources.
Optimize data processing and storage performance and cost efficiency by leveraging cloud-based technologies and services.
Collaborate with engineers, scientists and other stakeholders to share knowledge and build expertise.
Lead and participate in development life cycle activities like design, coding, testing, and production release.
Contribute to our evolving engineering culture, standards, tooling, and processes.
Mentor and support other engineers and deeply review code.
Technical Qualifications
Strong software engineering skill set.
2+ years experience in data engineering or a related field.
Experience with distributed data processing frameworks and tools (e.g., Airflow, Hadoop, Spark).
Proficiency with Python or an equivalent language. Ability to write clean, high-quality code and tests to keep our system fast, reliable, and monitorable.
Bonus Qualifications - Absolutely not required, but nice to have
Airflow
Pandas / Geopandas
Jupyter notebooks
Geospatial processing
Lidar data
Satellite imagery
Experience with ArcGIS/QGIS
Other Expectations:
Strong communication skills; discussing complex technical concepts to engineers and non-engineers is no problem to you.
Collaborative and supportive team player, with a desire to enrich our engineering culture.
Eagerness to learn, think creatively, and share knowledge with others.
Ability to write understandable, testable code with an eye towards maintainability.
Proactive and empathetic mindset - you love to roll up your sleeves to fix problems
Location - We are a remote first company with the majority of the team in the US west coast time zone. We also have a small and growing presence in New Zealand. Location can be flexible, but we are primarily targeting those two time zones and anything in between.",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Kanini,#N/A,"North Druid Hills, GA",Senior Data Engineer,"About Kanini
Kanini provides Agile Software Development, Cloud Computing, Data Science, and Location Intelligence services to public and private organizations. We have successfully served our clients in government, finance, transportation, utility, and software industries since 2003.
Why you should join
Working at Kanini is flexible and personal. We are a highly motivated, collaborative team experimenting with the latest technologies. We are committed to everyone having a healthy work/life balance, and we provide extensive mentorship and training resources to help you succeed.
Kanini is looking for Data Engineer who has a deep experience in Data Engineering, Agile planning, Snowflake, SQL, MySQL, PostgreSQL, Git, AWS Data Tools, Python, Jira
Job Description
The Senior Data Engineer is responsible for processing structured and unstructured data, validating data quality, and developing and supporting data products. The Senior Data Engineer also plays a role in Agile planning, providing advice and guidance, and monitoring emerging technologies.
Responsibilities
Gathers and processes raw, structured, semi-structured, and unstructured data using batch and real-time data processing frameworks.
Understands and enforces appropriate data master management techniques.
Ensures data quality and implements tools and frameworks for automating the identification of data quality issues.
Work with internal and external data providers on data validation providing feedback and making customized changes to data feeds and data mappings.
Understands the challenges that the analytics organization faces in their day-to-day work and partner with them to design viable data solutions.
Recommends improvements to processes, technology, and interfaces that improve the effectiveness of the team and reduce technical debt.
Implements and optimizes data solutions in enterprise data warehouses and big data repositories.
Installs, maintains, monitors, and supports business intelligence, distributed computation, and big data analytics tools.
Provides ongoing support, monitoring, and maintenance of deployed products.
Actively works with less experienced data engineers providing technical guidance and oversight.
Actively participates in the engineering community, staying up to date on new data technologies and best practices and shares insights with others in the organization.
Qualifications:
Bachelor’s degree in computer science or related field and 4 years work experience
Working experience with batch and real-time data processing frameworks
Working experience with data modelling, data access, schemas, and data storage techniques within Snowflake
Working experience in design, development, and implementation of highly scalable, high-volume software systems and components, client-facing web applications, and major Internet-oriented applications and systems
Working experience working with relational databases such as SQL, MySQL, Postgres/PostgreSQL
Working experience with business intelligence tools and platforms
Working experience with data quality tools
Working experience with application lifecycle methodologies (e.g. waterfall, agile, iterative)
Experience with ETL processes and tools
Experience working with Git.
Desirable Qualifications
Scrum Developer Certification or equivalent
Working experience with Sisense platform
Working experience with SQL Server Integration Services
Working experience with AWS data tools (Database Migration Service)
Working experience with Python
Experience working with Jira, Rally or similar tools
Kanini Software Solutions, Inc. does not discriminate in employment matters based on race, gender, religion, age, national origin, citizenship, veteran status, family status, disability status, or any other protected class. We support workplace diversity. If you have a disability, please let us know if there is anything we can do to improve the interview process for you; we’re happy to accommodate.
Kanini Software Solutions, Inc., 25 Century Blvd., Ste. 602, Nashville, TN 37214
Job Types: Full-time, Contract
Pay: $60.00 - $65.00 per hour
Application Question(s):
Current Location?
Work Location: Remote",$62.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Siri info solutions inc,#N/A,"Beaverton, OR",Sr. Data Engineer,"Must Have Skills
PySpark
Python on AWS
Responsibilities:
Understanding of advanced data warehousing concepts, data modeling and extract, transform and load development
Advanced SQL skills with experience querying large datasets from multiple sources and developing automated reporting
Python skills for scripting, data manipulation, custom extract, transform and loads and statistical/regression analysis particularly, as they apply to sales and marketing operations and performance
Job Type: Full-time
Salary: $47.00 - $50.00 per hour
Experience level:
7 years
Work Location: On the road",$48.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Petrolink International
2.9",2.9,"Houston, TX",Data Analysis Engineer,"Location:
Houston, Texas, U.S.A.
Job Type:
Full-time
Job Description:
Petrolink is a global company that specializes in user-driven technology for the Energy Industry.
We are looking for a Data Analysis Engineer to assist with creating and delivering algorithms primarily used by our engineering solutions.
This position is based in our U.S. office in Houston, Texas.
Duties & Responsibilities:
Research and provide logic for engineering solutions algorithms.
Collaborate with Subject Matter Experts (Drilling Engineers, Mud loggers, Pore Pressure Analysts, and other specialists from the Drilling Industry) to understand different challenges in the Drilling Industry and the practices in place to address/solve these challenges.
Code the algorithms using C# to run on Petrolink’s proprietary analytics engine.
Develop, test, and maintain algorithms.
Support escalations related to issues in algorithms.
Learn existing algorithms to provide support when required.
Prepare training material and sessions for stakeholders as part of the production release.
Skills & Qualifications:
Proficient in C#.
Experience with Python is an advantage.
Knowledge in drilling practices, and ability to speak to other drilling experts and capture requirements and logic.
Knowledge in drilling performance KPIs is an advantage.
Prior experience with WITSML is an advantage.
Function and communicate well in a team having diverse expertise.
Bi-lingual (English and Spanish) preferred.
At least 2 years’ relevant experience.
Benefits:
Health insurance
Dental insurance
401(k) with matching
Health savings account
Life insurance
Long-term disability
This job description is intended to explain the general nature and level of work being performed. Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.
Applicants must be legally authorized to work in the United States for any employer.
To apply for this position, applicants must meet the above criteria.
Interested applicants fulfilling the above criteria, please send your resume to careers.usa@petrolink.com.","$81,133 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
Radiant System,#N/A,"Pleasanton, CA",Data Engineer,"First Priority: Pleasanton, CA –
Second Priority for a superstar: Plano, TX
Experience in developing and deploying data pipelines, preferably in the Cloud
Experience working with Spark, Databricks, and other relevant technologies in Microsoft Azure cloud
Experience in productionizing and deploying Big Data platforms and applications
Expert in writing SQL statements
Job Type: Contract
Application Question(s):
Are you willing to work on W2?
Work Location: In person",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Hiscox
4.1",4.1,"Atlanta, GA",Senior Data Engineer,"job description:
You will be a senior member of our US IT Team data engineering practice. You will be an early member of this team and will work closely with the Director of Data Engineering to help establish it. We will have our work cut out for us delivering planned work, building the practice and evolving our technical landscape. This is an exciting opportunity for the right individual to join at the very beginning and grow with the new team.
As a senior you will be expected to lead, mentor others and deputise for the director of data engineering when required. Yoi will also continue to contribute as a hands-on engineer. You will have the knowledge, best earned through 5+ years of experience, to truly inhabit this senior role in an ambitious team.
What you’ll be doing:
Working with business stakeholders and our delivery team to understand high value business problems that can be solved through the application of data processing and analytical systems.
Developing, expanding and evolving our existing databases and ETL pipelines.
Working with architects to design, build and support a transformational cloud-based data platform for the business.
Being a core and professional member of the new data engineering practice
Understand business requirements and help refine into development tasks and estimate their complexity.
Research, evaluate and adopt new technologies with a right tool for the job mentality.
Focus on both speed of delivery and quality, with suitable pragmatism – ensuring your solutions are always appropriate and not too complex or over-engineered.
Quick progression of projects from proof-of-concept to post-production stage.
Communication and presentation of ideas to colleagues in all parts of the wider tech team.
Participating in code reviews for the data engineering practice.
Act as a servant leader in our practice – ensuring team members have
person specification:
Knowledge of, or ability to rapidly adopt our core languages for data engineering – SQL and/or Python.
Experience working using a modern DevOps approach. You should not just have written data code but also the CI/CD pipelines necessary to test and deploy that code in a professional environment.
A robust understanding of core data engineering topics – ETL vs ELT, structured and unstructured data, data quality and data governance.
Ability to contribute to all aspects of a solution – design, infrastructure, development, testing and maintenance.
The ability to design and advocate for technical solutions to business problems.
Effective collaboration with technical and non-technical team members through agile ceremonies – roadmap planning, feature workshops, backlog elaboration, code review.
Understanding of cloud technology and significant hands-on experience working with it. You must understand how cloud-native solutions must be built differently from traditional ones.
Track record of taking initiative and delivering projects end-to-end; clear evidence of being self-driven and motivated
Immense curiosity, high energy and desire to go the extra mile to make a difference.

Desirable Skills
Beyond the required skills we are open to individuals of diverse talents. Experience with additional technologies, data science knowledge or a business background are all valued. We want to know how your unique abilities can contribute to our team.
Our Technology
We operate in a diverse technical landscape and are looking for flexible engineers who can adapt to and use many different tools. We would not expect any engineer to be familiar with the entire tech stack – no engineer can. Instead, we seek people with a good understanding of data structures and algorithms and the ability to apply this knowledge in learning new tools.
We have an existing data landscape involving both MS SQL Server and Oracle databases. We use SSIS for ETL. We will also need to source data from our integration layer – webMethods.
If some of that sounds like it needs bringing up to date, we know! We hope to start building a modern cloud data platform to realise the full value of our data. The technologies we use for that platform are still to be decided. Likely it will be based on Snowflake with Azure Data Factory for ELT. Other parts of our business have had success using Prefect for orchestration and dbt for transformation and we will reuse where it is sensible.
We prepare data for use by analysts working with a variety of tools – Tableau, PowerBI, and even Excel.
We take a DevOps approach and strive for continuous integration / continuous deployment. We use Azure Pipelines to deploy our code. We deploy infrastructure the same way using Terraform and Docker.
Our technology landscape is not fixed. Our engineering and architecture teams drive the technology we adopt.


Salary range $140,000 - $170,000","$155,000 /yr (est.)",1001 to 5000 Employees,Company - Public,Insurance,Insurance Carriers,1901,$1 to $5 billion (USD)
"DSMH LLC
5.0",5.0,Remote,"Data Engineer (AWS, Python)","Title: Data Engineer
Duration: 12+ Months
Technical Skills
(Required)
Four years or more of experience in data engineering and/or software
development.
Three years or more of experience with development, operations, or
architecture in AWS using Python.
Experience with development and delivery of microservices using serverless
AWS services (S3, RDS, Aurora, DynamoDB, Lambda, SNS, SQS, Kinesis,
IAM)
Strong competency in testing, including unit testing to coverage standards and
integration testing.
(Desired)
Software development experience with object-oriented development and design
patterns
Experience with serverless design in AWS
AWS technical certifications (Developer Associate, Solution Architect Associate)
Familiarity with machine learning and data design to support machine learning
Experience with productionizing machine learning workloads
Experience with ElasticSearch/ELK, Kibana, Grafana, and/or Prometheus
Experience with OpenTelemetry
Soft Skills
(Required)
Verbal and written communication skills, problem solving skills, customer
service and interpersonal skills.
Basic ability to work independently and manage one’s time.
(Desired)
Ability to work collaboratively in a complex, rapidly changing, and culturally
diverse environment.
Ability to clearly communicate complex technical ideas.
Comfortable working in a dynamic environment where digital is still evolving as
a core offering.
Job Type: Contract
Pay: $50.00 - $55.00 per hour
Experience level:
6 years
Schedule:
Monday to Friday
Experience:
AWS: 5 years (Required)
Python: 5 years (Required)
Work Location: Remote",$52.50 /hr (est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,$1 to $5 million (USD)
"Iron Bow Technologies
3.5",3.5,"Washington, DC",Sr. Data Center Engineer,"Iron Bow Technologies is seeking a Sr. Data Center Engineer to support our Federal customer, The Department of Justice (DOJ), Criminal Division (CRM), in Washington, DC. The selected candidate will support our flexible and scalable solution with the DOJ CRM to extend cloud-native features to existing legacy and next-generation applications while also future proofing the infrastructure to easily accommodate future requirements, such as transitioning to a public cloud.
This position is hybrid and requires 2 days a week onsite at the customer location in NW Washington, DC and support at the customer data center as needed in Sterling, VA.
Essential Duties and Responsibilities:
Assess and provide VMware solutions to include vCenter, vSphere, SRM, ESXi
Primary storage solutions and data file storage management, NAS file storage solutions
Data Center blade servers and rack servers
Server virtualization
Experience:
10 years of related experience providing system administration support in a virtualized environment, including at least 6 years of VMWare solutioning, including vCenter, vSphere, SRM, and ESXi
Data Center operations experience, with SAN and NAS
Experience supporting primary storage solutions and data file storage management, NAS file storage solutions
Hands on experience with Data Center blade and rack servers
Ability to work staggered shift hours when needed to support critical coverage and/or time-sensitive needs
Be a member of our Team providing coordinated 24x7 IaaS on-call support
U.S. Citizen
Currently hold or obtain/maintain a Public Trust High clearance
Education:
BS in Computer Science, Information Science, or other relevant discipline is preferred – additional years of experience may be considered in lieu of the degree
Preferred Certifications:
VMware Certified Professional
Primary storage certifications such as HPE, Dell-EMC, NetApp, or Pure Storage
VCP-DCV
Travel Requirements:
No travel
PRINCIPALS ONLY. NO AGENCIES PLEASE. MUST BE A US CITIZEN ABLE TO OBTAIN A PUBLIC TRUST SECURITY CLEARANCE.
Iron Bow Technologies is a leading IT solution provider dedicated to successfully transforming technology investments into business capabilities for government, commercial and healthcare clients. Our depth of technical expertise, global reach and strategic partnerships with industry leaders as well as disruptive technology partners uniquely positions Iron Bow to target the right solution to ensure your successful business outcome. We partner with our clients from planning and implementation through ongoing maintenance and management to deliver solutions that are strong, flexible and on target .
Iron Bow Technologies is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.
#LI-TM1",#N/A,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,1983,$500 million to $1 billion (USD)
Infosoft,#N/A,"Peachtree Corners, GA",Data Pipeline Engineer - 2173,"Here are the job position details for your review:
Job Title: Data Pipeline Engineer (GStreamer/ DeepStream)
Pay Rate: $90 to 120/Hr.
Duration: 3 Months
Location: Peachtree Corners, GA
This position is for the Building Automation division.
We are looking for Data Pipeline Engineer (GStreamer) to participate in the development of Streaming data pipelines using NVIDIA Deepstream, GStreamer, Python programming, messaging queues, and related tools.
Preference is for candidates located in Atlanta, GA metro area. However, ok with non-local candidates who are open to 75% travel to Atlanta, GA.
Understanding the basics of our architecture/setup to be able to do setup/support for demos, testing, etc.
Basic computer hardware assembly and set-up
Possible set-up/installation of the application(s)
Possible development tasks depending on what the rest of the development team has for them
Develops business software applications, system software, and/or IT solutions on the basis of functional specifications.
Scope of position embraces both the customization or reuse/modification of system software and/or solutions as well as their development from scratch.
Consults with customers in creating functional specifications and work estimates, and supports Make-or-Buy decisions.
Analyses performance gaps of existing software environments and proposes measures to address efficiency problems.
Is also responsible for the implementation of data-driven components supporting data modeling and engineering in the context of data analytics (Data Stewardship).
Skills:
Python
Linux
Docker and Docker Swarm
Zero MQ / a good understanding of messaging
Desired Skills:
NVIDIA Deepstream SDK
Web Sockets
Flask
Protobuf
Possible:
GStreamer (if different than RTSP / H264 / MJPEG)
Job Type: Contract
Salary: $90.00 - $120.00 per hour
Ability to commute/relocate:
Peachtree Corners, GA: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
""Please share your Email ID""
Work Location: One location",$105.00 /hr (est.),Unknown,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Titan America
3.8",3.8,"Roanoke, VA",Data Engineer,"Data Engineer
We are seeking a motivated and detail-oriented Data Engineer to join our team based in Roanoke, Virginia. As a Data Engineer, you will play a crucial role in designing, developing and maintaining scalable data pipelines and transformations for business-critical use cases. In addition, you may take on various Software Development and Data Science responsibilities as we frequently share responsibilities and knowledge across our growing team.
Responsibilities:
Design, develop and maintain scalable data pipelines for extracting, transforming, and loading data from various sources
Collaborate with cross-functional teams to identify data needs and determine the best data solutions
Develop and implement data models to support business requirements and ensure data quality
Ensure the security and privacy of sensitive data by implementing appropriate access controls
Monitor and optimize data pipeline performance to ensure timely and accurate data delivery
Document data pipeline processes, data dictionaries, and data storage solutions
Requirements:
Bachelor's degree in Computer Science, Computer Engineering, or a related technical field
Minimum of five years of professional experience working as a Data Engineer or Software Developer
Strong hands-on experience with data warehouse and transformation solutions, i.e. Domo, Snowflake or similar
Proficient in at least one scripting language such as Python, JavaScript, or R
Understanding of data modeling, data integration and data quality processes
Familiarity with cloud platforms such as AWS, Azure, or Google Cloud Platform
Strong analytical and problem solving skills
Full Stack Software Development experience in a professional setting is highly desired, but not required
This is an excellent opportunity for a driven and collaborative individual to make a significant impact in a dynamic and growing team. If you have a passion for data and a desire to work in a fast-paced and dynamic environment, we want to hear from you!
Success begins with hiring the right people to partner with us as we grow and develop our businesses. People are central to everything we do. It is through their efforts and talents that Titan has been successful for over 100 years.
Titan America is an Equal Employment Opportunity (EEO) / Affirmative Action employer and welcomes all qualified applicants. Applicants will receive fair and impartial consideration without regard to race, sex, color, religion, national origin, age, disability, veteran status, genetic data, or other legally protected status.
Job Type: Full-time
Pay: $75,000.00 - $85,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Referral program
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Roanoke, VA 24019: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Do you currently or will you in the future need sponsorship to work in the United States?
This position requires the successful candidate to be in the office in Roanoke, VA at least 3 days a week. Can you meet this requirement?
This position requires the successful candidates to be a full time, W2 employee. Can you meet this requirement?
Education:
Bachelor's (Preferred)
Experience:
Data Engineering: 5 years (Preferred)
Work Location: Hybrid remote in Roanoke, VA 24019","$80,000 /yr (est.)",1001 to 5000 Employees,Company - Public,"Energy, Mining & Utilities",Mining & Metals,1902,Unknown / Non-Applicable
Atika Technologies,#N/A,"Pittsburgh, PA",Data Engineer,"Role: Data Engineer/PySpark Developer, Pittsburgh, PA - Day One Onsite
Fulltime and Contract(W2)
LinkedIn Must
USC/GC/EADs
Three round of interview: First two on Zoom Video Call, End Client Round will be Onsite on the Client's Location (Pittsburgh, PA)
Job Description:
Python(PySpark), Jupiter hub, airflow, Java
Database knowledge - Oracle / SQL / Data Analytics
Regulatory reporting knowledge preferred
Good communication skills
Open / Ability to learn new technologies
Very good Analytical Skills
Experience in ETL is preferable
Job Types: Full-time, Contract
Salary: $120,000.00 - $130,000.00 per year
Benefits:
Dental insurance
Health insurance
Life insurance
Compensation package:
Bonus pay
Performance bonus
Yearly pay
Experience level:
10 years
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Pittsburgh, PA 15259: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Required)
Spark: 4 years (Required)
Regulatory reporting: 3 years (Required)
SQL: 4 years (Required)
IT: 10 years (Required)
Airflow: 4 years (Required)
Jupiter: 4 years (Required)
Work Location: One location","$125,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Take Command Health
4.9",4.9,"Dallas, TX",Data Engineer,"Company: Take Command
Position: Data Engineer
Salary: $120,000-$130,000 plus bonus
Location: Dallas or Austin, TX
About Take Command
Take Command is a start-up on a mission to improve the healthcare system, starting with health insurance. Pragmatically speaking, we help employers reimburse employees for individual insurance instead of offering a traditional one-size-fits-all group plan. We believe this model can empower employees (when they have the right support) to be savvy healthcare consumers and have a transformative impact on the entire healthcare system.
Let’s be honest—health insurance is usually a confusing, frustrating, and even emotional experience for people. We want to fix that with a new model, great technology, and a superior user experience. We have made a great start, but we need your help to fully realize our long-term vision.
As a part of the dev team, you'll build the framework for both our clients and our team to succeed. You will be a pillar in our company as we grow and make our product the best product on the market!
In this role, you will:
Take ownership for delivering multiple features, their quality and assist team members in delivering their features
Recommend improvements to data quality, processes and code to make it easier to use, manage and maintain
Experience designing and developing data models and data migrations
Experience managing and maintaining ETL or ELT processes (Hevo, BigQuery, Looker, etc.)
Hands-on development experience with SQL, relational database design (ERDs)
Must be able to write SQL & contribute code
Be product-minded; focus not only on what needs to be built but why something needs to be built
You may be a great fit for this role if you have:
3 or more years of professional experience as a data analyst or data engineer
Cloud-Based Data Tools e.g. Hevo, BigQuery, Looker, etc.
Database technologies: SQL, RDBMS / Relational databases (Postgres, Amazon RDS / Aurora, etc.)
Nice to have experience: Python, Java, Typescript or another modern OOP language
Cloud infrastructure: AWS
Source control: Git
Working at Take Command
We’re excited to build a team and culture that reflects our values! We offer competitive pay, health benefits, and equity options to share with this position. We want you to be an owner along with us in revolutionizing healthcare.
A generously funded ICHRA for medical, dental, and vision premiums and medical expenses. You get to use our own product and we think that’s so exciting and rare!
Unlimited personal vacation in addition to regular company holidays.
401(k): 90-day eligibility for 2% match that vests immediately!
Access to LinkedIn Learning.
We have two beautiful offices conveniently located near The Hill (Dallas) and Holly Commons (Austin). The kitchen is well-stocked and we've designed the space to have lots of different areas to work!
Paid parental leave for new parents.
Flexible on where you work – we hope to see you around the office on average 3 days a week especially when white-boarding or building relationships with your colleagues, but you also have the ability to work from home or wherever you'd like when you need to get focus work done.
More About Us
We secured our Series A funding in 2021 and are thrilled to be able to expand our team. Despite being a small startup in a land of health insurance giants, we’re the recognized industry leader for what we do (health insurance reimbursements) and passionate about bringing it to market because we know we can help fix a broken system and improve our clients’ wellbeing and health outcomes.
We’ve been featured in The New York Times, The Wall Street Journal, The Dallas Morning News, and other national healthcare publications and are excited about our growth opportunities.
Take Command knows diversity and inclusion among our teammates is integral to our company’s success and growth. Our vision is to recruit, develop, and retain the best team from a diverse candidate pool.
This has mostly been about us, but we’d love to hear from you-we can’t wait to hear your story!
Take Command is an equal opportunity employer! We celebrate diversity and are committed to creating an inclusive environment for all employees.
Xrygn4NXMr","$125,000 /yr (est.)",1 to 50 Employees,Company - Private,Insurance,Insurance Agencies & Brokerages,2014,Unknown / Non-Applicable
WellTrust Medical Group,#N/A,"Charlotte, NC",Data Engineer,"The Data Engineer will assist the senior analyst in complex modeling and projects to ensure their completion within the allotted timeline. Helps track measures of success and develop reports that measure operational and customer metrics. Demonstrates ability to troubleshoot data consumption and data quality problems.

Responsibilities:
Build and maintain reports using My SQL Server
Identify optimal data sources, structure data validation, and cache optimization.
Performs business problem modeling and quantitative analytics including regression analysis, time-series, hierarchical cluster analysis, and data mining.
Helps track measures of success and develop reports that measure operational and customer metrics.
Demonstrates ability to troubleshoot data consumption and data quality problems.
Evaluates and recommend improvements to current models, metrics, and analytic processes to improve operational efficiency and insight.
Design, document, and communicate effective technical solutions meeting end-user requirements
Perform analyses with large amounts of complex data, and uses data visualization tools to communicate results.
Requirements:

Bachelor's degree or equivalent experience in a quantitative field (Mathematics, Statistics, Computer Science, Economics, Finance, Physics, Engineering)
3+ years of professional working experience as a Data Engineer or Analyst
1-3 years of data warehouse experience
Build and maintain reports using My SQL Server
Strong analytical and creative problem-solving skills
Design, document, and communicate effective technical solutions meeting end-user
SQL and Excel Macro development
Strong understanding of various data environments such as SQL, Oracle, Big Query, etc.
Experience with data visualization tools such as Looker Studio, Tableau, D3.js, etc.
Proficient in R, Python, MATLAB, Java, or another programming language.",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Apolis
3.9",3.9,"Saint Louis, MO",Data Engineer (Python Focused),"Role: Data Engineer (Python Focused) (W2 ONLY)
Location : St. Louis, MO 63102
Duration: 18 Months
The primary responsibilities of this role will be to work as a part of the data and analytics team of the client. The ideal candidate would have experience using ETL and other data engineering tools in a cloud environment. Additionally, this candidate would have coding and scripting experience creating code to move and manipulate data using tools such in Python such as Spark or Pandas. Microsoft Azure experience is a plus but relevant experience with other cloud-based environments will be considered.
This team uses Microsoft Azure to build and productionize data sets and processes to the product owners and product users and this role will be expected to contribute to those objectives. As a member of this team, you will work in conjunction with other data engineers, platform engineers, solution developers and others to execute and deliver working solutions to the end users.
Technical Skills
High focus on Python
More Programmer/Coder experience, less Cloud experience needed.
Must have experience with data analytics/engineering and cloud technologies and an in-depth working knowledge and understanding of data engineering, design, and integration.
Intermediate or higher level of proficiency with Python Must have the proven ability to learn quickly and apply cloud data engineering principles and skills to new tools and use cases.
Ability to define and take technical ownership of detailed requirements and collaborate effectively with various team members such as Business Analysts and Project Managers and end users.
Excellent communication (verbal and written) and problem-solving skills are essential for this position.
Initiative, Proactive Cooperation, and curiosity are mandatory behavioral skills required for this position.
Experience with several of the following technologies is preferred:
Experience with Microsoft Azure Platform. In particular, the data processing toolsets including Azure Data Factory Pipelines, Azure Databricks, Azure Data Lake Storage.
Experience using Notebooks (Such as Jupyter, DataBricks) also desired.
Experience using code languages such as Python and C#
Experience using query languages such as SQL, MDX and DAX
Experience using scripting languages such as PowerShell, M-Query (Power Query), and Windows batch commands.
Understanding of Agile and DevOps principles for software development and ownership.
Familiar with compliance and controls for software development and deployment.
Education Required
A bachelor's in computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field [or equivalent work experience] is not required but is preferred.
Job Type: Contract
Salary: $60.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Vision insurance
Compensation package:
Overtime pay
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
St. Louis, MO: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Preferred)
ETL: 5 years (Preferred)
Cloud infrastructure: 4 years (Preferred)
Work Location: One location",$65.00 /hr (est.),501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,1996,$25 to $100 million (USD)
"Loyal
3.9",3.9,"Atlanta, GA",Senior Cloud Data Operations Engineer,"Loyal is an organization centered on experience and building a platform that allows consumers to make meaningful decisions when it comes to healthcare. We deeply understand providers, locations, services, appointments, business rules, and moreover, we understand patients - who they are, the preferred method of communication, upcoming appointments, lapsed appointments, outstanding bills, health risks, and more. With this intelligence, our platform fuels highly relevant and personalized experiences across all mediums (website, email, voice…) allowing patients to get healthy, stay healthy, and have a better relationship with the health care provider.
**This is a remote role**
Summary
The Senior Cloud Data Operations Engineer is responsible for delivering Loyal's cloud-based data warehouse. They interface with groups across the organization to ensure we are collecting and storing data in a way that makes it easy to create and consume. They identify best practices for big data management, select the right tooling, create a workable data architecture, and drive an implementation roadmap others can depend on. The Senior Cloud Data Operations Engineer also works closely with our security group to ensure data security and privacy. They also mentor others on the team, providing technical leadership to those they work with.
You will
Create data streams and consolidate data sources into a single cloud-based data warehouse
Architect and catalog data warehouse storage schema
Work with product teams to enable easy data aggregation
Work with data consumers (analysts, data scientists, and AI/ML engineers) to ensure data is easily accessible and consumable
Create and execute a roadmap that meets data storage and availability goals
Ensure data is secure through data analysis, access control, and encryption
You have
Bachelor's degree in computer science, information systems, applied math or related field, or equivalent work experience
Minimum 5 years of experience working with cloud data processing services (AWS, Azure, or GCP).
Minimum 3 year experience with Big Data technologies, such as Databricks, Snowflake, Spark, Hadoop, etc.
Strong experience in creating data architecture and storage schemas required
Significant working knowledge of SQL and other query languages and tools
Significant experience with building relational database systems strongly preferred
Experience with ETL tools a plus
Knowledge of Python preferred
Software development, data analytics, or data science experience is a plus.
Experience working within a software as a service (SaaS) company preferred.
Experience working within a high growth and/or ambiguous environment, with proven experience to be dynamic preferred.
#LI-REMOTE
We know that potential candidates are often less likely to apply to a position if they don't match 100% of the job qualifications. Don't let that be why you miss out on this opportunity! We encourage you to apply if you can demonstrate many of these skills and competencies.
Loyal to our employees
We are a remote-friendly company! We encourage you to apply from anywhere in the United States. We also believe in a work/life balance that fulfills you while you're here and supports you when you're not. We built our benefits package to prove that we're committed to you having everything you need (including a little fun). Here is what we offer full-time employees:
Flexible paid time off, sick and personal days
At least one holiday per month (sometimes, more!)
Full health, dental, and vision insurance - Loyal pays the premium for all employees!
One Time Home Office Setup Stipend For Remote & Hybrid Roles
Monthly Internet Stipend for Remote & Hybrid Roles
Long term & short term disability
401[k] plan
16 Weeks Paid Parental Leave
2 Volunteer days per year
Matching Gift Program
Participation Grant Program
Annual Travel/Team Events up to twice per year (post-COVID)
Our Commitment
We believe that the key to Loyal's success is you. Your unique background, life experience, knowledge, self-expression, and talent make you uniquely you. Who you are, what you have experienced, and how you think inspires us to be innovative and bold.
Loyal is an equal opportunity employer. We hire great people from a wide variety of backgrounds, not just because it's the right thing to do, but because it makes our company stronger. We welcome the unique contributions that you can bring in terms of your education, opinions, culture, ethnicity, race, ancestry, sex, gender identity and expression, national origin, citizenship, marital status, age, languages spoken, veteran status, color, religion, disability, sexual orientation, and beliefs.
We consider qualified applicants regardless of criminal histories, consistent with legal requirements.
Further, consistent with applicable federal and state law, Loyal provides reasonable accommodations when requested by qualified applicants or employees with disabilities, unless doing so would cause an undue hardship. Loyal's policy regarding requests for reasonable accommodation applies to all aspects of employment, including the application process. If you require a reasonable accommodation in connection with the application process, please contact the Talent Acquisition Department at talentacquisition@loyalhealth.com.
E-Verify
This employer participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S. If E-Verify cannot confirm that you are authorized to work, this employer is required to give you written instructions and an opportunity to contact Department of Homeland Security (DHS) or Social Security Administration (SSA) so you can begin to resolve the issue before the employer can take any action against you, including terminating your employment. Employers can only use E-Verify once you have accepted a job offer and completed the I-9 Form.
COVID-19 Vaccinations
Consistent with Loyal policy, candidates performing in-person work will be required to show proof of being fully vaccinated against COVID-19 upon commencing employment. Reasonable accommodations will be considered on a case-by-case basis for exemptions to this requirement in accordance with applicable law. If you require a reasonable accommodation to Loyal's vaccination requirement, please contact the People Department at people@loyalhealth.com.",#N/A,51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2015,$5 to $25 million (USD)
"Tipico - North America
4.0",4.0,"Denver, CO",Data Engineer,"Company Description

Tipico is an energetic, passionate, bold tech company, and we love sports! Tipico has recently expanded to the US market, establishing offices in Hoboken, New Jersey and Denver, Colorado. We are driven by our core values: passion, progress and trust. Our goal is to build the best possible mobile sports betting product in the industry, in order to amplify the emotions of millions of sports fans every day. Let’s Make This Interesting ™
Please note: this role is located in our Denver, CO office; we work off a hybrid model and come into the office 2 days per week.

Job Description

The Data Engineer will report directly to the Data Engineering Manager and work together with rest of the data engineers, BI analysts, and product teams. The position holder ensures this is done by doing the following:
Create and maintain both batch (ETL/ELT) and real-time data pipelines and architecture
Ensure all data provided by Data Platform is of the highest quality and is delivered in a timely manner and in line with agreed SLAs
Assemble large and complex data sets that meet functional and non-functional business requirements
Identify, design, and implement internal process improvements, including automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies
Collaborate with stakeholders including data architects, Executive, Product, Data and IT team members from the beginning to the delivery of a project
Build a scalable, modern, and trusted data ecosystem, making sure it is responsive, available and in line with business requirements
Take part in sprints, workloads, and deliverables through agile methodologies
Manage quality assurance, verification of accuracy and consistency of data, specifically KPI measures sent to the business
Thrive working in a fast-paced and challenging environment
Open to continuous learning individually and with the team
Comfortable with being challenged / seeking challenge
Willing to work flexible hours if needed

Qualifications
3+ years in Data Platforms / Datawarehouse / BI experience
Must have experience in successfully implementing data-centric applications, such as data warehouses, operational data stores, and data integration projects
Strong analytical and problem-solving skills that balance with creative approaches
Ability to learn new and complex concepts quickly
Strong experience with large-scale production databases and SQL
Solid understanding of cloud data services: AWS services such as S3, Athena, EC2, RedShift, EMR, EKS, RDS and Lambda
Proficient with one or more big data & new data technologies such as DBT, NIFI, Airflow, Kafka
Experience with data modelling
Experience with ETL development (extractions, data load, aggregation, transformation)
Knowledge in working with time-series/analytics databases such as Elasticsearch
Understanding of containerization and orchestration technologies like Docker/Kubernetes
Must be able to take ownership of projects with a high level of attention to detail and priority
Team player with a hands-on approach and attitude
Must be comfortable with on call rotation
Nice to have:
Familiarity with Marketing tooling and/or CRM systems is preferred
Familiarity with using business intelligence tools such as Domo, Tableau or Looker
Knowledge about and / or experience with sports betting
Background in digital companies, online, gambling, gaming, e-commerce, marketing, or mobile apps
Familiarity with one scripting language, preferably Python

Additional Information

What's in it for you:
Work in an environment where you, your work and ideas matter and have an impact.
You will be a be part of the newest and up and coming USA online iGaming and sports betting companies in the market.
Work in a new and young business with high growth potential.
Build your own success story together with us.
Work with self-organized, self-responsible and entrepreneurial employees.
Start-up feeling, backed-up by a leading European sports betting house. We are a high-volume business and are taking off in the US!
Competitive salary, Medical, Vision, Dental Benefits, Unlimited PTO, and more
Salary for this position is $120k+ & annual performance bonus","$120,000 /yr (est.)",201 to 500 Employees,Company - Private,"Arts, Entertainment & Recreation",Gambling,2004,Unknown / Non-Applicable
"Barracuda Networks Inc.
3.6",3.6,"Alpharetta, GA",Data Engineer,"Job ID: 23-579

Come Join Our Passionate Team! At Barracuda, we make the world a safer place. We believe every business deserves access to cloud-enabled, enterprise-grade security solutions that are easy to buy, deploy, and use. We protect email, networks, data and applications with innovative solutions that grow and adapt with our customers’ journey. More than 200,000 organizations worldwide trust Barracuda to protect them — in ways they may not even know they are at risk — so they can focus on taking their business to the next level.
We know a diverse workforce adds to our collective value and strength as an organization. Barracuda Networks is proud to be an Equal Opportunity Employer, committed to equal employment opportunity and equitable compensation regardless of race, gender, religion, sex, sexual orientation, national origin, or disability.
Envision yourself at Barracuda
We currently have an exciting new position open for a Data Engineer. This person will assist in driving technology innovation for engineering projects and work on architecture of AWS, Elastic and Kubernetes. They will also work closely with the Technology Architect to create and drive the Architecture Review Board.
What you’ll be working on
Determine internal / external customer data needs and identify system specifications
Analyze the needs of large systems and breaking them down into smaller manageable parts
Responsible for designing the journey of data from various sources to the database and everything in between
Plan and design the structure of data, brainstorm with team
Analyze best technology products and systems for internal teams
Help provide build vs buy decisions on data architecture
Communicate data requirements to engineers; explain data architecture to them and provide assistance throughout SDLC
Choose / maintain suitable software, hardware and integration methods
Help resolve technical problems as and when they arise
Review testing procedures to ensure data performant and error free
Ensure that database satisfy quality standards and procedures. Setup such standards.
Work with senior IT personnel to devise plans for future IT requirements of the organization
Produce progress reports on key infrastructures - Elastic and Data Pipelines
What you bring to the role
Bachelor Degree or Post Graduate in Computer Science, or relevant work experience
5+ years of experience in an Engineering / Operations heavy organization
1+ years of experience as a Data Analysis and Data Design responsible for data architecture of a SaaS application
Experience in understanding and designing data sharding in a distributed database
Some Hands-on experience in software development and system administration
Knowledge of strategic IT solutions
Familiarity with programming languages like JavaScript or Java or Python
Familiarity with Linux OS and Windows alike
Experience in cloud computing and cloud Technologies (AWS, GCP, Azure)
Excellent communication skills – you should be adept at listening to, understanding, and explaining key concepts to managerial and technical resources.
Good Anticipation and problem-solving skills
Ability to prioritize and manage time – responsible for multiple streams of work at the same time
Relationship building - Ability to form a good rapport with internal and external stakeholders
In-depth knowledge about quality standards and security best practices

Preferred Skills:
Expert level Understanding of Elastic eco system - Elasticsearch, Logstash, Kibana, Filebeats
Understanding of secure tunnel communication using VPN
Experience designing data at scale
AWS experience is preferred in Cloud Technologies
What you’ll get from us
A team where you can voice your opinion, make an impact, and where you and your experience are valued. Internal mobility – there are opportunities for cross training and the ability to attain your next career step within Barracuda.
Equity, in the form of non-qualified options
High-quality health benefits
Retirement Plan with employer match
Career-growth opportunities
Flexible Time Off and Paid Time Off benefits
Volunteer opportunities
#LI-Remote","$98,161 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,$100 to $500 million (USD)
"Merkle
3.7",3.7,,Data Engineer,"Company Description

We are Dentsu International. We innovate the way brands are built. That means we do things differently so they’re better than before. In this way, we make our clients’ most important marketing assets—their brands—win in a changing world.
Dentsu International is a modern marketing solutions company. Our mission is to help clients navigate, progress, and thrive in a world of change. Businesses rely on our integrated network of agencies and specialized practices to champion meaningful progress through creative, media, commerce, data, and technology. Part of Dentsu Group, our global network comprises 66,000 diverse people in 143 countries, who are dedicated to teaming for growth and good. Follow us on Twitter @DentsuUSA and visit dentsu.com/us.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.

Job Description
Develop onboarding process to receive and process information related to onboarding a new subscriber
Convert Snowflake Stored Procedures to Python Scripts directly in Airflow
Develop metadata driven solutions for managing data processing and cadence
End to end automation development and testing
Building and maintaining stored procedures and scripts in Snowflake Data Cloud
Building and maintaining complex SQL queries for ETL/ELT processing
Develop Dashboard for data processed for audit and governance needs
Follow Agile methodologies to deliver iterative feature sets rapidly
Maintain codebase using source control methodologies, Git
Collaborate with business analysts, data analysts, and other developers to deliver powerful new features
Coordinate with teams as part of a larger data-sharing system
Contribute to a collaborative, positive, and enjoyable environment for your operations team
Performing quality assurance and testing
Writing user and technical documentation such as runbooks

Qualifications
Strong communication skills both verbal and written
Strong understanding of conditional programming logic
Knowledge of software engineering and development methodologies, techniques, and tools; includes Issue Tracking (JIRA) Software Development Lifecycle SDLC), and Continuous Integration / Continuous Development (CI/CD)
Experience working with large databases and data warehouses utilizing both relational and dimensional design concepts
Eagerness to learn new technologies, programming languages and platforms to solve complex data challenges
Experience with Python required
Experience with SQL required
Experience with Snowflake Data Cloud is highly desirable
Experience with Amazon Web Services (AWS) a plus
Experience with Airflow (or other orchestration) a plus
Experience in database marketing is a plus

Additional Information

Perks:
Great compensation package
Comprehensive healthcare plans
401(k) with employer match
Flexible time-off
16 weeks paid parental leave
Additional Information
The anticipated salary range for this position is $94,000-$146,000. Salary is based on a range of factors that include relevant experience, knowledge, skills, other job-related qualifications, and geography. A range of medical, dental, vision, 401(k) matching, paid time off, and/or other benefits also are available. For more information regarding dentsu benefits, please visit dentsubenefitsplus.com
Employees from diverse or underrepresented backgrounds encouraged to apply.
Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact recruiting@dentsuaegis.com if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.
About dentsu
Dentsu is the network designed for what’s next, helping clients predict and plan for disruptive future opportunities in the sustainable economy. Taking a people-centered approach to business transformation, dentsu combines Japanese innovation with a diverse, global perspective to drive client growth and to shape society www.dentsu.com.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.
Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact your recruiter if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.","$120,000 /yr (est.)",5001 to 10000 Employees,Company - Private,Media & Communication,Advertising & Public Relations,1971,$500 million to $1 billion (USD)
Diamondpick,#N/A,"Alpharetta, GA",Java Software Engineer with Big-Data,"About us
There is someone out there who fits your requirements like a glove! We are an innovative Talent Solutions company with a vision to build talent supply chain models without compromise for the technology industry.
We are a new-age talent solutions company with a vision to build an enterprise recruitment model without compromise. We love to solve complex talent challenges and strive to be an enabler of business success for our clients using data, technology and our team’s vast experience in the technology market.
Senior Java Developer with Big data
Alpharetta, GA (Hybrid Work from Day1)
Job Description:
The ideal candidate must possess strong background on frontend and backend development technologies.
The candidate must possess excellent written and verbal communication skills with the ability to collaborate effectively with domain experts and technical experts in the team.
Responsibilities:
As a Java Senior Developer, you will
Maintain active relationships with Product Owner to understand business requirements, lead requirement gathering meetings and review designs with the product owner
Own his backlog items and coordinate with other team members to develop the features planned for each sprint
Perform technical design reviews and code reviews
Be Responsible for prototyping, developing, and troubleshooting software in the user interface or service layers
Perform peer reviews on source code to ensure reuse, scalability and the use of best practices
Participate in collaborative technical discussions that focus on software user experience, design, architecture, and development
Perform demonstrations for client stakeholders on project features and sub features, which utilizes the latest Front end and Backend development technologies
Requirements:
6+ years of experience in Java/JEE development
Skills in developing applications using multi-tier architecture
Knowledge of google/AWS cloud
Java/JEE, Spring, Spring boot, REST/SOAP web services, Hibernate, SQL, Tomcat, Application servers (WebSphere), SONAR, Agile, AJAX, Jenkins..etc
Skills in UML, application designing/architecture, Design Patterns..etc
Skills in Unit testing application using Junit or similar technologies
Good communication skills
Leadership skills
Provide overlap coverage with onsite/customer teams
Capability to support QA teams with test plans, root cause analysis and defect fixing
Strong experience in Responsive design, cross browser web applications
Strong knowledge of web service models
Strong knowledge in creating and working with APIs
Experience with Cloud services, specifically on Google cloud
Strong exposure in Agile, Scaled Agile based development models
Familiar with Interfaces such as REST web services, swagger profiles, JSON payloads.
Familiar with tools/utilities such as Bitbucket / Jira / Confluence
Job Types: Full-time, Contract, Temporary
Pay: From $60.00 per hour
Benefits:
Flexible schedule
Compensation package:
Hourly pay
Experience level:
8 years
Schedule:
8 hour shift
Day shift
Work Location: Hybrid remote in Alpharetta, GA 30005",$60.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"MyCare Medical Group
4.1",4.1,"Lutz, FL",Data Engineer,"Job Summary
The Data Engineer will design, develop, optimize, and maintain data architecture and pipelines that adhere to ELT principles and business goals.
Job Responsibilities:
Designs, develops, optimizes, and maintains data architecture and pipelines that adhere to ELT principles and business goals.
Solves complex data problems to deliver insights that helps business achieve its goals.
Creates data products for engineer, analyst, and data scientist team members to accelerate their productivity.
Engineers effective features for modelling in close collaboration with business analysts and stakeholders.
Leads the evaluation, implementation and deployment of emerging tools and process for analytics data engineering in order to improve productivity as a team and quality.
Partners with engineers, BI, and solutions architects to develop technical architectures for strategic enterprise projects and initiatives.
Fosters a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions.
Advises, consults, mentors, and coaches other data and analytic professionals on data standards and practices.
Develops and delivers communication and education plans on analytic data engineering capabilities, standards, and processes.
Job Qualifications:
Education and Work Experience: Bachelor’s degree in computer science, information systems, engineering, or a related field; Masters preferred.
3+ years of experience in data engineering role.
Expertise in ELT and data analysis, SQL and at least one programming language (e.g., SQL, Python).
Conceptual knowledge of data and analytics, such as dimensional modelling, reporting tools, data governance, and structured and unstructured data.
Skills: Strong understanding of agile methodologies and experience as a Data Engineer on a cross-functional agile team preferred.
Experience with designing and maintaining data warehouses and/or data lakes with technologies, such as Hadoop, Spark, or distributed databases, like Redshift and Snowflake, and experience with housing, accessing, and transforming data in a variety of relational databases.
Experience with cloud database technologies (e.g., Azure) and developing solutions on cloud computing services and infrastructure in the data and analytics space.
Experience in building data pipelines and deploying/maintaining them with best-in-class tools (e.g., Git and Jenkins).
Familiarity with the Linux operating system.
Basic familiarity with BI tools (e.g., Tableau, Power BI, Looker).
Experience working in data engineering or data architect role.
Candidate Qualities: Focuses on business agility and agile delivery with a “fail-fast, succeed early” mindset and emphasis on measurable outcomes.
Brings a high-energy and passionate outlook to the job and can influence those around them.
Able to build a sense of trust and rapport that creates a comfortable & effective workplace.
Passion for innovation and “can do” attitude.
BENEFITS
Comprehensive benefits package, including Health, Vision, Dental, and Life insurancesFSA and Life Assistance Program (EAP)
401(k) Retirement Plan
Health Advocacy, Travel Assistance, and My Secure Advantage
PTO Accrual and Holidays
#ZIP
Job Type: Full-time
Pay: $125,000.00 - $150,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Employee discount
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Referral program
Relocation assistance
Retirement plan
Tuition reimbursement
Vision insurance
Compensation package:
Performance bonus
Yearly bonus
Experience level:
3 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Lutz, FL 33558: Reliably commute or planning to relocate before starting work (Preferred)
Experience:
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person","$137,500 /yr (est.)",501 to 1000 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2018,Unknown / Non-Applicable
Redzara.us,#N/A,"Atlanta, GA",Azure Data Engineer,"Responsible for developing new and modernizing existing applications to Azure Cloud Platform.
The ideal candidate must demonstrate in-depth knowledge and understanding of Database concepts.
Requirements:
7+ years of experience in Data Warehousing and Business Intelligence
5+ Years in BI development leveraging the Microsoft SQL Server. Needs to be an expert in T-SQL
5+ Years in SSIS and use of Stored Procedures.
1-2 Years in Azure Data Factory, SQL VMs, build and maintain ETL
Ingest and aggregate data from both internal and external data sources to build our data lake
Strong understanding data modeling (i.e., conceptual, logical, and physical model design, experience with Operation Data Stores, Enterprise Data warehouses and Data Marts).
Job Type: Contract
Pay: $43.49 - $57.00 per hour
Compensation package:
1099 contract
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Atlanta, GA 30334: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure Data Factory: 3 years (Required)
Azure: 3 years (Required)
Data warehouse: 1 year (Preferred)
Work Location: Hybrid remote in Atlanta, GA 30334",$50.25 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"shaped.ai Inc.
4.0",4.0,"New York, NY",Data Engineer,"Hello! We’re building the future of content discovery infrastructure with AI — come join us!

Shaped (YC W22) is an API for developers to seamlessly add personalized ranking and recommendation to their products. These frictionless discovery experiences help end-users find what they want faster, and, consequently, grow conversion and engagement business metrics. Behind the scenes, we’re building state-of-the-art ML infrastructure for training and deploying large AI models to power our personalization engine.
We’re a seed stage start-up backed by top-tier investors (e.g. Y-Combinator, Susa Ventures, Tribe Capital etc…) and executives (e.g. Google, Amazon, Uber, Dropbox etc…). Our team comes from Meta, Google, Apple and Uber. We’re a remote team but have a small office in Brooklyn, New York.
We’ve been lucky to have a huge amount of success since our initial launch earlier this year and now looking to expand our real-time data engineering capabilities. We are looking for a data engineer to design, build and optimize Shaped's real-time streaming data ingestion endpoints. You will be a founding engineer working on state-of-the-art infrastructure. As one of Shaped’s early employees you will help shape our product, culture and vision.
This role is for a full-time data engineer at $120k - $200k a year and 0.1 - 0.5% equity.

Responsibilities:
Manage the unification of our batch and real-time data processing pipelines.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Requirements:
Bachelor's in computer science, data science or mathematics related field. Master's degree or PhD will be advantageous.
5+ years of data engineering experience, ideally applied to machine learning use-cases.
Experience with AWS or common cloud technologies
Proficient with Python, SQL.
Extensive knowledge of both offline and online large scale data processing frameworks: Spark, Beam, Flink.
Extensive knowledge with stream technologies like Kakfa or Kinesis.
Excellent written and verbal communication skills.
We’re excited to work with you. Come build the future of AI and discovery with us!","$160,000 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Fracsys Inc,#N/A,"Washington, DC",Data Engineer,"Fracsys Inc is hiring a Data Engineer position. The ideal candidate must have at least 12+ years of industry experience. He or she must be responsible for successful technical delivery of Data Warehousing and Business Intelligence projects using a formal project management methodology.
Responsibilities
Directs, motivates and develops key technical personnel
Provides technical direction across the full lifecycle system development process
Provides strategic planning and comprehensive senior level technical consulting to IT senior management and senior technical staff
Working independently within guidelines
Provides detailed inputs for plans and schedules, including goals, risks, and resource allocation
Conduct diagnostic, troubleshooting steps and present conclusions to a varied audience
comfortable slinging some code to solve a problem
Build Data Marts and Data warehouses to facilitate analytical capabilities across the varied sources of data
Design and implement data patterns on cloud using cloud Data warehouse and data virtualization tools
Evaluate new tools and technologies for target state on cloud
Learn and implement new ETL, BI tools and data warehousing technologies
Qualifications
Minimum twelve (12) years in a Technical Lead or Consulting role leading Information Management related initiatives (system integration, data warehouse build, data mart build, or similar)
MUST have proven leadership experience for team of at least 3 technical resources.
Experience creating Data Marts, Data warehouses on-premise and on cloud for real time and batch processing frameworks
Experience building and optimizing big-data data pipelines and data sets including Postgres, AWS Relational Data Service
extensive hands-on experience building and optimizing data structures for data analytics, data science and business intelligence
Experience building self-service data consumption patterns and knowledge of cloud-based data Lake platforms
Experience wrangling data (structured and unstructured), in-depth knowledge of database architecture
Experience utilizing or leading implementations leveraging ETL tools (Informatica / Talend), BI Reporting tools such as MicroStrategy, Microsoft Power BI, data modeling tools such as Erwin, Oracle, SQL server, NoSQL, JDBC, UNIX shell scripting, PERL and JAVA, XML/JSON files, SAS , Python, AWS cloud-native technologies, S3, Athena, Redshift
Familiarity with the following technologies: Hadoop, Kafka, Airflow, Hive, Presto, Athena, S3, Aurora, EMR, Spark
Ability to drive, contribute to, and communicate solutions to technical product challenges
Ability to roll-up your sleeves and work in process or resource gaps and fill it in yourself
Excellent written and oral communication skills in English
Education
Bachelor’s degree in computer science, Data Analytics, Information Systems, or a related degree or equivalent experience. Master’s degree is preferred.
Job Type: Full-time
Pay: From $96,336.72 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Experience level:
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Washington, DC: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person","$96,337 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Intersec
4.5",4.5,Remote,Senior Data Engineer,"Architect and implement highly scalable data pipeline to streamline migration of data from source data repositories to the centralized data platform
Implement agreed upon data models (e.g., data marts) to enable faster and efficient consumption of data by downstream consumers
Conduct performance tuning to optimize data migration techniques and ensure adherence to identified service-level agreements (SLAs)
Define and implement automated validation checks to ensure data integrity between source and target
Assist in onboarding consumers with existing technology to connect to the platform
Setup predictive alert mechanism for pipeline degradation for IMF pipelines.
Enhance data quality framework for data pipelines and derived datasets
Enhance Data Audit Balance and Control mechanism for data pipelines
Enhance pipelines to be certified copies of authoritative source
Populate metadata for the Enterprise with particular focus on the Data Sources being onboarded to EDP (Technical and Business Glossaries) related to IMF Modernization Programs
Validate correct lineage for datasets
Required:
*8+ years of Databricks, Data Modeling, and/or Metadata management experience
* Familiarity with architecting and developing real and near-real-time solutions
* Bachelor's Degree in Engineering, Mathematics, computer Science, Statistics or other quantitative discipline, or 4 years equivalent professional experience
* US Citizenship and ability to gain a public trust
Preferred:
* Familiarity with IRS systems and ETL pipelines
* Ability to work independently and manage multiple tasks
* IRS MBI Clearance with IRS Laptop and Informatica Installed
Job Type: Full-time
Salary: $70.00 - $75.00 per hour
Schedule:
Monday to Friday
Application Question(s):
Are you a US/GC card holder?
How many years of Databricks, Data Modeling, and/or Metadata management experience do you have?
Work Location: Remote",$72.50 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,Unknown / Non-Applicable
"Trella Health
4.8",4.8,"Philadelphia, PA",Senior Data Engineer,"At Trella Health, we are passionate and committed to our mission – empowering meaningful change in healthcare. Since our founding in 2015, we continue to grow our team, enhance our solution and services offerings, accelerate into new markets, and expand our customer base. We are rapidly growing and are looking for new Trellavators to join our team!
""What is a Trellavator?"" you ask. Innovate and elevate is the name of our game! We go above and beyond to collaborate with and support each other – we believe that when a colleague or customers succeeds, we succeed. By learning from others, building on our successes, and taking risks, we constantly raise the bar – continuous improvement is in our DNA. Our word is paramount, we keep our commitments, and we always follow through. We have a strong, reliable support system that fuels growth, collaboration, and passion – and together, we create a positive environment where everyone at Trella Health, including the customers we support, can thrive. Are you ready to learn more about the opportunities with our team? Trell-yeah you are!
Position Overview:
The Senior Data Engineer will be responsible for designing, implementing, and maintaining an effective data pipeline for Trella Health's applications. This position is responsible for turning data concepts into effective data models that are efficient and scalable. The Senior Data Engineer will ensure a smooth data release and deployment process throughout the data development lifecycle.
Successful candidate will be a highly organized, self-starter that can roll up their sleeves and solve problems efficiently. Ability to document procedures, manage time, and have a knack for optimizing data flows.
Location: Atlanta HQ; Raleigh, NC; Nashville, TN; Philadelphia, PA; Remote considered
Reports to: Manager, Data Engineering
As a member of the Data team at Trella, you will:
Champion a comprehensive data vision and operating principles that improve data quality, consistency, accessibility, decision making, use, and stewardship.
Manage data pipeline by developing processes for data input, transformation, and storage
Act as an interface between the Data Science team and Application Developers
Architect and implement front-end (application layer) and back-end (ETL layer) database technologies in a highly customized data environment
Develop automated processes for moving data between systems
Implement ETL processes that transform source data into functional data stores consumed by front-end applications
Scale data pipeline by utilizing the most efficient tools and technologies
This job might be a fit for you if you have:
Bachelor's degree in Computer Science, Engineering, Mathematics, or other technical discipline (proven professional experience will be considered in lieu of a technical degree)
2+ full years of recent advanced Snowflake development experience
Experience manipulating healthcare data
In-depth knowledge of Amazon Web Services (AWS) and Platform as a Service (PAAS)
Demonstrable experience with Snowflake utilities including SnowSQL & Snowflake Javascript
Analyze all aspects of the Snowflake Query Engine to improve bottlenecks that may exist
Ability to automate data ingestion from Amazon S3, FTP, and other technologies
Experience with scripting languages, extensive Python experience, Node.js experience a plus
Working knowledge of Bitbucket and Git
Experience managing and loading data in AWS RDS (MySQL/MariaDb)
Ability to optimize MySQL query execution plans for robust performance
Technical Job Requirements (Nice to Have)
Experience manipulating healthcare data including claims, ADT data
Foundational knowledge of CI/CD platforms, preferably TeamCity
Development and consumption of API services
Experience with data science tools (Jupyter, SageMaker, R)
Understanding of Atlassian suite of tools, including Jira and Confluence
Soft Skill Job Requirements (Nice to Have)
Strong analytical skills and ability to rapidly switch contexts
Excellent communication and problem-solving skills
Sole contributor that is willing to independently uncover and improve processes that are inefficient or sub-optimal
Track record of implementing creative solutions to complex problems
Experience working in a nimble, fast-paced, start-up environment
Knowledge of HIPPA, PHI, PII policies
About Trella Health:
Trella Health provides unmatched, actionable market intelligence to post-acute care and value-based care providers of all sizes. Our industry leading analytics paired with CRM and EHR integration workflows positions us as the most advanced sales enablement platform for the post-acute care market. One of only a few companies to be deemed both a Qualified Entity by CMS and an Innovator under its Virtual Research Data Center Program, Trella has elite access to billions of performance metrics and referral data to enable competitive positioning across the market. Its standardized insights, representing 90% of all lives 65+ U.S. population, help customers identify, engage, and manage critical relationships and advance their organizations with certainty.
Since launching our first product in 2016, we have experienced tremendous growth and have highlighted a few key data points to get an at-a-glance view of Trella:
Over 700 customers including 10 of the top 10 post-acute providers and 4 of the top 10 health systems
Over 120 team members and growing
Recognized as one of Modern Healthcare's ""Best Places to Work"" in consecutive years and were ranked in the Top 50 startups in the US by LinkedIn
Headquartered in Atlanta, GA with remote hubs in Nashville, Philadelphia, Raleigh, and the Philippines
The Trellavator Experience:
At Trella Health, you can expect to join a welcoming team that truly embodies our core values. Our collaborative culture is anchored by trust, transparency, and inclusion — and we also have a lot of fun. No matter where you work, at Trella Health you can expect an awesome team, frequent virtual gatherings, engaging events, casual attire, and more.
We offer competitive salaries with a comprehensive benefits package to all employees and provide an environment that fosters work-life harmonization with unlimited PTO, company-observed holidays, and Summer Fridays. As we continue to see exponential growth, our goal is to continue to put team members first and strive to offer our team members the best culture and benefits possible. Some of the benefits we provide are:
Health, Dental, Vision & Voluntary Benefits
Competitive Salary & Bonus Plans
401k Retirement Savings
Unlimited PTO & 10 Paid Holidays
Flexible Work Hours
Equity Shares
Paid Leave Programs
Marketplace for discounted retail and entertainment
Equal Opportunity Employer
Trella Health is an equal opportunity employer. All persons will receive consideration for employment without regard to race, color, gender, sexual orientation, gender identity or expression, religion, national origin, marital status, age, disability, handicap, veteran status, genetic information, or any other protected status as recognized by federal, state, or local laws.","$113,846 /yr (est.)",51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2015,Unknown / Non-Applicable
Proits Hub LLC,#N/A,Remote,Data Analytics Engineer,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote
Speak with the employer
+91 9417599933",$60.00 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Radiant System,#N/A,"Pleasanton, CA",Data Engineer,"First Priority: Pleasanton, CA –
Second Priority for a superstar: Plano, TX
Experience in developing and deploying data pipelines, preferably in the Cloud
Experience working with Spark, Databricks, and other relevant technologies in Microsoft Azure cloud
Experience in productionizing and deploying Big Data platforms and applications
Expert in writing SQL statements
Job Type: Contract
Application Question(s):
Are you willing to work on W2?
Work Location: In person",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Atalan Tech,#N/A,Remote,Data Engineer,"Atalan Tech is a start-up with a mission to mitigate the impact of clinician burnout and turnover in our nation’s health system. We are expanding our team and looking to hire the critical role of Data Engineer. The Data Engineer will work closely with the Chief Technology Officer, Chief Scientific Officer (UCLA Ph.D.), Chief Product Officer (Oxford Ph.D.), and Chief Executive Officer to deliver actionable insights from data in a variety of situations.
We are a small, close-knit team. This is an excellent opportunity for someone to join a mission-oriented, early-stage company at the ground floor and grow with us. There will be plenty of room for growth and creativity, and opportunities to tackle challenges that present one of the toughest problems in our society.
Requirements
Strong hands-on experience of designing, developing and managing data pipelines
Strong coding skills in Python (required) and SQL (required) and R (strongly preferred)
Experience building and maintaining data pipelines on AWS using EC2, EBS, Redshift, Postgres, S3, Data Pipeline, Kinesis, Lambda, etc.
Thorough understanding and experience with task management tools (Airflow, NiFi, etc.)
Knowledge of data management best practices for ETL, data governance, data warehousing, data modeling and data science
Basic knowledge of Linux (Redhat preferred) administration
Minimum B.S in Computer Science, software engineering or related field (or equivalent in related professional experience)
Strong communication skills
Strong problem-solving/critical thinking skills
Ability to work in a fast-paced, dynamic start-up environment
Some knowledge of healthcare, health tech, or healthcare data analytics (preferred)
Typical Tasks and Responsibilities include:
Working closely with CTO and CSO to build data pipelines for machine learning
Assisting the CPO in managing and supporting analysis of enterprise data to drive better products and solutions
Managing company’s data infrastructure on AWS (Postgres/RDS, Redshift, S3, etc.)
Synthesizing multiple data sources, such as internal medical records and HR data
Integrating with client systems including EHR (Epic, Cerner), HRIS, flat files, etc.
Working closely with other key members on other projects
Developing and designing data security standards and policies in line with industry best practices
About Us:
Atalan Tech’s mission is to improve the wellness of clinicians and their patients. We blend labor economics, machine learning, and clinical psychology with a team from Harvard, Yale, and UCLA. Atalan Tech is looking to contribute innovative and sustainable solutions to one of the most pressing problems in the healthcare industry. Building upon our proof-of-concept success, we have recently secured multiple engagements with nationwide healthcare organizations and have successfully raised institutional funding to expand our reach.
Contact Us:
If you’re interested in this position, please send us your CV/resume and cover letter.
Job Type: Full-time
Pay: $108,448.20 - $130,604.27 per year
Experience level:
2 years
Schedule:
Monday to Friday
Application Question(s):
Do you know the role is full time?
Can you start within the next month?
What are your salary expectations?
Have you held the title of a data engineer? If not, what experience do you have as a data engineer? Please be very specific.
Have you released Python code to a production environment in the last year?
Experience:
Data analytics: 2 years (Required)
Work Location: Remote","$119,526 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Hiscox
4.1",4.1,"Atlanta, GA",Senior Data Engineer,"job description:
You will be a senior member of our US IT Team data engineering practice. You will be an early member of this team and will work closely with the Director of Data Engineering to help establish it. We will have our work cut out for us delivering planned work, building the practice and evolving our technical landscape. This is an exciting opportunity for the right individual to join at the very beginning and grow with the new team.
As a senior you will be expected to lead, mentor others and deputise for the director of data engineering when required. Yoi will also continue to contribute as a hands-on engineer. You will have the knowledge, best earned through 5+ years of experience, to truly inhabit this senior role in an ambitious team.
What you’ll be doing:
Working with business stakeholders and our delivery team to understand high value business problems that can be solved through the application of data processing and analytical systems.
Developing, expanding and evolving our existing databases and ETL pipelines.
Working with architects to design, build and support a transformational cloud-based data platform for the business.
Being a core and professional member of the new data engineering practice
Understand business requirements and help refine into development tasks and estimate their complexity.
Research, evaluate and adopt new technologies with a right tool for the job mentality.
Focus on both speed of delivery and quality, with suitable pragmatism – ensuring your solutions are always appropriate and not too complex or over-engineered.
Quick progression of projects from proof-of-concept to post-production stage.
Communication and presentation of ideas to colleagues in all parts of the wider tech team.
Participating in code reviews for the data engineering practice.
Act as a servant leader in our practice – ensuring team members have
person specification:
Knowledge of, or ability to rapidly adopt our core languages for data engineering – SQL and/or Python.
Experience working using a modern DevOps approach. You should not just have written data code but also the CI/CD pipelines necessary to test and deploy that code in a professional environment.
A robust understanding of core data engineering topics – ETL vs ELT, structured and unstructured data, data quality and data governance.
Ability to contribute to all aspects of a solution – design, infrastructure, development, testing and maintenance.
The ability to design and advocate for technical solutions to business problems.
Effective collaboration with technical and non-technical team members through agile ceremonies – roadmap planning, feature workshops, backlog elaboration, code review.
Understanding of cloud technology and significant hands-on experience working with it. You must understand how cloud-native solutions must be built differently from traditional ones.
Track record of taking initiative and delivering projects end-to-end; clear evidence of being self-driven and motivated
Immense curiosity, high energy and desire to go the extra mile to make a difference.

Desirable Skills
Beyond the required skills we are open to individuals of diverse talents. Experience with additional technologies, data science knowledge or a business background are all valued. We want to know how your unique abilities can contribute to our team.
Our Technology
We operate in a diverse technical landscape and are looking for flexible engineers who can adapt to and use many different tools. We would not expect any engineer to be familiar with the entire tech stack – no engineer can. Instead, we seek people with a good understanding of data structures and algorithms and the ability to apply this knowledge in learning new tools.
We have an existing data landscape involving both MS SQL Server and Oracle databases. We use SSIS for ETL. We will also need to source data from our integration layer – webMethods.
If some of that sounds like it needs bringing up to date, we know! We hope to start building a modern cloud data platform to realise the full value of our data. The technologies we use for that platform are still to be decided. Likely it will be based on Snowflake with Azure Data Factory for ELT. Other parts of our business have had success using Prefect for orchestration and dbt for transformation and we will reuse where it is sensible.
We prepare data for use by analysts working with a variety of tools – Tableau, PowerBI, and even Excel.
We take a DevOps approach and strive for continuous integration / continuous deployment. We use Azure Pipelines to deploy our code. We deploy infrastructure the same way using Terraform and Docker.
Our technology landscape is not fixed. Our engineering and architecture teams drive the technology we adopt.


Salary range $140,000 - $170,000","$155,000 /yr (est.)",1001 to 5000 Employees,Company - Public,Insurance,Insurance Carriers,1901,$1 to $5 billion (USD)
"DiNi Communications, Inc.
3.0",3.0,"New York, NY",Data Center Engineer,"DiNi Communications, Inc. is seeking a Data Center Engineer. This person needs to be a team player with exceptional communication skills. Maintaining the confidentiality of the client's information is of utmost importance.
Required Education and Experience
· At least 8 years of experience as a large data center technician
· Knowledge of Networking,
· Ability to lift equipment at least (50lbs) Rack and Stack (Network Switches, Servers, Storage,
Appliances)
· Knowledge of structured Cabling in a Data Center.
· General knowledge of Operating Systems, (Windows, Unix).
· General knowledge of networking in N Tier Environment (App, Web, DMZ,).
· Level 1 and Level 2 troubleshooting in a N Tier Environment.
· Ability to follow and execute commands to start up and shutdown systems.
· Physically travel to and be available at site B to perform technical changes that require a
person at Site B.
· Performing backup and restore activities.
· Asset Management.
· Maintaining document.
· Work with vendor on circuit issues, (WAN and Internet).
· Cable Patch the equipment in the environment.
· Participate in Site B fall-over testing
Preferred Network Engineering Experience
· Knowledge of N Tier networking.
· Certification in Networking (CISCO).
· Certification in various Operating systems (Linux and Windows), Storage, Messaging
(Microsoft).
· Certification in IT Security.
· Knowledge of Databases.
· Experience working with CISCO, EMC, Symantec, HP, Oracle, Bluecoat, MacAfee, and other vendors' equipment and software in a networked environment.
Responsibilities include but are not limited to:
Adding equipment to support agency applications in site B Environment,
Coordinating Site B testing with Site A technical resources.
Monitoring the B environment.
Rack and Stack new and replacement equipment
Troubleshooting and resolving incidents and problems in the
Physically travel to and be available at Site B to perform technical changes that require a person at Site B.
Upgrading and expanding the environment
Performing backup and restore activities
Manage Site B assets
Maintaining document
Cable Patch the equipment in the environment
Participate in Site B fall-over testing.
Job Type: Full-time
Pay: $60.00 - $70.00 per hour
Schedule:
Monday to Friday
Experience:
large data center technician: 8 years (Required)
Networking in N Tier Environment (App, Web, DMZ,): 3 years (Required)
Ability to lift equipment at least (50lbs): 1 year (Required)
structured Cabling in a Data Center: 3 years (Required)
Level 1 and 2 troubleshooting in a N Tier Environment: 3 years (Required)
Performing backup and restore activities: 3 years (Required)
Cable Patch the equipment: 3 years (Required)
Certification in Networking (CISCO): 1 year (Preferred)
Certification in IT Security: 3 years (Preferred)
CISCO, EMC, Symantec, HP, Oracle, Bluecoat, MacAfee: 3 years (Preferred)
Work Location: In person",$65.00 /hr (est.),Unknown,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"ECI - Sacramento
4.4",4.4,"Sacramento, CA",Lead Data Quality Engineer (C),"Senior Lead Data Quality Engineer with Snowflake experience:
12-24 months.
Public sector experience is preferred.
Must have these specific experiences in data management, data integration, data qualify and Lead experience.
Must have Snowflake platform experience.
Project will start end of June.

Mandatory Qualifications:
Must have a bachelor's degree in computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field. Please attach degree(s).
Must have 10 or more years of experience working in Information Technology.
Must have at least seven (7) years or more of work experience in data management disciplines including data integration, modeling, optimization, and data quality, and/or other areas directly relevant to data engineering responsibilities and tasks.
Must have at least five (5) years of experience working in cross-functional teams and collaborating with business stakeholders in support of a departmental and/or multi-departmental data management and analytics initiative.
Must have at least three (3) years of experience in data integration, data warehouse, big-data-related initiatives, development, and implementation.
Must have at least three (3) years of experience in architecture patterns and data integration design principles.
Must have at least three (3) years of experience with database technologies (e.g., SQL, NoSQL, SQL Server).
Must have at least three (3) years of experience with the Snowflake Data Warehouse Platform.
Must have at least three (3) years of experience with extract, transform, and load (ETL) and other business intelligence tools (e.g., SSIS, Azure Data Factory, Power BI, Tableau).

Here are the desired which is best to get as many as possible:
Desired Qualifications:
Possess an advanced degree in computer science (MS), statistics, applied mathematics (Ph.D.), information science (MIS), data management, information systems, information science (post-graduation diploma or related) or a related quantitative field.
Possess Azure Cloud and Snowflake certifications.
Three (3) years working experience in the Azure Cloud platform using Synapse, Azure BLOB Storage/ADLS, Azure Data Factory, and Purview.
One (1) year experience with Azure DevOps and CI/CD pipelines.



About ECI - Sacramento:

Estrada Consulting, Inc. (ECI) delivers technology-enabled services and solutions to clients all over the USA and British Columbia. We provide system integration, custom application development, data warehouse and business intelligence, project management, custom reporting solutions and consulting services to mid-size and large enterprises in all major industries. The Company headquarters is in Sacramento, California, and was established in year 2000. Visit http://www.estradaci.com/ to learn about our projects, managed services, awards and certifications delivering value for a range of businesses and government agencies.",$75.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2000,Unknown / Non-Applicable
"Integration Developer Network LLC
4.9",4.9,Remote,Data Engineer with PL/SQL Background,"Requirements:
Recent PL/SQL development work experience with Oracle 19.
Must have working experience with tools like Airflow, Git.
Must have python coding experience and REST API development experience.
Plus would be experience with microservices, docker, Kubernetes.
Plus would be experience with accounting and financial data
Experience in event driven and cloud based architectures and working knowledge of containerization and streaming technologies.
Detailed work experience on Oracle, SingleStore or IBM I DB2 database, SQL & PL/SQL, including ANSI SQL.
Experience in database analysis, development and maintenance of business applications specific to oil & gas back-office operations. Expertise in various ETL tools such as SQL*Loader, Equalum, Python scripts.
Familiarity with No-SQL databases and pipelines (Elasticsearch, MongoDB).
2+ years’ experience in the design, implementation and support of full life-cycle data engineering projects.
Bachelor's degree in Computer Science, Engineering or other equivalent degree, with 5+ years data engineering experience or Master's degree in Computer Science, Engineering or other equivalent degree with 2+ years data engineering preferred.
High school diploma or GED is required.
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Experience level:
9 years
Schedule:
8 hour shift
Experience:
Python: 8 years (Required)
PL/SQL: 5 years (Required)
ETL: 5 years (Required)
Work Location: Remote",$62.50 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2011,$1 to $5 million (USD)
Kanini,#N/A,"North Druid Hills, GA",Senior Data Engineer,"About Kanini
Kanini provides Agile Software Development, Cloud Computing, Data Science, and Location Intelligence services to public and private organizations. We have successfully served our clients in government, finance, transportation, utility, and software industries since 2003.
Why you should join
Working at Kanini is flexible and personal. We are a highly motivated, collaborative team experimenting with the latest technologies. We are committed to everyone having a healthy work/life balance, and we provide extensive mentorship and training resources to help you succeed.
Kanini is looking for Data Engineer who has a deep experience in Data Engineering, Agile planning, Snowflake, SQL, MySQL, PostgreSQL, Git, AWS Data Tools, Python, Jira
Job Description
The Senior Data Engineer is responsible for processing structured and unstructured data, validating data quality, and developing and supporting data products. The Senior Data Engineer also plays a role in Agile planning, providing advice and guidance, and monitoring emerging technologies.
Responsibilities
Gathers and processes raw, structured, semi-structured, and unstructured data using batch and real-time data processing frameworks.
Understands and enforces appropriate data master management techniques.
Ensures data quality and implements tools and frameworks for automating the identification of data quality issues.
Work with internal and external data providers on data validation providing feedback and making customized changes to data feeds and data mappings.
Understands the challenges that the analytics organization faces in their day-to-day work and partner with them to design viable data solutions.
Recommends improvements to processes, technology, and interfaces that improve the effectiveness of the team and reduce technical debt.
Implements and optimizes data solutions in enterprise data warehouses and big data repositories.
Installs, maintains, monitors, and supports business intelligence, distributed computation, and big data analytics tools.
Provides ongoing support, monitoring, and maintenance of deployed products.
Actively works with less experienced data engineers providing technical guidance and oversight.
Actively participates in the engineering community, staying up to date on new data technologies and best practices and shares insights with others in the organization.
Qualifications:
Bachelor’s degree in computer science or related field and 4 years work experience
Working experience with batch and real-time data processing frameworks
Working experience with data modelling, data access, schemas, and data storage techniques within Snowflake
Working experience in design, development, and implementation of highly scalable, high-volume software systems and components, client-facing web applications, and major Internet-oriented applications and systems
Working experience working with relational databases such as SQL, MySQL, Postgres/PostgreSQL
Working experience with business intelligence tools and platforms
Working experience with data quality tools
Working experience with application lifecycle methodologies (e.g. waterfall, agile, iterative)
Experience with ETL processes and tools
Experience working with Git.
Desirable Qualifications
Scrum Developer Certification or equivalent
Working experience with Sisense platform
Working experience with SQL Server Integration Services
Working experience with AWS data tools (Database Migration Service)
Working experience with Python
Experience working with Jira, Rally or similar tools
Kanini Software Solutions, Inc. does not discriminate in employment matters based on race, gender, religion, age, national origin, citizenship, veteran status, family status, disability status, or any other protected class. We support workplace diversity. If you have a disability, please let us know if there is anything we can do to improve the interview process for you; we’re happy to accommodate.
Kanini Software Solutions, Inc., 25 Century Blvd., Ste. 602, Nashville, TN 37214
Job Types: Full-time, Contract
Pay: $60.00 - $65.00 per hour
Application Question(s):
Current Location?
Work Location: Remote",$62.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"RelMap Consulting
4.8",4.8,"Addison, TX",Senior Data Engineer (Hybrid),"The data engineer entails:
Advanced SQL Server Development
Ability to create and modify views
Ability to create and modify custom functions
Ability to write complex queries, including joins, temp table processing, and common table expressions (CTE’s)
Ability to create and modify stored procedures using TSQL
Ability to analyze and optimize query performance using indices
Ability to use Red Gate’s SQL Compare to perform a schema (AKA “do a manual roll”) between one server an another, and between revision control and a database
Ability to stand-up new database server VM’s in Linux and Windows using VMWare
Ability to backup and restore a database in a Linux and Windows environment
Ability to manage and script automated database jobs
Ability to use PowerShell / Bash to interact with databases from the command line
Addition skills that would be desired but not required
· Experience with Redis
· Experience with MongoDB / CosmosDB
· Experience with Azure and Azure SQL
· C#, JavaScript (NodeJS / ES6), or Python experience
Job Type: Contract
Pay: $70.00 - $80.00 per hour
Compensation package:
Hourly pay
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Addison, TX: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person",$75.00 /hr (est.),1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2010,$1 to $5 million (USD)
Siri info solutions inc,#N/A,"Beaverton, OR",Sr. Data Engineer,"Must Have Skills
PySpark
Python on AWS
Responsibilities:
Understanding of advanced data warehousing concepts, data modeling and extract, transform and load development
Advanced SQL skills with experience querying large datasets from multiple sources and developing automated reporting
Python skills for scripting, data manipulation, custom extract, transform and loads and statistical/regression analysis particularly, as they apply to sales and marketing operations and performance
Job Type: Full-time
Salary: $47.00 - $50.00 per hour
Experience level:
7 years
Work Location: On the road",$48.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"DataArt
4.6",4.6,Remote,Data Engineer,"CLIENT
Our client is a pioneer in US schools’ education since 2000, it is leading the way in next-generation curriculum and formative assessment. They develop a number of solutions and interactive web products for teachers, students and their parents.

The company is technology-driven with a huge number of software engineers involved in product development. They have a very solid approach to technology, employ best practices and processes, with a focus on cutting-edge frameworks, languages and tools.

We invite to the company, not a project
TEAM
There are multiple autonomous and empowered teams who work with their own part of the product and choose their own way of doing things and they expect people to be proactive and good team players.
Responsibilities
Building well-tested and optimized ETL data pipelines for both full and delta extraction
Collaborating with data analysts and learning scientists to store, aggregate, and calculate captured students’ work
Contributing to leading industry data standards, such as Caliper Analytics or xAPI
Improving our deployment and testing automation data pipelines
Requirements
3+ years of professional software development or data engineering experience
Strong CS and data engineering fundamentals
Proven fluency in SQL and a development language such as Python
Experience with the following storages: Snowflake, AWS Storage Services (S3, RDS, Glacier, DynamoDB)
Experience with ETL/BI: Airflow, DBT, Matillion, Looker
Experience with Cloud Infrastructure: AWS Kinesis, Lambda, API Gateway, Terraform
Understanding of ETL/ELT pipelines and Data Warehousing design, tooling, and support
Understanding of different data formatting (JSON, CSV, XML) and data storage techniques (3NF, EAV Model, Star Schema, Data Lake)
BS in Computer Science, Data Science, or equivalent
Experience in education or ed-tech
Proven passion and talent for teaching fellow engineers and non-engineers
Proven passion for building and learning: open source contributions, pet projects, self-education, Stack Overflow
Strong communication skills in writing, conversation",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,1997,$100 to $500 million (USD)
OpenSecrets.org,#N/A,Remote,Snowflake Data Engineer,"About us
Our goal is to is to track the flow of money in American politics and provide the data and analysis to strengthen democracy. Our vision is for Americans to use this knowledge to create a more vibrant, representative and accountable democracy..
Our work environment includes:
Modern office setting
Work-from-home days
Wellness programs
Data Engineer
OpenSecrets is seeking to add a new member to our data team. The Data Engineer will be responsible for helping build and maintain our data systems.
Responsibilities include:
Implementing methods to improve data reliability and quality
Analyzing raw data and implementing that data into our system using data pipelines
Troubleshooting data errors and pipeline issues
REQUIRED QUALIFICATIONS:
Experience working with and analyzing large datasets, equivalent to 2-3 years in a similar position
Proven ability to build complex data pipelines using Snowpipe and SSIS
The ability to work with diverse populations
Experience with MSSQL Server and Snowflake
Advanced knowledge of ANSI SQL
PREFERRED QUALIFICATIONS:
Excellent written and verbal communication skills.
The ability to manage multiple priorities and projects both independently and as a member of a team
Snowflake SnowPro Data Engineer certification
Experience with Apache Spark
Experience with machine learning matching models
Knowledge of and interest in American government and money in politics.
SALARY AND BENEFITS:
We are a fully remote organization and this person can be based anywhere in the U.S.
OpenSecrets offers an excellent benefits package, including full medical, dental, 403(b) match, generous vacation and sick leave, and all federal holidays paid.
TO APPLY:
Please visit this link: https://airtable.com/shrGe2bugeapKVr8a
People of diverse backgrounds are strongly encouraged to apply. OpenSecrets is an equal opportunity employer committed to creating a diverse and inclusive organizational culture. We do not discriminate against candidates and employees because of their disability, sex, race, gender identity, sexual orientation, religion, national origin, age, veteran status, or any other protected status under the law.
ABOUT US: OpenSecrets is the nation's premier research group tracking money in U.S. politics and its effect on elections and public policy. Our mission is to track the flow of money in American politics and provide the data and analysis to strengthen democracy. Our vision is for Americans to use this knowledge to create a more vibrant, representative and accountable democracy.
Job Type: Full-time
Pay: $83,874.82 - $100,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Retirement plan
Vision insurance
Compensation package:
Yearly pay
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Experience:
SQL: 3 years (Required)
Snowflake: 1 year (Preferred)
ETL: 2 years (Required)
Work Location: Remote","$91,937 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Argo Data
2.7",2.7,"Richardson, TX",Senior Data Engineer,"ARGO is a leading provider of software and analytics solutions for both the financial services and healthcare industries. ARGO transforms business processes for financial service providers and healthcare organizations using proven business models and software innovation informed by real customer challenges, breakthrough technology, and rich analytics.
ARGO- Data Engineer II (Markham, ON; Memphis, TN; ...) (theapplicantmanager.com)
JOB SUMMARY
The Data Engineer II is responsible for designing and implementing data solutions to support business needs. The successful candidate will have a strong background in software engineering and database design, as well as experience with working with cloud-based tools and technologies.
EXPECTED WORK AND PERFORMANCE
This section is intended to provide a more detailed overview of what the expectations of the job are to include: work to be performed, expected performance results and how they will be measured, as well as connection to corporate strategy, values, and mission. Job duties and responsibilities include the following. Other duties may be assigned as needed.
Design, build, and maintain data pipelines for data extraction, transformation, and loading (ETL) from various sources.
Ensure data security and privacy protocols are implemented and enforced.
Monitor and optimize data storage and processing performance to ensure high performance and cost-effectiveness.
Collaborate with data scientists, analysts, and other stakeholders to understand their data needs and translate them into technical solutions.
Maintain documentation of data systems, processes, and policies.
Stay current with the latest data tools, technologies, and trends and recommend improvements.
Mentor and train junior data engineers as needed.
QUALIFICATIONS
Bachelor's degree in Computer Science, Software Engineering, or related field.
3-5 years of experience in software engineering and SQL database design.
Proficiency in programming languages such as Python.
Strong understanding of data warehousing, ETL processes, and data modeling.
Experience with cloud infrastructure technologies such as Microsoft Azure.
Excellent problem-solving and analytical skills.
Ability to work well in a team-oriented environment and communicate effectively with both technical and non-technical stakeholders.
PHYSICAL DEMANDS
The candidate will work at our corporate headquarters in Richardson, TX. While performing the duties of this job, the employee is regularly required to stand; walk; sit; use hands to finger, handle, or feel; reach with hands and arms; talk or hear. Specific vision abilities required by this job include close vision, distance vision, color vision, peripheral vision, depth perception, and ability to adjust focus.
ADDITIONAL REQUIREMENTS:
Applicants for U.S. or Canadian based positions with ARGO must be legally authorized to work in the United States or Canada. Verification of employment eligibility will be required at the time of hire. Visa sponsorship is not available at this time for ARGO positions.
Pre-employment background screening will be conducted.
Job Type: Full-time
Pay: $70,745.42 - $141,624.04 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Life insurance
Paid time off
Professional development assistance
Referral program
Relocation assistance
Retirement plan
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Performance bonus
Yearly bonus
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Richardson, TX 75081: Reliably commute or willing to relocate with an employer-provided relocation package (Required)
Experience:
Informatica: 3 years (Required)
SQL: 5 years (Required)
Data warehouse: 3 years (Required)
Python: 3 years (Required)
Azure: 3 years (Required)
Work Location: In person","$106,185 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Computer Hardware Development,1980,$25 to $100 million (USD)
Infosoft,#N/A,"Peachtree Corners, GA",Data Pipeline Engineer - 2173,"Here are the job position details for your review:
Job Title: Data Pipeline Engineer (GStreamer/ DeepStream)
Pay Rate: $90 to 120/Hr.
Duration: 3 Months
Location: Peachtree Corners, GA
This position is for the Building Automation division.
We are looking for Data Pipeline Engineer (GStreamer) to participate in the development of Streaming data pipelines using NVIDIA Deepstream, GStreamer, Python programming, messaging queues, and related tools.
Preference is for candidates located in Atlanta, GA metro area. However, ok with non-local candidates who are open to 75% travel to Atlanta, GA.
Understanding the basics of our architecture/setup to be able to do setup/support for demos, testing, etc.
Basic computer hardware assembly and set-up
Possible set-up/installation of the application(s)
Possible development tasks depending on what the rest of the development team has for them
Develops business software applications, system software, and/or IT solutions on the basis of functional specifications.
Scope of position embraces both the customization or reuse/modification of system software and/or solutions as well as their development from scratch.
Consults with customers in creating functional specifications and work estimates, and supports Make-or-Buy decisions.
Analyses performance gaps of existing software environments and proposes measures to address efficiency problems.
Is also responsible for the implementation of data-driven components supporting data modeling and engineering in the context of data analytics (Data Stewardship).
Skills:
Python
Linux
Docker and Docker Swarm
Zero MQ / a good understanding of messaging
Desired Skills:
NVIDIA Deepstream SDK
Web Sockets
Flask
Protobuf
Possible:
GStreamer (if different than RTSP / H264 / MJPEG)
Job Type: Contract
Salary: $90.00 - $120.00 per hour
Ability to commute/relocate:
Peachtree Corners, GA: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
""Please share your Email ID""
Work Location: One location",$105.00 /hr (est.),Unknown,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Titan America
3.8",3.8,"Roanoke, VA",Data Engineer,"Data Engineer
We are seeking a motivated and detail-oriented Data Engineer to join our team based in Roanoke, Virginia. As a Data Engineer, you will play a crucial role in designing, developing and maintaining scalable data pipelines and transformations for business-critical use cases. In addition, you may take on various Software Development and Data Science responsibilities as we frequently share responsibilities and knowledge across our growing team.
Responsibilities:
Design, develop and maintain scalable data pipelines for extracting, transforming, and loading data from various sources
Collaborate with cross-functional teams to identify data needs and determine the best data solutions
Develop and implement data models to support business requirements and ensure data quality
Ensure the security and privacy of sensitive data by implementing appropriate access controls
Monitor and optimize data pipeline performance to ensure timely and accurate data delivery
Document data pipeline processes, data dictionaries, and data storage solutions
Requirements:
Bachelor's degree in Computer Science, Computer Engineering, or a related technical field
Minimum of five years of professional experience working as a Data Engineer or Software Developer
Strong hands-on experience with data warehouse and transformation solutions, i.e. Domo, Snowflake or similar
Proficient in at least one scripting language such as Python, JavaScript, or R
Understanding of data modeling, data integration and data quality processes
Familiarity with cloud platforms such as AWS, Azure, or Google Cloud Platform
Strong analytical and problem solving skills
Full Stack Software Development experience in a professional setting is highly desired, but not required
This is an excellent opportunity for a driven and collaborative individual to make a significant impact in a dynamic and growing team. If you have a passion for data and a desire to work in a fast-paced and dynamic environment, we want to hear from you!
Success begins with hiring the right people to partner with us as we grow and develop our businesses. People are central to everything we do. It is through their efforts and talents that Titan has been successful for over 100 years.
Titan America is an Equal Employment Opportunity (EEO) / Affirmative Action employer and welcomes all qualified applicants. Applicants will receive fair and impartial consideration without regard to race, sex, color, religion, national origin, age, disability, veteran status, genetic data, or other legally protected status.
Job Type: Full-time
Pay: $75,000.00 - $85,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Referral program
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Roanoke, VA 24019: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Do you currently or will you in the future need sponsorship to work in the United States?
This position requires the successful candidate to be in the office in Roanoke, VA at least 3 days a week. Can you meet this requirement?
This position requires the successful candidates to be a full time, W2 employee. Can you meet this requirement?
Education:
Bachelor's (Preferred)
Experience:
Data Engineering: 5 years (Preferred)
Work Location: Hybrid remote in Roanoke, VA 24019","$80,000 /yr (est.)",1001 to 5000 Employees,Company - Public,"Energy, Mining & Utilities",Mining & Metals,1902,Unknown / Non-Applicable
Atika Technologies,#N/A,"Pittsburgh, PA",Data Engineer,"Role: Data Engineer/PySpark Developer, Pittsburgh, PA - Day One Onsite
Fulltime and Contract(W2)
LinkedIn Must
USC/GC/EADs
Three round of interview: First two on Zoom Video Call, End Client Round will be Onsite on the Client's Location (Pittsburgh, PA)
Job Description:
Python(PySpark), Jupiter hub, airflow, Java
Database knowledge - Oracle / SQL / Data Analytics
Regulatory reporting knowledge preferred
Good communication skills
Open / Ability to learn new technologies
Very good Analytical Skills
Experience in ETL is preferable
Job Types: Full-time, Contract
Salary: $120,000.00 - $130,000.00 per year
Benefits:
Dental insurance
Health insurance
Life insurance
Compensation package:
Bonus pay
Performance bonus
Yearly pay
Experience level:
10 years
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Pittsburgh, PA 15259: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Required)
Spark: 4 years (Required)
Regulatory reporting: 3 years (Required)
SQL: 4 years (Required)
IT: 10 years (Required)
Airflow: 4 years (Required)
Jupiter: 4 years (Required)
Work Location: One location","$125,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"MyCare Medical Group
4.1",4.1,"Lutz, FL",Data Engineer,"Job Summary
The Data Engineer will design, develop, optimize, and maintain data architecture and pipelines that adhere to ELT principles and business goals.
Job Responsibilities:
Designs, develops, optimizes, and maintains data architecture and pipelines that adhere to ELT principles and business goals.
Solves complex data problems to deliver insights that helps business achieve its goals.
Creates data products for engineer, analyst, and data scientist team members to accelerate their productivity.
Engineers effective features for modelling in close collaboration with business analysts and stakeholders.
Leads the evaluation, implementation and deployment of emerging tools and process for analytics data engineering in order to improve productivity as a team and quality.
Partners with engineers, BI, and solutions architects to develop technical architectures for strategic enterprise projects and initiatives.
Fosters a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions.
Advises, consults, mentors, and coaches other data and analytic professionals on data standards and practices.
Develops and delivers communication and education plans on analytic data engineering capabilities, standards, and processes.
Job Qualifications:
Education and Work Experience: Bachelor’s degree in computer science, information systems, engineering, or a related field; Masters preferred.
3+ years of experience in data engineering role.
Expertise in ELT and data analysis, SQL and at least one programming language (e.g., SQL, Python).
Conceptual knowledge of data and analytics, such as dimensional modelling, reporting tools, data governance, and structured and unstructured data.
Skills: Strong understanding of agile methodologies and experience as a Data Engineer on a cross-functional agile team preferred.
Experience with designing and maintaining data warehouses and/or data lakes with technologies, such as Hadoop, Spark, or distributed databases, like Redshift and Snowflake, and experience with housing, accessing, and transforming data in a variety of relational databases.
Experience with cloud database technologies (e.g., Azure) and developing solutions on cloud computing services and infrastructure in the data and analytics space.
Experience in building data pipelines and deploying/maintaining them with best-in-class tools (e.g., Git and Jenkins).
Familiarity with the Linux operating system.
Basic familiarity with BI tools (e.g., Tableau, Power BI, Looker).
Experience working in data engineering or data architect role.
Candidate Qualities: Focuses on business agility and agile delivery with a “fail-fast, succeed early” mindset and emphasis on measurable outcomes.
Brings a high-energy and passionate outlook to the job and can influence those around them.
Able to build a sense of trust and rapport that creates a comfortable & effective workplace.
Passion for innovation and “can do” attitude.
BENEFITS
Comprehensive benefits package, including Health, Vision, Dental, and Life insurancesFSA and Life Assistance Program (EAP)
401(k) Retirement Plan
Health Advocacy, Travel Assistance, and My Secure Advantage
PTO Accrual and Holidays
#ZIP
Job Type: Full-time
Pay: $125,000.00 - $150,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Employee discount
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Referral program
Relocation assistance
Retirement plan
Tuition reimbursement
Vision insurance
Compensation package:
Performance bonus
Yearly bonus
Experience level:
3 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Lutz, FL 33558: Reliably commute or planning to relocate before starting work (Preferred)
Experience:
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person","$137,500 /yr (est.)",501 to 1000 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2018,Unknown / Non-Applicable
"Loyal
3.9",3.9,"Atlanta, GA",Senior Cloud Data Operations Engineer,"Loyal is an organization centered on experience and building a platform that allows consumers to make meaningful decisions when it comes to healthcare. We deeply understand providers, locations, services, appointments, business rules, and moreover, we understand patients - who they are, the preferred method of communication, upcoming appointments, lapsed appointments, outstanding bills, health risks, and more. With this intelligence, our platform fuels highly relevant and personalized experiences across all mediums (website, email, voice…) allowing patients to get healthy, stay healthy, and have a better relationship with the health care provider.
**This is a remote role**
Summary
The Senior Cloud Data Operations Engineer is responsible for delivering Loyal's cloud-based data warehouse. They interface with groups across the organization to ensure we are collecting and storing data in a way that makes it easy to create and consume. They identify best practices for big data management, select the right tooling, create a workable data architecture, and drive an implementation roadmap others can depend on. The Senior Cloud Data Operations Engineer also works closely with our security group to ensure data security and privacy. They also mentor others on the team, providing technical leadership to those they work with.
You will
Create data streams and consolidate data sources into a single cloud-based data warehouse
Architect and catalog data warehouse storage schema
Work with product teams to enable easy data aggregation
Work with data consumers (analysts, data scientists, and AI/ML engineers) to ensure data is easily accessible and consumable
Create and execute a roadmap that meets data storage and availability goals
Ensure data is secure through data analysis, access control, and encryption
You have
Bachelor's degree in computer science, information systems, applied math or related field, or equivalent work experience
Minimum 5 years of experience working with cloud data processing services (AWS, Azure, or GCP).
Minimum 3 year experience with Big Data technologies, such as Databricks, Snowflake, Spark, Hadoop, etc.
Strong experience in creating data architecture and storage schemas required
Significant working knowledge of SQL and other query languages and tools
Significant experience with building relational database systems strongly preferred
Experience with ETL tools a plus
Knowledge of Python preferred
Software development, data analytics, or data science experience is a plus.
Experience working within a software as a service (SaaS) company preferred.
Experience working within a high growth and/or ambiguous environment, with proven experience to be dynamic preferred.
#LI-REMOTE
We know that potential candidates are often less likely to apply to a position if they don't match 100% of the job qualifications. Don't let that be why you miss out on this opportunity! We encourage you to apply if you can demonstrate many of these skills and competencies.
Loyal to our employees
We are a remote-friendly company! We encourage you to apply from anywhere in the United States. We also believe in a work/life balance that fulfills you while you're here and supports you when you're not. We built our benefits package to prove that we're committed to you having everything you need (including a little fun). Here is what we offer full-time employees:
Flexible paid time off, sick and personal days
At least one holiday per month (sometimes, more!)
Full health, dental, and vision insurance - Loyal pays the premium for all employees!
One Time Home Office Setup Stipend For Remote & Hybrid Roles
Monthly Internet Stipend for Remote & Hybrid Roles
Long term & short term disability
401[k] plan
16 Weeks Paid Parental Leave
2 Volunteer days per year
Matching Gift Program
Participation Grant Program
Annual Travel/Team Events up to twice per year (post-COVID)
Our Commitment
We believe that the key to Loyal's success is you. Your unique background, life experience, knowledge, self-expression, and talent make you uniquely you. Who you are, what you have experienced, and how you think inspires us to be innovative and bold.
Loyal is an equal opportunity employer. We hire great people from a wide variety of backgrounds, not just because it's the right thing to do, but because it makes our company stronger. We welcome the unique contributions that you can bring in terms of your education, opinions, culture, ethnicity, race, ancestry, sex, gender identity and expression, national origin, citizenship, marital status, age, languages spoken, veteran status, color, religion, disability, sexual orientation, and beliefs.
We consider qualified applicants regardless of criminal histories, consistent with legal requirements.
Further, consistent with applicable federal and state law, Loyal provides reasonable accommodations when requested by qualified applicants or employees with disabilities, unless doing so would cause an undue hardship. Loyal's policy regarding requests for reasonable accommodation applies to all aspects of employment, including the application process. If you require a reasonable accommodation in connection with the application process, please contact the Talent Acquisition Department at talentacquisition@loyalhealth.com.
E-Verify
This employer participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S. If E-Verify cannot confirm that you are authorized to work, this employer is required to give you written instructions and an opportunity to contact Department of Homeland Security (DHS) or Social Security Administration (SSA) so you can begin to resolve the issue before the employer can take any action against you, including terminating your employment. Employers can only use E-Verify once you have accepted a job offer and completed the I-9 Form.
COVID-19 Vaccinations
Consistent with Loyal policy, candidates performing in-person work will be required to show proof of being fully vaccinated against COVID-19 upon commencing employment. Reasonable accommodations will be considered on a case-by-case basis for exemptions to this requirement in accordance with applicable law. If you require a reasonable accommodation to Loyal's vaccination requirement, please contact the People Department at people@loyalhealth.com.",#N/A,51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2015,$5 to $25 million (USD)
"Apolis
3.9",3.9,"Saint Louis, MO",Data Engineer (Python Focused),"Role: Data Engineer (Python Focused) (W2 ONLY)
Location : St. Louis, MO 63102
Duration: 18 Months
The primary responsibilities of this role will be to work as a part of the data and analytics team of the client. The ideal candidate would have experience using ETL and other data engineering tools in a cloud environment. Additionally, this candidate would have coding and scripting experience creating code to move and manipulate data using tools such in Python such as Spark or Pandas. Microsoft Azure experience is a plus but relevant experience with other cloud-based environments will be considered.
This team uses Microsoft Azure to build and productionize data sets and processes to the product owners and product users and this role will be expected to contribute to those objectives. As a member of this team, you will work in conjunction with other data engineers, platform engineers, solution developers and others to execute and deliver working solutions to the end users.
Technical Skills
High focus on Python
More Programmer/Coder experience, less Cloud experience needed.
Must have experience with data analytics/engineering and cloud technologies and an in-depth working knowledge and understanding of data engineering, design, and integration.
Intermediate or higher level of proficiency with Python Must have the proven ability to learn quickly and apply cloud data engineering principles and skills to new tools and use cases.
Ability to define and take technical ownership of detailed requirements and collaborate effectively with various team members such as Business Analysts and Project Managers and end users.
Excellent communication (verbal and written) and problem-solving skills are essential for this position.
Initiative, Proactive Cooperation, and curiosity are mandatory behavioral skills required for this position.
Experience with several of the following technologies is preferred:
Experience with Microsoft Azure Platform. In particular, the data processing toolsets including Azure Data Factory Pipelines, Azure Databricks, Azure Data Lake Storage.
Experience using Notebooks (Such as Jupyter, DataBricks) also desired.
Experience using code languages such as Python and C#
Experience using query languages such as SQL, MDX and DAX
Experience using scripting languages such as PowerShell, M-Query (Power Query), and Windows batch commands.
Understanding of Agile and DevOps principles for software development and ownership.
Familiar with compliance and controls for software development and deployment.
Education Required
A bachelor's in computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field [or equivalent work experience] is not required but is preferred.
Job Type: Contract
Salary: $60.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Vision insurance
Compensation package:
Overtime pay
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
St. Louis, MO: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Preferred)
ETL: 5 years (Preferred)
Cloud infrastructure: 4 years (Preferred)
Work Location: One location",$65.00 /hr (est.),501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,1996,$25 to $100 million (USD)
WellTrust Medical Group,#N/A,"Charlotte, NC",Data Engineer,"The Data Engineer will assist the senior analyst in complex modeling and projects to ensure their completion within the allotted timeline. Helps track measures of success and develop reports that measure operational and customer metrics. Demonstrates ability to troubleshoot data consumption and data quality problems.

Responsibilities:
Build and maintain reports using My SQL Server
Identify optimal data sources, structure data validation, and cache optimization.
Performs business problem modeling and quantitative analytics including regression analysis, time-series, hierarchical cluster analysis, and data mining.
Helps track measures of success and develop reports that measure operational and customer metrics.
Demonstrates ability to troubleshoot data consumption and data quality problems.
Evaluates and recommend improvements to current models, metrics, and analytic processes to improve operational efficiency and insight.
Design, document, and communicate effective technical solutions meeting end-user requirements
Perform analyses with large amounts of complex data, and uses data visualization tools to communicate results.
Requirements:

Bachelor's degree or equivalent experience in a quantitative field (Mathematics, Statistics, Computer Science, Economics, Finance, Physics, Engineering)
3+ years of professional working experience as a Data Engineer or Analyst
1-3 years of data warehouse experience
Build and maintain reports using My SQL Server
Strong analytical and creative problem-solving skills
Design, document, and communicate effective technical solutions meeting end-user
SQL and Excel Macro development
Strong understanding of various data environments such as SQL, Oracle, Big Query, etc.
Experience with data visualization tools such as Looker Studio, Tableau, D3.js, etc.
Proficient in R, Python, MATLAB, Java, or another programming language.",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Take Command Health
4.9",4.9,"Dallas, TX",Data Engineer,"Company: Take Command
Position: Data Engineer
Salary: $120,000-$130,000 plus bonus
Location: Dallas or Austin, TX
About Take Command
Take Command is a start-up on a mission to improve the healthcare system, starting with health insurance. Pragmatically speaking, we help employers reimburse employees for individual insurance instead of offering a traditional one-size-fits-all group plan. We believe this model can empower employees (when they have the right support) to be savvy healthcare consumers and have a transformative impact on the entire healthcare system.
Let’s be honest—health insurance is usually a confusing, frustrating, and even emotional experience for people. We want to fix that with a new model, great technology, and a superior user experience. We have made a great start, but we need your help to fully realize our long-term vision.
As a part of the dev team, you'll build the framework for both our clients and our team to succeed. You will be a pillar in our company as we grow and make our product the best product on the market!
In this role, you will:
Take ownership for delivering multiple features, their quality and assist team members in delivering their features
Recommend improvements to data quality, processes and code to make it easier to use, manage and maintain
Experience designing and developing data models and data migrations
Experience managing and maintaining ETL or ELT processes (Hevo, BigQuery, Looker, etc.)
Hands-on development experience with SQL, relational database design (ERDs)
Must be able to write SQL & contribute code
Be product-minded; focus not only on what needs to be built but why something needs to be built
You may be a great fit for this role if you have:
3 or more years of professional experience as a data analyst or data engineer
Cloud-Based Data Tools e.g. Hevo, BigQuery, Looker, etc.
Database technologies: SQL, RDBMS / Relational databases (Postgres, Amazon RDS / Aurora, etc.)
Nice to have experience: Python, Java, Typescript or another modern OOP language
Cloud infrastructure: AWS
Source control: Git
Working at Take Command
We’re excited to build a team and culture that reflects our values! We offer competitive pay, health benefits, and equity options to share with this position. We want you to be an owner along with us in revolutionizing healthcare.
A generously funded ICHRA for medical, dental, and vision premiums and medical expenses. You get to use our own product and we think that’s so exciting and rare!
Unlimited personal vacation in addition to regular company holidays.
401(k): 90-day eligibility for 2% match that vests immediately!
Access to LinkedIn Learning.
We have two beautiful offices conveniently located near The Hill (Dallas) and Holly Commons (Austin). The kitchen is well-stocked and we've designed the space to have lots of different areas to work!
Paid parental leave for new parents.
Flexible on where you work – we hope to see you around the office on average 3 days a week especially when white-boarding or building relationships with your colleagues, but you also have the ability to work from home or wherever you'd like when you need to get focus work done.
More About Us
We secured our Series A funding in 2021 and are thrilled to be able to expand our team. Despite being a small startup in a land of health insurance giants, we’re the recognized industry leader for what we do (health insurance reimbursements) and passionate about bringing it to market because we know we can help fix a broken system and improve our clients’ wellbeing and health outcomes.
We’ve been featured in The New York Times, The Wall Street Journal, The Dallas Morning News, and other national healthcare publications and are excited about our growth opportunities.
Take Command knows diversity and inclusion among our teammates is integral to our company’s success and growth. Our vision is to recruit, develop, and retain the best team from a diverse candidate pool.
This has mostly been about us, but we’d love to hear from you-we can’t wait to hear your story!
Take Command is an equal opportunity employer! We celebrate diversity and are committed to creating an inclusive environment for all employees.
Xrygn4NXMr","$125,000 /yr (est.)",1 to 50 Employees,Company - Private,Insurance,Insurance Agencies & Brokerages,2014,Unknown / Non-Applicable
Redzara.us,#N/A,"Atlanta, GA",Azure Data Engineer,"Responsible for developing new and modernizing existing applications to Azure Cloud Platform.
The ideal candidate must demonstrate in-depth knowledge and understanding of Database concepts.
Requirements:
7+ years of experience in Data Warehousing and Business Intelligence
5+ Years in BI development leveraging the Microsoft SQL Server. Needs to be an expert in T-SQL
5+ Years in SSIS and use of Stored Procedures.
1-2 Years in Azure Data Factory, SQL VMs, build and maintain ETL
Ingest and aggregate data from both internal and external data sources to build our data lake
Strong understanding data modeling (i.e., conceptual, logical, and physical model design, experience with Operation Data Stores, Enterprise Data warehouses and Data Marts).
Job Type: Contract
Pay: $43.49 - $57.00 per hour
Compensation package:
1099 contract
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Atlanta, GA 30334: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure Data Factory: 3 years (Required)
Azure: 3 years (Required)
Data warehouse: 1 year (Preferred)
Work Location: Hybrid remote in Atlanta, GA 30334",$50.25 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Intersec
4.5",4.5,Remote,Senior Data Engineer,"Architect and implement highly scalable data pipeline to streamline migration of data from source data repositories to the centralized data platform
Implement agreed upon data models (e.g., data marts) to enable faster and efficient consumption of data by downstream consumers
Conduct performance tuning to optimize data migration techniques and ensure adherence to identified service-level agreements (SLAs)
Define and implement automated validation checks to ensure data integrity between source and target
Assist in onboarding consumers with existing technology to connect to the platform
Setup predictive alert mechanism for pipeline degradation for IMF pipelines.
Enhance data quality framework for data pipelines and derived datasets
Enhance Data Audit Balance and Control mechanism for data pipelines
Enhance pipelines to be certified copies of authoritative source
Populate metadata for the Enterprise with particular focus on the Data Sources being onboarded to EDP (Technical and Business Glossaries) related to IMF Modernization Programs
Validate correct lineage for datasets
Required:
*8+ years of Databricks, Data Modeling, and/or Metadata management experience
* Familiarity with architecting and developing real and near-real-time solutions
* Bachelor's Degree in Engineering, Mathematics, computer Science, Statistics or other quantitative discipline, or 4 years equivalent professional experience
* US Citizenship and ability to gain a public trust
Preferred:
* Familiarity with IRS systems and ETL pipelines
* Ability to work independently and manage multiple tasks
* IRS MBI Clearance with IRS Laptop and Informatica Installed
Job Type: Full-time
Salary: $70.00 - $75.00 per hour
Schedule:
Monday to Friday
Application Question(s):
Are you a US/GC card holder?
How many years of Databricks, Data Modeling, and/or Metadata management experience do you have?
Work Location: Remote",$72.50 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,Unknown / Non-Applicable
"DataArt
4.6",4.6,Remote,Data Engineer,"CLIENT
Our client is a pioneer in US schools’ education since 2000, it is leading the way in next-generation curriculum and formative assessment. They develop a number of solutions and interactive web products for teachers, students and their parents.

The company is technology-driven with a huge number of software engineers involved in product development. They have a very solid approach to technology, employ best practices and processes, with a focus on cutting-edge frameworks, languages and tools.

We invite to the company, not a project
TEAM
There are multiple autonomous and empowered teams who work with their own part of the product and choose their own way of doing things and they expect people to be proactive and good team players.
Responsibilities
Building well-tested and optimized ETL data pipelines for both full and delta extraction
Collaborating with data analysts and learning scientists to store, aggregate, and calculate captured students’ work
Contributing to leading industry data standards, such as Caliper Analytics or xAPI
Improving our deployment and testing automation data pipelines
Requirements
3+ years of professional software development or data engineering experience
Strong CS and data engineering fundamentals
Proven fluency in SQL and a development language such as Python
Experience with the following storages: Snowflake, AWS Storage Services (S3, RDS, Glacier, DynamoDB)
Experience with ETL/BI: Airflow, DBT, Matillion, Looker
Experience with Cloud Infrastructure: AWS Kinesis, Lambda, API Gateway, Terraform
Understanding of ETL/ELT pipelines and Data Warehousing design, tooling, and support
Understanding of different data formatting (JSON, CSV, XML) and data storage techniques (3NF, EAV Model, Star Schema, Data Lake)
BS in Computer Science, Data Science, or equivalent
Experience in education or ed-tech
Proven passion and talent for teaching fellow engineers and non-engineers
Proven passion for building and learning: open source contributions, pet projects, self-education, Stack Overflow
Strong communication skills in writing, conversation",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,1997,$100 to $500 million (USD)
"shaped.ai Inc.
4.0",4.0,"New York, NY",Data Engineer,"Hello! We’re building the future of content discovery infrastructure with AI — come join us!

Shaped (YC W22) is an API for developers to seamlessly add personalized ranking and recommendation to their products. These frictionless discovery experiences help end-users find what they want faster, and, consequently, grow conversion and engagement business metrics. Behind the scenes, we’re building state-of-the-art ML infrastructure for training and deploying large AI models to power our personalization engine.
We’re a seed stage start-up backed by top-tier investors (e.g. Y-Combinator, Susa Ventures, Tribe Capital etc…) and executives (e.g. Google, Amazon, Uber, Dropbox etc…). Our team comes from Meta, Google, Apple and Uber. We’re a remote team but have a small office in Brooklyn, New York.
We’ve been lucky to have a huge amount of success since our initial launch earlier this year and now looking to expand our real-time data engineering capabilities. We are looking for a data engineer to design, build and optimize Shaped's real-time streaming data ingestion endpoints. You will be a founding engineer working on state-of-the-art infrastructure. As one of Shaped’s early employees you will help shape our product, culture and vision.
This role is for a full-time data engineer at $120k - $200k a year and 0.1 - 0.5% equity.

Responsibilities:
Manage the unification of our batch and real-time data processing pipelines.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Requirements:
Bachelor's in computer science, data science or mathematics related field. Master's degree or PhD will be advantageous.
5+ years of data engineering experience, ideally applied to machine learning use-cases.
Experience with AWS or common cloud technologies
Proficient with Python, SQL.
Extensive knowledge of both offline and online large scale data processing frameworks: Spark, Beam, Flink.
Extensive knowledge with stream technologies like Kakfa or Kinesis.
Excellent written and verbal communication skills.
We’re excited to work with you. Come build the future of AI and discovery with us!","$160,000 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"DSMH LLC
5.0",5.0,Remote,"Data Engineer (AWS, Python)","Title: Data Engineer
Duration: 12+ Months
Technical Skills
(Required)
Four years or more of experience in data engineering and/or software
development.
Three years or more of experience with development, operations, or
architecture in AWS using Python.
Experience with development and delivery of microservices using serverless
AWS services (S3, RDS, Aurora, DynamoDB, Lambda, SNS, SQS, Kinesis,
IAM)
Strong competency in testing, including unit testing to coverage standards and
integration testing.
(Desired)
Software development experience with object-oriented development and design
patterns
Experience with serverless design in AWS
AWS technical certifications (Developer Associate, Solution Architect Associate)
Familiarity with machine learning and data design to support machine learning
Experience with productionizing machine learning workloads
Experience with ElasticSearch/ELK, Kibana, Grafana, and/or Prometheus
Experience with OpenTelemetry
Soft Skills
(Required)
Verbal and written communication skills, problem solving skills, customer
service and interpersonal skills.
Basic ability to work independently and manage one’s time.
(Desired)
Ability to work collaboratively in a complex, rapidly changing, and culturally
diverse environment.
Ability to clearly communicate complex technical ideas.
Comfortable working in a dynamic environment where digital is still evolving as
a core offering.
Job Type: Contract
Pay: $50.00 - $55.00 per hour
Experience level:
6 years
Schedule:
Monday to Friday
Experience:
AWS: 5 years (Required)
Python: 5 years (Required)
Work Location: Remote",$52.50 /hr (est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,$1 to $5 million (USD)
Kanini,#N/A,"North Druid Hills, GA",Senior Data Engineer,"About Kanini
Kanini provides Agile Software Development, Cloud Computing, Data Science, and Location Intelligence services to public and private organizations. We have successfully served our clients in government, finance, transportation, utility, and software industries since 2003.
Why you should join
Working at Kanini is flexible and personal. We are a highly motivated, collaborative team experimenting with the latest technologies. We are committed to everyone having a healthy work/life balance, and we provide extensive mentorship and training resources to help you succeed.
Kanini is looking for Data Engineer who has a deep experience in Data Engineering, Agile planning, Snowflake, SQL, MySQL, PostgreSQL, Git, AWS Data Tools, Python, Jira
Job Description
The Senior Data Engineer is responsible for processing structured and unstructured data, validating data quality, and developing and supporting data products. The Senior Data Engineer also plays a role in Agile planning, providing advice and guidance, and monitoring emerging technologies.
Responsibilities
Gathers and processes raw, structured, semi-structured, and unstructured data using batch and real-time data processing frameworks.
Understands and enforces appropriate data master management techniques.
Ensures data quality and implements tools and frameworks for automating the identification of data quality issues.
Work with internal and external data providers on data validation providing feedback and making customized changes to data feeds and data mappings.
Understands the challenges that the analytics organization faces in their day-to-day work and partner with them to design viable data solutions.
Recommends improvements to processes, technology, and interfaces that improve the effectiveness of the team and reduce technical debt.
Implements and optimizes data solutions in enterprise data warehouses and big data repositories.
Installs, maintains, monitors, and supports business intelligence, distributed computation, and big data analytics tools.
Provides ongoing support, monitoring, and maintenance of deployed products.
Actively works with less experienced data engineers providing technical guidance and oversight.
Actively participates in the engineering community, staying up to date on new data technologies and best practices and shares insights with others in the organization.
Qualifications:
Bachelor’s degree in computer science or related field and 4 years work experience
Working experience with batch and real-time data processing frameworks
Working experience with data modelling, data access, schemas, and data storage techniques within Snowflake
Working experience in design, development, and implementation of highly scalable, high-volume software systems and components, client-facing web applications, and major Internet-oriented applications and systems
Working experience working with relational databases such as SQL, MySQL, Postgres/PostgreSQL
Working experience with business intelligence tools and platforms
Working experience with data quality tools
Working experience with application lifecycle methodologies (e.g. waterfall, agile, iterative)
Experience with ETL processes and tools
Experience working with Git.
Desirable Qualifications
Scrum Developer Certification or equivalent
Working experience with Sisense platform
Working experience with SQL Server Integration Services
Working experience with AWS data tools (Database Migration Service)
Working experience with Python
Experience working with Jira, Rally or similar tools
Kanini Software Solutions, Inc. does not discriminate in employment matters based on race, gender, religion, age, national origin, citizenship, veteran status, family status, disability status, or any other protected class. We support workplace diversity. If you have a disability, please let us know if there is anything we can do to improve the interview process for you; we’re happy to accommodate.
Kanini Software Solutions, Inc., 25 Century Blvd., Ste. 602, Nashville, TN 37214
Job Types: Full-time, Contract
Pay: $60.00 - $65.00 per hour
Application Question(s):
Current Location?
Work Location: Remote",$62.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Integration Developer Network LLC
4.9",4.9,Remote,Data Engineer with PL/SQL Background,"Requirements:
Recent PL/SQL development work experience with Oracle 19.
Must have working experience with tools like Airflow, Git.
Must have python coding experience and REST API development experience.
Plus would be experience with microservices, docker, Kubernetes.
Plus would be experience with accounting and financial data
Experience in event driven and cloud based architectures and working knowledge of containerization and streaming technologies.
Detailed work experience on Oracle, SingleStore or IBM I DB2 database, SQL & PL/SQL, including ANSI SQL.
Experience in database analysis, development and maintenance of business applications specific to oil & gas back-office operations. Expertise in various ETL tools such as SQL*Loader, Equalum, Python scripts.
Familiarity with No-SQL databases and pipelines (Elasticsearch, MongoDB).
2+ years’ experience in the design, implementation and support of full life-cycle data engineering projects.
Bachelor's degree in Computer Science, Engineering or other equivalent degree, with 5+ years data engineering experience or Master's degree in Computer Science, Engineering or other equivalent degree with 2+ years data engineering preferred.
High school diploma or GED is required.
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Experience level:
9 years
Schedule:
8 hour shift
Experience:
Python: 8 years (Required)
PL/SQL: 5 years (Required)
ETL: 5 years (Required)
Work Location: Remote",$62.50 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2011,$1 to $5 million (USD)
"Petrolink International
2.9",2.9,"Houston, TX",Data Analysis Engineer,"Location:
Houston, Texas, U.S.A.
Job Type:
Full-time
Job Description:
Petrolink is a global company that specializes in user-driven technology for the Energy Industry.
We are looking for a Data Analysis Engineer to assist with creating and delivering algorithms primarily used by our engineering solutions.
This position is based in our U.S. office in Houston, Texas.
Duties & Responsibilities:
Research and provide logic for engineering solutions algorithms.
Collaborate with Subject Matter Experts (Drilling Engineers, Mud loggers, Pore Pressure Analysts, and other specialists from the Drilling Industry) to understand different challenges in the Drilling Industry and the practices in place to address/solve these challenges.
Code the algorithms using C# to run on Petrolink’s proprietary analytics engine.
Develop, test, and maintain algorithms.
Support escalations related to issues in algorithms.
Learn existing algorithms to provide support when required.
Prepare training material and sessions for stakeholders as part of the production release.
Skills & Qualifications:
Proficient in C#.
Experience with Python is an advantage.
Knowledge in drilling practices, and ability to speak to other drilling experts and capture requirements and logic.
Knowledge in drilling performance KPIs is an advantage.
Prior experience with WITSML is an advantage.
Function and communicate well in a team having diverse expertise.
Bi-lingual (English and Spanish) preferred.
At least 2 years’ relevant experience.
Benefits:
Health insurance
Dental insurance
401(k) with matching
Health savings account
Life insurance
Long-term disability
This job description is intended to explain the general nature and level of work being performed. Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.
Applicants must be legally authorized to work in the United States for any employer.
To apply for this position, applicants must meet the above criteria.
Interested applicants fulfilling the above criteria, please send your resume to careers.usa@petrolink.com.","$81,133 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"Trella Health
4.8",4.8,"Philadelphia, PA",Senior Data Engineer,"At Trella Health, we are passionate and committed to our mission – empowering meaningful change in healthcare. Since our founding in 2015, we continue to grow our team, enhance our solution and services offerings, accelerate into new markets, and expand our customer base. We are rapidly growing and are looking for new Trellavators to join our team!
""What is a Trellavator?"" you ask. Innovate and elevate is the name of our game! We go above and beyond to collaborate with and support each other – we believe that when a colleague or customers succeeds, we succeed. By learning from others, building on our successes, and taking risks, we constantly raise the bar – continuous improvement is in our DNA. Our word is paramount, we keep our commitments, and we always follow through. We have a strong, reliable support system that fuels growth, collaboration, and passion – and together, we create a positive environment where everyone at Trella Health, including the customers we support, can thrive. Are you ready to learn more about the opportunities with our team? Trell-yeah you are!
Position Overview:
The Senior Data Engineer will be responsible for designing, implementing, and maintaining an effective data pipeline for Trella Health's applications. This position is responsible for turning data concepts into effective data models that are efficient and scalable. The Senior Data Engineer will ensure a smooth data release and deployment process throughout the data development lifecycle.
Successful candidate will be a highly organized, self-starter that can roll up their sleeves and solve problems efficiently. Ability to document procedures, manage time, and have a knack for optimizing data flows.
Location: Atlanta HQ; Raleigh, NC; Nashville, TN; Philadelphia, PA; Remote considered
Reports to: Manager, Data Engineering
As a member of the Data team at Trella, you will:
Champion a comprehensive data vision and operating principles that improve data quality, consistency, accessibility, decision making, use, and stewardship.
Manage data pipeline by developing processes for data input, transformation, and storage
Act as an interface between the Data Science team and Application Developers
Architect and implement front-end (application layer) and back-end (ETL layer) database technologies in a highly customized data environment
Develop automated processes for moving data between systems
Implement ETL processes that transform source data into functional data stores consumed by front-end applications
Scale data pipeline by utilizing the most efficient tools and technologies
This job might be a fit for you if you have:
Bachelor's degree in Computer Science, Engineering, Mathematics, or other technical discipline (proven professional experience will be considered in lieu of a technical degree)
2+ full years of recent advanced Snowflake development experience
Experience manipulating healthcare data
In-depth knowledge of Amazon Web Services (AWS) and Platform as a Service (PAAS)
Demonstrable experience with Snowflake utilities including SnowSQL & Snowflake Javascript
Analyze all aspects of the Snowflake Query Engine to improve bottlenecks that may exist
Ability to automate data ingestion from Amazon S3, FTP, and other technologies
Experience with scripting languages, extensive Python experience, Node.js experience a plus
Working knowledge of Bitbucket and Git
Experience managing and loading data in AWS RDS (MySQL/MariaDb)
Ability to optimize MySQL query execution plans for robust performance
Technical Job Requirements (Nice to Have)
Experience manipulating healthcare data including claims, ADT data
Foundational knowledge of CI/CD platforms, preferably TeamCity
Development and consumption of API services
Experience with data science tools (Jupyter, SageMaker, R)
Understanding of Atlassian suite of tools, including Jira and Confluence
Soft Skill Job Requirements (Nice to Have)
Strong analytical skills and ability to rapidly switch contexts
Excellent communication and problem-solving skills
Sole contributor that is willing to independently uncover and improve processes that are inefficient or sub-optimal
Track record of implementing creative solutions to complex problems
Experience working in a nimble, fast-paced, start-up environment
Knowledge of HIPPA, PHI, PII policies
About Trella Health:
Trella Health provides unmatched, actionable market intelligence to post-acute care and value-based care providers of all sizes. Our industry leading analytics paired with CRM and EHR integration workflows positions us as the most advanced sales enablement platform for the post-acute care market. One of only a few companies to be deemed both a Qualified Entity by CMS and an Innovator under its Virtual Research Data Center Program, Trella has elite access to billions of performance metrics and referral data to enable competitive positioning across the market. Its standardized insights, representing 90% of all lives 65+ U.S. population, help customers identify, engage, and manage critical relationships and advance their organizations with certainty.
Since launching our first product in 2016, we have experienced tremendous growth and have highlighted a few key data points to get an at-a-glance view of Trella:
Over 700 customers including 10 of the top 10 post-acute providers and 4 of the top 10 health systems
Over 120 team members and growing
Recognized as one of Modern Healthcare's ""Best Places to Work"" in consecutive years and were ranked in the Top 50 startups in the US by LinkedIn
Headquartered in Atlanta, GA with remote hubs in Nashville, Philadelphia, Raleigh, and the Philippines
The Trellavator Experience:
At Trella Health, you can expect to join a welcoming team that truly embodies our core values. Our collaborative culture is anchored by trust, transparency, and inclusion — and we also have a lot of fun. No matter where you work, at Trella Health you can expect an awesome team, frequent virtual gatherings, engaging events, casual attire, and more.
We offer competitive salaries with a comprehensive benefits package to all employees and provide an environment that fosters work-life harmonization with unlimited PTO, company-observed holidays, and Summer Fridays. As we continue to see exponential growth, our goal is to continue to put team members first and strive to offer our team members the best culture and benefits possible. Some of the benefits we provide are:
Health, Dental, Vision & Voluntary Benefits
Competitive Salary & Bonus Plans
401k Retirement Savings
Unlimited PTO & 10 Paid Holidays
Flexible Work Hours
Equity Shares
Paid Leave Programs
Marketplace for discounted retail and entertainment
Equal Opportunity Employer
Trella Health is an equal opportunity employer. All persons will receive consideration for employment without regard to race, color, gender, sexual orientation, gender identity or expression, religion, national origin, marital status, age, disability, handicap, veteran status, genetic information, or any other protected status as recognized by federal, state, or local laws.","$113,846 /yr (est.)",51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2015,Unknown / Non-Applicable
"Iron Bow Technologies
3.5",3.5,"Washington, DC",Sr. Data Center Engineer,"Iron Bow Technologies is seeking a Sr. Data Center Engineer to support our Federal customer, The Department of Justice (DOJ), Criminal Division (CRM), in Washington, DC. The selected candidate will support our flexible and scalable solution with the DOJ CRM to extend cloud-native features to existing legacy and next-generation applications while also future proofing the infrastructure to easily accommodate future requirements, such as transitioning to a public cloud.
This position is hybrid and requires 2 days a week onsite at the customer location in NW Washington, DC and support at the customer data center as needed in Sterling, VA.
Essential Duties and Responsibilities:
Assess and provide VMware solutions to include vCenter, vSphere, SRM, ESXi
Primary storage solutions and data file storage management, NAS file storage solutions
Data Center blade servers and rack servers
Server virtualization
Experience:
10 years of related experience providing system administration support in a virtualized environment, including at least 6 years of VMWare solutioning, including vCenter, vSphere, SRM, and ESXi
Data Center operations experience, with SAN and NAS
Experience supporting primary storage solutions and data file storage management, NAS file storage solutions
Hands on experience with Data Center blade and rack servers
Ability to work staggered shift hours when needed to support critical coverage and/or time-sensitive needs
Be a member of our Team providing coordinated 24x7 IaaS on-call support
U.S. Citizen
Currently hold or obtain/maintain a Public Trust High clearance
Education:
BS in Computer Science, Information Science, or other relevant discipline is preferred – additional years of experience may be considered in lieu of the degree
Preferred Certifications:
VMware Certified Professional
Primary storage certifications such as HPE, Dell-EMC, NetApp, or Pure Storage
VCP-DCV
Travel Requirements:
No travel
PRINCIPALS ONLY. NO AGENCIES PLEASE. MUST BE A US CITIZEN ABLE TO OBTAIN A PUBLIC TRUST SECURITY CLEARANCE.
Iron Bow Technologies is a leading IT solution provider dedicated to successfully transforming technology investments into business capabilities for government, commercial and healthcare clients. Our depth of technical expertise, global reach and strategic partnerships with industry leaders as well as disruptive technology partners uniquely positions Iron Bow to target the right solution to ensure your successful business outcome. We partner with our clients from planning and implementation through ongoing maintenance and management to deliver solutions that are strong, flexible and on target .
Iron Bow Technologies is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.
#LI-TM1",#N/A,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,1983,$500 million to $1 billion (USD)
"DiNi Communications, Inc.
3.0",3.0,"New York, NY",Data Center Engineer,"DiNi Communications, Inc. is seeking a Data Center Engineer. This person needs to be a team player with exceptional communication skills. Maintaining the confidentiality of the client's information is of utmost importance.
Required Education and Experience
· At least 8 years of experience as a large data center technician
· Knowledge of Networking,
· Ability to lift equipment at least (50lbs) Rack and Stack (Network Switches, Servers, Storage,
Appliances)
· Knowledge of structured Cabling in a Data Center.
· General knowledge of Operating Systems, (Windows, Unix).
· General knowledge of networking in N Tier Environment (App, Web, DMZ,).
· Level 1 and Level 2 troubleshooting in a N Tier Environment.
· Ability to follow and execute commands to start up and shutdown systems.
· Physically travel to and be available at site B to perform technical changes that require a
person at Site B.
· Performing backup and restore activities.
· Asset Management.
· Maintaining document.
· Work with vendor on circuit issues, (WAN and Internet).
· Cable Patch the equipment in the environment.
· Participate in Site B fall-over testing
Preferred Network Engineering Experience
· Knowledge of N Tier networking.
· Certification in Networking (CISCO).
· Certification in various Operating systems (Linux and Windows), Storage, Messaging
(Microsoft).
· Certification in IT Security.
· Knowledge of Databases.
· Experience working with CISCO, EMC, Symantec, HP, Oracle, Bluecoat, MacAfee, and other vendors' equipment and software in a networked environment.
Responsibilities include but are not limited to:
Adding equipment to support agency applications in site B Environment,
Coordinating Site B testing with Site A technical resources.
Monitoring the B environment.
Rack and Stack new and replacement equipment
Troubleshooting and resolving incidents and problems in the
Physically travel to and be available at Site B to perform technical changes that require a person at Site B.
Upgrading and expanding the environment
Performing backup and restore activities
Manage Site B assets
Maintaining document
Cable Patch the equipment in the environment
Participate in Site B fall-over testing.
Job Type: Full-time
Pay: $60.00 - $70.00 per hour
Schedule:
Monday to Friday
Experience:
large data center technician: 8 years (Required)
Networking in N Tier Environment (App, Web, DMZ,): 3 years (Required)
Ability to lift equipment at least (50lbs): 1 year (Required)
structured Cabling in a Data Center: 3 years (Required)
Level 1 and 2 troubleshooting in a N Tier Environment: 3 years (Required)
Performing backup and restore activities: 3 years (Required)
Cable Patch the equipment: 3 years (Required)
Certification in Networking (CISCO): 1 year (Preferred)
Certification in IT Security: 3 years (Preferred)
CISCO, EMC, Symantec, HP, Oracle, Bluecoat, MacAfee: 3 years (Preferred)
Work Location: In person",$65.00 /hr (est.),Unknown,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
Diamondpick,#N/A,"Alpharetta, GA",Java Software Engineer with Big-Data,"About us
There is someone out there who fits your requirements like a glove! We are an innovative Talent Solutions company with a vision to build talent supply chain models without compromise for the technology industry.
We are a new-age talent solutions company with a vision to build an enterprise recruitment model without compromise. We love to solve complex talent challenges and strive to be an enabler of business success for our clients using data, technology and our team’s vast experience in the technology market.
Senior Java Developer with Big data
Alpharetta, GA (Hybrid Work from Day1)
Job Description:
The ideal candidate must possess strong background on frontend and backend development technologies.
The candidate must possess excellent written and verbal communication skills with the ability to collaborate effectively with domain experts and technical experts in the team.
Responsibilities:
As a Java Senior Developer, you will
Maintain active relationships with Product Owner to understand business requirements, lead requirement gathering meetings and review designs with the product owner
Own his backlog items and coordinate with other team members to develop the features planned for each sprint
Perform technical design reviews and code reviews
Be Responsible for prototyping, developing, and troubleshooting software in the user interface or service layers
Perform peer reviews on source code to ensure reuse, scalability and the use of best practices
Participate in collaborative technical discussions that focus on software user experience, design, architecture, and development
Perform demonstrations for client stakeholders on project features and sub features, which utilizes the latest Front end and Backend development technologies
Requirements:
6+ years of experience in Java/JEE development
Skills in developing applications using multi-tier architecture
Knowledge of google/AWS cloud
Java/JEE, Spring, Spring boot, REST/SOAP web services, Hibernate, SQL, Tomcat, Application servers (WebSphere), SONAR, Agile, AJAX, Jenkins..etc
Skills in UML, application designing/architecture, Design Patterns..etc
Skills in Unit testing application using Junit or similar technologies
Good communication skills
Leadership skills
Provide overlap coverage with onsite/customer teams
Capability to support QA teams with test plans, root cause analysis and defect fixing
Strong experience in Responsive design, cross browser web applications
Strong knowledge of web service models
Strong knowledge in creating and working with APIs
Experience with Cloud services, specifically on Google cloud
Strong exposure in Agile, Scaled Agile based development models
Familiar with Interfaces such as REST web services, swagger profiles, JSON payloads.
Familiar with tools/utilities such as Bitbucket / Jira / Confluence
Job Types: Full-time, Contract, Temporary
Pay: From $60.00 per hour
Benefits:
Flexible schedule
Compensation package:
Hourly pay
Experience level:
8 years
Schedule:
8 hour shift
Day shift
Work Location: Hybrid remote in Alpharetta, GA 30005",$60.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Barracuda Networks Inc.
3.6",3.6,"Alpharetta, GA",Data Engineer,"Job ID: 23-579

Come Join Our Passionate Team! At Barracuda, we make the world a safer place. We believe every business deserves access to cloud-enabled, enterprise-grade security solutions that are easy to buy, deploy, and use. We protect email, networks, data and applications with innovative solutions that grow and adapt with our customers’ journey. More than 200,000 organizations worldwide trust Barracuda to protect them — in ways they may not even know they are at risk — so they can focus on taking their business to the next level.
We know a diverse workforce adds to our collective value and strength as an organization. Barracuda Networks is proud to be an Equal Opportunity Employer, committed to equal employment opportunity and equitable compensation regardless of race, gender, religion, sex, sexual orientation, national origin, or disability.
Envision yourself at Barracuda
We currently have an exciting new position open for a Data Engineer. This person will assist in driving technology innovation for engineering projects and work on architecture of AWS, Elastic and Kubernetes. They will also work closely with the Technology Architect to create and drive the Architecture Review Board.
What you’ll be working on
Determine internal / external customer data needs and identify system specifications
Analyze the needs of large systems and breaking them down into smaller manageable parts
Responsible for designing the journey of data from various sources to the database and everything in between
Plan and design the structure of data, brainstorm with team
Analyze best technology products and systems for internal teams
Help provide build vs buy decisions on data architecture
Communicate data requirements to engineers; explain data architecture to them and provide assistance throughout SDLC
Choose / maintain suitable software, hardware and integration methods
Help resolve technical problems as and when they arise
Review testing procedures to ensure data performant and error free
Ensure that database satisfy quality standards and procedures. Setup such standards.
Work with senior IT personnel to devise plans for future IT requirements of the organization
Produce progress reports on key infrastructures - Elastic and Data Pipelines
What you bring to the role
Bachelor Degree or Post Graduate in Computer Science, or relevant work experience
5+ years of experience in an Engineering / Operations heavy organization
1+ years of experience as a Data Analysis and Data Design responsible for data architecture of a SaaS application
Experience in understanding and designing data sharding in a distributed database
Some Hands-on experience in software development and system administration
Knowledge of strategic IT solutions
Familiarity with programming languages like JavaScript or Java or Python
Familiarity with Linux OS and Windows alike
Experience in cloud computing and cloud Technologies (AWS, GCP, Azure)
Excellent communication skills – you should be adept at listening to, understanding, and explaining key concepts to managerial and technical resources.
Good Anticipation and problem-solving skills
Ability to prioritize and manage time – responsible for multiple streams of work at the same time
Relationship building - Ability to form a good rapport with internal and external stakeholders
In-depth knowledge about quality standards and security best practices

Preferred Skills:
Expert level Understanding of Elastic eco system - Elasticsearch, Logstash, Kibana, Filebeats
Understanding of secure tunnel communication using VPN
Experience designing data at scale
AWS experience is preferred in Cloud Technologies
What you’ll get from us
A team where you can voice your opinion, make an impact, and where you and your experience are valued. Internal mobility – there are opportunities for cross training and the ability to attain your next career step within Barracuda.
Equity, in the form of non-qualified options
High-quality health benefits
Retirement Plan with employer match
Career-growth opportunities
Flexible Time Off and Paid Time Off benefits
Volunteer opportunities
#LI-Remote","$98,161 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,$100 to $500 million (USD)
OpenSecrets.org,#N/A,Remote,Snowflake Data Engineer,"About us
Our goal is to is to track the flow of money in American politics and provide the data and analysis to strengthen democracy. Our vision is for Americans to use this knowledge to create a more vibrant, representative and accountable democracy..
Our work environment includes:
Modern office setting
Work-from-home days
Wellness programs
Data Engineer
OpenSecrets is seeking to add a new member to our data team. The Data Engineer will be responsible for helping build and maintain our data systems.
Responsibilities include:
Implementing methods to improve data reliability and quality
Analyzing raw data and implementing that data into our system using data pipelines
Troubleshooting data errors and pipeline issues
REQUIRED QUALIFICATIONS:
Experience working with and analyzing large datasets, equivalent to 2-3 years in a similar position
Proven ability to build complex data pipelines using Snowpipe and SSIS
The ability to work with diverse populations
Experience with MSSQL Server and Snowflake
Advanced knowledge of ANSI SQL
PREFERRED QUALIFICATIONS:
Excellent written and verbal communication skills.
The ability to manage multiple priorities and projects both independently and as a member of a team
Snowflake SnowPro Data Engineer certification
Experience with Apache Spark
Experience with machine learning matching models
Knowledge of and interest in American government and money in politics.
SALARY AND BENEFITS:
We are a fully remote organization and this person can be based anywhere in the U.S.
OpenSecrets offers an excellent benefits package, including full medical, dental, 403(b) match, generous vacation and sick leave, and all federal holidays paid.
TO APPLY:
Please visit this link: https://airtable.com/shrGe2bugeapKVr8a
People of diverse backgrounds are strongly encouraged to apply. OpenSecrets is an equal opportunity employer committed to creating a diverse and inclusive organizational culture. We do not discriminate against candidates and employees because of their disability, sex, race, gender identity, sexual orientation, religion, national origin, age, veteran status, or any other protected status under the law.
ABOUT US: OpenSecrets is the nation's premier research group tracking money in U.S. politics and its effect on elections and public policy. Our mission is to track the flow of money in American politics and provide the data and analysis to strengthen democracy. Our vision is for Americans to use this knowledge to create a more vibrant, representative and accountable democracy.
Job Type: Full-time
Pay: $83,874.82 - $100,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Retirement plan
Vision insurance
Compensation package:
Yearly pay
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Experience:
SQL: 3 years (Required)
Snowflake: 1 year (Preferred)
ETL: 2 years (Required)
Work Location: Remote","$91,937 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Infosoft,#N/A,"Peachtree Corners, GA",Data Pipeline Engineer - 2173,"Here are the job position details for your review:
Job Title: Data Pipeline Engineer (GStreamer/ DeepStream)
Pay Rate: $90 to 120/Hr.
Duration: 3 Months
Location: Peachtree Corners, GA
This position is for the Building Automation division.
We are looking for Data Pipeline Engineer (GStreamer) to participate in the development of Streaming data pipelines using NVIDIA Deepstream, GStreamer, Python programming, messaging queues, and related tools.
Preference is for candidates located in Atlanta, GA metro area. However, ok with non-local candidates who are open to 75% travel to Atlanta, GA.
Understanding the basics of our architecture/setup to be able to do setup/support for demos, testing, etc.
Basic computer hardware assembly and set-up
Possible set-up/installation of the application(s)
Possible development tasks depending on what the rest of the development team has for them
Develops business software applications, system software, and/or IT solutions on the basis of functional specifications.
Scope of position embraces both the customization or reuse/modification of system software and/or solutions as well as their development from scratch.
Consults with customers in creating functional specifications and work estimates, and supports Make-or-Buy decisions.
Analyses performance gaps of existing software environments and proposes measures to address efficiency problems.
Is also responsible for the implementation of data-driven components supporting data modeling and engineering in the context of data analytics (Data Stewardship).
Skills:
Python
Linux
Docker and Docker Swarm
Zero MQ / a good understanding of messaging
Desired Skills:
NVIDIA Deepstream SDK
Web Sockets
Flask
Protobuf
Possible:
GStreamer (if different than RTSP / H264 / MJPEG)
Job Type: Contract
Salary: $90.00 - $120.00 per hour
Ability to commute/relocate:
Peachtree Corners, GA: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
""Please share your Email ID""
Work Location: One location",$105.00 /hr (est.),Unknown,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Fracsys Inc,#N/A,"Washington, DC",Data Engineer,"Fracsys Inc is hiring a Data Engineer position. The ideal candidate must have at least 12+ years of industry experience. He or she must be responsible for successful technical delivery of Data Warehousing and Business Intelligence projects using a formal project management methodology.
Responsibilities
Directs, motivates and develops key technical personnel
Provides technical direction across the full lifecycle system development process
Provides strategic planning and comprehensive senior level technical consulting to IT senior management and senior technical staff
Working independently within guidelines
Provides detailed inputs for plans and schedules, including goals, risks, and resource allocation
Conduct diagnostic, troubleshooting steps and present conclusions to a varied audience
comfortable slinging some code to solve a problem
Build Data Marts and Data warehouses to facilitate analytical capabilities across the varied sources of data
Design and implement data patterns on cloud using cloud Data warehouse and data virtualization tools
Evaluate new tools and technologies for target state on cloud
Learn and implement new ETL, BI tools and data warehousing technologies
Qualifications
Minimum twelve (12) years in a Technical Lead or Consulting role leading Information Management related initiatives (system integration, data warehouse build, data mart build, or similar)
MUST have proven leadership experience for team of at least 3 technical resources.
Experience creating Data Marts, Data warehouses on-premise and on cloud for real time and batch processing frameworks
Experience building and optimizing big-data data pipelines and data sets including Postgres, AWS Relational Data Service
extensive hands-on experience building and optimizing data structures for data analytics, data science and business intelligence
Experience building self-service data consumption patterns and knowledge of cloud-based data Lake platforms
Experience wrangling data (structured and unstructured), in-depth knowledge of database architecture
Experience utilizing or leading implementations leveraging ETL tools (Informatica / Talend), BI Reporting tools such as MicroStrategy, Microsoft Power BI, data modeling tools such as Erwin, Oracle, SQL server, NoSQL, JDBC, UNIX shell scripting, PERL and JAVA, XML/JSON files, SAS , Python, AWS cloud-native technologies, S3, Athena, Redshift
Familiarity with the following technologies: Hadoop, Kafka, Airflow, Hive, Presto, Athena, S3, Aurora, EMR, Spark
Ability to drive, contribute to, and communicate solutions to technical product challenges
Ability to roll-up your sleeves and work in process or resource gaps and fill it in yourself
Excellent written and oral communication skills in English
Education
Bachelor’s degree in computer science, Data Analytics, Information Systems, or a related degree or equivalent experience. Master’s degree is preferred.
Job Type: Full-time
Pay: From $96,336.72 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Experience level:
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Washington, DC: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person","$96,337 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"WEX Inc.
3.7",3.7,Remote,Data Engineer,"We are seeking a Data Engineer to play a critical role in the development of WEX's enterprise data & analytics capabilities. You will be part of an organization focusing on the development and delivery of data solutions and capabilities for WEX’s data platform. The successful candidate is motivated by thinking big data, technically proficient, and enjoys working in a fast paced environment.
The base pay range represents the anticipated low and high end of the pay range for this position. Actual pay rates will vary and will be based on various factors, such as your qualifications, skills, competencies, and proficiency for the role. Base pay is one component of WEX's total compensation package. Most sales positions are eligible for commission under the terms of an applicable plan. Non-sales roles are typically eligible for a quarterly or annual bonus based on their role and applicable plan. WEX's comprehensive and market competitive benefits are designed to support your personal and professional well-being. Benefits include health, dental and vision insurances, retirement savings plan, paid time off, health savings account, flexible spending accounts, life insurance, disability insurance, tuition reimbursement, and more. For more information, check out the ""About Us"" section.
Salary Pay Range: $80,500.00 - $108,000.00","$94,250 /yr (est.)",1001 to 5000 Employees,Company - Public,Financial Services,Investment & Asset Management,1983,$1 to $5 billion (USD)
"Tipico - North America
4.0",4.0,"Denver, CO",Data Engineer,"Company Description

Tipico is an energetic, passionate, bold tech company, and we love sports! Tipico has recently expanded to the US market, establishing offices in Hoboken, New Jersey and Denver, Colorado. We are driven by our core values: passion, progress and trust. Our goal is to build the best possible mobile sports betting product in the industry, in order to amplify the emotions of millions of sports fans every day. Let’s Make This Interesting ™
Please note: this role is located in our Denver, CO office; we work off a hybrid model and come into the office 2 days per week.

Job Description

The Data Engineer will report directly to the Data Engineering Manager and work together with rest of the data engineers, BI analysts, and product teams. The position holder ensures this is done by doing the following:
Create and maintain both batch (ETL/ELT) and real-time data pipelines and architecture
Ensure all data provided by Data Platform is of the highest quality and is delivered in a timely manner and in line with agreed SLAs
Assemble large and complex data sets that meet functional and non-functional business requirements
Identify, design, and implement internal process improvements, including automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies
Collaborate with stakeholders including data architects, Executive, Product, Data and IT team members from the beginning to the delivery of a project
Build a scalable, modern, and trusted data ecosystem, making sure it is responsive, available and in line with business requirements
Take part in sprints, workloads, and deliverables through agile methodologies
Manage quality assurance, verification of accuracy and consistency of data, specifically KPI measures sent to the business
Thrive working in a fast-paced and challenging environment
Open to continuous learning individually and with the team
Comfortable with being challenged / seeking challenge
Willing to work flexible hours if needed

Qualifications
3+ years in Data Platforms / Datawarehouse / BI experience
Must have experience in successfully implementing data-centric applications, such as data warehouses, operational data stores, and data integration projects
Strong analytical and problem-solving skills that balance with creative approaches
Ability to learn new and complex concepts quickly
Strong experience with large-scale production databases and SQL
Solid understanding of cloud data services: AWS services such as S3, Athena, EC2, RedShift, EMR, EKS, RDS and Lambda
Proficient with one or more big data & new data technologies such as DBT, NIFI, Airflow, Kafka
Experience with data modelling
Experience with ETL development (extractions, data load, aggregation, transformation)
Knowledge in working with time-series/analytics databases such as Elasticsearch
Understanding of containerization and orchestration technologies like Docker/Kubernetes
Must be able to take ownership of projects with a high level of attention to detail and priority
Team player with a hands-on approach and attitude
Must be comfortable with on call rotation
Nice to have:
Familiarity with Marketing tooling and/or CRM systems is preferred
Familiarity with using business intelligence tools such as Domo, Tableau or Looker
Knowledge about and / or experience with sports betting
Background in digital companies, online, gambling, gaming, e-commerce, marketing, or mobile apps
Familiarity with one scripting language, preferably Python

Additional Information

What's in it for you:
Work in an environment where you, your work and ideas matter and have an impact.
You will be a be part of the newest and up and coming USA online iGaming and sports betting companies in the market.
Work in a new and young business with high growth potential.
Build your own success story together with us.
Work with self-organized, self-responsible and entrepreneurial employees.
Start-up feeling, backed-up by a leading European sports betting house. We are a high-volume business and are taking off in the US!
Competitive salary, Medical, Vision, Dental Benefits, Unlimited PTO, and more
Salary for this position is $120k+ & annual performance bonus","$120,000 /yr (est.)",201 to 500 Employees,Company - Private,"Arts, Entertainment & Recreation",Gambling,2004,Unknown / Non-Applicable
"Hiscox
4.1",4.1,"Atlanta, GA",Senior Data Engineer,"job description:
You will be a senior member of our US IT Team data engineering practice. You will be an early member of this team and will work closely with the Director of Data Engineering to help establish it. We will have our work cut out for us delivering planned work, building the practice and evolving our technical landscape. This is an exciting opportunity for the right individual to join at the very beginning and grow with the new team.
As a senior you will be expected to lead, mentor others and deputise for the director of data engineering when required. Yoi will also continue to contribute as a hands-on engineer. You will have the knowledge, best earned through 5+ years of experience, to truly inhabit this senior role in an ambitious team.
What you’ll be doing:
Working with business stakeholders and our delivery team to understand high value business problems that can be solved through the application of data processing and analytical systems.
Developing, expanding and evolving our existing databases and ETL pipelines.
Working with architects to design, build and support a transformational cloud-based data platform for the business.
Being a core and professional member of the new data engineering practice
Understand business requirements and help refine into development tasks and estimate their complexity.
Research, evaluate and adopt new technologies with a right tool for the job mentality.
Focus on both speed of delivery and quality, with suitable pragmatism – ensuring your solutions are always appropriate and not too complex or over-engineered.
Quick progression of projects from proof-of-concept to post-production stage.
Communication and presentation of ideas to colleagues in all parts of the wider tech team.
Participating in code reviews for the data engineering practice.
Act as a servant leader in our practice – ensuring team members have
person specification:
Knowledge of, or ability to rapidly adopt our core languages for data engineering – SQL and/or Python.
Experience working using a modern DevOps approach. You should not just have written data code but also the CI/CD pipelines necessary to test and deploy that code in a professional environment.
A robust understanding of core data engineering topics – ETL vs ELT, structured and unstructured data, data quality and data governance.
Ability to contribute to all aspects of a solution – design, infrastructure, development, testing and maintenance.
The ability to design and advocate for technical solutions to business problems.
Effective collaboration with technical and non-technical team members through agile ceremonies – roadmap planning, feature workshops, backlog elaboration, code review.
Understanding of cloud technology and significant hands-on experience working with it. You must understand how cloud-native solutions must be built differently from traditional ones.
Track record of taking initiative and delivering projects end-to-end; clear evidence of being self-driven and motivated
Immense curiosity, high energy and desire to go the extra mile to make a difference.

Desirable Skills
Beyond the required skills we are open to individuals of diverse talents. Experience with additional technologies, data science knowledge or a business background are all valued. We want to know how your unique abilities can contribute to our team.
Our Technology
We operate in a diverse technical landscape and are looking for flexible engineers who can adapt to and use many different tools. We would not expect any engineer to be familiar with the entire tech stack – no engineer can. Instead, we seek people with a good understanding of data structures and algorithms and the ability to apply this knowledge in learning new tools.
We have an existing data landscape involving both MS SQL Server and Oracle databases. We use SSIS for ETL. We will also need to source data from our integration layer – webMethods.
If some of that sounds like it needs bringing up to date, we know! We hope to start building a modern cloud data platform to realise the full value of our data. The technologies we use for that platform are still to be decided. Likely it will be based on Snowflake with Azure Data Factory for ELT. Other parts of our business have had success using Prefect for orchestration and dbt for transformation and we will reuse where it is sensible.
We prepare data for use by analysts working with a variety of tools – Tableau, PowerBI, and even Excel.
We take a DevOps approach and strive for continuous integration / continuous deployment. We use Azure Pipelines to deploy our code. We deploy infrastructure the same way using Terraform and Docker.
Our technology landscape is not fixed. Our engineering and architecture teams drive the technology we adopt.


Salary range $140,000 - $170,000","$155,000 /yr (est.)",1001 to 5000 Employees,Company - Public,Insurance,Insurance Carriers,1901,$1 to $5 billion (USD)
Proits Hub LLC,#N/A,Remote,Data Analytics Engineer,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote
Speak with the employer
+91 9417599933",$60.00 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Seamless.AI
3.4",3.4,"Columbus, OH",Senior Data Engineer - Remote US,"Role: Senior Data Engineer
The Opportunity
We are seeking a Senior Data Engineer with a strong background in data ingestion and processing for large data sets. The ideal candidate is maniacal about data accuracy and has a proven track record of building robust and scalable data pipelines. This role requires an experienced professional who can work collaboratively with cross-functional teams to design and implement solutions that meet business requirements and adhere to data security and privacy policies.
If you are a senior data engineer with a passion for data accuracy and a proven track record of building robust and scalable data pipelines, we encourage you to apply for this role. We offer competitive compensation, a flexible work environment, and opportunities for growth and advancement within the organization.
About Seamless
Seamless delivers the world's best sales leads. Through our product, we help sales teams maximize revenue, increase sales, and easily acquire their total addressable market using artificial intelligence; by development of a robust real-time contact and company search engine as well as a suite of technically-advanced tools to support sales and lead generation. We have been recognized as one of Ohio's fastest growing companies and have been recently ranked No. 7 in LinkedIn's Top 50 Startups of 2022, featured in Forbes as #1 Software company in Ohio in 2022, and on G2's ""Top 100 Highest Satisfaction Products for 2022"" list!
The Seamless Team
We have an amazing culture and work environment that anyone would want to be a part of. We encourage a culture of positivity. We thrive off of continuous feedback and do whatever it takes to help our team and customers be successful. You will grow as an individual, professionally, and be able to see and feel the impact you are making to the growth of Seamless every day.
Role Responsibilities
Design and develop data ingestion and processing pipelines that ensure data accuracy and completeness.
Work closely with cross-functional teams to understand business requirements and design solutions that meet those requirements.
Lead efforts to identify and implement data quality control measures to ensure consistency and reliability of data.
Monitor data pipelines to identify issues and proactively resolve them.
Evaluate and recommend new technologies and techniques to improve data pipeline performance and scalability.
Ensure compliance with data security and privacy policies and procedures.
Mentor junior engineers and provide technical leadership in data engineering best practices.

Candidate Requirements
Bachelor's or Master's degree in Computer Science, Software Engineering, or a related field.
3+ years of experience in data engineering, with a focus on data ingestion and accuracy.
Strong experience with large-scale data processing frameworks such as Hadoop, Spark, or Flink.
Proficiency in programming languages such as Python, Java, or Scala.
Experience with SQL and NoSQL databases.
Familiarity with cloud-based data processing and storage platforms such as AWS, Google Cloud, or Azure.
Strong experience in designing and implementing data quality control measures.
Excellent problem-solving and analytical skills.
Strong communication and collaboration skills.
Experience mentoring and leading teams of data engineers.
Seamless.AI has been delivering the world's best sales leads since 2015. Our product is the first real time, B2B search engine helping sales teams maximize revenue, increase sales, and easily acquire their total addressable market using artificial intelligence. We have been recognized as one of Ohio's fastest growing companies and won 2020 Best Places to Work and LinkedIn's Top 50 Tech Startups in 2020 and 2022. We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. Visa Sponsorship is not included in our hiring package. Applicants will need to be authorized to work in the U.S.","$93,575 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable
Siri info solutions inc,#N/A,"Beaverton, OR",Sr. Data Engineer,"Must Have Skills
PySpark
Python on AWS
Responsibilities:
Understanding of advanced data warehousing concepts, data modeling and extract, transform and load development
Advanced SQL skills with experience querying large datasets from multiple sources and developing automated reporting
Python skills for scripting, data manipulation, custom extract, transform and loads and statistical/regression analysis particularly, as they apply to sales and marketing operations and performance
Job Type: Full-time
Salary: $47.00 - $50.00 per hour
Experience level:
7 years
Work Location: On the road",$48.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Radiant System,#N/A,"Pleasanton, CA",Data Engineer,"First Priority: Pleasanton, CA –
Second Priority for a superstar: Plano, TX
Experience in developing and deploying data pipelines, preferably in the Cloud
Experience working with Spark, Databricks, and other relevant technologies in Microsoft Azure cloud
Experience in productionizing and deploying Big Data platforms and applications
Expert in writing SQL statements
Job Type: Contract
Application Question(s):
Are you willing to work on W2?
Work Location: In person",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Titan America
3.8",3.8,"Roanoke, VA",Data Engineer,"Data Engineer
We are seeking a motivated and detail-oriented Data Engineer to join our team based in Roanoke, Virginia. As a Data Engineer, you will play a crucial role in designing, developing and maintaining scalable data pipelines and transformations for business-critical use cases. In addition, you may take on various Software Development and Data Science responsibilities as we frequently share responsibilities and knowledge across our growing team.
Responsibilities:
Design, develop and maintain scalable data pipelines for extracting, transforming, and loading data from various sources
Collaborate with cross-functional teams to identify data needs and determine the best data solutions
Develop and implement data models to support business requirements and ensure data quality
Ensure the security and privacy of sensitive data by implementing appropriate access controls
Monitor and optimize data pipeline performance to ensure timely and accurate data delivery
Document data pipeline processes, data dictionaries, and data storage solutions
Requirements:
Bachelor's degree in Computer Science, Computer Engineering, or a related technical field
Minimum of five years of professional experience working as a Data Engineer or Software Developer
Strong hands-on experience with data warehouse and transformation solutions, i.e. Domo, Snowflake or similar
Proficient in at least one scripting language such as Python, JavaScript, or R
Understanding of data modeling, data integration and data quality processes
Familiarity with cloud platforms such as AWS, Azure, or Google Cloud Platform
Strong analytical and problem solving skills
Full Stack Software Development experience in a professional setting is highly desired, but not required
This is an excellent opportunity for a driven and collaborative individual to make a significant impact in a dynamic and growing team. If you have a passion for data and a desire to work in a fast-paced and dynamic environment, we want to hear from you!
Success begins with hiring the right people to partner with us as we grow and develop our businesses. People are central to everything we do. It is through their efforts and talents that Titan has been successful for over 100 years.
Titan America is an Equal Employment Opportunity (EEO) / Affirmative Action employer and welcomes all qualified applicants. Applicants will receive fair and impartial consideration without regard to race, sex, color, religion, national origin, age, disability, veteran status, genetic data, or other legally protected status.
Job Type: Full-time
Pay: $75,000.00 - $85,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Referral program
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Roanoke, VA 24019: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Do you currently or will you in the future need sponsorship to work in the United States?
This position requires the successful candidate to be in the office in Roanoke, VA at least 3 days a week. Can you meet this requirement?
This position requires the successful candidates to be a full time, W2 employee. Can you meet this requirement?
Education:
Bachelor's (Preferred)
Experience:
Data Engineering: 5 years (Preferred)
Work Location: Hybrid remote in Roanoke, VA 24019","$80,000 /yr (est.)",1001 to 5000 Employees,Company - Public,"Energy, Mining & Utilities",Mining & Metals,1902,Unknown / Non-Applicable
"MyCare Medical Group
4.1",4.1,"Lutz, FL",Data Engineer,"Job Summary
The Data Engineer will design, develop, optimize, and maintain data architecture and pipelines that adhere to ELT principles and business goals.
Job Responsibilities:
Designs, develops, optimizes, and maintains data architecture and pipelines that adhere to ELT principles and business goals.
Solves complex data problems to deliver insights that helps business achieve its goals.
Creates data products for engineer, analyst, and data scientist team members to accelerate their productivity.
Engineers effective features for modelling in close collaboration with business analysts and stakeholders.
Leads the evaluation, implementation and deployment of emerging tools and process for analytics data engineering in order to improve productivity as a team and quality.
Partners with engineers, BI, and solutions architects to develop technical architectures for strategic enterprise projects and initiatives.
Fosters a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions.
Advises, consults, mentors, and coaches other data and analytic professionals on data standards and practices.
Develops and delivers communication and education plans on analytic data engineering capabilities, standards, and processes.
Job Qualifications:
Education and Work Experience: Bachelor’s degree in computer science, information systems, engineering, or a related field; Masters preferred.
3+ years of experience in data engineering role.
Expertise in ELT and data analysis, SQL and at least one programming language (e.g., SQL, Python).
Conceptual knowledge of data and analytics, such as dimensional modelling, reporting tools, data governance, and structured and unstructured data.
Skills: Strong understanding of agile methodologies and experience as a Data Engineer on a cross-functional agile team preferred.
Experience with designing and maintaining data warehouses and/or data lakes with technologies, such as Hadoop, Spark, or distributed databases, like Redshift and Snowflake, and experience with housing, accessing, and transforming data in a variety of relational databases.
Experience with cloud database technologies (e.g., Azure) and developing solutions on cloud computing services and infrastructure in the data and analytics space.
Experience in building data pipelines and deploying/maintaining them with best-in-class tools (e.g., Git and Jenkins).
Familiarity with the Linux operating system.
Basic familiarity with BI tools (e.g., Tableau, Power BI, Looker).
Experience working in data engineering or data architect role.
Candidate Qualities: Focuses on business agility and agile delivery with a “fail-fast, succeed early” mindset and emphasis on measurable outcomes.
Brings a high-energy and passionate outlook to the job and can influence those around them.
Able to build a sense of trust and rapport that creates a comfortable & effective workplace.
Passion for innovation and “can do” attitude.
BENEFITS
Comprehensive benefits package, including Health, Vision, Dental, and Life insurancesFSA and Life Assistance Program (EAP)
401(k) Retirement Plan
Health Advocacy, Travel Assistance, and My Secure Advantage
PTO Accrual and Holidays
#ZIP
Job Type: Full-time
Pay: $125,000.00 - $150,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Employee discount
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Referral program
Relocation assistance
Retirement plan
Tuition reimbursement
Vision insurance
Compensation package:
Performance bonus
Yearly bonus
Experience level:
3 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Lutz, FL 33558: Reliably commute or planning to relocate before starting work (Preferred)
Experience:
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person","$137,500 /yr (est.)",501 to 1000 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2018,Unknown / Non-Applicable
WellTrust Medical Group,#N/A,"Charlotte, NC",Data Engineer,"The Data Engineer will assist the senior analyst in complex modeling and projects to ensure their completion within the allotted timeline. Helps track measures of success and develop reports that measure operational and customer metrics. Demonstrates ability to troubleshoot data consumption and data quality problems.

Responsibilities:
Build and maintain reports using My SQL Server
Identify optimal data sources, structure data validation, and cache optimization.
Performs business problem modeling and quantitative analytics including regression analysis, time-series, hierarchical cluster analysis, and data mining.
Helps track measures of success and develop reports that measure operational and customer metrics.
Demonstrates ability to troubleshoot data consumption and data quality problems.
Evaluates and recommend improvements to current models, metrics, and analytic processes to improve operational efficiency and insight.
Design, document, and communicate effective technical solutions meeting end-user requirements
Perform analyses with large amounts of complex data, and uses data visualization tools to communicate results.
Requirements:

Bachelor's degree or equivalent experience in a quantitative field (Mathematics, Statistics, Computer Science, Economics, Finance, Physics, Engineering)
3+ years of professional working experience as a Data Engineer or Analyst
1-3 years of data warehouse experience
Build and maintain reports using My SQL Server
Strong analytical and creative problem-solving skills
Design, document, and communicate effective technical solutions meeting end-user
SQL and Excel Macro development
Strong understanding of various data environments such as SQL, Oracle, Big Query, etc.
Experience with data visualization tools such as Looker Studio, Tableau, D3.js, etc.
Proficient in R, Python, MATLAB, Java, or another programming language.",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Loyal
3.9",3.9,"Atlanta, GA",Senior Cloud Data Operations Engineer,"Loyal is an organization centered on experience and building a platform that allows consumers to make meaningful decisions when it comes to healthcare. We deeply understand providers, locations, services, appointments, business rules, and moreover, we understand patients - who they are, the preferred method of communication, upcoming appointments, lapsed appointments, outstanding bills, health risks, and more. With this intelligence, our platform fuels highly relevant and personalized experiences across all mediums (website, email, voice…) allowing patients to get healthy, stay healthy, and have a better relationship with the health care provider.
**This is a remote role**
Summary
The Senior Cloud Data Operations Engineer is responsible for delivering Loyal's cloud-based data warehouse. They interface with groups across the organization to ensure we are collecting and storing data in a way that makes it easy to create and consume. They identify best practices for big data management, select the right tooling, create a workable data architecture, and drive an implementation roadmap others can depend on. The Senior Cloud Data Operations Engineer also works closely with our security group to ensure data security and privacy. They also mentor others on the team, providing technical leadership to those they work with.
You will
Create data streams and consolidate data sources into a single cloud-based data warehouse
Architect and catalog data warehouse storage schema
Work with product teams to enable easy data aggregation
Work with data consumers (analysts, data scientists, and AI/ML engineers) to ensure data is easily accessible and consumable
Create and execute a roadmap that meets data storage and availability goals
Ensure data is secure through data analysis, access control, and encryption
You have
Bachelor's degree in computer science, information systems, applied math or related field, or equivalent work experience
Minimum 5 years of experience working with cloud data processing services (AWS, Azure, or GCP).
Minimum 3 year experience with Big Data technologies, such as Databricks, Snowflake, Spark, Hadoop, etc.
Strong experience in creating data architecture and storage schemas required
Significant working knowledge of SQL and other query languages and tools
Significant experience with building relational database systems strongly preferred
Experience with ETL tools a plus
Knowledge of Python preferred
Software development, data analytics, or data science experience is a plus.
Experience working within a software as a service (SaaS) company preferred.
Experience working within a high growth and/or ambiguous environment, with proven experience to be dynamic preferred.
#LI-REMOTE
We know that potential candidates are often less likely to apply to a position if they don't match 100% of the job qualifications. Don't let that be why you miss out on this opportunity! We encourage you to apply if you can demonstrate many of these skills and competencies.
Loyal to our employees
We are a remote-friendly company! We encourage you to apply from anywhere in the United States. We also believe in a work/life balance that fulfills you while you're here and supports you when you're not. We built our benefits package to prove that we're committed to you having everything you need (including a little fun). Here is what we offer full-time employees:
Flexible paid time off, sick and personal days
At least one holiday per month (sometimes, more!)
Full health, dental, and vision insurance - Loyal pays the premium for all employees!
One Time Home Office Setup Stipend For Remote & Hybrid Roles
Monthly Internet Stipend for Remote & Hybrid Roles
Long term & short term disability
401[k] plan
16 Weeks Paid Parental Leave
2 Volunteer days per year
Matching Gift Program
Participation Grant Program
Annual Travel/Team Events up to twice per year (post-COVID)
Our Commitment
We believe that the key to Loyal's success is you. Your unique background, life experience, knowledge, self-expression, and talent make you uniquely you. Who you are, what you have experienced, and how you think inspires us to be innovative and bold.
Loyal is an equal opportunity employer. We hire great people from a wide variety of backgrounds, not just because it's the right thing to do, but because it makes our company stronger. We welcome the unique contributions that you can bring in terms of your education, opinions, culture, ethnicity, race, ancestry, sex, gender identity and expression, national origin, citizenship, marital status, age, languages spoken, veteran status, color, religion, disability, sexual orientation, and beliefs.
We consider qualified applicants regardless of criminal histories, consistent with legal requirements.
Further, consistent with applicable federal and state law, Loyal provides reasonable accommodations when requested by qualified applicants or employees with disabilities, unless doing so would cause an undue hardship. Loyal's policy regarding requests for reasonable accommodation applies to all aspects of employment, including the application process. If you require a reasonable accommodation in connection with the application process, please contact the Talent Acquisition Department at talentacquisition@loyalhealth.com.
E-Verify
This employer participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S. If E-Verify cannot confirm that you are authorized to work, this employer is required to give you written instructions and an opportunity to contact Department of Homeland Security (DHS) or Social Security Administration (SSA) so you can begin to resolve the issue before the employer can take any action against you, including terminating your employment. Employers can only use E-Verify once you have accepted a job offer and completed the I-9 Form.
COVID-19 Vaccinations
Consistent with Loyal policy, candidates performing in-person work will be required to show proof of being fully vaccinated against COVID-19 upon commencing employment. Reasonable accommodations will be considered on a case-by-case basis for exemptions to this requirement in accordance with applicable law. If you require a reasonable accommodation to Loyal's vaccination requirement, please contact the People Department at people@loyalhealth.com.",#N/A,51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2015,$5 to $25 million (USD)
"Apolis
3.9",3.9,"Saint Louis, MO",Data Engineer (Python Focused),"Role: Data Engineer (Python Focused) (W2 ONLY)
Location : St. Louis, MO 63102
Duration: 18 Months
The primary responsibilities of this role will be to work as a part of the data and analytics team of the client. The ideal candidate would have experience using ETL and other data engineering tools in a cloud environment. Additionally, this candidate would have coding and scripting experience creating code to move and manipulate data using tools such in Python such as Spark or Pandas. Microsoft Azure experience is a plus but relevant experience with other cloud-based environments will be considered.
This team uses Microsoft Azure to build and productionize data sets and processes to the product owners and product users and this role will be expected to contribute to those objectives. As a member of this team, you will work in conjunction with other data engineers, platform engineers, solution developers and others to execute and deliver working solutions to the end users.
Technical Skills
High focus on Python
More Programmer/Coder experience, less Cloud experience needed.
Must have experience with data analytics/engineering and cloud technologies and an in-depth working knowledge and understanding of data engineering, design, and integration.
Intermediate or higher level of proficiency with Python Must have the proven ability to learn quickly and apply cloud data engineering principles and skills to new tools and use cases.
Ability to define and take technical ownership of detailed requirements and collaborate effectively with various team members such as Business Analysts and Project Managers and end users.
Excellent communication (verbal and written) and problem-solving skills are essential for this position.
Initiative, Proactive Cooperation, and curiosity are mandatory behavioral skills required for this position.
Experience with several of the following technologies is preferred:
Experience with Microsoft Azure Platform. In particular, the data processing toolsets including Azure Data Factory Pipelines, Azure Databricks, Azure Data Lake Storage.
Experience using Notebooks (Such as Jupyter, DataBricks) also desired.
Experience using code languages such as Python and C#
Experience using query languages such as SQL, MDX and DAX
Experience using scripting languages such as PowerShell, M-Query (Power Query), and Windows batch commands.
Understanding of Agile and DevOps principles for software development and ownership.
Familiar with compliance and controls for software development and deployment.
Education Required
A bachelor's in computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field [or equivalent work experience] is not required but is preferred.
Job Type: Contract
Salary: $60.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Vision insurance
Compensation package:
Overtime pay
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
St. Louis, MO: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Preferred)
ETL: 5 years (Preferred)
Cloud infrastructure: 4 years (Preferred)
Work Location: One location",$65.00 /hr (est.),501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,1996,$25 to $100 million (USD)
"Intersec
4.5",4.5,Remote,Senior Data Engineer,"Architect and implement highly scalable data pipeline to streamline migration of data from source data repositories to the centralized data platform
Implement agreed upon data models (e.g., data marts) to enable faster and efficient consumption of data by downstream consumers
Conduct performance tuning to optimize data migration techniques and ensure adherence to identified service-level agreements (SLAs)
Define and implement automated validation checks to ensure data integrity between source and target
Assist in onboarding consumers with existing technology to connect to the platform
Setup predictive alert mechanism for pipeline degradation for IMF pipelines.
Enhance data quality framework for data pipelines and derived datasets
Enhance Data Audit Balance and Control mechanism for data pipelines
Enhance pipelines to be certified copies of authoritative source
Populate metadata for the Enterprise with particular focus on the Data Sources being onboarded to EDP (Technical and Business Glossaries) related to IMF Modernization Programs
Validate correct lineage for datasets
Required:
*8+ years of Databricks, Data Modeling, and/or Metadata management experience
* Familiarity with architecting and developing real and near-real-time solutions
* Bachelor's Degree in Engineering, Mathematics, computer Science, Statistics or other quantitative discipline, or 4 years equivalent professional experience
* US Citizenship and ability to gain a public trust
Preferred:
* Familiarity with IRS systems and ETL pipelines
* Ability to work independently and manage multiple tasks
* IRS MBI Clearance with IRS Laptop and Informatica Installed
Job Type: Full-time
Salary: $70.00 - $75.00 per hour
Schedule:
Monday to Friday
Application Question(s):
Are you a US/GC card holder?
How many years of Databricks, Data Modeling, and/or Metadata management experience do you have?
Work Location: Remote",$72.50 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,Unknown / Non-Applicable
Fracsys Inc,#N/A,"Washington, DC",Data Engineer,"Fracsys Inc is hiring a Data Engineer position. The ideal candidate must have at least 12+ years of industry experience. He or she must be responsible for successful technical delivery of Data Warehousing and Business Intelligence projects using a formal project management methodology.
Responsibilities
Directs, motivates and develops key technical personnel
Provides technical direction across the full lifecycle system development process
Provides strategic planning and comprehensive senior level technical consulting to IT senior management and senior technical staff
Working independently within guidelines
Provides detailed inputs for plans and schedules, including goals, risks, and resource allocation
Conduct diagnostic, troubleshooting steps and present conclusions to a varied audience
comfortable slinging some code to solve a problem
Build Data Marts and Data warehouses to facilitate analytical capabilities across the varied sources of data
Design and implement data patterns on cloud using cloud Data warehouse and data virtualization tools
Evaluate new tools and technologies for target state on cloud
Learn and implement new ETL, BI tools and data warehousing technologies
Qualifications
Minimum twelve (12) years in a Technical Lead or Consulting role leading Information Management related initiatives (system integration, data warehouse build, data mart build, or similar)
MUST have proven leadership experience for team of at least 3 technical resources.
Experience creating Data Marts, Data warehouses on-premise and on cloud for real time and batch processing frameworks
Experience building and optimizing big-data data pipelines and data sets including Postgres, AWS Relational Data Service
extensive hands-on experience building and optimizing data structures for data analytics, data science and business intelligence
Experience building self-service data consumption patterns and knowledge of cloud-based data Lake platforms
Experience wrangling data (structured and unstructured), in-depth knowledge of database architecture
Experience utilizing or leading implementations leveraging ETL tools (Informatica / Talend), BI Reporting tools such as MicroStrategy, Microsoft Power BI, data modeling tools such as Erwin, Oracle, SQL server, NoSQL, JDBC, UNIX shell scripting, PERL and JAVA, XML/JSON files, SAS , Python, AWS cloud-native technologies, S3, Athena, Redshift
Familiarity with the following technologies: Hadoop, Kafka, Airflow, Hive, Presto, Athena, S3, Aurora, EMR, Spark
Ability to drive, contribute to, and communicate solutions to technical product challenges
Ability to roll-up your sleeves and work in process or resource gaps and fill it in yourself
Excellent written and oral communication skills in English
Education
Bachelor’s degree in computer science, Data Analytics, Information Systems, or a related degree or equivalent experience. Master’s degree is preferred.
Job Type: Full-time
Pay: From $96,336.72 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Experience level:
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Washington, DC: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person","$96,337 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Atika Technologies,#N/A,"Pittsburgh, PA",Data Engineer,"Role: Data Engineer/PySpark Developer, Pittsburgh, PA - Day One Onsite
Fulltime and Contract(W2)
LinkedIn Must
USC/GC/EADs
Three round of interview: First two on Zoom Video Call, End Client Round will be Onsite on the Client's Location (Pittsburgh, PA)
Job Description:
Python(PySpark), Jupiter hub, airflow, Java
Database knowledge - Oracle / SQL / Data Analytics
Regulatory reporting knowledge preferred
Good communication skills
Open / Ability to learn new technologies
Very good Analytical Skills
Experience in ETL is preferable
Job Types: Full-time, Contract
Salary: $120,000.00 - $130,000.00 per year
Benefits:
Dental insurance
Health insurance
Life insurance
Compensation package:
Bonus pay
Performance bonus
Yearly pay
Experience level:
10 years
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Pittsburgh, PA 15259: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Required)
Spark: 4 years (Required)
Regulatory reporting: 3 years (Required)
SQL: 4 years (Required)
IT: 10 years (Required)
Airflow: 4 years (Required)
Jupiter: 4 years (Required)
Work Location: One location","$125,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"DataArt
4.6",4.6,Remote,Data Engineer,"CLIENT
Our client is a pioneer in US schools’ education since 2000, it is leading the way in next-generation curriculum and formative assessment. They develop a number of solutions and interactive web products for teachers, students and their parents.

The company is technology-driven with a huge number of software engineers involved in product development. They have a very solid approach to technology, employ best practices and processes, with a focus on cutting-edge frameworks, languages and tools.

We invite to the company, not a project
TEAM
There are multiple autonomous and empowered teams who work with their own part of the product and choose their own way of doing things and they expect people to be proactive and good team players.
Responsibilities
Building well-tested and optimized ETL data pipelines for both full and delta extraction
Collaborating with data analysts and learning scientists to store, aggregate, and calculate captured students’ work
Contributing to leading industry data standards, such as Caliper Analytics or xAPI
Improving our deployment and testing automation data pipelines
Requirements
3+ years of professional software development or data engineering experience
Strong CS and data engineering fundamentals
Proven fluency in SQL and a development language such as Python
Experience with the following storages: Snowflake, AWS Storage Services (S3, RDS, Glacier, DynamoDB)
Experience with ETL/BI: Airflow, DBT, Matillion, Looker
Experience with Cloud Infrastructure: AWS Kinesis, Lambda, API Gateway, Terraform
Understanding of ETL/ELT pipelines and Data Warehousing design, tooling, and support
Understanding of different data formatting (JSON, CSV, XML) and data storage techniques (3NF, EAV Model, Star Schema, Data Lake)
BS in Computer Science, Data Science, or equivalent
Experience in education or ed-tech
Proven passion and talent for teaching fellow engineers and non-engineers
Proven passion for building and learning: open source contributions, pet projects, self-education, Stack Overflow
Strong communication skills in writing, conversation",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,1997,$100 to $500 million (USD)
"DSMH LLC
5.0",5.0,Remote,"Data Engineer (AWS, Python)","Title: Data Engineer
Duration: 12+ Months
Technical Skills
(Required)
Four years or more of experience in data engineering and/or software
development.
Three years or more of experience with development, operations, or
architecture in AWS using Python.
Experience with development and delivery of microservices using serverless
AWS services (S3, RDS, Aurora, DynamoDB, Lambda, SNS, SQS, Kinesis,
IAM)
Strong competency in testing, including unit testing to coverage standards and
integration testing.
(Desired)
Software development experience with object-oriented development and design
patterns
Experience with serverless design in AWS
AWS technical certifications (Developer Associate, Solution Architect Associate)
Familiarity with machine learning and data design to support machine learning
Experience with productionizing machine learning workloads
Experience with ElasticSearch/ELK, Kibana, Grafana, and/or Prometheus
Experience with OpenTelemetry
Soft Skills
(Required)
Verbal and written communication skills, problem solving skills, customer
service and interpersonal skills.
Basic ability to work independently and manage one’s time.
(Desired)
Ability to work collaboratively in a complex, rapidly changing, and culturally
diverse environment.
Ability to clearly communicate complex technical ideas.
Comfortable working in a dynamic environment where digital is still evolving as
a core offering.
Job Type: Contract
Pay: $50.00 - $55.00 per hour
Experience level:
6 years
Schedule:
Monday to Friday
Experience:
AWS: 5 years (Required)
Python: 5 years (Required)
Work Location: Remote",$52.50 /hr (est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,$1 to $5 million (USD)
"Tipico - North America
4.0",4.0,"Denver, CO",Data Engineer,"Company Description

Tipico is an energetic, passionate, bold tech company, and we love sports! Tipico has recently expanded to the US market, establishing offices in Hoboken, New Jersey and Denver, Colorado. We are driven by our core values: passion, progress and trust. Our goal is to build the best possible mobile sports betting product in the industry, in order to amplify the emotions of millions of sports fans every day. Let’s Make This Interesting ™
Please note: this role is located in our Denver, CO office; we work off a hybrid model and come into the office 2 days per week.

Job Description

The Data Engineer will report directly to the Data Engineering Manager and work together with rest of the data engineers, BI analysts, and product teams. The position holder ensures this is done by doing the following:
Create and maintain both batch (ETL/ELT) and real-time data pipelines and architecture
Ensure all data provided by Data Platform is of the highest quality and is delivered in a timely manner and in line with agreed SLAs
Assemble large and complex data sets that meet functional and non-functional business requirements
Identify, design, and implement internal process improvements, including automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies
Collaborate with stakeholders including data architects, Executive, Product, Data and IT team members from the beginning to the delivery of a project
Build a scalable, modern, and trusted data ecosystem, making sure it is responsive, available and in line with business requirements
Take part in sprints, workloads, and deliverables through agile methodologies
Manage quality assurance, verification of accuracy and consistency of data, specifically KPI measures sent to the business
Thrive working in a fast-paced and challenging environment
Open to continuous learning individually and with the team
Comfortable with being challenged / seeking challenge
Willing to work flexible hours if needed

Qualifications
3+ years in Data Platforms / Datawarehouse / BI experience
Must have experience in successfully implementing data-centric applications, such as data warehouses, operational data stores, and data integration projects
Strong analytical and problem-solving skills that balance with creative approaches
Ability to learn new and complex concepts quickly
Strong experience with large-scale production databases and SQL
Solid understanding of cloud data services: AWS services such as S3, Athena, EC2, RedShift, EMR, EKS, RDS and Lambda
Proficient with one or more big data & new data technologies such as DBT, NIFI, Airflow, Kafka
Experience with data modelling
Experience with ETL development (extractions, data load, aggregation, transformation)
Knowledge in working with time-series/analytics databases such as Elasticsearch
Understanding of containerization and orchestration technologies like Docker/Kubernetes
Must be able to take ownership of projects with a high level of attention to detail and priority
Team player with a hands-on approach and attitude
Must be comfortable with on call rotation
Nice to have:
Familiarity with Marketing tooling and/or CRM systems is preferred
Familiarity with using business intelligence tools such as Domo, Tableau or Looker
Knowledge about and / or experience with sports betting
Background in digital companies, online, gambling, gaming, e-commerce, marketing, or mobile apps
Familiarity with one scripting language, preferably Python

Additional Information

What's in it for you:
Work in an environment where you, your work and ideas matter and have an impact.
You will be a be part of the newest and up and coming USA online iGaming and sports betting companies in the market.
Work in a new and young business with high growth potential.
Build your own success story together with us.
Work with self-organized, self-responsible and entrepreneurial employees.
Start-up feeling, backed-up by a leading European sports betting house. We are a high-volume business and are taking off in the US!
Competitive salary, Medical, Vision, Dental Benefits, Unlimited PTO, and more
Salary for this position is $120k+ & annual performance bonus","$120,000 /yr (est.)",201 to 500 Employees,Company - Private,"Arts, Entertainment & Recreation",Gambling,2004,Unknown / Non-Applicable
Siri info solutions inc,#N/A,"Beaverton, OR",Sr. Data Engineer,"Must Have Skills
PySpark
Python on AWS
Responsibilities:
Understanding of advanced data warehousing concepts, data modeling and extract, transform and load development
Advanced SQL skills with experience querying large datasets from multiple sources and developing automated reporting
Python skills for scripting, data manipulation, custom extract, transform and loads and statistical/regression analysis particularly, as they apply to sales and marketing operations and performance
Job Type: Full-time
Salary: $47.00 - $50.00 per hour
Experience level:
7 years
Work Location: On the road",$48.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Redzara.us,#N/A,"Atlanta, GA",Azure Data Engineer,"Responsible for developing new and modernizing existing applications to Azure Cloud Platform.
The ideal candidate must demonstrate in-depth knowledge and understanding of Database concepts.
Requirements:
7+ years of experience in Data Warehousing and Business Intelligence
5+ Years in BI development leveraging the Microsoft SQL Server. Needs to be an expert in T-SQL
5+ Years in SSIS and use of Stored Procedures.
1-2 Years in Azure Data Factory, SQL VMs, build and maintain ETL
Ingest and aggregate data from both internal and external data sources to build our data lake
Strong understanding data modeling (i.e., conceptual, logical, and physical model design, experience with Operation Data Stores, Enterprise Data warehouses and Data Marts).
Job Type: Contract
Pay: $43.49 - $57.00 per hour
Compensation package:
1099 contract
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Atlanta, GA 30334: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure Data Factory: 3 years (Required)
Azure: 3 years (Required)
Data warehouse: 1 year (Preferred)
Work Location: Hybrid remote in Atlanta, GA 30334",$50.25 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"shaped.ai Inc.
4.0",4.0,"New York, NY",Data Engineer,"Hello! We’re building the future of content discovery infrastructure with AI — come join us!

Shaped (YC W22) is an API for developers to seamlessly add personalized ranking and recommendation to their products. These frictionless discovery experiences help end-users find what they want faster, and, consequently, grow conversion and engagement business metrics. Behind the scenes, we’re building state-of-the-art ML infrastructure for training and deploying large AI models to power our personalization engine.
We’re a seed stage start-up backed by top-tier investors (e.g. Y-Combinator, Susa Ventures, Tribe Capital etc…) and executives (e.g. Google, Amazon, Uber, Dropbox etc…). Our team comes from Meta, Google, Apple and Uber. We’re a remote team but have a small office in Brooklyn, New York.
We’ve been lucky to have a huge amount of success since our initial launch earlier this year and now looking to expand our real-time data engineering capabilities. We are looking for a data engineer to design, build and optimize Shaped's real-time streaming data ingestion endpoints. You will be a founding engineer working on state-of-the-art infrastructure. As one of Shaped’s early employees you will help shape our product, culture and vision.
This role is for a full-time data engineer at $120k - $200k a year and 0.1 - 0.5% equity.

Responsibilities:
Manage the unification of our batch and real-time data processing pipelines.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Requirements:
Bachelor's in computer science, data science or mathematics related field. Master's degree or PhD will be advantageous.
5+ years of data engineering experience, ideally applied to machine learning use-cases.
Experience with AWS or common cloud technologies
Proficient with Python, SQL.
Extensive knowledge of both offline and online large scale data processing frameworks: Spark, Beam, Flink.
Extensive knowledge with stream technologies like Kakfa or Kinesis.
Excellent written and verbal communication skills.
We’re excited to work with you. Come build the future of AI and discovery with us!","$160,000 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Infosoft,#N/A,"Peachtree Corners, GA",Data Pipeline Engineer - 2173,"Here are the job position details for your review:
Job Title: Data Pipeline Engineer (GStreamer/ DeepStream)
Pay Rate: $90 to 120/Hr.
Duration: 3 Months
Location: Peachtree Corners, GA
This position is for the Building Automation division.
We are looking for Data Pipeline Engineer (GStreamer) to participate in the development of Streaming data pipelines using NVIDIA Deepstream, GStreamer, Python programming, messaging queues, and related tools.
Preference is for candidates located in Atlanta, GA metro area. However, ok with non-local candidates who are open to 75% travel to Atlanta, GA.
Understanding the basics of our architecture/setup to be able to do setup/support for demos, testing, etc.
Basic computer hardware assembly and set-up
Possible set-up/installation of the application(s)
Possible development tasks depending on what the rest of the development team has for them
Develops business software applications, system software, and/or IT solutions on the basis of functional specifications.
Scope of position embraces both the customization or reuse/modification of system software and/or solutions as well as their development from scratch.
Consults with customers in creating functional specifications and work estimates, and supports Make-or-Buy decisions.
Analyses performance gaps of existing software environments and proposes measures to address efficiency problems.
Is also responsible for the implementation of data-driven components supporting data modeling and engineering in the context of data analytics (Data Stewardship).
Skills:
Python
Linux
Docker and Docker Swarm
Zero MQ / a good understanding of messaging
Desired Skills:
NVIDIA Deepstream SDK
Web Sockets
Flask
Protobuf
Possible:
GStreamer (if different than RTSP / H264 / MJPEG)
Job Type: Contract
Salary: $90.00 - $120.00 per hour
Ability to commute/relocate:
Peachtree Corners, GA: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
""Please share your Email ID""
Work Location: One location",$105.00 /hr (est.),Unknown,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Trella Health
4.8",4.8,"Philadelphia, PA",Senior Data Engineer,"At Trella Health, we are passionate and committed to our mission – empowering meaningful change in healthcare. Since our founding in 2015, we continue to grow our team, enhance our solution and services offerings, accelerate into new markets, and expand our customer base. We are rapidly growing and are looking for new Trellavators to join our team!
""What is a Trellavator?"" you ask. Innovate and elevate is the name of our game! We go above and beyond to collaborate with and support each other – we believe that when a colleague or customers succeeds, we succeed. By learning from others, building on our successes, and taking risks, we constantly raise the bar – continuous improvement is in our DNA. Our word is paramount, we keep our commitments, and we always follow through. We have a strong, reliable support system that fuels growth, collaboration, and passion – and together, we create a positive environment where everyone at Trella Health, including the customers we support, can thrive. Are you ready to learn more about the opportunities with our team? Trell-yeah you are!
Position Overview:
The Senior Data Engineer will be responsible for designing, implementing, and maintaining an effective data pipeline for Trella Health's applications. This position is responsible for turning data concepts into effective data models that are efficient and scalable. The Senior Data Engineer will ensure a smooth data release and deployment process throughout the data development lifecycle.
Successful candidate will be a highly organized, self-starter that can roll up their sleeves and solve problems efficiently. Ability to document procedures, manage time, and have a knack for optimizing data flows.
Location: Atlanta HQ; Raleigh, NC; Nashville, TN; Philadelphia, PA; Remote considered
Reports to: Manager, Data Engineering
As a member of the Data team at Trella, you will:
Champion a comprehensive data vision and operating principles that improve data quality, consistency, accessibility, decision making, use, and stewardship.
Manage data pipeline by developing processes for data input, transformation, and storage
Act as an interface between the Data Science team and Application Developers
Architect and implement front-end (application layer) and back-end (ETL layer) database technologies in a highly customized data environment
Develop automated processes for moving data between systems
Implement ETL processes that transform source data into functional data stores consumed by front-end applications
Scale data pipeline by utilizing the most efficient tools and technologies
This job might be a fit for you if you have:
Bachelor's degree in Computer Science, Engineering, Mathematics, or other technical discipline (proven professional experience will be considered in lieu of a technical degree)
2+ full years of recent advanced Snowflake development experience
Experience manipulating healthcare data
In-depth knowledge of Amazon Web Services (AWS) and Platform as a Service (PAAS)
Demonstrable experience with Snowflake utilities including SnowSQL & Snowflake Javascript
Analyze all aspects of the Snowflake Query Engine to improve bottlenecks that may exist
Ability to automate data ingestion from Amazon S3, FTP, and other technologies
Experience with scripting languages, extensive Python experience, Node.js experience a plus
Working knowledge of Bitbucket and Git
Experience managing and loading data in AWS RDS (MySQL/MariaDb)
Ability to optimize MySQL query execution plans for robust performance
Technical Job Requirements (Nice to Have)
Experience manipulating healthcare data including claims, ADT data
Foundational knowledge of CI/CD platforms, preferably TeamCity
Development and consumption of API services
Experience with data science tools (Jupyter, SageMaker, R)
Understanding of Atlassian suite of tools, including Jira and Confluence
Soft Skill Job Requirements (Nice to Have)
Strong analytical skills and ability to rapidly switch contexts
Excellent communication and problem-solving skills
Sole contributor that is willing to independently uncover and improve processes that are inefficient or sub-optimal
Track record of implementing creative solutions to complex problems
Experience working in a nimble, fast-paced, start-up environment
Knowledge of HIPPA, PHI, PII policies
About Trella Health:
Trella Health provides unmatched, actionable market intelligence to post-acute care and value-based care providers of all sizes. Our industry leading analytics paired with CRM and EHR integration workflows positions us as the most advanced sales enablement platform for the post-acute care market. One of only a few companies to be deemed both a Qualified Entity by CMS and an Innovator under its Virtual Research Data Center Program, Trella has elite access to billions of performance metrics and referral data to enable competitive positioning across the market. Its standardized insights, representing 90% of all lives 65+ U.S. population, help customers identify, engage, and manage critical relationships and advance their organizations with certainty.
Since launching our first product in 2016, we have experienced tremendous growth and have highlighted a few key data points to get an at-a-glance view of Trella:
Over 700 customers including 10 of the top 10 post-acute providers and 4 of the top 10 health systems
Over 120 team members and growing
Recognized as one of Modern Healthcare's ""Best Places to Work"" in consecutive years and were ranked in the Top 50 startups in the US by LinkedIn
Headquartered in Atlanta, GA with remote hubs in Nashville, Philadelphia, Raleigh, and the Philippines
The Trellavator Experience:
At Trella Health, you can expect to join a welcoming team that truly embodies our core values. Our collaborative culture is anchored by trust, transparency, and inclusion — and we also have a lot of fun. No matter where you work, at Trella Health you can expect an awesome team, frequent virtual gatherings, engaging events, casual attire, and more.
We offer competitive salaries with a comprehensive benefits package to all employees and provide an environment that fosters work-life harmonization with unlimited PTO, company-observed holidays, and Summer Fridays. As we continue to see exponential growth, our goal is to continue to put team members first and strive to offer our team members the best culture and benefits possible. Some of the benefits we provide are:
Health, Dental, Vision & Voluntary Benefits
Competitive Salary & Bonus Plans
401k Retirement Savings
Unlimited PTO & 10 Paid Holidays
Flexible Work Hours
Equity Shares
Paid Leave Programs
Marketplace for discounted retail and entertainment
Equal Opportunity Employer
Trella Health is an equal opportunity employer. All persons will receive consideration for employment without regard to race, color, gender, sexual orientation, gender identity or expression, religion, national origin, marital status, age, disability, handicap, veteran status, genetic information, or any other protected status as recognized by federal, state, or local laws.","$113,846 /yr (est.)",51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2015,Unknown / Non-Applicable
"Barracuda Networks Inc.
3.6",3.6,"Alpharetta, GA",Data Engineer,"Job ID: 23-579

Come Join Our Passionate Team! At Barracuda, we make the world a safer place. We believe every business deserves access to cloud-enabled, enterprise-grade security solutions that are easy to buy, deploy, and use. We protect email, networks, data and applications with innovative solutions that grow and adapt with our customers’ journey. More than 200,000 organizations worldwide trust Barracuda to protect them — in ways they may not even know they are at risk — so they can focus on taking their business to the next level.
We know a diverse workforce adds to our collective value and strength as an organization. Barracuda Networks is proud to be an Equal Opportunity Employer, committed to equal employment opportunity and equitable compensation regardless of race, gender, religion, sex, sexual orientation, national origin, or disability.
Envision yourself at Barracuda
We currently have an exciting new position open for a Data Engineer. This person will assist in driving technology innovation for engineering projects and work on architecture of AWS, Elastic and Kubernetes. They will also work closely with the Technology Architect to create and drive the Architecture Review Board.
What you’ll be working on
Determine internal / external customer data needs and identify system specifications
Analyze the needs of large systems and breaking them down into smaller manageable parts
Responsible for designing the journey of data from various sources to the database and everything in between
Plan and design the structure of data, brainstorm with team
Analyze best technology products and systems for internal teams
Help provide build vs buy decisions on data architecture
Communicate data requirements to engineers; explain data architecture to them and provide assistance throughout SDLC
Choose / maintain suitable software, hardware and integration methods
Help resolve technical problems as and when they arise
Review testing procedures to ensure data performant and error free
Ensure that database satisfy quality standards and procedures. Setup such standards.
Work with senior IT personnel to devise plans for future IT requirements of the organization
Produce progress reports on key infrastructures - Elastic and Data Pipelines
What you bring to the role
Bachelor Degree or Post Graduate in Computer Science, or relevant work experience
5+ years of experience in an Engineering / Operations heavy organization
1+ years of experience as a Data Analysis and Data Design responsible for data architecture of a SaaS application
Experience in understanding and designing data sharding in a distributed database
Some Hands-on experience in software development and system administration
Knowledge of strategic IT solutions
Familiarity with programming languages like JavaScript or Java or Python
Familiarity with Linux OS and Windows alike
Experience in cloud computing and cloud Technologies (AWS, GCP, Azure)
Excellent communication skills – you should be adept at listening to, understanding, and explaining key concepts to managerial and technical resources.
Good Anticipation and problem-solving skills
Ability to prioritize and manage time – responsible for multiple streams of work at the same time
Relationship building - Ability to form a good rapport with internal and external stakeholders
In-depth knowledge about quality standards and security best practices

Preferred Skills:
Expert level Understanding of Elastic eco system - Elasticsearch, Logstash, Kibana, Filebeats
Understanding of secure tunnel communication using VPN
Experience designing data at scale
AWS experience is preferred in Cloud Technologies
What you’ll get from us
A team where you can voice your opinion, make an impact, and where you and your experience are valued. Internal mobility – there are opportunities for cross training and the ability to attain your next career step within Barracuda.
Equity, in the form of non-qualified options
High-quality health benefits
Retirement Plan with employer match
Career-growth opportunities
Flexible Time Off and Paid Time Off benefits
Volunteer opportunities
#LI-Remote","$98,161 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,$100 to $500 million (USD)
"Merkle
3.7",3.7,"Norristown, PA",Data Engineer,"Company Description

We are Dentsu International. We innovate the way brands are built. That means we do things differently so they’re better than before. In this way, we make our clients’ most important marketing assets—their brands—win in a changing world.
Dentsu International is a modern marketing solutions company. Our mission is to help clients navigate, progress, and thrive in a world of change. Businesses rely on our integrated network of agencies and specialized practices to champion meaningful progress through creative, media, commerce, data, and technology. Part of Dentsu Group, our global network comprises 66,000 diverse people in 143 countries, who are dedicated to teaming for growth and good. Follow us on Twitter @DentsuUSA and visit dentsu.com/us.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.

Job Description
Develop onboarding process to receive and process information related to onboarding a new subscriber
Convert Snowflake Stored Procedures to Python Scripts directly in Airflow
Develop metadata driven solutions for managing data processing and cadence
End to end automation development and testing
Building and maintaining stored procedures and scripts in Snowflake Data Cloud
Building and maintaining complex SQL queries for ETL/ELT processing
Develop Dashboard for data processed for audit and governance needs
Follow Agile methodologies to deliver iterative feature sets rapidly
Maintain codebase using source control methodologies, Git
Collaborate with business analysts, data analysts, and other developers to deliver powerful new features
Coordinate with teams as part of a larger data-sharing system
Contribute to a collaborative, positive, and enjoyable environment for your operations team
Performing quality assurance and testing
Writing user and technical documentation such as runbooks

Qualifications
Strong communication skills both verbal and written
Strong understanding of conditional programming logic
Knowledge of software engineering and development methodologies, techniques, and tools; includes Issue Tracking (JIRA) Software Development Lifecycle SDLC), and Continuous Integration / Continuous Development (CI/CD)
Experience working with large databases and data warehouses utilizing both relational and dimensional design concepts
Eagerness to learn new technologies, programming languages and platforms to solve complex data challenges
Experience with Python required
Experience with SQL required
Experience with Snowflake Data Cloud is highly desirable
Experience with Amazon Web Services (AWS) a plus
Experience with Airflow (or other orchestration) a plus
Experience in database marketing is a plus

Additional Information

Perks:
Great compensation package
Comprehensive healthcare plans
401(k) with employer match
Flexible time-off
16 weeks paid parental leave
Additional Information
The anticipated salary range for this position is $94,000-$146,000. Salary is based on a range of factors that include relevant experience, knowledge, skills, other job-related qualifications, and geography. A range of medical, dental, vision, 401(k) matching, paid time off, and/or other benefits also are available. For more information regarding dentsu benefits, please visit dentsubenefitsplus.com
Employees from diverse or underrepresented backgrounds encouraged to apply.
Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact recruiting@dentsuaegis.com if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.
About dentsu
Dentsu is the network designed for what’s next, helping clients predict and plan for disruptive future opportunities in the sustainable economy. Taking a people-centered approach to business transformation, dentsu combines Japanese innovation with a diverse, global perspective to drive client growth and to shape society www.dentsu.com.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.
Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact your recruiter if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.","$120,000 /yr (est.)",5001 to 10000 Employees,Company - Private,Media & Communication,Advertising & Public Relations,1971,$500 million to $1 billion (USD)
Proits Hub LLC,#N/A,Remote,Data Analytics Engineer,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote
Speak with the employer
+91 9417599933",$60.00 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Iron Bow Technologies
3.5",3.5,"Washington, DC",Sr. Data Center Engineer,"Iron Bow Technologies is seeking a Sr. Data Center Engineer to support our Federal customer, The Department of Justice (DOJ), Criminal Division (CRM), in Washington, DC. The selected candidate will support our flexible and scalable solution with the DOJ CRM to extend cloud-native features to existing legacy and next-generation applications while also future proofing the infrastructure to easily accommodate future requirements, such as transitioning to a public cloud.
This position is hybrid and requires 2 days a week onsite at the customer location in NW Washington, DC and support at the customer data center as needed in Sterling, VA.
Essential Duties and Responsibilities:
Assess and provide VMware solutions to include vCenter, vSphere, SRM, ESXi
Primary storage solutions and data file storage management, NAS file storage solutions
Data Center blade servers and rack servers
Server virtualization
Experience:
10 years of related experience providing system administration support in a virtualized environment, including at least 6 years of VMWare solutioning, including vCenter, vSphere, SRM, and ESXi
Data Center operations experience, with SAN and NAS
Experience supporting primary storage solutions and data file storage management, NAS file storage solutions
Hands on experience with Data Center blade and rack servers
Ability to work staggered shift hours when needed to support critical coverage and/or time-sensitive needs
Be a member of our Team providing coordinated 24x7 IaaS on-call support
U.S. Citizen
Currently hold or obtain/maintain a Public Trust High clearance
Education:
BS in Computer Science, Information Science, or other relevant discipline is preferred – additional years of experience may be considered in lieu of the degree
Preferred Certifications:
VMware Certified Professional
Primary storage certifications such as HPE, Dell-EMC, NetApp, or Pure Storage
VCP-DCV
Travel Requirements:
No travel
PRINCIPALS ONLY. NO AGENCIES PLEASE. MUST BE A US CITIZEN ABLE TO OBTAIN A PUBLIC TRUST SECURITY CLEARANCE.
Iron Bow Technologies is a leading IT solution provider dedicated to successfully transforming technology investments into business capabilities for government, commercial and healthcare clients. Our depth of technical expertise, global reach and strategic partnerships with industry leaders as well as disruptive technology partners uniquely positions Iron Bow to target the right solution to ensure your successful business outcome. We partner with our clients from planning and implementation through ongoing maintenance and management to deliver solutions that are strong, flexible and on target .
Iron Bow Technologies is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.
#LI-TM1",#N/A,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,1983,$500 million to $1 billion (USD)
"Quadrant Resource
4.1",4.1,Remote,Data Engineer,"Job Description:
4-6 years of experience as a Data Engineer in the Snowflake-Fivetran-dbt stack
• Possess a solid understanding of end to end Data ingestion, ETL, data analysis, Reporting processes
• Expert in ETL development using Fivetran and experienced in implementing data pipelines for Snowflake Cloud DWH
•Experienced in using dbt for data transformation
•Expertise on Tableau/Metabase for building reports/dashboards
•Expert in troubleshooting and resolving ETL issues, data load failures/problems and transformation translation problems
•Excellent communicator with the ability to work effectively with distributed teams
Job Type: Full-time
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Data Engineer: 9 years (Required)
Fivetran: 8 years (Required)
DBT: 7 years (Required)
Snowflake: 6 years (Required)
Work Location: Remote",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,2004,Unknown / Non-Applicable
"DiNi Communications, Inc.
3.0",3.0,"New York, NY",Data Center Engineer,"DiNi Communications, Inc. is seeking a Data Center Engineer. This person needs to be a team player with exceptional communication skills. Maintaining the confidentiality of the client's information is of utmost importance.
Required Education and Experience
· At least 8 years of experience as a large data center technician
· Knowledge of Networking,
· Ability to lift equipment at least (50lbs) Rack and Stack (Network Switches, Servers, Storage,
Appliances)
· Knowledge of structured Cabling in a Data Center.
· General knowledge of Operating Systems, (Windows, Unix).
· General knowledge of networking in N Tier Environment (App, Web, DMZ,).
· Level 1 and Level 2 troubleshooting in a N Tier Environment.
· Ability to follow and execute commands to start up and shutdown systems.
· Physically travel to and be available at site B to perform technical changes that require a
person at Site B.
· Performing backup and restore activities.
· Asset Management.
· Maintaining document.
· Work with vendor on circuit issues, (WAN and Internet).
· Cable Patch the equipment in the environment.
· Participate in Site B fall-over testing
Preferred Network Engineering Experience
· Knowledge of N Tier networking.
· Certification in Networking (CISCO).
· Certification in various Operating systems (Linux and Windows), Storage, Messaging
(Microsoft).
· Certification in IT Security.
· Knowledge of Databases.
· Experience working with CISCO, EMC, Symantec, HP, Oracle, Bluecoat, MacAfee, and other vendors' equipment and software in a networked environment.
Responsibilities include but are not limited to:
Adding equipment to support agency applications in site B Environment,
Coordinating Site B testing with Site A technical resources.
Monitoring the B environment.
Rack and Stack new and replacement equipment
Troubleshooting and resolving incidents and problems in the
Physically travel to and be available at Site B to perform technical changes that require a person at Site B.
Upgrading and expanding the environment
Performing backup and restore activities
Manage Site B assets
Maintaining document
Cable Patch the equipment in the environment
Participate in Site B fall-over testing.
Job Type: Full-time
Pay: $60.00 - $70.00 per hour
Schedule:
Monday to Friday
Experience:
large data center technician: 8 years (Required)
Networking in N Tier Environment (App, Web, DMZ,): 3 years (Required)
Ability to lift equipment at least (50lbs): 1 year (Required)
structured Cabling in a Data Center: 3 years (Required)
Level 1 and 2 troubleshooting in a N Tier Environment: 3 years (Required)
Performing backup and restore activities: 3 years (Required)
Cable Patch the equipment: 3 years (Required)
Certification in Networking (CISCO): 1 year (Preferred)
Certification in IT Security: 3 years (Preferred)
CISCO, EMC, Symantec, HP, Oracle, Bluecoat, MacAfee: 3 years (Preferred)
Work Location: In person",$65.00 /hr (est.),Unknown,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
Atalan Tech,#N/A,Remote,Data Engineer,"Atalan Tech is a start-up with a mission to mitigate the impact of clinician burnout and turnover in our nation’s health system. We are expanding our team and looking to hire the critical role of Data Engineer. The Data Engineer will work closely with the Chief Technology Officer, Chief Scientific Officer (UCLA Ph.D.), Chief Product Officer (Oxford Ph.D.), and Chief Executive Officer to deliver actionable insights from data in a variety of situations.
We are a small, close-knit team. This is an excellent opportunity for someone to join a mission-oriented, early-stage company at the ground floor and grow with us. There will be plenty of room for growth and creativity, and opportunities to tackle challenges that present one of the toughest problems in our society.
Requirements
Strong hands-on experience of designing, developing and managing data pipelines
Strong coding skills in Python (required) and SQL (required) and R (strongly preferred)
Experience building and maintaining data pipelines on AWS using EC2, EBS, Redshift, Postgres, S3, Data Pipeline, Kinesis, Lambda, etc.
Thorough understanding and experience with task management tools (Airflow, NiFi, etc.)
Knowledge of data management best practices for ETL, data governance, data warehousing, data modeling and data science
Basic knowledge of Linux (Redhat preferred) administration
Minimum B.S in Computer Science, software engineering or related field (or equivalent in related professional experience)
Strong communication skills
Strong problem-solving/critical thinking skills
Ability to work in a fast-paced, dynamic start-up environment
Some knowledge of healthcare, health tech, or healthcare data analytics (preferred)
Typical Tasks and Responsibilities include:
Working closely with CTO and CSO to build data pipelines for machine learning
Assisting the CPO in managing and supporting analysis of enterprise data to drive better products and solutions
Managing company’s data infrastructure on AWS (Postgres/RDS, Redshift, S3, etc.)
Synthesizing multiple data sources, such as internal medical records and HR data
Integrating with client systems including EHR (Epic, Cerner), HRIS, flat files, etc.
Working closely with other key members on other projects
Developing and designing data security standards and policies in line with industry best practices
About Us:
Atalan Tech’s mission is to improve the wellness of clinicians and their patients. We blend labor economics, machine learning, and clinical psychology with a team from Harvard, Yale, and UCLA. Atalan Tech is looking to contribute innovative and sustainable solutions to one of the most pressing problems in the healthcare industry. Building upon our proof-of-concept success, we have recently secured multiple engagements with nationwide healthcare organizations and have successfully raised institutional funding to expand our reach.
Contact Us:
If you’re interested in this position, please send us your CV/resume and cover letter.
Job Type: Full-time
Pay: $108,448.20 - $130,604.27 per year
Experience level:
2 years
Schedule:
Monday to Friday
Application Question(s):
Do you know the role is full time?
Can you start within the next month?
What are your salary expectations?
Have you held the title of a data engineer? If not, what experience do you have as a data engineer? Please be very specific.
Have you released Python code to a production environment in the last year?
Experience:
Data analytics: 2 years (Required)
Work Location: Remote","$119,526 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Take Command Health
4.9",4.9,"Dallas, TX",Data Engineer,"Company: Take Command
Position: Data Engineer
Salary: $120,000-$130,000 plus bonus
Location: Dallas or Austin, TX
About Take Command
Take Command is a start-up on a mission to improve the healthcare system, starting with health insurance. Pragmatically speaking, we help employers reimburse employees for individual insurance instead of offering a traditional one-size-fits-all group plan. We believe this model can empower employees (when they have the right support) to be savvy healthcare consumers and have a transformative impact on the entire healthcare system.
Let’s be honest—health insurance is usually a confusing, frustrating, and even emotional experience for people. We want to fix that with a new model, great technology, and a superior user experience. We have made a great start, but we need your help to fully realize our long-term vision.
As a part of the dev team, you'll build the framework for both our clients and our team to succeed. You will be a pillar in our company as we grow and make our product the best product on the market!
In this role, you will:
Take ownership for delivering multiple features, their quality and assist team members in delivering their features
Recommend improvements to data quality, processes and code to make it easier to use, manage and maintain
Experience designing and developing data models and data migrations
Experience managing and maintaining ETL or ELT processes (Hevo, BigQuery, Looker, etc.)
Hands-on development experience with SQL, relational database design (ERDs)
Must be able to write SQL & contribute code
Be product-minded; focus not only on what needs to be built but why something needs to be built
You may be a great fit for this role if you have:
3 or more years of professional experience as a data analyst or data engineer
Cloud-Based Data Tools e.g. Hevo, BigQuery, Looker, etc.
Database technologies: SQL, RDBMS / Relational databases (Postgres, Amazon RDS / Aurora, etc.)
Nice to have experience: Python, Java, Typescript or another modern OOP language
Cloud infrastructure: AWS
Source control: Git
Working at Take Command
We’re excited to build a team and culture that reflects our values! We offer competitive pay, health benefits, and equity options to share with this position. We want you to be an owner along with us in revolutionizing healthcare.
A generously funded ICHRA for medical, dental, and vision premiums and medical expenses. You get to use our own product and we think that’s so exciting and rare!
Unlimited personal vacation in addition to regular company holidays.
401(k): 90-day eligibility for 2% match that vests immediately!
Access to LinkedIn Learning.
We have two beautiful offices conveniently located near The Hill (Dallas) and Holly Commons (Austin). The kitchen is well-stocked and we've designed the space to have lots of different areas to work!
Paid parental leave for new parents.
Flexible on where you work – we hope to see you around the office on average 3 days a week especially when white-boarding or building relationships with your colleagues, but you also have the ability to work from home or wherever you'd like when you need to get focus work done.
More About Us
We secured our Series A funding in 2021 and are thrilled to be able to expand our team. Despite being a small startup in a land of health insurance giants, we’re the recognized industry leader for what we do (health insurance reimbursements) and passionate about bringing it to market because we know we can help fix a broken system and improve our clients’ wellbeing and health outcomes.
We’ve been featured in The New York Times, The Wall Street Journal, The Dallas Morning News, and other national healthcare publications and are excited about our growth opportunities.
Take Command knows diversity and inclusion among our teammates is integral to our company’s success and growth. Our vision is to recruit, develop, and retain the best team from a diverse candidate pool.
This has mostly been about us, but we’d love to hear from you-we can’t wait to hear your story!
Take Command is an equal opportunity employer! We celebrate diversity and are committed to creating an inclusive environment for all employees.
Xrygn4NXMr","$125,000 /yr (est.)",1 to 50 Employees,Company - Private,Insurance,Insurance Agencies & Brokerages,2014,Unknown / Non-Applicable
"Petrolink International
2.9",2.9,"Houston, TX",Data Analysis Engineer,"Location:
Houston, Texas, U.S.A.
Job Type:
Full-time
Job Description:
Petrolink is a global company that specializes in user-driven technology for the Energy Industry.
We are looking for a Data Analysis Engineer to assist with creating and delivering algorithms primarily used by our engineering solutions.
This position is based in our U.S. office in Houston, Texas.
Duties & Responsibilities:
Research and provide logic for engineering solutions algorithms.
Collaborate with Subject Matter Experts (Drilling Engineers, Mud loggers, Pore Pressure Analysts, and other specialists from the Drilling Industry) to understand different challenges in the Drilling Industry and the practices in place to address/solve these challenges.
Code the algorithms using C# to run on Petrolink’s proprietary analytics engine.
Develop, test, and maintain algorithms.
Support escalations related to issues in algorithms.
Learn existing algorithms to provide support when required.
Prepare training material and sessions for stakeholders as part of the production release.
Skills & Qualifications:
Proficient in C#.
Experience with Python is an advantage.
Knowledge in drilling practices, and ability to speak to other drilling experts and capture requirements and logic.
Knowledge in drilling performance KPIs is an advantage.
Prior experience with WITSML is an advantage.
Function and communicate well in a team having diverse expertise.
Bi-lingual (English and Spanish) preferred.
At least 2 years’ relevant experience.
Benefits:
Health insurance
Dental insurance
401(k) with matching
Health savings account
Life insurance
Long-term disability
This job description is intended to explain the general nature and level of work being performed. Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.
Applicants must be legally authorized to work in the United States for any employer.
To apply for this position, applicants must meet the above criteria.
Interested applicants fulfilling the above criteria, please send your resume to careers.usa@petrolink.com.","$81,133 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"Hiscox
4.1",4.1,"Atlanta, GA",Senior Data Engineer,"job description:
You will be a senior member of our US IT Team data engineering practice. You will be an early member of this team and will work closely with the Director of Data Engineering to help establish it. We will have our work cut out for us delivering planned work, building the practice and evolving our technical landscape. This is an exciting opportunity for the right individual to join at the very beginning and grow with the new team.
As a senior you will be expected to lead, mentor others and deputise for the director of data engineering when required. Yoi will also continue to contribute as a hands-on engineer. You will have the knowledge, best earned through 5+ years of experience, to truly inhabit this senior role in an ambitious team.
What you’ll be doing:
Working with business stakeholders and our delivery team to understand high value business problems that can be solved through the application of data processing and analytical systems.
Developing, expanding and evolving our existing databases and ETL pipelines.
Working with architects to design, build and support a transformational cloud-based data platform for the business.
Being a core and professional member of the new data engineering practice
Understand business requirements and help refine into development tasks and estimate their complexity.
Research, evaluate and adopt new technologies with a right tool for the job mentality.
Focus on both speed of delivery and quality, with suitable pragmatism – ensuring your solutions are always appropriate and not too complex or over-engineered.
Quick progression of projects from proof-of-concept to post-production stage.
Communication and presentation of ideas to colleagues in all parts of the wider tech team.
Participating in code reviews for the data engineering practice.
Act as a servant leader in our practice – ensuring team members have
person specification:
Knowledge of, or ability to rapidly adopt our core languages for data engineering – SQL and/or Python.
Experience working using a modern DevOps approach. You should not just have written data code but also the CI/CD pipelines necessary to test and deploy that code in a professional environment.
A robust understanding of core data engineering topics – ETL vs ELT, structured and unstructured data, data quality and data governance.
Ability to contribute to all aspects of a solution – design, infrastructure, development, testing and maintenance.
The ability to design and advocate for technical solutions to business problems.
Effective collaboration with technical and non-technical team members through agile ceremonies – roadmap planning, feature workshops, backlog elaboration, code review.
Understanding of cloud technology and significant hands-on experience working with it. You must understand how cloud-native solutions must be built differently from traditional ones.
Track record of taking initiative and delivering projects end-to-end; clear evidence of being self-driven and motivated
Immense curiosity, high energy and desire to go the extra mile to make a difference.

Desirable Skills
Beyond the required skills we are open to individuals of diverse talents. Experience with additional technologies, data science knowledge or a business background are all valued. We want to know how your unique abilities can contribute to our team.
Our Technology
We operate in a diverse technical landscape and are looking for flexible engineers who can adapt to and use many different tools. We would not expect any engineer to be familiar with the entire tech stack – no engineer can. Instead, we seek people with a good understanding of data structures and algorithms and the ability to apply this knowledge in learning new tools.
We have an existing data landscape involving both MS SQL Server and Oracle databases. We use SSIS for ETL. We will also need to source data from our integration layer – webMethods.
If some of that sounds like it needs bringing up to date, we know! We hope to start building a modern cloud data platform to realise the full value of our data. The technologies we use for that platform are still to be decided. Likely it will be based on Snowflake with Azure Data Factory for ELT. Other parts of our business have had success using Prefect for orchestration and dbt for transformation and we will reuse where it is sensible.
We prepare data for use by analysts working with a variety of tools – Tableau, PowerBI, and even Excel.
We take a DevOps approach and strive for continuous integration / continuous deployment. We use Azure Pipelines to deploy our code. We deploy infrastructure the same way using Terraform and Docker.
Our technology landscape is not fixed. Our engineering and architecture teams drive the technology we adopt.


Salary range $140,000 - $170,000","$155,000 /yr (est.)",1001 to 5000 Employees,Company - Public,Insurance,Insurance Carriers,1901,$1 to $5 billion (USD)
Staff Bees Solutions,#N/A,"Dallas, TX",Data Engineer,"We are looking for OPT/CPT individuals, Helping Them to Train, Providing knowledge, and Placing Them in Fortune companies with the help of our Direct Clients in Big Data, Machine Learning, And Data Engineering Suitable Positions. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross-functional teams.
Responsibilities for Data Engineer
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics..
Qualifications for Data Engineer
An individual who has valid visa, Opt/Cpt are Applicable
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Strong project management and organizational skills.
Job Types: Full-time, Part-time, Contract
Salary: $70,000.00 - $80,000.00 per year
Benefits:
Employee assistance program
Health insurance
Professional development assistance
Relocation assistance
Compensation package:
Yearly pay
Experience level:
1 year
No experience needed
Under 1 year
Schedule:
10 hour shift
4 hour shift
8 hour shift
Monday to Friday
Ability to commute/relocate:
Dallas, TX 75243: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$75,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Diamondpick,#N/A,"Alpharetta, GA",Java Software Engineer with Big-Data,"About us
There is someone out there who fits your requirements like a glove! We are an innovative Talent Solutions company with a vision to build talent supply chain models without compromise for the technology industry.
We are a new-age talent solutions company with a vision to build an enterprise recruitment model without compromise. We love to solve complex talent challenges and strive to be an enabler of business success for our clients using data, technology and our team’s vast experience in the technology market.
Senior Java Developer with Big data
Alpharetta, GA (Hybrid Work from Day1)
Job Description:
The ideal candidate must possess strong background on frontend and backend development technologies.
The candidate must possess excellent written and verbal communication skills with the ability to collaborate effectively with domain experts and technical experts in the team.
Responsibilities:
As a Java Senior Developer, you will
Maintain active relationships with Product Owner to understand business requirements, lead requirement gathering meetings and review designs with the product owner
Own his backlog items and coordinate with other team members to develop the features planned for each sprint
Perform technical design reviews and code reviews
Be Responsible for prototyping, developing, and troubleshooting software in the user interface or service layers
Perform peer reviews on source code to ensure reuse, scalability and the use of best practices
Participate in collaborative technical discussions that focus on software user experience, design, architecture, and development
Perform demonstrations for client stakeholders on project features and sub features, which utilizes the latest Front end and Backend development technologies
Requirements:
6+ years of experience in Java/JEE development
Skills in developing applications using multi-tier architecture
Knowledge of google/AWS cloud
Java/JEE, Spring, Spring boot, REST/SOAP web services, Hibernate, SQL, Tomcat, Application servers (WebSphere), SONAR, Agile, AJAX, Jenkins..etc
Skills in UML, application designing/architecture, Design Patterns..etc
Skills in Unit testing application using Junit or similar technologies
Good communication skills
Leadership skills
Provide overlap coverage with onsite/customer teams
Capability to support QA teams with test plans, root cause analysis and defect fixing
Strong experience in Responsive design, cross browser web applications
Strong knowledge of web service models
Strong knowledge in creating and working with APIs
Experience with Cloud services, specifically on Google cloud
Strong exposure in Agile, Scaled Agile based development models
Familiar with Interfaces such as REST web services, swagger profiles, JSON payloads.
Familiar with tools/utilities such as Bitbucket / Jira / Confluence
Job Types: Full-time, Contract, Temporary
Pay: From $60.00 per hour
Benefits:
Flexible schedule
Compensation package:
Hourly pay
Experience level:
8 years
Schedule:
8 hour shift
Day shift
Work Location: Hybrid remote in Alpharetta, GA 30005",$60.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
TekValue IT Solutions,#N/A,"Houston, TX",Data Engineer with PL/SQL,"Data Engineer
Day 1 Onsite
Loc Houston Texas
Required Skills:
Requirements:
Recent PL/SQL development work experience with Oracle 19.
Must have working experience with tools like Airflow, Git.
Must have python coding experience and REST API development experience.
Plus would be experience with microservices, docker, Kubernetes.
Plus would be experience with accounting and financial data
Experience in event driven and cloud based architectures and working knowledge of containerization and streaming technologies.
Detailed work experience on Oracle, SingleStore or IBM I DB2 database, SQL & PL/SQL, including ANSI SQL.
Experience in database analysis, development and maintenance of business applications specific to oil & gas back-office operations. Expertise in various ETL tools such as SQL*Loader, Equalum, Python scripts.
Familiarity with No-SQL databases and pipelines (Elasticsearch, MongoDB).
2+ years’ experience in the design, implementation and support of full life-cycle data engineering projects.
Bachelor's degree in Computer Science, Engineering or other equivalent degree, with 5+ years data engineering experience or Master's degree in Computer Science, Engineering or other equivalent degree with 2+ years data engineering preferred.
High school diploma or GED is required.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Houston, TX 77003: Reliably commute or planning to relocate before starting work (Required)
Experience:
PL/SQL: 9 years (Preferred)
Python: 8 years (Preferred)
Database development: 8 years (Preferred)
Work Location: One location",$72.50 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Seamless.AI
3.4",3.4,"Columbus, OH",Senior Data Engineer - Remote US,"Role: Senior Data Engineer
The Opportunity
We are seeking a Senior Data Engineer with a strong background in data ingestion and processing for large data sets. The ideal candidate is maniacal about data accuracy and has a proven track record of building robust and scalable data pipelines. This role requires an experienced professional who can work collaboratively with cross-functional teams to design and implement solutions that meet business requirements and adhere to data security and privacy policies.
If you are a senior data engineer with a passion for data accuracy and a proven track record of building robust and scalable data pipelines, we encourage you to apply for this role. We offer competitive compensation, a flexible work environment, and opportunities for growth and advancement within the organization.
About Seamless
Seamless delivers the world's best sales leads. Through our product, we help sales teams maximize revenue, increase sales, and easily acquire their total addressable market using artificial intelligence; by development of a robust real-time contact and company search engine as well as a suite of technically-advanced tools to support sales and lead generation. We have been recognized as one of Ohio's fastest growing companies and have been recently ranked No. 7 in LinkedIn's Top 50 Startups of 2022, featured in Forbes as #1 Software company in Ohio in 2022, and on G2's ""Top 100 Highest Satisfaction Products for 2022"" list!
The Seamless Team
We have an amazing culture and work environment that anyone would want to be a part of. We encourage a culture of positivity. We thrive off of continuous feedback and do whatever it takes to help our team and customers be successful. You will grow as an individual, professionally, and be able to see and feel the impact you are making to the growth of Seamless every day.
Role Responsibilities
Design and develop data ingestion and processing pipelines that ensure data accuracy and completeness.
Work closely with cross-functional teams to understand business requirements and design solutions that meet those requirements.
Lead efforts to identify and implement data quality control measures to ensure consistency and reliability of data.
Monitor data pipelines to identify issues and proactively resolve them.
Evaluate and recommend new technologies and techniques to improve data pipeline performance and scalability.
Ensure compliance with data security and privacy policies and procedures.
Mentor junior engineers and provide technical leadership in data engineering best practices.

Candidate Requirements
Bachelor's or Master's degree in Computer Science, Software Engineering, or a related field.
3+ years of experience in data engineering, with a focus on data ingestion and accuracy.
Strong experience with large-scale data processing frameworks such as Hadoop, Spark, or Flink.
Proficiency in programming languages such as Python, Java, or Scala.
Experience with SQL and NoSQL databases.
Familiarity with cloud-based data processing and storage platforms such as AWS, Google Cloud, or Azure.
Strong experience in designing and implementing data quality control measures.
Excellent problem-solving and analytical skills.
Strong communication and collaboration skills.
Experience mentoring and leading teams of data engineers.
Seamless.AI has been delivering the world's best sales leads since 2015. Our product is the first real time, B2B search engine helping sales teams maximize revenue, increase sales, and easily acquire their total addressable market using artificial intelligence. We have been recognized as one of Ohio's fastest growing companies and won 2020 Best Places to Work and LinkedIn's Top 50 Tech Startups in 2020 and 2022. We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. Visa Sponsorship is not included in our hiring package. Applicants will need to be authorized to work in the U.S.","$93,575 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable
"Titan America
3.8",3.8,"Roanoke, VA",Data Engineer,"Data Engineer
We are seeking a motivated and detail-oriented Data Engineer to join our team based in Roanoke, Virginia. As a Data Engineer, you will play a crucial role in designing, developing and maintaining scalable data pipelines and transformations for business-critical use cases. In addition, you may take on various Software Development and Data Science responsibilities as we frequently share responsibilities and knowledge across our growing team.
Responsibilities:
Design, develop and maintain scalable data pipelines for extracting, transforming, and loading data from various sources
Collaborate with cross-functional teams to identify data needs and determine the best data solutions
Develop and implement data models to support business requirements and ensure data quality
Ensure the security and privacy of sensitive data by implementing appropriate access controls
Monitor and optimize data pipeline performance to ensure timely and accurate data delivery
Document data pipeline processes, data dictionaries, and data storage solutions
Requirements:
Bachelor's degree in Computer Science, Computer Engineering, or a related technical field
Minimum of five years of professional experience working as a Data Engineer or Software Developer
Strong hands-on experience with data warehouse and transformation solutions, i.e. Domo, Snowflake or similar
Proficient in at least one scripting language such as Python, JavaScript, or R
Understanding of data modeling, data integration and data quality processes
Familiarity with cloud platforms such as AWS, Azure, or Google Cloud Platform
Strong analytical and problem solving skills
Full Stack Software Development experience in a professional setting is highly desired, but not required
This is an excellent opportunity for a driven and collaborative individual to make a significant impact in a dynamic and growing team. If you have a passion for data and a desire to work in a fast-paced and dynamic environment, we want to hear from you!
Success begins with hiring the right people to partner with us as we grow and develop our businesses. People are central to everything we do. It is through their efforts and talents that Titan has been successful for over 100 years.
Titan America is an Equal Employment Opportunity (EEO) / Affirmative Action employer and welcomes all qualified applicants. Applicants will receive fair and impartial consideration without regard to race, sex, color, religion, national origin, age, disability, veteran status, genetic data, or other legally protected status.
Job Type: Full-time
Pay: $75,000.00 - $85,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Referral program
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Roanoke, VA 24019: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Do you currently or will you in the future need sponsorship to work in the United States?
This position requires the successful candidate to be in the office in Roanoke, VA at least 3 days a week. Can you meet this requirement?
This position requires the successful candidates to be a full time, W2 employee. Can you meet this requirement?
Education:
Bachelor's (Preferred)
Experience:
Data Engineering: 5 years (Preferred)
Work Location: Hybrid remote in Roanoke, VA 24019","$80,000 /yr (est.)",1001 to 5000 Employees,Company - Public,"Energy, Mining & Utilities",Mining & Metals,1902,Unknown / Non-Applicable
"Apolis
3.9",3.9,"Saint Louis, MO",Data Engineer (Python Focused),"Role: Data Engineer (Python Focused) (W2 ONLY)
Location : St. Louis, MO 63102
Duration: 18 Months
The primary responsibilities of this role will be to work as a part of the data and analytics team of the client. The ideal candidate would have experience using ETL and other data engineering tools in a cloud environment. Additionally, this candidate would have coding and scripting experience creating code to move and manipulate data using tools such in Python such as Spark or Pandas. Microsoft Azure experience is a plus but relevant experience with other cloud-based environments will be considered.
This team uses Microsoft Azure to build and productionize data sets and processes to the product owners and product users and this role will be expected to contribute to those objectives. As a member of this team, you will work in conjunction with other data engineers, platform engineers, solution developers and others to execute and deliver working solutions to the end users.
Technical Skills
High focus on Python
More Programmer/Coder experience, less Cloud experience needed.
Must have experience with data analytics/engineering and cloud technologies and an in-depth working knowledge and understanding of data engineering, design, and integration.
Intermediate or higher level of proficiency with Python Must have the proven ability to learn quickly and apply cloud data engineering principles and skills to new tools and use cases.
Ability to define and take technical ownership of detailed requirements and collaborate effectively with various team members such as Business Analysts and Project Managers and end users.
Excellent communication (verbal and written) and problem-solving skills are essential for this position.
Initiative, Proactive Cooperation, and curiosity are mandatory behavioral skills required for this position.
Experience with several of the following technologies is preferred:
Experience with Microsoft Azure Platform. In particular, the data processing toolsets including Azure Data Factory Pipelines, Azure Databricks, Azure Data Lake Storage.
Experience using Notebooks (Such as Jupyter, DataBricks) also desired.
Experience using code languages such as Python and C#
Experience using query languages such as SQL, MDX and DAX
Experience using scripting languages such as PowerShell, M-Query (Power Query), and Windows batch commands.
Understanding of Agile and DevOps principles for software development and ownership.
Familiar with compliance and controls for software development and deployment.
Education Required
A bachelor's in computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field [or equivalent work experience] is not required but is preferred.
Job Type: Contract
Salary: $60.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Vision insurance
Compensation package:
Overtime pay
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
St. Louis, MO: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Preferred)
ETL: 5 years (Preferred)
Cloud infrastructure: 4 years (Preferred)
Work Location: One location",$65.00 /hr (est.),501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,1996,$25 to $100 million (USD)
"CDNetworks Inc.
2.9",2.9,"Monrovia, CA",Jr. Data Center Engineer,"CDNetworks is one of the top leading CDN & Edge Service Providers with global offices in Korea, Japan, Singapore, Malaysia, China, Russia, London, and Canada. We focus on delivering integrated cloud and edge computing solutions with unparalleled speed, ultra-low latency, rigorous security, and reliability so that our clients can focus on what’s most important – growing their business.
The Jr. Data Center Engineer is a Full-Time position based in Monrovia, CA. Job is performed indoors in a Data Center or Warehouse environment. 75% Travel required for this position.
Job Responsibilities
Ensure all incidents are logged and resolved, gather all relevant data, and ensure all incidents and tasks follow the appropriate procedures.
Support data center activities and work closely with our system and network team to complete tasks/projects.
First responder to all alerts and problem reports while managing communications between departments and handling crisis documentation and dissemination after the fact.
Utilize internal systems such as JIRA/Wiki to manage project plans and progress.
Performing general system administration duties including OS patching and upgrades, batch job monitoring, system and hardware diagnostics, and other activities to ensure optimal health and performance of all systems as required.
Resolve complex problems related to Server and H/W areas.
Assisting/working closely with Network, System Engineers to configure customer requirements.
Physical deployment of network devices, servers, cables, etc.
Assembling/dissembling server hardware for deployment and OS installation and network equipment testing.
Maintain existing department and system documentation (update workflow, process, training documentation).
Other duties as assigned.
Abilities Required
Good verbal and written communication skills, and ability to work independently with minimum instruction.
Basic degree of mentorship, training, and direction team members skills.
Knowledge of IDC industry.
Intermediate degree of analytical and project management skills.
Other Features of Job
Job is performed indoors in a Data Center or Warehouse environment.
Language Skills
Excellent communication skills (English) – written and verbal. Bilingual Chinese or Korean is a plus!!
Job Type: Full-time
Salary: $20.00 - $25.00 per hour
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Health insurance
Life insurance
Paid time off
Professional development assistance
Referral program
Vision insurance
Schedule:
10 hour shift
8 hour shift
Evening shift
Monday to Friday
Night shift
On call
Overtime
Supplemental pay types:
Bonus pay
Ability to commute/relocate:
Monrovia, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Computer networking: 1 year (Preferred)
Shift availability:
Overnight Shift (Required)
Night Shift (Required)
Day Shift (Required)
Willingness to travel:
75% (Preferred)
Work Location: In person",$22.50 /hr (est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2000,$10+ billion (USD)
"MyCare Medical Group
4.1",4.1,"Lutz, FL",Data Engineer,"Job Summary
The Data Engineer will design, develop, optimize, and maintain data architecture and pipelines that adhere to ELT principles and business goals.
Job Responsibilities:
Designs, develops, optimizes, and maintains data architecture and pipelines that adhere to ELT principles and business goals.
Solves complex data problems to deliver insights that helps business achieve its goals.
Creates data products for engineer, analyst, and data scientist team members to accelerate their productivity.
Engineers effective features for modelling in close collaboration with business analysts and stakeholders.
Leads the evaluation, implementation and deployment of emerging tools and process for analytics data engineering in order to improve productivity as a team and quality.
Partners with engineers, BI, and solutions architects to develop technical architectures for strategic enterprise projects and initiatives.
Fosters a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions.
Advises, consults, mentors, and coaches other data and analytic professionals on data standards and practices.
Develops and delivers communication and education plans on analytic data engineering capabilities, standards, and processes.
Job Qualifications:
Education and Work Experience: Bachelor’s degree in computer science, information systems, engineering, or a related field; Masters preferred.
3+ years of experience in data engineering role.
Expertise in ELT and data analysis, SQL and at least one programming language (e.g., SQL, Python).
Conceptual knowledge of data and analytics, such as dimensional modelling, reporting tools, data governance, and structured and unstructured data.
Skills: Strong understanding of agile methodologies and experience as a Data Engineer on a cross-functional agile team preferred.
Experience with designing and maintaining data warehouses and/or data lakes with technologies, such as Hadoop, Spark, or distributed databases, like Redshift and Snowflake, and experience with housing, accessing, and transforming data in a variety of relational databases.
Experience with cloud database technologies (e.g., Azure) and developing solutions on cloud computing services and infrastructure in the data and analytics space.
Experience in building data pipelines and deploying/maintaining them with best-in-class tools (e.g., Git and Jenkins).
Familiarity with the Linux operating system.
Basic familiarity with BI tools (e.g., Tableau, Power BI, Looker).
Experience working in data engineering or data architect role.
Candidate Qualities: Focuses on business agility and agile delivery with a “fail-fast, succeed early” mindset and emphasis on measurable outcomes.
Brings a high-energy and passionate outlook to the job and can influence those around them.
Able to build a sense of trust and rapport that creates a comfortable & effective workplace.
Passion for innovation and “can do” attitude.
BENEFITS
Comprehensive benefits package, including Health, Vision, Dental, and Life insurancesFSA and Life Assistance Program (EAP)
401(k) Retirement Plan
Health Advocacy, Travel Assistance, and My Secure Advantage
PTO Accrual and Holidays
#ZIP
Job Type: Full-time
Pay: $125,000.00 - $150,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Employee discount
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Referral program
Relocation assistance
Retirement plan
Tuition reimbursement
Vision insurance
Compensation package:
Performance bonus
Yearly bonus
Experience level:
3 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Lutz, FL 33558: Reliably commute or planning to relocate before starting work (Preferred)
Experience:
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person","$137,500 /yr (est.)",501 to 1000 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2018,Unknown / Non-Applicable
Infosoft,#N/A,"Peachtree Corners, GA",Data Pipeline Engineer - 2173,"Here are the job position details for your review:
Job Title: Data Pipeline Engineer (GStreamer/ DeepStream)
Pay Rate: $90 to 120/Hr.
Duration: 3 Months
Location: Peachtree Corners, GA
This position is for the Building Automation division.
We are looking for Data Pipeline Engineer (GStreamer) to participate in the development of Streaming data pipelines using NVIDIA Deepstream, GStreamer, Python programming, messaging queues, and related tools.
Preference is for candidates located in Atlanta, GA metro area. However, ok with non-local candidates who are open to 75% travel to Atlanta, GA.
Understanding the basics of our architecture/setup to be able to do setup/support for demos, testing, etc.
Basic computer hardware assembly and set-up
Possible set-up/installation of the application(s)
Possible development tasks depending on what the rest of the development team has for them
Develops business software applications, system software, and/or IT solutions on the basis of functional specifications.
Scope of position embraces both the customization or reuse/modification of system software and/or solutions as well as their development from scratch.
Consults with customers in creating functional specifications and work estimates, and supports Make-or-Buy decisions.
Analyses performance gaps of existing software environments and proposes measures to address efficiency problems.
Is also responsible for the implementation of data-driven components supporting data modeling and engineering in the context of data analytics (Data Stewardship).
Skills:
Python
Linux
Docker and Docker Swarm
Zero MQ / a good understanding of messaging
Desired Skills:
NVIDIA Deepstream SDK
Web Sockets
Flask
Protobuf
Possible:
GStreamer (if different than RTSP / H264 / MJPEG)
Job Type: Contract
Salary: $90.00 - $120.00 per hour
Ability to commute/relocate:
Peachtree Corners, GA: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
""Please share your Email ID""
Work Location: One location",$105.00 /hr (est.),Unknown,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"DiNi Communications, Inc.
3.0",3.0,"New York, NY",Data Center Engineer,"DiNi Communications, Inc. is seeking a Data Center Engineer. This person needs to be a team player with exceptional communication skills. Maintaining the confidentiality of the client's information is of utmost importance.
Required Education and Experience
· At least 8 years of experience as a large data center technician
· Knowledge of Networking,
· Ability to lift equipment at least (50lbs) Rack and Stack (Network Switches, Servers, Storage,
Appliances)
· Knowledge of structured Cabling in a Data Center.
· General knowledge of Operating Systems, (Windows, Unix).
· General knowledge of networking in N Tier Environment (App, Web, DMZ,).
· Level 1 and Level 2 troubleshooting in a N Tier Environment.
· Ability to follow and execute commands to start up and shutdown systems.
· Physically travel to and be available at site B to perform technical changes that require a
person at Site B.
· Performing backup and restore activities.
· Asset Management.
· Maintaining document.
· Work with vendor on circuit issues, (WAN and Internet).
· Cable Patch the equipment in the environment.
· Participate in Site B fall-over testing
Preferred Network Engineering Experience
· Knowledge of N Tier networking.
· Certification in Networking (CISCO).
· Certification in various Operating systems (Linux and Windows), Storage, Messaging
(Microsoft).
· Certification in IT Security.
· Knowledge of Databases.
· Experience working with CISCO, EMC, Symantec, HP, Oracle, Bluecoat, MacAfee, and other vendors' equipment and software in a networked environment.
Responsibilities include but are not limited to:
Adding equipment to support agency applications in site B Environment,
Coordinating Site B testing with Site A technical resources.
Monitoring the B environment.
Rack and Stack new and replacement equipment
Troubleshooting and resolving incidents and problems in the
Physically travel to and be available at Site B to perform technical changes that require a person at Site B.
Upgrading and expanding the environment
Performing backup and restore activities
Manage Site B assets
Maintaining document
Cable Patch the equipment in the environment
Participate in Site B fall-over testing.
Job Type: Full-time
Pay: $60.00 - $70.00 per hour
Schedule:
Monday to Friday
Experience:
large data center technician: 8 years (Required)
Networking in N Tier Environment (App, Web, DMZ,): 3 years (Required)
Ability to lift equipment at least (50lbs): 1 year (Required)
structured Cabling in a Data Center: 3 years (Required)
Level 1 and 2 troubleshooting in a N Tier Environment: 3 years (Required)
Performing backup and restore activities: 3 years (Required)
Cable Patch the equipment: 3 years (Required)
Certification in Networking (CISCO): 1 year (Preferred)
Certification in IT Security: 3 years (Preferred)
CISCO, EMC, Symantec, HP, Oracle, Bluecoat, MacAfee: 3 years (Preferred)
Work Location: In person",$65.00 /hr (est.),Unknown,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
WellTrust Medical Group,#N/A,"Charlotte, NC",Data Engineer,"The Data Engineer will assist the senior analyst in complex modeling and projects to ensure their completion within the allotted timeline. Helps track measures of success and develop reports that measure operational and customer metrics. Demonstrates ability to troubleshoot data consumption and data quality problems.

Responsibilities:
Build and maintain reports using My SQL Server
Identify optimal data sources, structure data validation, and cache optimization.
Performs business problem modeling and quantitative analytics including regression analysis, time-series, hierarchical cluster analysis, and data mining.
Helps track measures of success and develop reports that measure operational and customer metrics.
Demonstrates ability to troubleshoot data consumption and data quality problems.
Evaluates and recommend improvements to current models, metrics, and analytic processes to improve operational efficiency and insight.
Design, document, and communicate effective technical solutions meeting end-user requirements
Perform analyses with large amounts of complex data, and uses data visualization tools to communicate results.
Requirements:

Bachelor's degree or equivalent experience in a quantitative field (Mathematics, Statistics, Computer Science, Economics, Finance, Physics, Engineering)
3+ years of professional working experience as a Data Engineer or Analyst
1-3 years of data warehouse experience
Build and maintain reports using My SQL Server
Strong analytical and creative problem-solving skills
Design, document, and communicate effective technical solutions meeting end-user
SQL and Excel Macro development
Strong understanding of various data environments such as SQL, Oracle, Big Query, etc.
Experience with data visualization tools such as Looker Studio, Tableau, D3.js, etc.
Proficient in R, Python, MATLAB, Java, or another programming language.",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Trella Health
4.8",4.8,"Philadelphia, PA",Senior Data Engineer,"At Trella Health, we are passionate and committed to our mission – empowering meaningful change in healthcare. Since our founding in 2015, we continue to grow our team, enhance our solution and services offerings, accelerate into new markets, and expand our customer base. We are rapidly growing and are looking for new Trellavators to join our team!
""What is a Trellavator?"" you ask. Innovate and elevate is the name of our game! We go above and beyond to collaborate with and support each other – we believe that when a colleague or customers succeeds, we succeed. By learning from others, building on our successes, and taking risks, we constantly raise the bar – continuous improvement is in our DNA. Our word is paramount, we keep our commitments, and we always follow through. We have a strong, reliable support system that fuels growth, collaboration, and passion – and together, we create a positive environment where everyone at Trella Health, including the customers we support, can thrive. Are you ready to learn more about the opportunities with our team? Trell-yeah you are!
Position Overview:
The Senior Data Engineer will be responsible for designing, implementing, and maintaining an effective data pipeline for Trella Health's applications. This position is responsible for turning data concepts into effective data models that are efficient and scalable. The Senior Data Engineer will ensure a smooth data release and deployment process throughout the data development lifecycle.
Successful candidate will be a highly organized, self-starter that can roll up their sleeves and solve problems efficiently. Ability to document procedures, manage time, and have a knack for optimizing data flows.
Location: Atlanta HQ; Raleigh, NC; Nashville, TN; Philadelphia, PA; Remote considered
Reports to: Manager, Data Engineering
As a member of the Data team at Trella, you will:
Champion a comprehensive data vision and operating principles that improve data quality, consistency, accessibility, decision making, use, and stewardship.
Manage data pipeline by developing processes for data input, transformation, and storage
Act as an interface between the Data Science team and Application Developers
Architect and implement front-end (application layer) and back-end (ETL layer) database technologies in a highly customized data environment
Develop automated processes for moving data between systems
Implement ETL processes that transform source data into functional data stores consumed by front-end applications
Scale data pipeline by utilizing the most efficient tools and technologies
This job might be a fit for you if you have:
Bachelor's degree in Computer Science, Engineering, Mathematics, or other technical discipline (proven professional experience will be considered in lieu of a technical degree)
2+ full years of recent advanced Snowflake development experience
Experience manipulating healthcare data
In-depth knowledge of Amazon Web Services (AWS) and Platform as a Service (PAAS)
Demonstrable experience with Snowflake utilities including SnowSQL & Snowflake Javascript
Analyze all aspects of the Snowflake Query Engine to improve bottlenecks that may exist
Ability to automate data ingestion from Amazon S3, FTP, and other technologies
Experience with scripting languages, extensive Python experience, Node.js experience a plus
Working knowledge of Bitbucket and Git
Experience managing and loading data in AWS RDS (MySQL/MariaDb)
Ability to optimize MySQL query execution plans for robust performance
Technical Job Requirements (Nice to Have)
Experience manipulating healthcare data including claims, ADT data
Foundational knowledge of CI/CD platforms, preferably TeamCity
Development and consumption of API services
Experience with data science tools (Jupyter, SageMaker, R)
Understanding of Atlassian suite of tools, including Jira and Confluence
Soft Skill Job Requirements (Nice to Have)
Strong analytical skills and ability to rapidly switch contexts
Excellent communication and problem-solving skills
Sole contributor that is willing to independently uncover and improve processes that are inefficient or sub-optimal
Track record of implementing creative solutions to complex problems
Experience working in a nimble, fast-paced, start-up environment
Knowledge of HIPPA, PHI, PII policies
About Trella Health:
Trella Health provides unmatched, actionable market intelligence to post-acute care and value-based care providers of all sizes. Our industry leading analytics paired with CRM and EHR integration workflows positions us as the most advanced sales enablement platform for the post-acute care market. One of only a few companies to be deemed both a Qualified Entity by CMS and an Innovator under its Virtual Research Data Center Program, Trella has elite access to billions of performance metrics and referral data to enable competitive positioning across the market. Its standardized insights, representing 90% of all lives 65+ U.S. population, help customers identify, engage, and manage critical relationships and advance their organizations with certainty.
Since launching our first product in 2016, we have experienced tremendous growth and have highlighted a few key data points to get an at-a-glance view of Trella:
Over 700 customers including 10 of the top 10 post-acute providers and 4 of the top 10 health systems
Over 120 team members and growing
Recognized as one of Modern Healthcare's ""Best Places to Work"" in consecutive years and were ranked in the Top 50 startups in the US by LinkedIn
Headquartered in Atlanta, GA with remote hubs in Nashville, Philadelphia, Raleigh, and the Philippines
The Trellavator Experience:
At Trella Health, you can expect to join a welcoming team that truly embodies our core values. Our collaborative culture is anchored by trust, transparency, and inclusion — and we also have a lot of fun. No matter where you work, at Trella Health you can expect an awesome team, frequent virtual gatherings, engaging events, casual attire, and more.
We offer competitive salaries with a comprehensive benefits package to all employees and provide an environment that fosters work-life harmonization with unlimited PTO, company-observed holidays, and Summer Fridays. As we continue to see exponential growth, our goal is to continue to put team members first and strive to offer our team members the best culture and benefits possible. Some of the benefits we provide are:
Health, Dental, Vision & Voluntary Benefits
Competitive Salary & Bonus Plans
401k Retirement Savings
Unlimited PTO & 10 Paid Holidays
Flexible Work Hours
Equity Shares
Paid Leave Programs
Marketplace for discounted retail and entertainment
Equal Opportunity Employer
Trella Health is an equal opportunity employer. All persons will receive consideration for employment without regard to race, color, gender, sexual orientation, gender identity or expression, religion, national origin, marital status, age, disability, handicap, veteran status, genetic information, or any other protected status as recognized by federal, state, or local laws.","$113,846 /yr (est.)",51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2015,Unknown / Non-Applicable
Fracsys Inc,#N/A,"Washington, DC",Data Engineer,"Fracsys Inc is hiring a Data Engineer position. The ideal candidate must have at least 12+ years of industry experience. He or she must be responsible for successful technical delivery of Data Warehousing and Business Intelligence projects using a formal project management methodology.
Responsibilities
Directs, motivates and develops key technical personnel
Provides technical direction across the full lifecycle system development process
Provides strategic planning and comprehensive senior level technical consulting to IT senior management and senior technical staff
Working independently within guidelines
Provides detailed inputs for plans and schedules, including goals, risks, and resource allocation
Conduct diagnostic, troubleshooting steps and present conclusions to a varied audience
comfortable slinging some code to solve a problem
Build Data Marts and Data warehouses to facilitate analytical capabilities across the varied sources of data
Design and implement data patterns on cloud using cloud Data warehouse and data virtualization tools
Evaluate new tools and technologies for target state on cloud
Learn and implement new ETL, BI tools and data warehousing technologies
Qualifications
Minimum twelve (12) years in a Technical Lead or Consulting role leading Information Management related initiatives (system integration, data warehouse build, data mart build, or similar)
MUST have proven leadership experience for team of at least 3 technical resources.
Experience creating Data Marts, Data warehouses on-premise and on cloud for real time and batch processing frameworks
Experience building and optimizing big-data data pipelines and data sets including Postgres, AWS Relational Data Service
extensive hands-on experience building and optimizing data structures for data analytics, data science and business intelligence
Experience building self-service data consumption patterns and knowledge of cloud-based data Lake platforms
Experience wrangling data (structured and unstructured), in-depth knowledge of database architecture
Experience utilizing or leading implementations leveraging ETL tools (Informatica / Talend), BI Reporting tools such as MicroStrategy, Microsoft Power BI, data modeling tools such as Erwin, Oracle, SQL server, NoSQL, JDBC, UNIX shell scripting, PERL and JAVA, XML/JSON files, SAS , Python, AWS cloud-native technologies, S3, Athena, Redshift
Familiarity with the following technologies: Hadoop, Kafka, Airflow, Hive, Presto, Athena, S3, Aurora, EMR, Spark
Ability to drive, contribute to, and communicate solutions to technical product challenges
Ability to roll-up your sleeves and work in process or resource gaps and fill it in yourself
Excellent written and oral communication skills in English
Education
Bachelor’s degree in computer science, Data Analytics, Information Systems, or a related degree or equivalent experience. Master’s degree is preferred.
Job Type: Full-time
Pay: From $96,336.72 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Experience level:
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Washington, DC: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person","$96,337 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Siri info solutions inc,#N/A,"Beaverton, OR",Sr. Data Engineer,"Must Have Skills
PySpark
Python on AWS
Responsibilities:
Understanding of advanced data warehousing concepts, data modeling and extract, transform and load development
Advanced SQL skills with experience querying large datasets from multiple sources and developing automated reporting
Python skills for scripting, data manipulation, custom extract, transform and loads and statistical/regression analysis particularly, as they apply to sales and marketing operations and performance
Job Type: Full-time
Salary: $47.00 - $50.00 per hour
Experience level:
7 years
Work Location: On the road",$48.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"shaped.ai Inc.
4.0",4.0,"New York, NY",Data Engineer,"Hello! We’re building the future of content discovery infrastructure with AI — come join us!

Shaped (YC W22) is an API for developers to seamlessly add personalized ranking and recommendation to their products. These frictionless discovery experiences help end-users find what they want faster, and, consequently, grow conversion and engagement business metrics. Behind the scenes, we’re building state-of-the-art ML infrastructure for training and deploying large AI models to power our personalization engine.
We’re a seed stage start-up backed by top-tier investors (e.g. Y-Combinator, Susa Ventures, Tribe Capital etc…) and executives (e.g. Google, Amazon, Uber, Dropbox etc…). Our team comes from Meta, Google, Apple and Uber. We’re a remote team but have a small office in Brooklyn, New York.
We’ve been lucky to have a huge amount of success since our initial launch earlier this year and now looking to expand our real-time data engineering capabilities. We are looking for a data engineer to design, build and optimize Shaped's real-time streaming data ingestion endpoints. You will be a founding engineer working on state-of-the-art infrastructure. As one of Shaped’s early employees you will help shape our product, culture and vision.
This role is for a full-time data engineer at $120k - $200k a year and 0.1 - 0.5% equity.

Responsibilities:
Manage the unification of our batch and real-time data processing pipelines.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Requirements:
Bachelor's in computer science, data science or mathematics related field. Master's degree or PhD will be advantageous.
5+ years of data engineering experience, ideally applied to machine learning use-cases.
Experience with AWS or common cloud technologies
Proficient with Python, SQL.
Extensive knowledge of both offline and online large scale data processing frameworks: Spark, Beam, Flink.
Extensive knowledge with stream technologies like Kakfa or Kinesis.
Excellent written and verbal communication skills.
We’re excited to work with you. Come build the future of AI and discovery with us!","$160,000 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Tipico - North America
4.0",4.0,"Denver, CO",Data Engineer,"Company Description

Tipico is an energetic, passionate, bold tech company, and we love sports! Tipico has recently expanded to the US market, establishing offices in Hoboken, New Jersey and Denver, Colorado. We are driven by our core values: passion, progress and trust. Our goal is to build the best possible mobile sports betting product in the industry, in order to amplify the emotions of millions of sports fans every day. Let’s Make This Interesting ™
Please note: this role is located in our Denver, CO office; we work off a hybrid model and come into the office 2 days per week.

Job Description

The Data Engineer will report directly to the Data Engineering Manager and work together with rest of the data engineers, BI analysts, and product teams. The position holder ensures this is done by doing the following:
Create and maintain both batch (ETL/ELT) and real-time data pipelines and architecture
Ensure all data provided by Data Platform is of the highest quality and is delivered in a timely manner and in line with agreed SLAs
Assemble large and complex data sets that meet functional and non-functional business requirements
Identify, design, and implement internal process improvements, including automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies
Collaborate with stakeholders including data architects, Executive, Product, Data and IT team members from the beginning to the delivery of a project
Build a scalable, modern, and trusted data ecosystem, making sure it is responsive, available and in line with business requirements
Take part in sprints, workloads, and deliverables through agile methodologies
Manage quality assurance, verification of accuracy and consistency of data, specifically KPI measures sent to the business
Thrive working in a fast-paced and challenging environment
Open to continuous learning individually and with the team
Comfortable with being challenged / seeking challenge
Willing to work flexible hours if needed

Qualifications
3+ years in Data Platforms / Datawarehouse / BI experience
Must have experience in successfully implementing data-centric applications, such as data warehouses, operational data stores, and data integration projects
Strong analytical and problem-solving skills that balance with creative approaches
Ability to learn new and complex concepts quickly
Strong experience with large-scale production databases and SQL
Solid understanding of cloud data services: AWS services such as S3, Athena, EC2, RedShift, EMR, EKS, RDS and Lambda
Proficient with one or more big data & new data technologies such as DBT, NIFI, Airflow, Kafka
Experience with data modelling
Experience with ETL development (extractions, data load, aggregation, transformation)
Knowledge in working with time-series/analytics databases such as Elasticsearch
Understanding of containerization and orchestration technologies like Docker/Kubernetes
Must be able to take ownership of projects with a high level of attention to detail and priority
Team player with a hands-on approach and attitude
Must be comfortable with on call rotation
Nice to have:
Familiarity with Marketing tooling and/or CRM systems is preferred
Familiarity with using business intelligence tools such as Domo, Tableau or Looker
Knowledge about and / or experience with sports betting
Background in digital companies, online, gambling, gaming, e-commerce, marketing, or mobile apps
Familiarity with one scripting language, preferably Python

Additional Information

What's in it for you:
Work in an environment where you, your work and ideas matter and have an impact.
You will be a be part of the newest and up and coming USA online iGaming and sports betting companies in the market.
Work in a new and young business with high growth potential.
Build your own success story together with us.
Work with self-organized, self-responsible and entrepreneurial employees.
Start-up feeling, backed-up by a leading European sports betting house. We are a high-volume business and are taking off in the US!
Competitive salary, Medical, Vision, Dental Benefits, Unlimited PTO, and more
Salary for this position is $120k+ & annual performance bonus","$120,000 /yr (est.)",201 to 500 Employees,Company - Private,"Arts, Entertainment & Recreation",Gambling,2004,Unknown / Non-Applicable
"Intersec
4.5",4.5,Remote,Senior Data Engineer,"Architect and implement highly scalable data pipeline to streamline migration of data from source data repositories to the centralized data platform
Implement agreed upon data models (e.g., data marts) to enable faster and efficient consumption of data by downstream consumers
Conduct performance tuning to optimize data migration techniques and ensure adherence to identified service-level agreements (SLAs)
Define and implement automated validation checks to ensure data integrity between source and target
Assist in onboarding consumers with existing technology to connect to the platform
Setup predictive alert mechanism for pipeline degradation for IMF pipelines.
Enhance data quality framework for data pipelines and derived datasets
Enhance Data Audit Balance and Control mechanism for data pipelines
Enhance pipelines to be certified copies of authoritative source
Populate metadata for the Enterprise with particular focus on the Data Sources being onboarded to EDP (Technical and Business Glossaries) related to IMF Modernization Programs
Validate correct lineage for datasets
Required:
*8+ years of Databricks, Data Modeling, and/or Metadata management experience
* Familiarity with architecting and developing real and near-real-time solutions
* Bachelor's Degree in Engineering, Mathematics, computer Science, Statistics or other quantitative discipline, or 4 years equivalent professional experience
* US Citizenship and ability to gain a public trust
Preferred:
* Familiarity with IRS systems and ETL pipelines
* Ability to work independently and manage multiple tasks
* IRS MBI Clearance with IRS Laptop and Informatica Installed
Job Type: Full-time
Salary: $70.00 - $75.00 per hour
Schedule:
Monday to Friday
Application Question(s):
Are you a US/GC card holder?
How many years of Databricks, Data Modeling, and/or Metadata management experience do you have?
Work Location: Remote",$72.50 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,Unknown / Non-Applicable
Atika Technologies,#N/A,"Pittsburgh, PA",Data Engineer,"Role: Data Engineer/PySpark Developer, Pittsburgh, PA - Day One Onsite
Fulltime and Contract(W2)
LinkedIn Must
USC/GC/EADs
Three round of interview: First two on Zoom Video Call, End Client Round will be Onsite on the Client's Location (Pittsburgh, PA)
Job Description:
Python(PySpark), Jupiter hub, airflow, Java
Database knowledge - Oracle / SQL / Data Analytics
Regulatory reporting knowledge preferred
Good communication skills
Open / Ability to learn new technologies
Very good Analytical Skills
Experience in ETL is preferable
Job Types: Full-time, Contract
Salary: $120,000.00 - $130,000.00 per year
Benefits:
Dental insurance
Health insurance
Life insurance
Compensation package:
Bonus pay
Performance bonus
Yearly pay
Experience level:
10 years
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Pittsburgh, PA 15259: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Required)
Spark: 4 years (Required)
Regulatory reporting: 3 years (Required)
SQL: 4 years (Required)
IT: 10 years (Required)
Airflow: 4 years (Required)
Jupiter: 4 years (Required)
Work Location: One location","$125,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"DSMH LLC
5.0",5.0,Remote,"Data Engineer (AWS, Python)","Title: Data Engineer
Duration: 12+ Months
Technical Skills
(Required)
Four years or more of experience in data engineering and/or software
development.
Three years or more of experience with development, operations, or
architecture in AWS using Python.
Experience with development and delivery of microservices using serverless
AWS services (S3, RDS, Aurora, DynamoDB, Lambda, SNS, SQS, Kinesis,
IAM)
Strong competency in testing, including unit testing to coverage standards and
integration testing.
(Desired)
Software development experience with object-oriented development and design
patterns
Experience with serverless design in AWS
AWS technical certifications (Developer Associate, Solution Architect Associate)
Familiarity with machine learning and data design to support machine learning
Experience with productionizing machine learning workloads
Experience with ElasticSearch/ELK, Kibana, Grafana, and/or Prometheus
Experience with OpenTelemetry
Soft Skills
(Required)
Verbal and written communication skills, problem solving skills, customer
service and interpersonal skills.
Basic ability to work independently and manage one’s time.
(Desired)
Ability to work collaboratively in a complex, rapidly changing, and culturally
diverse environment.
Ability to clearly communicate complex technical ideas.
Comfortable working in a dynamic environment where digital is still evolving as
a core offering.
Job Type: Contract
Pay: $50.00 - $55.00 per hour
Experience level:
6 years
Schedule:
Monday to Friday
Experience:
AWS: 5 years (Required)
Python: 5 years (Required)
Work Location: Remote",$52.50 /hr (est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,$1 to $5 million (USD)
"Iron Bow Technologies
3.5",3.5,"Washington, DC",Sr. Data Center Engineer,"Iron Bow Technologies is seeking a Sr. Data Center Engineer to support our Federal customer, The Department of Justice (DOJ), Criminal Division (CRM), in Washington, DC. The selected candidate will support our flexible and scalable solution with the DOJ CRM to extend cloud-native features to existing legacy and next-generation applications while also future proofing the infrastructure to easily accommodate future requirements, such as transitioning to a public cloud.
This position is hybrid and requires 2 days a week onsite at the customer location in NW Washington, DC and support at the customer data center as needed in Sterling, VA.
Essential Duties and Responsibilities:
Assess and provide VMware solutions to include vCenter, vSphere, SRM, ESXi
Primary storage solutions and data file storage management, NAS file storage solutions
Data Center blade servers and rack servers
Server virtualization
Experience:
10 years of related experience providing system administration support in a virtualized environment, including at least 6 years of VMWare solutioning, including vCenter, vSphere, SRM, and ESXi
Data Center operations experience, with SAN and NAS
Experience supporting primary storage solutions and data file storage management, NAS file storage solutions
Hands on experience with Data Center blade and rack servers
Ability to work staggered shift hours when needed to support critical coverage and/or time-sensitive needs
Be a member of our Team providing coordinated 24x7 IaaS on-call support
U.S. Citizen
Currently hold or obtain/maintain a Public Trust High clearance
Education:
BS in Computer Science, Information Science, or other relevant discipline is preferred – additional years of experience may be considered in lieu of the degree
Preferred Certifications:
VMware Certified Professional
Primary storage certifications such as HPE, Dell-EMC, NetApp, or Pure Storage
VCP-DCV
Travel Requirements:
No travel
PRINCIPALS ONLY. NO AGENCIES PLEASE. MUST BE A US CITIZEN ABLE TO OBTAIN A PUBLIC TRUST SECURITY CLEARANCE.
Iron Bow Technologies is a leading IT solution provider dedicated to successfully transforming technology investments into business capabilities for government, commercial and healthcare clients. Our depth of technical expertise, global reach and strategic partnerships with industry leaders as well as disruptive technology partners uniquely positions Iron Bow to target the right solution to ensure your successful business outcome. We partner with our clients from planning and implementation through ongoing maintenance and management to deliver solutions that are strong, flexible and on target .
Iron Bow Technologies is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.
#LI-TM1",#N/A,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,1983,$500 million to $1 billion (USD)
"Merkle
3.7",3.7,"Norristown, PA",Data Engineer,"Company Description

We are Dentsu International. We innovate the way brands are built. That means we do things differently so they’re better than before. In this way, we make our clients’ most important marketing assets—their brands—win in a changing world.
Dentsu International is a modern marketing solutions company. Our mission is to help clients navigate, progress, and thrive in a world of change. Businesses rely on our integrated network of agencies and specialized practices to champion meaningful progress through creative, media, commerce, data, and technology. Part of Dentsu Group, our global network comprises 66,000 diverse people in 143 countries, who are dedicated to teaming for growth and good. Follow us on Twitter @DentsuUSA and visit dentsu.com/us.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.

Job Description
Develop onboarding process to receive and process information related to onboarding a new subscriber
Convert Snowflake Stored Procedures to Python Scripts directly in Airflow
Develop metadata driven solutions for managing data processing and cadence
End to end automation development and testing
Building and maintaining stored procedures and scripts in Snowflake Data Cloud
Building and maintaining complex SQL queries for ETL/ELT processing
Develop Dashboard for data processed for audit and governance needs
Follow Agile methodologies to deliver iterative feature sets rapidly
Maintain codebase using source control methodologies, Git
Collaborate with business analysts, data analysts, and other developers to deliver powerful new features
Coordinate with teams as part of a larger data-sharing system
Contribute to a collaborative, positive, and enjoyable environment for your operations team
Performing quality assurance and testing
Writing user and technical documentation such as runbooks

Qualifications
Strong communication skills both verbal and written
Strong understanding of conditional programming logic
Knowledge of software engineering and development methodologies, techniques, and tools; includes Issue Tracking (JIRA) Software Development Lifecycle SDLC), and Continuous Integration / Continuous Development (CI/CD)
Experience working with large databases and data warehouses utilizing both relational and dimensional design concepts
Eagerness to learn new technologies, programming languages and platforms to solve complex data challenges
Experience with Python required
Experience with SQL required
Experience with Snowflake Data Cloud is highly desirable
Experience with Amazon Web Services (AWS) a plus
Experience with Airflow (or other orchestration) a plus
Experience in database marketing is a plus

Additional Information

Perks:
Great compensation package
Comprehensive healthcare plans
401(k) with employer match
Flexible time-off
16 weeks paid parental leave
Additional Information
The anticipated salary range for this position is $94,000-$146,000. Salary is based on a range of factors that include relevant experience, knowledge, skills, other job-related qualifications, and geography. A range of medical, dental, vision, 401(k) matching, paid time off, and/or other benefits also are available. For more information regarding dentsu benefits, please visit dentsubenefitsplus.com
Employees from diverse or underrepresented backgrounds encouraged to apply.
Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact recruiting@dentsuaegis.com if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.
About dentsu
Dentsu is the network designed for what’s next, helping clients predict and plan for disruptive future opportunities in the sustainable economy. Taking a people-centered approach to business transformation, dentsu combines Japanese innovation with a diverse, global perspective to drive client growth and to shape society www.dentsu.com.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.
Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact your recruiter if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.","$120,000 /yr (est.)",5001 to 10000 Employees,Company - Private,Media & Communication,Advertising & Public Relations,1971,$500 million to $1 billion (USD)
"Loyal
3.9",3.9,"Atlanta, GA",Senior Cloud Data Operations Engineer,"Loyal is an organization centered on experience and building a platform that allows consumers to make meaningful decisions when it comes to healthcare. We deeply understand providers, locations, services, appointments, business rules, and moreover, we understand patients - who they are, the preferred method of communication, upcoming appointments, lapsed appointments, outstanding bills, health risks, and more. With this intelligence, our platform fuels highly relevant and personalized experiences across all mediums (website, email, voice…) allowing patients to get healthy, stay healthy, and have a better relationship with the health care provider.
**This is a remote role**
Summary
The Senior Cloud Data Operations Engineer is responsible for delivering Loyal's cloud-based data warehouse. They interface with groups across the organization to ensure we are collecting and storing data in a way that makes it easy to create and consume. They identify best practices for big data management, select the right tooling, create a workable data architecture, and drive an implementation roadmap others can depend on. The Senior Cloud Data Operations Engineer also works closely with our security group to ensure data security and privacy. They also mentor others on the team, providing technical leadership to those they work with.
You will
Create data streams and consolidate data sources into a single cloud-based data warehouse
Architect and catalog data warehouse storage schema
Work with product teams to enable easy data aggregation
Work with data consumers (analysts, data scientists, and AI/ML engineers) to ensure data is easily accessible and consumable
Create and execute a roadmap that meets data storage and availability goals
Ensure data is secure through data analysis, access control, and encryption
You have
Bachelor's degree in computer science, information systems, applied math or related field, or equivalent work experience
Minimum 5 years of experience working with cloud data processing services (AWS, Azure, or GCP).
Minimum 3 year experience with Big Data technologies, such as Databricks, Snowflake, Spark, Hadoop, etc.
Strong experience in creating data architecture and storage schemas required
Significant working knowledge of SQL and other query languages and tools
Significant experience with building relational database systems strongly preferred
Experience with ETL tools a plus
Knowledge of Python preferred
Software development, data analytics, or data science experience is a plus.
Experience working within a software as a service (SaaS) company preferred.
Experience working within a high growth and/or ambiguous environment, with proven experience to be dynamic preferred.
#LI-REMOTE
We know that potential candidates are often less likely to apply to a position if they don't match 100% of the job qualifications. Don't let that be why you miss out on this opportunity! We encourage you to apply if you can demonstrate many of these skills and competencies.
Loyal to our employees
We are a remote-friendly company! We encourage you to apply from anywhere in the United States. We also believe in a work/life balance that fulfills you while you're here and supports you when you're not. We built our benefits package to prove that we're committed to you having everything you need (including a little fun). Here is what we offer full-time employees:
Flexible paid time off, sick and personal days
At least one holiday per month (sometimes, more!)
Full health, dental, and vision insurance - Loyal pays the premium for all employees!
One Time Home Office Setup Stipend For Remote & Hybrid Roles
Monthly Internet Stipend for Remote & Hybrid Roles
Long term & short term disability
401[k] plan
16 Weeks Paid Parental Leave
2 Volunteer days per year
Matching Gift Program
Participation Grant Program
Annual Travel/Team Events up to twice per year (post-COVID)
Our Commitment
We believe that the key to Loyal's success is you. Your unique background, life experience, knowledge, self-expression, and talent make you uniquely you. Who you are, what you have experienced, and how you think inspires us to be innovative and bold.
Loyal is an equal opportunity employer. We hire great people from a wide variety of backgrounds, not just because it's the right thing to do, but because it makes our company stronger. We welcome the unique contributions that you can bring in terms of your education, opinions, culture, ethnicity, race, ancestry, sex, gender identity and expression, national origin, citizenship, marital status, age, languages spoken, veteran status, color, religion, disability, sexual orientation, and beliefs.
We consider qualified applicants regardless of criminal histories, consistent with legal requirements.
Further, consistent with applicable federal and state law, Loyal provides reasonable accommodations when requested by qualified applicants or employees with disabilities, unless doing so would cause an undue hardship. Loyal's policy regarding requests for reasonable accommodation applies to all aspects of employment, including the application process. If you require a reasonable accommodation in connection with the application process, please contact the Talent Acquisition Department at talentacquisition@loyalhealth.com.
E-Verify
This employer participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S. If E-Verify cannot confirm that you are authorized to work, this employer is required to give you written instructions and an opportunity to contact Department of Homeland Security (DHS) or Social Security Administration (SSA) so you can begin to resolve the issue before the employer can take any action against you, including terminating your employment. Employers can only use E-Verify once you have accepted a job offer and completed the I-9 Form.
COVID-19 Vaccinations
Consistent with Loyal policy, candidates performing in-person work will be required to show proof of being fully vaccinated against COVID-19 upon commencing employment. Reasonable accommodations will be considered on a case-by-case basis for exemptions to this requirement in accordance with applicable law. If you require a reasonable accommodation to Loyal's vaccination requirement, please contact the People Department at people@loyalhealth.com.",#N/A,51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2015,$5 to $25 million (USD)
Redzara.us,#N/A,"Atlanta, GA",Azure Data Engineer,"Responsible for developing new and modernizing existing applications to Azure Cloud Platform.
The ideal candidate must demonstrate in-depth knowledge and understanding of Database concepts.
Requirements:
7+ years of experience in Data Warehousing and Business Intelligence
5+ Years in BI development leveraging the Microsoft SQL Server. Needs to be an expert in T-SQL
5+ Years in SSIS and use of Stored Procedures.
1-2 Years in Azure Data Factory, SQL VMs, build and maintain ETL
Ingest and aggregate data from both internal and external data sources to build our data lake
Strong understanding data modeling (i.e., conceptual, logical, and physical model design, experience with Operation Data Stores, Enterprise Data warehouses and Data Marts).
Job Type: Contract
Pay: $43.49 - $57.00 per hour
Compensation package:
1099 contract
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Atlanta, GA 30334: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure Data Factory: 3 years (Required)
Azure: 3 years (Required)
Data warehouse: 1 year (Preferred)
Work Location: Hybrid remote in Atlanta, GA 30334",$50.25 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Petrolink International
2.9",2.9,"Houston, TX",Data Analysis Engineer,"Location:
Houston, Texas, U.S.A.
Job Type:
Full-time
Job Description:
Petrolink is a global company that specializes in user-driven technology for the Energy Industry.
We are looking for a Data Analysis Engineer to assist with creating and delivering algorithms primarily used by our engineering solutions.
This position is based in our U.S. office in Houston, Texas.
Duties & Responsibilities:
Research and provide logic for engineering solutions algorithms.
Collaborate with Subject Matter Experts (Drilling Engineers, Mud loggers, Pore Pressure Analysts, and other specialists from the Drilling Industry) to understand different challenges in the Drilling Industry and the practices in place to address/solve these challenges.
Code the algorithms using C# to run on Petrolink’s proprietary analytics engine.
Develop, test, and maintain algorithms.
Support escalations related to issues in algorithms.
Learn existing algorithms to provide support when required.
Prepare training material and sessions for stakeholders as part of the production release.
Skills & Qualifications:
Proficient in C#.
Experience with Python is an advantage.
Knowledge in drilling practices, and ability to speak to other drilling experts and capture requirements and logic.
Knowledge in drilling performance KPIs is an advantage.
Prior experience with WITSML is an advantage.
Function and communicate well in a team having diverse expertise.
Bi-lingual (English and Spanish) preferred.
At least 2 years’ relevant experience.
Benefits:
Health insurance
Dental insurance
401(k) with matching
Health savings account
Life insurance
Long-term disability
This job description is intended to explain the general nature and level of work being performed. Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.
Applicants must be legally authorized to work in the United States for any employer.
To apply for this position, applicants must meet the above criteria.
Interested applicants fulfilling the above criteria, please send your resume to careers.usa@petrolink.com.","$81,133 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"FreightWaves, Inc.
3.5",3.5,Remote,Senior Data Engineer,"Are you smart, driven, curious, resourceful, and not afraid to fail? Then we want to meet you! Our team of bold, innovative, and creative teammates is what makes us a top startup to work for. FreightWaves delivers news and commentary as well as data and analytics which empower risk management and actionable market insights in the logistics and supply chain industry. If you are ready to join our team, it is time for YOU to apply!
FreightWaves is on the hunt for a curious, tenacious, and team-oriented Senior Data Engineer to join our fast paced engineering team. The ideal candidate is inquisitive, versatile, team oriented, thrives on change, and has a positive attitude. If you are ready to be challenged, learn new and exciting technologies, and have the unique opportunity to work with some of the most talented developers in the country, we want you to apply!
**This position is fully remote.**
**Must RESIDE in the United States and be eligible to work.**
What you will be doing:
Implementing ingestion pipelines, using Airflow as the orchestration platform, for consuming data from a wide variety of sources (API, SFTP, Cloud Storage Bucket, etc.).
Implementing transformation pipelines using software engineering best practices and tools (DBT)
Working closely with Software Engineering and DevOps to maintain reproducible infrastructure and data that serves both API-only customers and in-house SaaS products
Defining and implementing data ingestion/transformation quality control processes using established frameworks (Pytest, DBT)
Building pipelines that use multiple technologies and cloud environments (for example, an Airflow pipeline pulling a file from an S3 bucket and loading the data into BigQuery)
Create and ensure data automation stability with associated monitoring tools.
Review existing and proposed infrastructure for architectural enhancements that follow both software engineering and data analytics best practices.
Working closely with Data Science and facilitating advanced data analysis (like Machine Learning)
What you bring to the table:
Strong working knowledge of Apache Airflow
Experience supporting a SaaS or DaaS product, bonus points if you were creating new data products/features
Strong in Linux environments and experience in scripting languages
Python Expert
Strong understanding of software best practices and associated tools.
Experience in any major RDBMS (MySQL, Postgres, SQL Server, etc.).
Strong SQL Skills, bonus points for having used both T-SQL and Standard SQL
Experience with NoSQL (Elasticsearch, MongoDB, etc.)
Multi-cloud and/or hybrid-cloud experience
Strong interpersonal skills
Comfortable working directly with data providers, including non-technical individuals
Experience with the following (or transitioning from equivalent platform services):
Cloud Storage
Cloud Pubsub
BigQuery
Apache Airflow
dbt
DataFlow
Bonus knowledge/experience:
Experience implementing cloud architecture changes
Working knowledge of how to build and maintain APIs using Python/FastAPI
Transforming similar data from disparate sources to create canonical data structures
Surfacing data to BI platforms such as Looker Studio
Data Migration experience, especially from one cloud platform to another
Certification: Professional Google Cloud Certified Data Engineer
Our Benefits:
An excellent work environment, flat hierarchies, and short decision paths.
Work from home
A generous benefits package including 100% employer-paid health, dental, vision and Life insurance, STD, LTD
Stock options
Appealing 401k matching plan
Career Mentorship Opportunities
Personal Development Credit (Can be used toward Student loans or relevant PD Courses)
Annual life achievement bonus of $2000 for having a baby, buying a house, or getting married (max one per year)
No set days off Vacation policy (our team takes time off as needed with supervisor approval)
Up to $50 for Gym or Virtual Gym membership.
Audible or Kindle Unlimited subscription
Discount on Ford vehicles
oYXkhYiWkU",#N/A,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2017,Unknown / Non-Applicable
"OM Group Inc.
3.5",3.5,Remote,Senior Data Engineer,"Data Engineer
OM Group, Inc. is a fast-growing Microsoft Azure and AWS Partner delivering full-stack cloud services for Federal & DoD clients. We are currently expanding support for the Enterprise Data Warehouse to continue and evolve on-prem/cloud/hybrid data migration and enterprise reporting platforms.
We are hiring a Data Engineer with experience creating and maintaining complex shell and SQL scripts used to expand and populate data into the enterprise data warehouse. The successful candidate will be responsible for managing and maintaining the Extract, Transform and Load (ETL) processes to populate the Enterprise Data Warehouse/ Enterprise Virtual Viewer (EDW/EVV). The EDW/EVV is powered by a multi-tier environment encapsulation of Windows/Unix and Linux Environments. The primary data repository, as well as host to the virtualization/governance and catalog layers live on the IBM Cloud Pak for Data System (CP4D) and Netezza Performance Server (NPS) PostgreSQL based database. The CP4D system is Linux-based on a combined OpenShift and Redhat 7 platform. The EDW/EVV also utilizes SAP Business Objects (BOBJ) to do reporting based on a Windows Virtual Machine (VM) architecture. The Oracle 19c database is on Oracle Solaris VMs and acts as repository databases for SAP BOBJ.
This position is remote on Eastern Time zone schedule.
Responsibilities
Work with stakeholders to evaluate business needs and develop tasks to meet requirements and objectives
Provide quality assurance and identify bugs or required fixes and communicate to respective teams. Ensure data being migrated to the EDW/EVV production environment is documented
Enforce EDW/EVV standards / policies and provide quality assurance for universe, star schema/build, report, and dashboard migrations from the EDW/EVV development environment into the EDW/EVV production environment (see Appendix A for migration metrics)
Maintain existing Unix Shell and Structured Query Language (SQL) script based ETL processes. Design, Develop, document, and maintain new and/or expanding script based ETL processes e.g., Unix Shell and SQL scripts
Coordinate, test, implement, document, and manage required upgrades and capability enhancements for the EDW/EVV. Perform tasks for data repository expansion as more data content is transitioned to the warehouse
Work with infrastructure team to ensure that all the required monitoring, exception handling and fault tolerance is in place for a production-quality data platform
Team up with analysts, product managers, and other stakeholders to understand evolving business needs and translate reporting capabilities accordingly
Assist with process improvement with a customer-focused, progressive mindset
Understand data classification and adhere to the information protection and privacy restrictions
Troubleshoot and provide technical support for staff and back-end system users
Document and support code migrations and provide quality assurance / control of EDW/EVV star schemas/builds, universes, local data, and reports

Requirements
Active DoD Secret Security clearance
Hold DoD IAT-III, IAM-II and IASAE-II certifications and Data related industry certifications
5+ years experience with Linux Shell Script development, testing, debugging and deployment
5+ years experience with SQL, relational database and data warehouse technologies
3+ years experience developing and maintaining Python or similar scripting language
3+ years experience in multiple subversion technologies such as Subversion, GitHub or Tortoise
Knowledge of AWS technologies such as S3, EC2, Redshift, Glue, Athena and Step Functions preferred
Experience in data mining and development of ETL processes, distributed data architectures and big data processing technologies
Knowledge of production / environment control
Knowledge of Agile software development lifecycle
Excellent problem-solving skills and attention to detail
Strong documentation and training skills
OM Group, Inc. recently voted one of the top workplaces in the Washington, DC area, is a fast-growing Microsoft Azure and AWS Partner delivering full-stack cloud services for Federal & DoD clients. We are a growing company that values your skills, training, and ideas and strives to foster a welcoming, diverse, and inclusive environment. OM Group provides competitive compensation and benefits including health insurance coverage, 401(k), paid time off, as well as support for continuous education and training.
OM Group, Inc. is an equal opportunity employer (EEO) and does not discriminate on the basis of race, color, religion, sex, national origin, age, disability, veteran status, or any other characteristic protected by law. We are committed to creating a diverse and inclusive workplace where all employees are treated with respect and dignity",#N/A,1 to 50 Employees,Company - Public,Management & Consulting,Business Consulting,2001,Unknown / Non-Applicable
"Hiscox
4.1",4.1,"Atlanta, GA",Senior Data Engineer,"job description:
You will be a senior member of our US IT Team data engineering practice. You will be an early member of this team and will work closely with the Director of Data Engineering to help establish it. We will have our work cut out for us delivering planned work, building the practice and evolving our technical landscape. This is an exciting opportunity for the right individual to join at the very beginning and grow with the new team.
As a senior you will be expected to lead, mentor others and deputise for the director of data engineering when required. Yoi will also continue to contribute as a hands-on engineer. You will have the knowledge, best earned through 5+ years of experience, to truly inhabit this senior role in an ambitious team.
What you’ll be doing:
Working with business stakeholders and our delivery team to understand high value business problems that can be solved through the application of data processing and analytical systems.
Developing, expanding and evolving our existing databases and ETL pipelines.
Working with architects to design, build and support a transformational cloud-based data platform for the business.
Being a core and professional member of the new data engineering practice
Understand business requirements and help refine into development tasks and estimate their complexity.
Research, evaluate and adopt new technologies with a right tool for the job mentality.
Focus on both speed of delivery and quality, with suitable pragmatism – ensuring your solutions are always appropriate and not too complex or over-engineered.
Quick progression of projects from proof-of-concept to post-production stage.
Communication and presentation of ideas to colleagues in all parts of the wider tech team.
Participating in code reviews for the data engineering practice.
Act as a servant leader in our practice – ensuring team members have
person specification:
Knowledge of, or ability to rapidly adopt our core languages for data engineering – SQL and/or Python.
Experience working using a modern DevOps approach. You should not just have written data code but also the CI/CD pipelines necessary to test and deploy that code in a professional environment.
A robust understanding of core data engineering topics – ETL vs ELT, structured and unstructured data, data quality and data governance.
Ability to contribute to all aspects of a solution – design, infrastructure, development, testing and maintenance.
The ability to design and advocate for technical solutions to business problems.
Effective collaboration with technical and non-technical team members through agile ceremonies – roadmap planning, feature workshops, backlog elaboration, code review.
Understanding of cloud technology and significant hands-on experience working with it. You must understand how cloud-native solutions must be built differently from traditional ones.
Track record of taking initiative and delivering projects end-to-end; clear evidence of being self-driven and motivated
Immense curiosity, high energy and desire to go the extra mile to make a difference.

Desirable Skills
Beyond the required skills we are open to individuals of diverse talents. Experience with additional technologies, data science knowledge or a business background are all valued. We want to know how your unique abilities can contribute to our team.
Our Technology
We operate in a diverse technical landscape and are looking for flexible engineers who can adapt to and use many different tools. We would not expect any engineer to be familiar with the entire tech stack – no engineer can. Instead, we seek people with a good understanding of data structures and algorithms and the ability to apply this knowledge in learning new tools.
We have an existing data landscape involving both MS SQL Server and Oracle databases. We use SSIS for ETL. We will also need to source data from our integration layer – webMethods.
If some of that sounds like it needs bringing up to date, we know! We hope to start building a modern cloud data platform to realise the full value of our data. The technologies we use for that platform are still to be decided. Likely it will be based on Snowflake with Azure Data Factory for ELT. Other parts of our business have had success using Prefect for orchestration and dbt for transformation and we will reuse where it is sensible.
We prepare data for use by analysts working with a variety of tools – Tableau, PowerBI, and even Excel.
We take a DevOps approach and strive for continuous integration / continuous deployment. We use Azure Pipelines to deploy our code. We deploy infrastructure the same way using Terraform and Docker.
Our technology landscape is not fixed. Our engineering and architecture teams drive the technology we adopt.


Salary range $140,000 - $170,000","$155,000 /yr (est.)",1001 to 5000 Employees,Company - Public,Insurance,Insurance Carriers,1901,$1 to $5 billion (USD)
"DataArt
4.6",4.6,Remote,Data Engineer,"CLIENT
Our client is a pioneer in US schools’ education since 2000, it is leading the way in next-generation curriculum and formative assessment. They develop a number of solutions and interactive web products for teachers, students and their parents.

The company is technology-driven with a huge number of software engineers involved in product development. They have a very solid approach to technology, employ best practices and processes, with a focus on cutting-edge frameworks, languages and tools.

We invite to the company, not a project
TEAM
There are multiple autonomous and empowered teams who work with their own part of the product and choose their own way of doing things and they expect people to be proactive and good team players.
Responsibilities
Building well-tested and optimized ETL data pipelines for both full and delta extraction
Collaborating with data analysts and learning scientists to store, aggregate, and calculate captured students’ work
Contributing to leading industry data standards, such as Caliper Analytics or xAPI
Improving our deployment and testing automation data pipelines
Requirements
3+ years of professional software development or data engineering experience
Strong CS and data engineering fundamentals
Proven fluency in SQL and a development language such as Python
Experience with the following storages: Snowflake, AWS Storage Services (S3, RDS, Glacier, DynamoDB)
Experience with ETL/BI: Airflow, DBT, Matillion, Looker
Experience with Cloud Infrastructure: AWS Kinesis, Lambda, API Gateway, Terraform
Understanding of ETL/ELT pipelines and Data Warehousing design, tooling, and support
Understanding of different data formatting (JSON, CSV, XML) and data storage techniques (3NF, EAV Model, Star Schema, Data Lake)
BS in Computer Science, Data Science, or equivalent
Experience in education or ed-tech
Proven passion and talent for teaching fellow engineers and non-engineers
Proven passion for building and learning: open source contributions, pet projects, self-education, Stack Overflow
Strong communication skills in writing, conversation",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,1997,$100 to $500 million (USD)
"Take Command Health
4.9",4.9,"Dallas, TX",Data Engineer,"Company: Take Command
Position: Data Engineer
Salary: $120,000-$130,000 plus bonus
Location: Dallas or Austin, TX
About Take Command
Take Command is a start-up on a mission to improve the healthcare system, starting with health insurance. Pragmatically speaking, we help employers reimburse employees for individual insurance instead of offering a traditional one-size-fits-all group plan. We believe this model can empower employees (when they have the right support) to be savvy healthcare consumers and have a transformative impact on the entire healthcare system.
Let’s be honest—health insurance is usually a confusing, frustrating, and even emotional experience for people. We want to fix that with a new model, great technology, and a superior user experience. We have made a great start, but we need your help to fully realize our long-term vision.
As a part of the dev team, you'll build the framework for both our clients and our team to succeed. You will be a pillar in our company as we grow and make our product the best product on the market!
In this role, you will:
Take ownership for delivering multiple features, their quality and assist team members in delivering their features
Recommend improvements to data quality, processes and code to make it easier to use, manage and maintain
Experience designing and developing data models and data migrations
Experience managing and maintaining ETL or ELT processes (Hevo, BigQuery, Looker, etc.)
Hands-on development experience with SQL, relational database design (ERDs)
Must be able to write SQL & contribute code
Be product-minded; focus not only on what needs to be built but why something needs to be built
You may be a great fit for this role if you have:
3 or more years of professional experience as a data analyst or data engineer
Cloud-Based Data Tools e.g. Hevo, BigQuery, Looker, etc.
Database technologies: SQL, RDBMS / Relational databases (Postgres, Amazon RDS / Aurora, etc.)
Nice to have experience: Python, Java, Typescript or another modern OOP language
Cloud infrastructure: AWS
Source control: Git
Working at Take Command
We’re excited to build a team and culture that reflects our values! We offer competitive pay, health benefits, and equity options to share with this position. We want you to be an owner along with us in revolutionizing healthcare.
A generously funded ICHRA for medical, dental, and vision premiums and medical expenses. You get to use our own product and we think that’s so exciting and rare!
Unlimited personal vacation in addition to regular company holidays.
401(k): 90-day eligibility for 2% match that vests immediately!
Access to LinkedIn Learning.
We have two beautiful offices conveniently located near The Hill (Dallas) and Holly Commons (Austin). The kitchen is well-stocked and we've designed the space to have lots of different areas to work!
Paid parental leave for new parents.
Flexible on where you work – we hope to see you around the office on average 3 days a week especially when white-boarding or building relationships with your colleagues, but you also have the ability to work from home or wherever you'd like when you need to get focus work done.
More About Us
We secured our Series A funding in 2021 and are thrilled to be able to expand our team. Despite being a small startup in a land of health insurance giants, we’re the recognized industry leader for what we do (health insurance reimbursements) and passionate about bringing it to market because we know we can help fix a broken system and improve our clients’ wellbeing and health outcomes.
We’ve been featured in The New York Times, The Wall Street Journal, The Dallas Morning News, and other national healthcare publications and are excited about our growth opportunities.
Take Command knows diversity and inclusion among our teammates is integral to our company’s success and growth. Our vision is to recruit, develop, and retain the best team from a diverse candidate pool.
This has mostly been about us, but we’d love to hear from you-we can’t wait to hear your story!
Take Command is an equal opportunity employer! We celebrate diversity and are committed to creating an inclusive environment for all employees.
Xrygn4NXMr","$125,000 /yr (est.)",1 to 50 Employees,Company - Private,Insurance,Insurance Agencies & Brokerages,2014,Unknown / Non-Applicable
"Argo Data
2.7",2.7,"Richardson, TX",Senior Data Engineer,"ARGO is a leading provider of software and analytics solutions for both the financial services and healthcare industries. ARGO transforms business processes for financial service providers and healthcare organizations using proven business models and software innovation informed by real customer challenges, breakthrough technology, and rich analytics.
ARGO- Data Engineer II (Markham, ON; Memphis, TN; ...) (theapplicantmanager.com)
JOB SUMMARY
The Data Engineer II is responsible for designing and implementing data solutions to support business needs. The successful candidate will have a strong background in software engineering and database design, as well as experience with working with cloud-based tools and technologies.
EXPECTED WORK AND PERFORMANCE
This section is intended to provide a more detailed overview of what the expectations of the job are to include: work to be performed, expected performance results and how they will be measured, as well as connection to corporate strategy, values, and mission. Job duties and responsibilities include the following. Other duties may be assigned as needed.
Design, build, and maintain data pipelines for data extraction, transformation, and loading (ETL) from various sources.
Ensure data security and privacy protocols are implemented and enforced.
Monitor and optimize data storage and processing performance to ensure high performance and cost-effectiveness.
Collaborate with data scientists, analysts, and other stakeholders to understand their data needs and translate them into technical solutions.
Maintain documentation of data systems, processes, and policies.
Stay current with the latest data tools, technologies, and trends and recommend improvements.
Mentor and train junior data engineers as needed.
QUALIFICATIONS
Bachelor's degree in Computer Science, Software Engineering, or related field.
3-5 years of experience in software engineering and SQL database design.
Proficiency in programming languages such as Python.
Strong understanding of data warehousing, ETL processes, and data modeling.
Experience with cloud infrastructure technologies such as Microsoft Azure.
Excellent problem-solving and analytical skills.
Ability to work well in a team-oriented environment and communicate effectively with both technical and non-technical stakeholders.
PHYSICAL DEMANDS
The candidate will work at our corporate headquarters in Richardson, TX. While performing the duties of this job, the employee is regularly required to stand; walk; sit; use hands to finger, handle, or feel; reach with hands and arms; talk or hear. Specific vision abilities required by this job include close vision, distance vision, color vision, peripheral vision, depth perception, and ability to adjust focus.
ADDITIONAL REQUIREMENTS:
Applicants for U.S. or Canadian based positions with ARGO must be legally authorized to work in the United States or Canada. Verification of employment eligibility will be required at the time of hire. Visa sponsorship is not available at this time for ARGO positions.
Pre-employment background screening will be conducted.
Job Type: Full-time
Pay: $70,745.42 - $141,624.04 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Life insurance
Paid time off
Professional development assistance
Referral program
Relocation assistance
Retirement plan
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Performance bonus
Yearly bonus
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Richardson, TX 75081: Reliably commute or willing to relocate with an employer-provided relocation package (Required)
Experience:
Informatica: 3 years (Required)
SQL: 5 years (Required)
Data warehouse: 3 years (Required)
Python: 3 years (Required)
Azure: 3 years (Required)
Work Location: In person","$106,185 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Computer Hardware Development,1980,$25 to $100 million (USD)
"Quirch Foods
3.0",3.0,"Coral Gables, FL",Data Engineer,"About Quirch Foods
In business for over 50 years, Quirch Foods is a global distributor and exporter of fresh and frozen food products with a unique focus on ethnic cultures. We service our customers from 5 distribution centers strategically located in the United States. We have consistently been ranked among the top 50 exporters in the US and are one of the largest importers of seafood. Our customers include independent grocers, chain supermarkets, foodservice distributors, cruise lines, restaurants and food processors/manufacturers. We have an extensive product list with over 8,000 SKU's and carry niche brands such as Panamei, Mambo and Chiquita along with national brands like Tyson, IBP, Excel, JBS, Iowa Premium, Smithfield, Dietz & Watson, along with many others. We are headquartered in Coral Gables, FL.
Quirch is a portfolio company of Palladium Equity Partners, a leading mid-market private equity fund based in New York City, which partnered with the Executive Team to capitalize on Quirch Foods’ record of excellence and to grow it decisively beyond its current business.
Position Summary
At Quirch Foods we believe in leveraging technology as a source of competitive advantage. This includes customized ERP Solutions, Business Intelligence, EDI, Web and Mobile Solutions. We are currently looking for a Data Engineer to join our development efforts.
Essential Duties and Responsibilities:
Develop, test and maintain custom business solutions by leveraging state of the art software development practices using an agile methodology
Evaluate business requirement and help define problems and develop solutions
Participate in design and architecture discussions with business leaders, end users and IT team members
Document business requirements and solution, document code and provide support for creation of end user documentation
Update business knowledge, technical skills and soft skills, leverage educational opportunities, participate in professional organizations
Experience:
MS SQL Programming
Visual Studio\C#\WPS Experience
Microsoft SSAS, SSIS, SSRS desired but not required
Biztalk experience desired but not required
Skills and Requirements:
Bachelor’s degree in Computer Science, Engineering, Math or equivalent and\or related experience and training preferred
Strong analytical and problem-solving skills
Strong collaborator and team player
Great organizational skills, attention to detail and follow thru
Effective oral and written communication skills
Extensive familiarity with data management principles
Benefits:
Professional growth and developmental opportunities to grow your skills using state of the art technology
Comprehensive benefits package that includes: Medical, Dental, Prescription Drug Plan, Disability Plan, Life insurance Plan
401K savings Plan
Paid Holidays
Personal Time off
Employee Discounts
Quirch Foods is an Equal Opportunity Employer (EOE). Qualified applicants are considered for employment without regard to age, race, color, religion, sex, national origin, sexual orientation, disability, or veteran status. All applicants must be eligible to work in the United States.
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Vision insurance
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Coral Gables, FL 33134: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
What is your desired compensation?
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Data Engineering: 1 year (Preferred)
Work Location: In person","$89,340 /yr (est.)",201 to 500 Employees,Company - Private,Retail & Wholesale,Wholesale,1967,$500 million to $1 billion (USD)
"Quadrant Resource
4.1",4.1,Remote,Data Engineer,"Job Description:
4-6 years of experience as a Data Engineer in the Snowflake-Fivetran-dbt stack
• Possess a solid understanding of end to end Data ingestion, ETL, data analysis, Reporting processes
• Expert in ETL development using Fivetran and experienced in implementing data pipelines for Snowflake Cloud DWH
•Experienced in using dbt for data transformation
•Expertise on Tableau/Metabase for building reports/dashboards
•Expert in troubleshooting and resolving ETL issues, data load failures/problems and transformation translation problems
•Excellent communicator with the ability to work effectively with distributed teams
Job Type: Full-time
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Data Engineer: 9 years (Required)
Fivetran: 8 years (Required)
DBT: 7 years (Required)
Snowflake: 6 years (Required)
Work Location: Remote",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,2004,Unknown / Non-Applicable
"Barracuda Networks Inc.
3.6",3.6,"Alpharetta, GA",Data Engineer,"Job ID: 23-579

Come Join Our Passionate Team! At Barracuda, we make the world a safer place. We believe every business deserves access to cloud-enabled, enterprise-grade security solutions that are easy to buy, deploy, and use. We protect email, networks, data and applications with innovative solutions that grow and adapt with our customers’ journey. More than 200,000 organizations worldwide trust Barracuda to protect them — in ways they may not even know they are at risk — so they can focus on taking their business to the next level.
We know a diverse workforce adds to our collective value and strength as an organization. Barracuda Networks is proud to be an Equal Opportunity Employer, committed to equal employment opportunity and equitable compensation regardless of race, gender, religion, sex, sexual orientation, national origin, or disability.
Envision yourself at Barracuda
We currently have an exciting new position open for a Data Engineer. This person will assist in driving technology innovation for engineering projects and work on architecture of AWS, Elastic and Kubernetes. They will also work closely with the Technology Architect to create and drive the Architecture Review Board.
What you’ll be working on
Determine internal / external customer data needs and identify system specifications
Analyze the needs of large systems and breaking them down into smaller manageable parts
Responsible for designing the journey of data from various sources to the database and everything in between
Plan and design the structure of data, brainstorm with team
Analyze best technology products and systems for internal teams
Help provide build vs buy decisions on data architecture
Communicate data requirements to engineers; explain data architecture to them and provide assistance throughout SDLC
Choose / maintain suitable software, hardware and integration methods
Help resolve technical problems as and when they arise
Review testing procedures to ensure data performant and error free
Ensure that database satisfy quality standards and procedures. Setup such standards.
Work with senior IT personnel to devise plans for future IT requirements of the organization
Produce progress reports on key infrastructures - Elastic and Data Pipelines
What you bring to the role
Bachelor Degree or Post Graduate in Computer Science, or relevant work experience
5+ years of experience in an Engineering / Operations heavy organization
1+ years of experience as a Data Analysis and Data Design responsible for data architecture of a SaaS application
Experience in understanding and designing data sharding in a distributed database
Some Hands-on experience in software development and system administration
Knowledge of strategic IT solutions
Familiarity with programming languages like JavaScript or Java or Python
Familiarity with Linux OS and Windows alike
Experience in cloud computing and cloud Technologies (AWS, GCP, Azure)
Excellent communication skills – you should be adept at listening to, understanding, and explaining key concepts to managerial and technical resources.
Good Anticipation and problem-solving skills
Ability to prioritize and manage time – responsible for multiple streams of work at the same time
Relationship building - Ability to form a good rapport with internal and external stakeholders
In-depth knowledge about quality standards and security best practices

Preferred Skills:
Expert level Understanding of Elastic eco system - Elasticsearch, Logstash, Kibana, Filebeats
Understanding of secure tunnel communication using VPN
Experience designing data at scale
AWS experience is preferred in Cloud Technologies
What you’ll get from us
A team where you can voice your opinion, make an impact, and where you and your experience are valued. Internal mobility – there are opportunities for cross training and the ability to attain your next career step within Barracuda.
Equity, in the form of non-qualified options
High-quality health benefits
Retirement Plan with employer match
Career-growth opportunities
Flexible Time Off and Paid Time Off benefits
Volunteer opportunities
#LI-Remote","$98,161 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,$100 to $500 million (USD)
vebyond corp,#N/A,"Charlotte, NC",AWS Data Engineer,"AWS Data Engineer Lead, Core Technical Skills
5+ years of AWS experience
Experience in a regulated environment
AWS services - S3, EMR, Glue Jobs, Lambda, Athena, CloudTrail, SNS, SQS, CloudWatch, Step Functions
Experience with Kafka/Messaging preferably Confluent Kafka
Experience with databases such as DocumentDB, MySQL, Postgres, Glue Catalog, Lake Formation, Redshift, DynamoDB and Aurora and SQL
Tools and Languages – Python, Spark, PySpark
Experience with Secrets Management Platform like Vault and AWS Secrets manager
Experience with Event Driven Architecture
Experience with Rest APIs and API gateway
Experience with AWS workflow orchestration tool like Airflow or Step Functions
AWS Data Engineer Lead Additional Technical Skills (nice to have, but not required for the role)
Experience with native AWS technologies for data and analytics such as Kinesis, OpenSearch
Databases - Document DB, MongoDB Atlas
Data Lake platform (Hive, Druid, Apachi Hudi/Apache Iceberg/Databricks Delta)
Java, Scala, Node JS, Pandas
Workflow Automation
Experience transitioning on premise big data platforms into cloud-based platforms such as AWS
Strong Background in Kubernetes, Distributed Systems, Microservice architecture and containers
Day to Day Responsibilities/project specifics:
Provides technical direction, guides the team on key technical aspects and responsible for product tech delivery
Lead the Design, Build, Test and Deployment of components
i. Where applicable in collaboration with Lead Developers (Data Engineer, Software Engineer, Data Scientist, Technical Test Lead)
Understand requirements / use case to outline technical scope and lead delivery of technical solution
Confirm required developers and skillsets specific to product
Provides leadership, direction, peer review and accountability to developers on the product (key responsibility)
Works closely with the Product Owner to align on delivery goals and timing
Assists Product Owner with prioritizing and managing team backlog
Collaborates with Data and Solution architects on key technical decisions
i. The architecture and design to deliver the requirements and functionality
Mentor other developers in development of components and related processes
Job Type: Full-time
Salary: $80.00 - $85.00 per hour
Experience level:
10 years
11+ years
9 years
Schedule:
8 hour shift
Experience:
AWS Data Engineer: 5 years (Preferred)
Work Location: On the road",$82.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Titan America
3.8",3.8,"Roanoke, VA",Data Engineer,"Data Engineer
We are seeking a motivated and detail-oriented Data Engineer to join our team based in Roanoke, Virginia. As a Data Engineer, you will play a crucial role in designing, developing and maintaining scalable data pipelines and transformations for business-critical use cases. In addition, you may take on various Software Development and Data Science responsibilities as we frequently share responsibilities and knowledge across our growing team.
Responsibilities:
Design, develop and maintain scalable data pipelines for extracting, transforming, and loading data from various sources
Collaborate with cross-functional teams to identify data needs and determine the best data solutions
Develop and implement data models to support business requirements and ensure data quality
Ensure the security and privacy of sensitive data by implementing appropriate access controls
Monitor and optimize data pipeline performance to ensure timely and accurate data delivery
Document data pipeline processes, data dictionaries, and data storage solutions
Requirements:
Bachelor's degree in Computer Science, Computer Engineering, or a related technical field
Minimum of five years of professional experience working as a Data Engineer or Software Developer
Strong hands-on experience with data warehouse and transformation solutions, i.e. Domo, Snowflake or similar
Proficient in at least one scripting language such as Python, JavaScript, or R
Understanding of data modeling, data integration and data quality processes
Familiarity with cloud platforms such as AWS, Azure, or Google Cloud Platform
Strong analytical and problem solving skills
Full Stack Software Development experience in a professional setting is highly desired, but not required
This is an excellent opportunity for a driven and collaborative individual to make a significant impact in a dynamic and growing team. If you have a passion for data and a desire to work in a fast-paced and dynamic environment, we want to hear from you!
Success begins with hiring the right people to partner with us as we grow and develop our businesses. People are central to everything we do. It is through their efforts and talents that Titan has been successful for over 100 years.
Titan America is an Equal Employment Opportunity (EEO) / Affirmative Action employer and welcomes all qualified applicants. Applicants will receive fair and impartial consideration without regard to race, sex, color, religion, national origin, age, disability, veteran status, genetic data, or other legally protected status.
Job Type: Full-time
Pay: $75,000.00 - $85,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Referral program
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Roanoke, VA 24019: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Do you currently or will you in the future need sponsorship to work in the United States?
This position requires the successful candidate to be in the office in Roanoke, VA at least 3 days a week. Can you meet this requirement?
This position requires the successful candidates to be a full time, W2 employee. Can you meet this requirement?
Education:
Bachelor's (Preferred)
Experience:
Data Engineering: 5 years (Preferred)
Work Location: Hybrid remote in Roanoke, VA 24019","$80,000 /yr (est.)",1001 to 5000 Employees,Company - Public,"Energy, Mining & Utilities",Mining & Metals,1902,Unknown / Non-Applicable
"MyCare Medical Group
4.1",4.1,"Lutz, FL",Data Engineer,"Job Summary
The Data Engineer will design, develop, optimize, and maintain data architecture and pipelines that adhere to ELT principles and business goals.
Job Responsibilities:
Designs, develops, optimizes, and maintains data architecture and pipelines that adhere to ELT principles and business goals.
Solves complex data problems to deliver insights that helps business achieve its goals.
Creates data products for engineer, analyst, and data scientist team members to accelerate their productivity.
Engineers effective features for modelling in close collaboration with business analysts and stakeholders.
Leads the evaluation, implementation and deployment of emerging tools and process for analytics data engineering in order to improve productivity as a team and quality.
Partners with engineers, BI, and solutions architects to develop technical architectures for strategic enterprise projects and initiatives.
Fosters a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions.
Advises, consults, mentors, and coaches other data and analytic professionals on data standards and practices.
Develops and delivers communication and education plans on analytic data engineering capabilities, standards, and processes.
Job Qualifications:
Education and Work Experience: Bachelor’s degree in computer science, information systems, engineering, or a related field; Masters preferred.
3+ years of experience in data engineering role.
Expertise in ELT and data analysis, SQL and at least one programming language (e.g., SQL, Python).
Conceptual knowledge of data and analytics, such as dimensional modelling, reporting tools, data governance, and structured and unstructured data.
Skills: Strong understanding of agile methodologies and experience as a Data Engineer on a cross-functional agile team preferred.
Experience with designing and maintaining data warehouses and/or data lakes with technologies, such as Hadoop, Spark, or distributed databases, like Redshift and Snowflake, and experience with housing, accessing, and transforming data in a variety of relational databases.
Experience with cloud database technologies (e.g., Azure) and developing solutions on cloud computing services and infrastructure in the data and analytics space.
Experience in building data pipelines and deploying/maintaining them with best-in-class tools (e.g., Git and Jenkins).
Familiarity with the Linux operating system.
Basic familiarity with BI tools (e.g., Tableau, Power BI, Looker).
Experience working in data engineering or data architect role.
Candidate Qualities: Focuses on business agility and agile delivery with a “fail-fast, succeed early” mindset and emphasis on measurable outcomes.
Brings a high-energy and passionate outlook to the job and can influence those around them.
Able to build a sense of trust and rapport that creates a comfortable & effective workplace.
Passion for innovation and “can do” attitude.
BENEFITS
Comprehensive benefits package, including Health, Vision, Dental, and Life insurancesFSA and Life Assistance Program (EAP)
401(k) Retirement Plan
Health Advocacy, Travel Assistance, and My Secure Advantage
PTO Accrual and Holidays
#ZIP
Job Type: Full-time
Pay: $125,000.00 - $150,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Employee discount
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Referral program
Relocation assistance
Retirement plan
Tuition reimbursement
Vision insurance
Compensation package:
Performance bonus
Yearly bonus
Experience level:
3 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Lutz, FL 33558: Reliably commute or planning to relocate before starting work (Preferred)
Experience:
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person","$137,500 /yr (est.)",501 to 1000 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2018,Unknown / Non-Applicable
Redzara.us,#N/A,"Atlanta, GA",Azure Data Engineer,"Responsible for developing new and modernizing existing applications to Azure Cloud Platform.
The ideal candidate must demonstrate in-depth knowledge and understanding of Database concepts.
Requirements:
7+ years of experience in Data Warehousing and Business Intelligence
5+ Years in BI development leveraging the Microsoft SQL Server. Needs to be an expert in T-SQL
5+ Years in SSIS and use of Stored Procedures.
1-2 Years in Azure Data Factory, SQL VMs, build and maintain ETL
Ingest and aggregate data from both internal and external data sources to build our data lake
Strong understanding data modeling (i.e., conceptual, logical, and physical model design, experience with Operation Data Stores, Enterprise Data warehouses and Data Marts).
Job Type: Contract
Pay: $43.49 - $57.00 per hour
Compensation package:
1099 contract
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Atlanta, GA 30334: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure Data Factory: 3 years (Required)
Azure: 3 years (Required)
Data warehouse: 1 year (Preferred)
Work Location: Hybrid remote in Atlanta, GA 30334",$50.25 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Trella Health
4.8",4.8,"Philadelphia, PA",Senior Data Engineer,"At Trella Health, we are passionate and committed to our mission – empowering meaningful change in healthcare. Since our founding in 2015, we continue to grow our team, enhance our solution and services offerings, accelerate into new markets, and expand our customer base. We are rapidly growing and are looking for new Trellavators to join our team!
""What is a Trellavator?"" you ask. Innovate and elevate is the name of our game! We go above and beyond to collaborate with and support each other – we believe that when a colleague or customers succeeds, we succeed. By learning from others, building on our successes, and taking risks, we constantly raise the bar – continuous improvement is in our DNA. Our word is paramount, we keep our commitments, and we always follow through. We have a strong, reliable support system that fuels growth, collaboration, and passion – and together, we create a positive environment where everyone at Trella Health, including the customers we support, can thrive. Are you ready to learn more about the opportunities with our team? Trell-yeah you are!
Position Overview:
The Senior Data Engineer will be responsible for designing, implementing, and maintaining an effective data pipeline for Trella Health's applications. This position is responsible for turning data concepts into effective data models that are efficient and scalable. The Senior Data Engineer will ensure a smooth data release and deployment process throughout the data development lifecycle.
Successful candidate will be a highly organized, self-starter that can roll up their sleeves and solve problems efficiently. Ability to document procedures, manage time, and have a knack for optimizing data flows.
Location: Atlanta HQ; Raleigh, NC; Nashville, TN; Philadelphia, PA; Remote considered
Reports to: Manager, Data Engineering
As a member of the Data team at Trella, you will:
Champion a comprehensive data vision and operating principles that improve data quality, consistency, accessibility, decision making, use, and stewardship.
Manage data pipeline by developing processes for data input, transformation, and storage
Act as an interface between the Data Science team and Application Developers
Architect and implement front-end (application layer) and back-end (ETL layer) database technologies in a highly customized data environment
Develop automated processes for moving data between systems
Implement ETL processes that transform source data into functional data stores consumed by front-end applications
Scale data pipeline by utilizing the most efficient tools and technologies
This job might be a fit for you if you have:
Bachelor's degree in Computer Science, Engineering, Mathematics, or other technical discipline (proven professional experience will be considered in lieu of a technical degree)
2+ full years of recent advanced Snowflake development experience
Experience manipulating healthcare data
In-depth knowledge of Amazon Web Services (AWS) and Platform as a Service (PAAS)
Demonstrable experience with Snowflake utilities including SnowSQL & Snowflake Javascript
Analyze all aspects of the Snowflake Query Engine to improve bottlenecks that may exist
Ability to automate data ingestion from Amazon S3, FTP, and other technologies
Experience with scripting languages, extensive Python experience, Node.js experience a plus
Working knowledge of Bitbucket and Git
Experience managing and loading data in AWS RDS (MySQL/MariaDb)
Ability to optimize MySQL query execution plans for robust performance
Technical Job Requirements (Nice to Have)
Experience manipulating healthcare data including claims, ADT data
Foundational knowledge of CI/CD platforms, preferably TeamCity
Development and consumption of API services
Experience with data science tools (Jupyter, SageMaker, R)
Understanding of Atlassian suite of tools, including Jira and Confluence
Soft Skill Job Requirements (Nice to Have)
Strong analytical skills and ability to rapidly switch contexts
Excellent communication and problem-solving skills
Sole contributor that is willing to independently uncover and improve processes that are inefficient or sub-optimal
Track record of implementing creative solutions to complex problems
Experience working in a nimble, fast-paced, start-up environment
Knowledge of HIPPA, PHI, PII policies
About Trella Health:
Trella Health provides unmatched, actionable market intelligence to post-acute care and value-based care providers of all sizes. Our industry leading analytics paired with CRM and EHR integration workflows positions us as the most advanced sales enablement platform for the post-acute care market. One of only a few companies to be deemed both a Qualified Entity by CMS and an Innovator under its Virtual Research Data Center Program, Trella has elite access to billions of performance metrics and referral data to enable competitive positioning across the market. Its standardized insights, representing 90% of all lives 65+ U.S. population, help customers identify, engage, and manage critical relationships and advance their organizations with certainty.
Since launching our first product in 2016, we have experienced tremendous growth and have highlighted a few key data points to get an at-a-glance view of Trella:
Over 700 customers including 10 of the top 10 post-acute providers and 4 of the top 10 health systems
Over 120 team members and growing
Recognized as one of Modern Healthcare's ""Best Places to Work"" in consecutive years and were ranked in the Top 50 startups in the US by LinkedIn
Headquartered in Atlanta, GA with remote hubs in Nashville, Philadelphia, Raleigh, and the Philippines
The Trellavator Experience:
At Trella Health, you can expect to join a welcoming team that truly embodies our core values. Our collaborative culture is anchored by trust, transparency, and inclusion — and we also have a lot of fun. No matter where you work, at Trella Health you can expect an awesome team, frequent virtual gatherings, engaging events, casual attire, and more.
We offer competitive salaries with a comprehensive benefits package to all employees and provide an environment that fosters work-life harmonization with unlimited PTO, company-observed holidays, and Summer Fridays. As we continue to see exponential growth, our goal is to continue to put team members first and strive to offer our team members the best culture and benefits possible. Some of the benefits we provide are:
Health, Dental, Vision & Voluntary Benefits
Competitive Salary & Bonus Plans
401k Retirement Savings
Unlimited PTO & 10 Paid Holidays
Flexible Work Hours
Equity Shares
Paid Leave Programs
Marketplace for discounted retail and entertainment
Equal Opportunity Employer
Trella Health is an equal opportunity employer. All persons will receive consideration for employment without regard to race, color, gender, sexual orientation, gender identity or expression, religion, national origin, marital status, age, disability, handicap, veteran status, genetic information, or any other protected status as recognized by federal, state, or local laws.","$113,846 /yr (est.)",51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2015,Unknown / Non-Applicable
Atika Technologies,#N/A,"Pittsburgh, PA",Data Engineer,"Role: Data Engineer/PySpark Developer, Pittsburgh, PA - Day One Onsite
Fulltime and Contract(W2)
LinkedIn Must
USC/GC/EADs
Three round of interview: First two on Zoom Video Call, End Client Round will be Onsite on the Client's Location (Pittsburgh, PA)
Job Description:
Python(PySpark), Jupiter hub, airflow, Java
Database knowledge - Oracle / SQL / Data Analytics
Regulatory reporting knowledge preferred
Good communication skills
Open / Ability to learn new technologies
Very good Analytical Skills
Experience in ETL is preferable
Job Types: Full-time, Contract
Salary: $120,000.00 - $130,000.00 per year
Benefits:
Dental insurance
Health insurance
Life insurance
Compensation package:
Bonus pay
Performance bonus
Yearly pay
Experience level:
10 years
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Pittsburgh, PA 15259: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Required)
Spark: 4 years (Required)
Regulatory reporting: 3 years (Required)
SQL: 4 years (Required)
IT: 10 years (Required)
Airflow: 4 years (Required)
Jupiter: 4 years (Required)
Work Location: One location","$125,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Take Command Health
4.9",4.9,"Dallas, TX",Data Engineer,"Company: Take Command
Position: Data Engineer
Salary: $120,000-$130,000 plus bonus
Location: Dallas or Austin, TX
About Take Command
Take Command is a start-up on a mission to improve the healthcare system, starting with health insurance. Pragmatically speaking, we help employers reimburse employees for individual insurance instead of offering a traditional one-size-fits-all group plan. We believe this model can empower employees (when they have the right support) to be savvy healthcare consumers and have a transformative impact on the entire healthcare system.
Let’s be honest—health insurance is usually a confusing, frustrating, and even emotional experience for people. We want to fix that with a new model, great technology, and a superior user experience. We have made a great start, but we need your help to fully realize our long-term vision.
As a part of the dev team, you'll build the framework for both our clients and our team to succeed. You will be a pillar in our company as we grow and make our product the best product on the market!
In this role, you will:
Take ownership for delivering multiple features, their quality and assist team members in delivering their features
Recommend improvements to data quality, processes and code to make it easier to use, manage and maintain
Experience designing and developing data models and data migrations
Experience managing and maintaining ETL or ELT processes (Hevo, BigQuery, Looker, etc.)
Hands-on development experience with SQL, relational database design (ERDs)
Must be able to write SQL & contribute code
Be product-minded; focus not only on what needs to be built but why something needs to be built
You may be a great fit for this role if you have:
3 or more years of professional experience as a data analyst or data engineer
Cloud-Based Data Tools e.g. Hevo, BigQuery, Looker, etc.
Database technologies: SQL, RDBMS / Relational databases (Postgres, Amazon RDS / Aurora, etc.)
Nice to have experience: Python, Java, Typescript or another modern OOP language
Cloud infrastructure: AWS
Source control: Git
Working at Take Command
We’re excited to build a team and culture that reflects our values! We offer competitive pay, health benefits, and equity options to share with this position. We want you to be an owner along with us in revolutionizing healthcare.
A generously funded ICHRA for medical, dental, and vision premiums and medical expenses. You get to use our own product and we think that’s so exciting and rare!
Unlimited personal vacation in addition to regular company holidays.
401(k): 90-day eligibility for 2% match that vests immediately!
Access to LinkedIn Learning.
We have two beautiful offices conveniently located near The Hill (Dallas) and Holly Commons (Austin). The kitchen is well-stocked and we've designed the space to have lots of different areas to work!
Paid parental leave for new parents.
Flexible on where you work – we hope to see you around the office on average 3 days a week especially when white-boarding or building relationships with your colleagues, but you also have the ability to work from home or wherever you'd like when you need to get focus work done.
More About Us
We secured our Series A funding in 2021 and are thrilled to be able to expand our team. Despite being a small startup in a land of health insurance giants, we’re the recognized industry leader for what we do (health insurance reimbursements) and passionate about bringing it to market because we know we can help fix a broken system and improve our clients’ wellbeing and health outcomes.
We’ve been featured in The New York Times, The Wall Street Journal, The Dallas Morning News, and other national healthcare publications and are excited about our growth opportunities.
Take Command knows diversity and inclusion among our teammates is integral to our company’s success and growth. Our vision is to recruit, develop, and retain the best team from a diverse candidate pool.
This has mostly been about us, but we’d love to hear from you-we can’t wait to hear your story!
Take Command is an equal opportunity employer! We celebrate diversity and are committed to creating an inclusive environment for all employees.
Xrygn4NXMr","$125,000 /yr (est.)",1 to 50 Employees,Company - Private,Insurance,Insurance Agencies & Brokerages,2014,Unknown / Non-Applicable
Infosoft,#N/A,"Peachtree Corners, GA",Data Pipeline Engineer - 2173,"Here are the job position details for your review:
Job Title: Data Pipeline Engineer (GStreamer/ DeepStream)
Pay Rate: $90 to 120/Hr.
Duration: 3 Months
Location: Peachtree Corners, GA
This position is for the Building Automation division.
We are looking for Data Pipeline Engineer (GStreamer) to participate in the development of Streaming data pipelines using NVIDIA Deepstream, GStreamer, Python programming, messaging queues, and related tools.
Preference is for candidates located in Atlanta, GA metro area. However, ok with non-local candidates who are open to 75% travel to Atlanta, GA.
Understanding the basics of our architecture/setup to be able to do setup/support for demos, testing, etc.
Basic computer hardware assembly and set-up
Possible set-up/installation of the application(s)
Possible development tasks depending on what the rest of the development team has for them
Develops business software applications, system software, and/or IT solutions on the basis of functional specifications.
Scope of position embraces both the customization or reuse/modification of system software and/or solutions as well as their development from scratch.
Consults with customers in creating functional specifications and work estimates, and supports Make-or-Buy decisions.
Analyses performance gaps of existing software environments and proposes measures to address efficiency problems.
Is also responsible for the implementation of data-driven components supporting data modeling and engineering in the context of data analytics (Data Stewardship).
Skills:
Python
Linux
Docker and Docker Swarm
Zero MQ / a good understanding of messaging
Desired Skills:
NVIDIA Deepstream SDK
Web Sockets
Flask
Protobuf
Possible:
GStreamer (if different than RTSP / H264 / MJPEG)
Job Type: Contract
Salary: $90.00 - $120.00 per hour
Ability to commute/relocate:
Peachtree Corners, GA: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
""Please share your Email ID""
Work Location: One location",$105.00 /hr (est.),Unknown,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Tipico - North America
4.0",4.0,"Denver, CO",Data Engineer,"Company Description

Tipico is an energetic, passionate, bold tech company, and we love sports! Tipico has recently expanded to the US market, establishing offices in Hoboken, New Jersey and Denver, Colorado. We are driven by our core values: passion, progress and trust. Our goal is to build the best possible mobile sports betting product in the industry, in order to amplify the emotions of millions of sports fans every day. Let’s Make This Interesting ™
Please note: this role is located in our Denver, CO office; we work off a hybrid model and come into the office 2 days per week.

Job Description

The Data Engineer will report directly to the Data Engineering Manager and work together with rest of the data engineers, BI analysts, and product teams. The position holder ensures this is done by doing the following:
Create and maintain both batch (ETL/ELT) and real-time data pipelines and architecture
Ensure all data provided by Data Platform is of the highest quality and is delivered in a timely manner and in line with agreed SLAs
Assemble large and complex data sets that meet functional and non-functional business requirements
Identify, design, and implement internal process improvements, including automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies
Collaborate with stakeholders including data architects, Executive, Product, Data and IT team members from the beginning to the delivery of a project
Build a scalable, modern, and trusted data ecosystem, making sure it is responsive, available and in line with business requirements
Take part in sprints, workloads, and deliverables through agile methodologies
Manage quality assurance, verification of accuracy and consistency of data, specifically KPI measures sent to the business
Thrive working in a fast-paced and challenging environment
Open to continuous learning individually and with the team
Comfortable with being challenged / seeking challenge
Willing to work flexible hours if needed

Qualifications
3+ years in Data Platforms / Datawarehouse / BI experience
Must have experience in successfully implementing data-centric applications, such as data warehouses, operational data stores, and data integration projects
Strong analytical and problem-solving skills that balance with creative approaches
Ability to learn new and complex concepts quickly
Strong experience with large-scale production databases and SQL
Solid understanding of cloud data services: AWS services such as S3, Athena, EC2, RedShift, EMR, EKS, RDS and Lambda
Proficient with one or more big data & new data technologies such as DBT, NIFI, Airflow, Kafka
Experience with data modelling
Experience with ETL development (extractions, data load, aggregation, transformation)
Knowledge in working with time-series/analytics databases such as Elasticsearch
Understanding of containerization and orchestration technologies like Docker/Kubernetes
Must be able to take ownership of projects with a high level of attention to detail and priority
Team player with a hands-on approach and attitude
Must be comfortable with on call rotation
Nice to have:
Familiarity with Marketing tooling and/or CRM systems is preferred
Familiarity with using business intelligence tools such as Domo, Tableau or Looker
Knowledge about and / or experience with sports betting
Background in digital companies, online, gambling, gaming, e-commerce, marketing, or mobile apps
Familiarity with one scripting language, preferably Python

Additional Information

What's in it for you:
Work in an environment where you, your work and ideas matter and have an impact.
You will be a be part of the newest and up and coming USA online iGaming and sports betting companies in the market.
Work in a new and young business with high growth potential.
Build your own success story together with us.
Work with self-organized, self-responsible and entrepreneurial employees.
Start-up feeling, backed-up by a leading European sports betting house. We are a high-volume business and are taking off in the US!
Competitive salary, Medical, Vision, Dental Benefits, Unlimited PTO, and more
Salary for this position is $120k+ & annual performance bonus","$120,000 /yr (est.)",201 to 500 Employees,Company - Private,"Arts, Entertainment & Recreation",Gambling,2004,Unknown / Non-Applicable
"Intersec
4.5",4.5,Remote,Senior Data Engineer,"Architect and implement highly scalable data pipeline to streamline migration of data from source data repositories to the centralized data platform
Implement agreed upon data models (e.g., data marts) to enable faster and efficient consumption of data by downstream consumers
Conduct performance tuning to optimize data migration techniques and ensure adherence to identified service-level agreements (SLAs)
Define and implement automated validation checks to ensure data integrity between source and target
Assist in onboarding consumers with existing technology to connect to the platform
Setup predictive alert mechanism for pipeline degradation for IMF pipelines.
Enhance data quality framework for data pipelines and derived datasets
Enhance Data Audit Balance and Control mechanism for data pipelines
Enhance pipelines to be certified copies of authoritative source
Populate metadata for the Enterprise with particular focus on the Data Sources being onboarded to EDP (Technical and Business Glossaries) related to IMF Modernization Programs
Validate correct lineage for datasets
Required:
*8+ years of Databricks, Data Modeling, and/or Metadata management experience
* Familiarity with architecting and developing real and near-real-time solutions
* Bachelor's Degree in Engineering, Mathematics, computer Science, Statistics or other quantitative discipline, or 4 years equivalent professional experience
* US Citizenship and ability to gain a public trust
Preferred:
* Familiarity with IRS systems and ETL pipelines
* Ability to work independently and manage multiple tasks
* IRS MBI Clearance with IRS Laptop and Informatica Installed
Job Type: Full-time
Salary: $70.00 - $75.00 per hour
Schedule:
Monday to Friday
Application Question(s):
Are you a US/GC card holder?
How many years of Databricks, Data Modeling, and/or Metadata management experience do you have?
Work Location: Remote",$72.50 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,Unknown / Non-Applicable
"Apolis
3.9",3.9,"Saint Louis, MO",Data Engineer (Python Focused),"Role: Data Engineer (Python Focused) (W2 ONLY)
Location : St. Louis, MO 63102
Duration: 18 Months
The primary responsibilities of this role will be to work as a part of the data and analytics team of the client. The ideal candidate would have experience using ETL and other data engineering tools in a cloud environment. Additionally, this candidate would have coding and scripting experience creating code to move and manipulate data using tools such in Python such as Spark or Pandas. Microsoft Azure experience is a plus but relevant experience with other cloud-based environments will be considered.
This team uses Microsoft Azure to build and productionize data sets and processes to the product owners and product users and this role will be expected to contribute to those objectives. As a member of this team, you will work in conjunction with other data engineers, platform engineers, solution developers and others to execute and deliver working solutions to the end users.
Technical Skills
High focus on Python
More Programmer/Coder experience, less Cloud experience needed.
Must have experience with data analytics/engineering and cloud technologies and an in-depth working knowledge and understanding of data engineering, design, and integration.
Intermediate or higher level of proficiency with Python Must have the proven ability to learn quickly and apply cloud data engineering principles and skills to new tools and use cases.
Ability to define and take technical ownership of detailed requirements and collaborate effectively with various team members such as Business Analysts and Project Managers and end users.
Excellent communication (verbal and written) and problem-solving skills are essential for this position.
Initiative, Proactive Cooperation, and curiosity are mandatory behavioral skills required for this position.
Experience with several of the following technologies is preferred:
Experience with Microsoft Azure Platform. In particular, the data processing toolsets including Azure Data Factory Pipelines, Azure Databricks, Azure Data Lake Storage.
Experience using Notebooks (Such as Jupyter, DataBricks) also desired.
Experience using code languages such as Python and C#
Experience using query languages such as SQL, MDX and DAX
Experience using scripting languages such as PowerShell, M-Query (Power Query), and Windows batch commands.
Understanding of Agile and DevOps principles for software development and ownership.
Familiar with compliance and controls for software development and deployment.
Education Required
A bachelor's in computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field [or equivalent work experience] is not required but is preferred.
Job Type: Contract
Salary: $60.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Vision insurance
Compensation package:
Overtime pay
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
St. Louis, MO: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Preferred)
ETL: 5 years (Preferred)
Cloud infrastructure: 4 years (Preferred)
Work Location: One location",$65.00 /hr (est.),501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,1996,$25 to $100 million (USD)
"shaped.ai Inc.
4.0",4.0,"New York, NY",Data Engineer,"Hello! We’re building the future of content discovery infrastructure with AI — come join us!

Shaped (YC W22) is an API for developers to seamlessly add personalized ranking and recommendation to their products. These frictionless discovery experiences help end-users find what they want faster, and, consequently, grow conversion and engagement business metrics. Behind the scenes, we’re building state-of-the-art ML infrastructure for training and deploying large AI models to power our personalization engine.
We’re a seed stage start-up backed by top-tier investors (e.g. Y-Combinator, Susa Ventures, Tribe Capital etc…) and executives (e.g. Google, Amazon, Uber, Dropbox etc…). Our team comes from Meta, Google, Apple and Uber. We’re a remote team but have a small office in Brooklyn, New York.
We’ve been lucky to have a huge amount of success since our initial launch earlier this year and now looking to expand our real-time data engineering capabilities. We are looking for a data engineer to design, build and optimize Shaped's real-time streaming data ingestion endpoints. You will be a founding engineer working on state-of-the-art infrastructure. As one of Shaped’s early employees you will help shape our product, culture and vision.
This role is for a full-time data engineer at $120k - $200k a year and 0.1 - 0.5% equity.

Responsibilities:
Manage the unification of our batch and real-time data processing pipelines.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Requirements:
Bachelor's in computer science, data science or mathematics related field. Master's degree or PhD will be advantageous.
5+ years of data engineering experience, ideally applied to machine learning use-cases.
Experience with AWS or common cloud technologies
Proficient with Python, SQL.
Extensive knowledge of both offline and online large scale data processing frameworks: Spark, Beam, Flink.
Extensive knowledge with stream technologies like Kakfa or Kinesis.
Excellent written and verbal communication skills.
We’re excited to work with you. Come build the future of AI and discovery with us!","$160,000 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Merkle
3.7",3.7,"Norristown, PA",Data Engineer,"Company Description

We are Dentsu International. We innovate the way brands are built. That means we do things differently so they’re better than before. In this way, we make our clients’ most important marketing assets—their brands—win in a changing world.
Dentsu International is a modern marketing solutions company. Our mission is to help clients navigate, progress, and thrive in a world of change. Businesses rely on our integrated network of agencies and specialized practices to champion meaningful progress through creative, media, commerce, data, and technology. Part of Dentsu Group, our global network comprises 66,000 diverse people in 143 countries, who are dedicated to teaming for growth and good. Follow us on Twitter @DentsuUSA and visit dentsu.com/us.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.

Job Description
Develop onboarding process to receive and process information related to onboarding a new subscriber
Convert Snowflake Stored Procedures to Python Scripts directly in Airflow
Develop metadata driven solutions for managing data processing and cadence
End to end automation development and testing
Building and maintaining stored procedures and scripts in Snowflake Data Cloud
Building and maintaining complex SQL queries for ETL/ELT processing
Develop Dashboard for data processed for audit and governance needs
Follow Agile methodologies to deliver iterative feature sets rapidly
Maintain codebase using source control methodologies, Git
Collaborate with business analysts, data analysts, and other developers to deliver powerful new features
Coordinate with teams as part of a larger data-sharing system
Contribute to a collaborative, positive, and enjoyable environment for your operations team
Performing quality assurance and testing
Writing user and technical documentation such as runbooks

Qualifications
Strong communication skills both verbal and written
Strong understanding of conditional programming logic
Knowledge of software engineering and development methodologies, techniques, and tools; includes Issue Tracking (JIRA) Software Development Lifecycle SDLC), and Continuous Integration / Continuous Development (CI/CD)
Experience working with large databases and data warehouses utilizing both relational and dimensional design concepts
Eagerness to learn new technologies, programming languages and platforms to solve complex data challenges
Experience with Python required
Experience with SQL required
Experience with Snowflake Data Cloud is highly desirable
Experience with Amazon Web Services (AWS) a plus
Experience with Airflow (or other orchestration) a plus
Experience in database marketing is a plus

Additional Information

Perks:
Great compensation package
Comprehensive healthcare plans
401(k) with employer match
Flexible time-off
16 weeks paid parental leave
Additional Information
The anticipated salary range for this position is $94,000-$146,000. Salary is based on a range of factors that include relevant experience, knowledge, skills, other job-related qualifications, and geography. A range of medical, dental, vision, 401(k) matching, paid time off, and/or other benefits also are available. For more information regarding dentsu benefits, please visit dentsubenefitsplus.com
Employees from diverse or underrepresented backgrounds encouraged to apply.
Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact recruiting@dentsuaegis.com if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.
About dentsu
Dentsu is the network designed for what’s next, helping clients predict and plan for disruptive future opportunities in the sustainable economy. Taking a people-centered approach to business transformation, dentsu combines Japanese innovation with a diverse, global perspective to drive client growth and to shape society www.dentsu.com.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.
Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact your recruiter if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.","$120,000 /yr (est.)",5001 to 10000 Employees,Company - Private,Media & Communication,Advertising & Public Relations,1971,$500 million to $1 billion (USD)
"Loyal
3.9",3.9,"Atlanta, GA",Senior Cloud Data Operations Engineer,"Loyal is an organization centered on experience and building a platform that allows consumers to make meaningful decisions when it comes to healthcare. We deeply understand providers, locations, services, appointments, business rules, and moreover, we understand patients - who they are, the preferred method of communication, upcoming appointments, lapsed appointments, outstanding bills, health risks, and more. With this intelligence, our platform fuels highly relevant and personalized experiences across all mediums (website, email, voice…) allowing patients to get healthy, stay healthy, and have a better relationship with the health care provider.
**This is a remote role**
Summary
The Senior Cloud Data Operations Engineer is responsible for delivering Loyal's cloud-based data warehouse. They interface with groups across the organization to ensure we are collecting and storing data in a way that makes it easy to create and consume. They identify best practices for big data management, select the right tooling, create a workable data architecture, and drive an implementation roadmap others can depend on. The Senior Cloud Data Operations Engineer also works closely with our security group to ensure data security and privacy. They also mentor others on the team, providing technical leadership to those they work with.
You will
Create data streams and consolidate data sources into a single cloud-based data warehouse
Architect and catalog data warehouse storage schema
Work with product teams to enable easy data aggregation
Work with data consumers (analysts, data scientists, and AI/ML engineers) to ensure data is easily accessible and consumable
Create and execute a roadmap that meets data storage and availability goals
Ensure data is secure through data analysis, access control, and encryption
You have
Bachelor's degree in computer science, information systems, applied math or related field, or equivalent work experience
Minimum 5 years of experience working with cloud data processing services (AWS, Azure, or GCP).
Minimum 3 year experience with Big Data technologies, such as Databricks, Snowflake, Spark, Hadoop, etc.
Strong experience in creating data architecture and storage schemas required
Significant working knowledge of SQL and other query languages and tools
Significant experience with building relational database systems strongly preferred
Experience with ETL tools a plus
Knowledge of Python preferred
Software development, data analytics, or data science experience is a plus.
Experience working within a software as a service (SaaS) company preferred.
Experience working within a high growth and/or ambiguous environment, with proven experience to be dynamic preferred.
#LI-REMOTE
We know that potential candidates are often less likely to apply to a position if they don't match 100% of the job qualifications. Don't let that be why you miss out on this opportunity! We encourage you to apply if you can demonstrate many of these skills and competencies.
Loyal to our employees
We are a remote-friendly company! We encourage you to apply from anywhere in the United States. We also believe in a work/life balance that fulfills you while you're here and supports you when you're not. We built our benefits package to prove that we're committed to you having everything you need (including a little fun). Here is what we offer full-time employees:
Flexible paid time off, sick and personal days
At least one holiday per month (sometimes, more!)
Full health, dental, and vision insurance - Loyal pays the premium for all employees!
One Time Home Office Setup Stipend For Remote & Hybrid Roles
Monthly Internet Stipend for Remote & Hybrid Roles
Long term & short term disability
401[k] plan
16 Weeks Paid Parental Leave
2 Volunteer days per year
Matching Gift Program
Participation Grant Program
Annual Travel/Team Events up to twice per year (post-COVID)
Our Commitment
We believe that the key to Loyal's success is you. Your unique background, life experience, knowledge, self-expression, and talent make you uniquely you. Who you are, what you have experienced, and how you think inspires us to be innovative and bold.
Loyal is an equal opportunity employer. We hire great people from a wide variety of backgrounds, not just because it's the right thing to do, but because it makes our company stronger. We welcome the unique contributions that you can bring in terms of your education, opinions, culture, ethnicity, race, ancestry, sex, gender identity and expression, national origin, citizenship, marital status, age, languages spoken, veteran status, color, religion, disability, sexual orientation, and beliefs.
We consider qualified applicants regardless of criminal histories, consistent with legal requirements.
Further, consistent with applicable federal and state law, Loyal provides reasonable accommodations when requested by qualified applicants or employees with disabilities, unless doing so would cause an undue hardship. Loyal's policy regarding requests for reasonable accommodation applies to all aspects of employment, including the application process. If you require a reasonable accommodation in connection with the application process, please contact the Talent Acquisition Department at talentacquisition@loyalhealth.com.
E-Verify
This employer participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S. If E-Verify cannot confirm that you are authorized to work, this employer is required to give you written instructions and an opportunity to contact Department of Homeland Security (DHS) or Social Security Administration (SSA) so you can begin to resolve the issue before the employer can take any action against you, including terminating your employment. Employers can only use E-Verify once you have accepted a job offer and completed the I-9 Form.
COVID-19 Vaccinations
Consistent with Loyal policy, candidates performing in-person work will be required to show proof of being fully vaccinated against COVID-19 upon commencing employment. Reasonable accommodations will be considered on a case-by-case basis for exemptions to this requirement in accordance with applicable law. If you require a reasonable accommodation to Loyal's vaccination requirement, please contact the People Department at people@loyalhealth.com.",#N/A,51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2015,$5 to $25 million (USD)
WellTrust Medical Group,#N/A,"Charlotte, NC",Data Engineer,"The Data Engineer will assist the senior analyst in complex modeling and projects to ensure their completion within the allotted timeline. Helps track measures of success and develop reports that measure operational and customer metrics. Demonstrates ability to troubleshoot data consumption and data quality problems.

Responsibilities:
Build and maintain reports using My SQL Server
Identify optimal data sources, structure data validation, and cache optimization.
Performs business problem modeling and quantitative analytics including regression analysis, time-series, hierarchical cluster analysis, and data mining.
Helps track measures of success and develop reports that measure operational and customer metrics.
Demonstrates ability to troubleshoot data consumption and data quality problems.
Evaluates and recommend improvements to current models, metrics, and analytic processes to improve operational efficiency and insight.
Design, document, and communicate effective technical solutions meeting end-user requirements
Perform analyses with large amounts of complex data, and uses data visualization tools to communicate results.
Requirements:

Bachelor's degree or equivalent experience in a quantitative field (Mathematics, Statistics, Computer Science, Economics, Finance, Physics, Engineering)
3+ years of professional working experience as a Data Engineer or Analyst
1-3 years of data warehouse experience
Build and maintain reports using My SQL Server
Strong analytical and creative problem-solving skills
Design, document, and communicate effective technical solutions meeting end-user
SQL and Excel Macro development
Strong understanding of various data environments such as SQL, Oracle, Big Query, etc.
Experience with data visualization tools such as Looker Studio, Tableau, D3.js, etc.
Proficient in R, Python, MATLAB, Java, or another programming language.",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Iron Bow Technologies
3.5",3.5,"Washington, DC",Sr. Data Center Engineer,"Iron Bow Technologies is seeking a Sr. Data Center Engineer to support our Federal customer, The Department of Justice (DOJ), Criminal Division (CRM), in Washington, DC. The selected candidate will support our flexible and scalable solution with the DOJ CRM to extend cloud-native features to existing legacy and next-generation applications while also future proofing the infrastructure to easily accommodate future requirements, such as transitioning to a public cloud.
This position is hybrid and requires 2 days a week onsite at the customer location in NW Washington, DC and support at the customer data center as needed in Sterling, VA.
Essential Duties and Responsibilities:
Assess and provide VMware solutions to include vCenter, vSphere, SRM, ESXi
Primary storage solutions and data file storage management, NAS file storage solutions
Data Center blade servers and rack servers
Server virtualization
Experience:
10 years of related experience providing system administration support in a virtualized environment, including at least 6 years of VMWare solutioning, including vCenter, vSphere, SRM, and ESXi
Data Center operations experience, with SAN and NAS
Experience supporting primary storage solutions and data file storage management, NAS file storage solutions
Hands on experience with Data Center blade and rack servers
Ability to work staggered shift hours when needed to support critical coverage and/or time-sensitive needs
Be a member of our Team providing coordinated 24x7 IaaS on-call support
U.S. Citizen
Currently hold or obtain/maintain a Public Trust High clearance
Education:
BS in Computer Science, Information Science, or other relevant discipline is preferred – additional years of experience may be considered in lieu of the degree
Preferred Certifications:
VMware Certified Professional
Primary storage certifications such as HPE, Dell-EMC, NetApp, or Pure Storage
VCP-DCV
Travel Requirements:
No travel
PRINCIPALS ONLY. NO AGENCIES PLEASE. MUST BE A US CITIZEN ABLE TO OBTAIN A PUBLIC TRUST SECURITY CLEARANCE.
Iron Bow Technologies is a leading IT solution provider dedicated to successfully transforming technology investments into business capabilities for government, commercial and healthcare clients. Our depth of technical expertise, global reach and strategic partnerships with industry leaders as well as disruptive technology partners uniquely positions Iron Bow to target the right solution to ensure your successful business outcome. We partner with our clients from planning and implementation through ongoing maintenance and management to deliver solutions that are strong, flexible and on target .
Iron Bow Technologies is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.
#LI-TM1",#N/A,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,1983,$500 million to $1 billion (USD)
Fracsys Inc,#N/A,"Washington, DC",Data Engineer,"Fracsys Inc is hiring a Data Engineer position. The ideal candidate must have at least 12+ years of industry experience. He or she must be responsible for successful technical delivery of Data Warehousing and Business Intelligence projects using a formal project management methodology.
Responsibilities
Directs, motivates and develops key technical personnel
Provides technical direction across the full lifecycle system development process
Provides strategic planning and comprehensive senior level technical consulting to IT senior management and senior technical staff
Working independently within guidelines
Provides detailed inputs for plans and schedules, including goals, risks, and resource allocation
Conduct diagnostic, troubleshooting steps and present conclusions to a varied audience
comfortable slinging some code to solve a problem
Build Data Marts and Data warehouses to facilitate analytical capabilities across the varied sources of data
Design and implement data patterns on cloud using cloud Data warehouse and data virtualization tools
Evaluate new tools and technologies for target state on cloud
Learn and implement new ETL, BI tools and data warehousing technologies
Qualifications
Minimum twelve (12) years in a Technical Lead or Consulting role leading Information Management related initiatives (system integration, data warehouse build, data mart build, or similar)
MUST have proven leadership experience for team of at least 3 technical resources.
Experience creating Data Marts, Data warehouses on-premise and on cloud for real time and batch processing frameworks
Experience building and optimizing big-data data pipelines and data sets including Postgres, AWS Relational Data Service
extensive hands-on experience building and optimizing data structures for data analytics, data science and business intelligence
Experience building self-service data consumption patterns and knowledge of cloud-based data Lake platforms
Experience wrangling data (structured and unstructured), in-depth knowledge of database architecture
Experience utilizing or leading implementations leveraging ETL tools (Informatica / Talend), BI Reporting tools such as MicroStrategy, Microsoft Power BI, data modeling tools such as Erwin, Oracle, SQL server, NoSQL, JDBC, UNIX shell scripting, PERL and JAVA, XML/JSON files, SAS , Python, AWS cloud-native technologies, S3, Athena, Redshift
Familiarity with the following technologies: Hadoop, Kafka, Airflow, Hive, Presto, Athena, S3, Aurora, EMR, Spark
Ability to drive, contribute to, and communicate solutions to technical product challenges
Ability to roll-up your sleeves and work in process or resource gaps and fill it in yourself
Excellent written and oral communication skills in English
Education
Bachelor’s degree in computer science, Data Analytics, Information Systems, or a related degree or equivalent experience. Master’s degree is preferred.
Job Type: Full-time
Pay: From $96,336.72 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Experience level:
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Washington, DC: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person","$96,337 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Hiscox
4.1",4.1,"Atlanta, GA",Senior Data Engineer,"job description:
You will be a senior member of our US IT Team data engineering practice. You will be an early member of this team and will work closely with the Director of Data Engineering to help establish it. We will have our work cut out for us delivering planned work, building the practice and evolving our technical landscape. This is an exciting opportunity for the right individual to join at the very beginning and grow with the new team.
As a senior you will be expected to lead, mentor others and deputise for the director of data engineering when required. Yoi will also continue to contribute as a hands-on engineer. You will have the knowledge, best earned through 5+ years of experience, to truly inhabit this senior role in an ambitious team.
What you’ll be doing:
Working with business stakeholders and our delivery team to understand high value business problems that can be solved through the application of data processing and analytical systems.
Developing, expanding and evolving our existing databases and ETL pipelines.
Working with architects to design, build and support a transformational cloud-based data platform for the business.
Being a core and professional member of the new data engineering practice
Understand business requirements and help refine into development tasks and estimate their complexity.
Research, evaluate and adopt new technologies with a right tool for the job mentality.
Focus on both speed of delivery and quality, with suitable pragmatism – ensuring your solutions are always appropriate and not too complex or over-engineered.
Quick progression of projects from proof-of-concept to post-production stage.
Communication and presentation of ideas to colleagues in all parts of the wider tech team.
Participating in code reviews for the data engineering practice.
Act as a servant leader in our practice – ensuring team members have
person specification:
Knowledge of, or ability to rapidly adopt our core languages for data engineering – SQL and/or Python.
Experience working using a modern DevOps approach. You should not just have written data code but also the CI/CD pipelines necessary to test and deploy that code in a professional environment.
A robust understanding of core data engineering topics – ETL vs ELT, structured and unstructured data, data quality and data governance.
Ability to contribute to all aspects of a solution – design, infrastructure, development, testing and maintenance.
The ability to design and advocate for technical solutions to business problems.
Effective collaboration with technical and non-technical team members through agile ceremonies – roadmap planning, feature workshops, backlog elaboration, code review.
Understanding of cloud technology and significant hands-on experience working with it. You must understand how cloud-native solutions must be built differently from traditional ones.
Track record of taking initiative and delivering projects end-to-end; clear evidence of being self-driven and motivated
Immense curiosity, high energy and desire to go the extra mile to make a difference.

Desirable Skills
Beyond the required skills we are open to individuals of diverse talents. Experience with additional technologies, data science knowledge or a business background are all valued. We want to know how your unique abilities can contribute to our team.
Our Technology
We operate in a diverse technical landscape and are looking for flexible engineers who can adapt to and use many different tools. We would not expect any engineer to be familiar with the entire tech stack – no engineer can. Instead, we seek people with a good understanding of data structures and algorithms and the ability to apply this knowledge in learning new tools.
We have an existing data landscape involving both MS SQL Server and Oracle databases. We use SSIS for ETL. We will also need to source data from our integration layer – webMethods.
If some of that sounds like it needs bringing up to date, we know! We hope to start building a modern cloud data platform to realise the full value of our data. The technologies we use for that platform are still to be decided. Likely it will be based on Snowflake with Azure Data Factory for ELT. Other parts of our business have had success using Prefect for orchestration and dbt for transformation and we will reuse where it is sensible.
We prepare data for use by analysts working with a variety of tools – Tableau, PowerBI, and even Excel.
We take a DevOps approach and strive for continuous integration / continuous deployment. We use Azure Pipelines to deploy our code. We deploy infrastructure the same way using Terraform and Docker.
Our technology landscape is not fixed. Our engineering and architecture teams drive the technology we adopt.


Salary range $140,000 - $170,000","$155,000 /yr (est.)",1001 to 5000 Employees,Company - Public,Insurance,Insurance Carriers,1901,$1 to $5 billion (USD)
"Petrolink International
2.9",2.9,"Houston, TX",Data Analysis Engineer,"Location:
Houston, Texas, U.S.A.
Job Type:
Full-time
Job Description:
Petrolink is a global company that specializes in user-driven technology for the Energy Industry.
We are looking for a Data Analysis Engineer to assist with creating and delivering algorithms primarily used by our engineering solutions.
This position is based in our U.S. office in Houston, Texas.
Duties & Responsibilities:
Research and provide logic for engineering solutions algorithms.
Collaborate with Subject Matter Experts (Drilling Engineers, Mud loggers, Pore Pressure Analysts, and other specialists from the Drilling Industry) to understand different challenges in the Drilling Industry and the practices in place to address/solve these challenges.
Code the algorithms using C# to run on Petrolink’s proprietary analytics engine.
Develop, test, and maintain algorithms.
Support escalations related to issues in algorithms.
Learn existing algorithms to provide support when required.
Prepare training material and sessions for stakeholders as part of the production release.
Skills & Qualifications:
Proficient in C#.
Experience with Python is an advantage.
Knowledge in drilling practices, and ability to speak to other drilling experts and capture requirements and logic.
Knowledge in drilling performance KPIs is an advantage.
Prior experience with WITSML is an advantage.
Function and communicate well in a team having diverse expertise.
Bi-lingual (English and Spanish) preferred.
At least 2 years’ relevant experience.
Benefits:
Health insurance
Dental insurance
401(k) with matching
Health savings account
Life insurance
Long-term disability
This job description is intended to explain the general nature and level of work being performed. Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.
Applicants must be legally authorized to work in the United States for any employer.
To apply for this position, applicants must meet the above criteria.
Interested applicants fulfilling the above criteria, please send your resume to careers.usa@petrolink.com.","$81,133 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"CDNetworks Inc.
2.9",2.9,"Monrovia, CA",Jr. Data Center Engineer,"CDNetworks is one of the top leading CDN & Edge Service Providers with global offices in Korea, Japan, Singapore, Malaysia, China, Russia, London, and Canada. We focus on delivering integrated cloud and edge computing solutions with unparalleled speed, ultra-low latency, rigorous security, and reliability so that our clients can focus on what’s most important – growing their business.
The Jr. Data Center Engineer is a Full-Time position based in Monrovia, CA. Job is performed indoors in a Data Center or Warehouse environment. 75% Travel required for this position.
Job Responsibilities
Ensure all incidents are logged and resolved, gather all relevant data, and ensure all incidents and tasks follow the appropriate procedures.
Support data center activities and work closely with our system and network team to complete tasks/projects.
First responder to all alerts and problem reports while managing communications between departments and handling crisis documentation and dissemination after the fact.
Utilize internal systems such as JIRA/Wiki to manage project plans and progress.
Performing general system administration duties including OS patching and upgrades, batch job monitoring, system and hardware diagnostics, and other activities to ensure optimal health and performance of all systems as required.
Resolve complex problems related to Server and H/W areas.
Assisting/working closely with Network, System Engineers to configure customer requirements.
Physical deployment of network devices, servers, cables, etc.
Assembling/dissembling server hardware for deployment and OS installation and network equipment testing.
Maintain existing department and system documentation (update workflow, process, training documentation).
Other duties as assigned.
Abilities Required
Good verbal and written communication skills, and ability to work independently with minimum instruction.
Basic degree of mentorship, training, and direction team members skills.
Knowledge of IDC industry.
Intermediate degree of analytical and project management skills.
Other Features of Job
Job is performed indoors in a Data Center or Warehouse environment.
Language Skills
Excellent communication skills (English) – written and verbal. Bilingual Chinese or Korean is a plus!!
Job Type: Full-time
Salary: $20.00 - $25.00 per hour
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Health insurance
Life insurance
Paid time off
Professional development assistance
Referral program
Vision insurance
Schedule:
10 hour shift
8 hour shift
Evening shift
Monday to Friday
Night shift
On call
Overtime
Supplemental pay types:
Bonus pay
Ability to commute/relocate:
Monrovia, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Computer networking: 1 year (Preferred)
Shift availability:
Overnight Shift (Required)
Night Shift (Required)
Day Shift (Required)
Willingness to travel:
75% (Preferred)
Work Location: In person",$22.50 /hr (est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2000,$10+ billion (USD)
Siri info solutions inc,#N/A,"Beaverton, OR",Sr. Data Engineer,"Must Have Skills
PySpark
Python on AWS
Responsibilities:
Understanding of advanced data warehousing concepts, data modeling and extract, transform and load development
Advanced SQL skills with experience querying large datasets from multiple sources and developing automated reporting
Python skills for scripting, data manipulation, custom extract, transform and loads and statistical/regression analysis particularly, as they apply to sales and marketing operations and performance
Job Type: Full-time
Salary: $47.00 - $50.00 per hour
Experience level:
7 years
Work Location: On the road",$48.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"PSEG
3.9",3.9,"Edison, NJ",AWS Data Engineer,"Requisition: 73755
PSEG Company: PSEG Services Corp.
Salary Range: $ 78,600 - $ 149,400
Incentive: PIP 10%
Work Location Category: Remote Local
PSEG operates under a Flexible Work Model where flexible work is offered when job requirements allow. In support of this model, roles have been categorized into one of four work location categories: onsite roles, hybrid roles that are a blend of onsite and remote work, remote local roles that are primarily home-based but require some level of purpose-driven in-person interaction and living within a commutable distance, and remote non-local roles that can be effectively performed remotely with the ability to work in approved states.
PSEG offers a unique experience to our more than 12,000 employees – we provide the resources and opportunities for career development that come with being a Fortune 500 company, as well as the attention, camaraderie and care for one another you might typically associate with a small business. Our focus on combatting climate change through clean energy technology, our new net zero climate vision for 2030 and enhanced commitment to diversity, equity and inclusion; and supporting the communities we serve make this a particularly exciting time to join PSEG.
Job Summary
The Technology Engineer is a direct report to the Senior Technology Engineer with understanding of business goals, business processes, technology expertise in one or more domains, technology solution design and implementation.
The Technology Engineer is involved in the full technology life cycle and responsible for designing, coding, configuring, testing, implementing and supporting application software. Technology Engineers work closely with Analysts and Product Managers to understand the business requirements that drive the analysis and physical design of technical solutions. Technology Engineers may be assigned to either implementation or support functions. Implementation involves creating new technology based solutions whereas support involves upgrades, maintenance or issue resolution to existing technology solutions.

Job Responsibilities
Primary Responsibilities:
Design, configuration, development, documentation and testing of technology solutions to meet business or technology requirements. Evaluation of existing technology solutions to determine fit for purpose for the new business or technology requirements. Recommendation of technology alternatives. Collaboration with other individuals to ensure proper integration of the new technology solution with the existing technology solutions.
Analysis of end user’s needs, business and technology requirements. Translation of these requirements into technology solution capabilities and design. Alternatively, using the user needs, business and technology knowledge to peer review designs, implementation plans, software code, configuration settings or other artifacts to ensure technology solution being created by others meets the user needs, business and technology requirements.
Provides support toward resolution of escalated support tickets. May perform the needed analysis to identify root cause of reported incidents, identify the short and long term remediation for the identified incidents
Specific responsibilities include:
Analyzes end-user needs and designs, configures, develops and tests technology solutions to satisfy demand. Performs build versus buy analysis for business demand.
Partners with business analysts to translate business needs to technology solution requirements.
Evaluates existing technology solutions and platforms and provides recommendations for improving technology performance by conducting gap analysis, identifying feasible alternative solutions, and assisting in the scope of needed modifications.
Collaborates with all stakeholders such as enterprise architects, software development, operations, cybersecurity and infrastructure to integrate applications and hardware.
Ensures that the design and technology solution implementation meets security and QA standards. Suggests fixes to issues by doing a thorough analysis of root cause and impact of any defect(s).
Provides support toward resolution of escalated support tickets. Applies operation break fixes and performs other proactive maintenance activities until permanent solutions can be implemented. Supports and participates in the solution deployment process for new functionality/ subsystems/modules, upgrades, updates and fixes to the production environment.
Makes solutions production-ready by following the standard change management processes, completing required forms, following procedures, completing version control documents, etc.

Job Specific Qualifications
Bachelor’s degree in Computer Science or a related field 4 years of professional technology solution engineering
Demonstrated technology solution ownership and adoption, projects or other work experiences.
Demonstrated experience in analysis of end-user needs to configure, develop and test low to medium complexity technology solutions to satisfy business demand.
Demonstrated track record of implementing technology solutions using structured methodologies such as agile (SCRUM, Kanban etc.) and waterfall.

Required Competencies:
AWS Data Cloud development experience
Experience developing and sustaining data load jobs using Extract, Transform, Load (ETL/ELT) methodologies and tools such as, Python, SQL, Shell Scripts and AWS Services.
Developing and sustaining a data ingestion pipeline to AWS leveraging services such as Athena, Glue, Lambda, S3, Relational Database Service (RDS) and Redshift.
Integration of AWS Data Lake with reporting tools such as PowerBI.
Experience using databases such as Oracle, MS SQL Server, and developing software code in one or more programming languages (Java, Python, etc.)
Experience with production support of mission critical technology solutions.
Experience documenting technical solutions and system process flow techniques.
Ability to work independently, multi-task effectively, and be flexible to accommodate change in priorities as necessary.
Strong analytical ability, communication skills, excellent problem solving skills, and ability to learn new technical concepts.
Ability to foster working relationships with Client departments, IT Management and Software Service Providers.
Desired Qualifications:
Graduate degree or MBA
AWS Developer/Architect certification
Minimum Years of Experience
4 years of experience
Education
Bachelors in Engineering or Computer Science
Bachelor in Information Technology
Certifications
None Noted
Disclaimer
Certain positions at the Company may require you to have access to Part 810-Controlled Information. Under the law, the Company is limited in who it can share this information with and in certain circumstances it is necessary to obtain specific authorization before the Company can share this information. Accordingly, if the position does require access to this information, you must complete a 10 CFR Part 810 Export Control Compliance Nationality Request Form, a copy of which will be provided to you by Talent Acquisition if an offer is made. If there is a need for specific authorization, due to the time it takes to obtain authorization from the government, we will likely not be able to further proceed with an offer
Candidates must foster an inclusive work environment and respect all aspects of diversity. Successful candidates must demonstrate and value differences in others' strengths, perspectives, approaches, and personal choices.
As an employee of PSE&G or PSEG LI, you should be aware that during storm restoration efforts, you may be required to perform functions outside of your routine duties and on a schedule that may be different from normal operations.
Certain positions at the Company may require you to have access to 10 CFR Part 810 controlled information. If the position does require access to this information, the Talent Acquisition representative will provide further details upon making an offer.
PSEG is an equal opportunity employer, dedicated to a policy of non-discrimination in employment, including the hiring process, based on any legally protected characteristic. Legally protected characteristics include race, color, religion, national origin, sex, age, marital status, sexual orientation, disability or veteran status or any other characteristic protected by federal, state, or local law in locations where PSEG employs individuals.
Business needs may cause PSEG to cancel or delay filling position at any time during the selection process.
This site (http://www.pseg.com) is strictly for candidates who are not currently PSEG employees. PSEG employees must apply for jobs internally through emPower which can be accessed through sharepoint.pseg.com by clicking on the emPower icon, then selecting careers.

PEOPLE WITH DISABILITIES:
PSEG is committed to providing reasonable accommodations to individuals with disabilities. If you have a disability and need assistance applying for a position, please call 973-430-3845 or email accommodations@pseg.com. If you need to request a reasonable accommodation to perform the essential functions of the job, email accommodations@pseg.com. Any information provided regarding a disability will be kept strictly confidential and will not be shared with anyone involved in making a hiring decision.

ADDITIONAL EEO/AA INFORMATION (Click link below)
Know your Rights: Workplace Discrimination is Illegal
Pay Transparency Nondiscrimination Provision","$114,000 /yr (est.)",10000+ Employees,Company - Public,"Energy, Mining & Utilities",Energy & Utilities,1903,$5 to $10 billion (USD)
"GOBankingRates
3.3",3.3,"North, SC",Staff Data Engineer,"GOBankingRates™ is unique in the digital marketing and media industry - we combine marketing, digital, content and fintech. Our performance based approach increases brand awareness and generates targeted audience engagement on our internal web properties and partner sites.
Learn More About What We Do

What's interesting about this role?
GOBankingRates has big growth plans ahead and is looking for a strong Staff Data Engineer experienced in Data warehousing and Python to join our Data Engineering team. The GOBankingRates Data Engineering team is responsible for designing and developing the Data lake, enterprise database, data warehouse, reporting solutions, and pipelines for data processing. If you are a critical thinker with a solid track record of developing data solutions and solving complex problems with SQL and Python, we want you to join our team! You will play a vital role in designing and developing our next generation data pipelines and data platform. Join our team and prototype new data product ideas and concepts!
How will you make an impact?
Build and maintain multiple data pipelines to ingest new data sources (API and file-based) and support products used by external users and internal teams.
Optimize by building tools to evaluate and automatically monitor data quality and develop automated scheduling, testing, and distribution of feeds.
Work with data engineers, data scientists, and product managers to design, rapid prototype, and productize new data product ideas and capabilities.
Design and build cloud-based data lakes and data warehouses.
Conquer complex problems by finding new ways to solve them with simple, efficient approaches focusing on our platforms' reliability, scalability, quality, and cost.
Collaborate with the team to perform root cause analysis and audit internal and external data and processes to help answer specific business questions.
What will you bring to us?
Master's Degree (or a B.S. degree with relevant industry experience) in math, statistics, computer science, or equivalent technical field
Experience with dimensional data modeling and schema design in a database or data warehouse
Expertise with scripting languages such as Python and writing efficient and optimized SQL.
Working experience in building data warehouses and data lakes.
Experience working directly with data analytics to bridge business requirements with data engineering.
Experience with AWS infrastructure
Ability to operate in an agile, entrepreneurial start-up environment and prioritize
Excellent communication and teamwork, and a passion for learning
Curiosity and passion for data, visualization, and solving problems
Willingness to question the validity, accuracy of data, and assumptions
Preferred Qualifications:
Experience building data warehouse, data lake, and data pipeline using Snowflake/Redshift and other AWS Technologies.
Experience with large-scale distributed systems with large datasets.
Experience with event streams and stream processing (e.g., Kafka, Spark, Kinesis)
Hands-on experience with event streaming with modern event streaming tools like Pulsar, Kafka, and Kinesis. Understanding when streaming vs. batch processing is appropriate, and tradeoffs in a given context
Knowledge of advertising platforms.

Benefits
Competitive salary with excellent growth opportunity; we pride ourselves in having a team that exudes leadership, high initiative, creativity and passion.
Awesome medical, dental and vision plans with heavy employer contribution
Paid maternity leave and paternity leave programs
Paid vacation, sick days and holidays
Company funding for outside classes and conferences to help you improve your skills
Contribution to student loan debt payments after the first year of employment
401(k) - employees can start contributing immediately. After the first year, GOBankingRates matches your contribution up to 4% of your salary
A note about our response to COVID -19 and our new norm: The world has changed and we know it's important to adapt and to do our part to take care of our teams in this global pandemic. Our number one priority is to have our team feel safe, balanced and connected. We're committed to providing our teams with the best resources and tools to navigate this new virtual world that we're living in. We've also reinvented the ways in which we recognize, celebrate, and engage with each other to keep our culture strong!
Here's a peek into our world at GOBankingRates -
Our teams are working remotely 100% for the foreseeable future and have flex time. We're in the digital media space so we're mobile and flexible!
Option to work from an office (if you need to get away!)
Tools & resources are available to keep our team connected across North America. (JIRA, Trello, Airtable, Slack, Zoom and so much more!)
To keep our community engaged and connected, virtual team building events are held weekly and monthly.
For wellness and balance, weekly virtual fitness classes such as yoga are available.
To care for the local communities that we're a part of across the U.S our team members host socially distanced philanthropic events every quarter.
And most importantly, we've committed to consistent and transparent communication to help us all stay informed, engaged and to keep us on our path to success and #greatness.
We are an equal-opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.","$134,519 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Internet & Web Services,#N/A,$25 to $100 million (USD)
"Contact Government Services, LLC
4.7",4.7,"Washington, DC",Data Engineer,"CGS is seeking a passionate and driven Data Engineer to support a rapidly growing Data Analytics and Business Intelligence platform focused on providing solutions that empower our Federal customers with the tools and capabilities needed to turn data into actionable insights. The ideal candidate is a critical thinker and perpetual learner; excited to gain exposure and build skillsets across a range of technologies while solving some of our clients’ toughest challenges. As a Data Engineer, you will be integral to data operations for the development and integration of multiple data types across a range of data sets and sources.

You will be responsible for the day-to-day operations of systems that depend on data, ensuring data is properly processed and securely transferred to its appropriate location, in a timely manner. Processing data will include managing, manipulating, storing and parsing data in a data pipeline for variety of target sources. You will also support maintenance of applications and tools that reside on these systems such as upgrades, patches, configuration changes, etc. The work is performed in a multidisciplinary team environment using agile methodologies. The candidate we seek must be highly motivated and enthusiastic about implementing new technologies and learning about new data in a small team environment where deadlines are important.
Responsibilities:
Complete development efforts across data pipeline to store, manage, store, and provision to data consumers
Being an active and collaborating member of an Agile/Scrum team and following all Agile/Scrum best practices
Write code to ensure the performance and reliability of data extraction and processing
Support continuous process automation for data ingest
Achieve technical excellence by advocating for and adhering to lean-agile engineering principles and practices such as API-first design, simple design, continuous integration, version control, and automated testing
Work with program management and engineers to implement and document complex and evolving requirements
Help cultivate an environment that promotes customer service excellence, innovation, collaboration, and teamwork
Collaborate with others as part of a cross-functional team that includes user experience researchers and designers, product managers, engineers, and other functional specialists
Required Skills:
Must be a US Citizen
Must be able to obtain a Public Trust Clearance
7+ years of IT experience including experience in design, management, and solutioning of large, complex data sets and models.
Experience with developing data pipelines from many sources from structured and unstructured data sets in a variety of formats
Proficiency developing ETL processes, and performing test and validation steps
Proficiency to manipulate data (Python, R, SQL, SAS)
Strong knowledge of big data analysis and storage tools and technologies
Strong understanding of the agile principles and ability to apply them
Strong understanding of the CI/CD pipelines and ability to apply them
Experience with relational database, such as, PostgreSQL
Work comfortably in version control systems, such as, Git Repositories
Contact Government Services strives to simplify and enhance government bureaucracy through the optimization of human, technical, and financial resources. We combine cutting edge technology with world class personnel to deliver customized solutions that fit specific needs and solve the most challenging, dynamic problems. Our team emphasizes technical expertise and an empathy to public service to provide unique top tier support for our government engagements.
SOME BENEFITS OF THIS POSITION INCLUDE:
Health, Dental, & Vision insurance
Life Insurance
401k
Flexible Spending Account (Health, Dependent Care, and Commuter)
Paid Time Off & Observance of State/Federal Holidays
For more information about CGS please visit: https://www.cgsfederal.com
39U0b7kJu8","$103,213 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Kanini,#N/A,"North Druid Hills, GA",Senior Data Engineer,"About Kanini
Kanini provides Agile Software Development, Cloud Computing, Data Science, and Location Intelligence services to public and private organizations. We have successfully served our clients in government, finance, transportation, utility, and software industries since 2003.
Why you should join
Working at Kanini is flexible and personal. We are a highly motivated, collaborative team experimenting with the latest technologies. We are committed to everyone having a healthy work/life balance, and we provide extensive mentorship and training resources to help you succeed.
Kanini is looking for Data Engineer who has a deep experience in Data Engineering, Agile planning, Snowflake, SQL, MySQL, PostgreSQL, Git, AWS Data Tools, Python, Jira
Job Description
The Senior Data Engineer is responsible for processing structured and unstructured data, validating data quality, and developing and supporting data products. The Senior Data Engineer also plays a role in Agile planning, providing advice and guidance, and monitoring emerging technologies.
Responsibilities
Gathers and processes raw, structured, semi-structured, and unstructured data using batch and real-time data processing frameworks.
Understands and enforces appropriate data master management techniques.
Ensures data quality and implements tools and frameworks for automating the identification of data quality issues.
Work with internal and external data providers on data validation providing feedback and making customized changes to data feeds and data mappings.
Understands the challenges that the analytics organization faces in their day-to-day work and partner with them to design viable data solutions.
Recommends improvements to processes, technology, and interfaces that improve the effectiveness of the team and reduce technical debt.
Implements and optimizes data solutions in enterprise data warehouses and big data repositories.
Installs, maintains, monitors, and supports business intelligence, distributed computation, and big data analytics tools.
Provides ongoing support, monitoring, and maintenance of deployed products.
Actively works with less experienced data engineers providing technical guidance and oversight.
Actively participates in the engineering community, staying up to date on new data technologies and best practices and shares insights with others in the organization.
Qualifications:
Bachelor’s degree in computer science or related field and 4 years work experience
Working experience with batch and real-time data processing frameworks
Working experience with data modelling, data access, schemas, and data storage techniques within Snowflake
Working experience in design, development, and implementation of highly scalable, high-volume software systems and components, client-facing web applications, and major Internet-oriented applications and systems
Working experience working with relational databases such as SQL, MySQL, Postgres/PostgreSQL
Working experience with business intelligence tools and platforms
Working experience with data quality tools
Working experience with application lifecycle methodologies (e.g. waterfall, agile, iterative)
Experience with ETL processes and tools
Experience working with Git.
Desirable Qualifications
Scrum Developer Certification or equivalent
Working experience with Sisense platform
Working experience with SQL Server Integration Services
Working experience with AWS data tools (Database Migration Service)
Working experience with Python
Experience working with Jira, Rally or similar tools
Kanini Software Solutions, Inc. does not discriminate in employment matters based on race, gender, religion, age, national origin, citizenship, veteran status, family status, disability status, or any other protected class. We support workplace diversity. If you have a disability, please let us know if there is anything we can do to improve the interview process for you; we’re happy to accommodate.
Kanini Software Solutions, Inc., 25 Century Blvd., Ste. 602, Nashville, TN 37214
Job Types: Full-time, Contract
Pay: $60.00 - $65.00 per hour
Application Question(s):
Current Location?
Work Location: Remote",$62.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Atalan Tech,#N/A,Remote,Data Engineer,"Atalan Tech is a start-up with a mission to mitigate the impact of clinician burnout and turnover in our nation’s health system. We are expanding our team and looking to hire the critical role of Data Engineer. The Data Engineer will work closely with the Chief Technology Officer, Chief Scientific Officer (UCLA Ph.D.), Chief Product Officer (Oxford Ph.D.), and Chief Executive Officer to deliver actionable insights from data in a variety of situations.
We are a small, close-knit team. This is an excellent opportunity for someone to join a mission-oriented, early-stage company at the ground floor and grow with us. There will be plenty of room for growth and creativity, and opportunities to tackle challenges that present one of the toughest problems in our society.
Requirements
Strong hands-on experience of designing, developing and managing data pipelines
Strong coding skills in Python (required) and SQL (required) and R (strongly preferred)
Experience building and maintaining data pipelines on AWS using EC2, EBS, Redshift, Postgres, S3, Data Pipeline, Kinesis, Lambda, etc.
Thorough understanding and experience with task management tools (Airflow, NiFi, etc.)
Knowledge of data management best practices for ETL, data governance, data warehousing, data modeling and data science
Basic knowledge of Linux (Redhat preferred) administration
Minimum B.S in Computer Science, software engineering or related field (or equivalent in related professional experience)
Strong communication skills
Strong problem-solving/critical thinking skills
Ability to work in a fast-paced, dynamic start-up environment
Some knowledge of healthcare, health tech, or healthcare data analytics (preferred)
Typical Tasks and Responsibilities include:
Working closely with CTO and CSO to build data pipelines for machine learning
Assisting the CPO in managing and supporting analysis of enterprise data to drive better products and solutions
Managing company’s data infrastructure on AWS (Postgres/RDS, Redshift, S3, etc.)
Synthesizing multiple data sources, such as internal medical records and HR data
Integrating with client systems including EHR (Epic, Cerner), HRIS, flat files, etc.
Working closely with other key members on other projects
Developing and designing data security standards and policies in line with industry best practices
About Us:
Atalan Tech’s mission is to improve the wellness of clinicians and their patients. We blend labor economics, machine learning, and clinical psychology with a team from Harvard, Yale, and UCLA. Atalan Tech is looking to contribute innovative and sustainable solutions to one of the most pressing problems in the healthcare industry. Building upon our proof-of-concept success, we have recently secured multiple engagements with nationwide healthcare organizations and have successfully raised institutional funding to expand our reach.
Contact Us:
If you’re interested in this position, please send us your CV/resume and cover letter.
Job Type: Full-time
Pay: $108,448.20 - $130,604.27 per year
Experience level:
2 years
Schedule:
Monday to Friday
Application Question(s):
Do you know the role is full time?
Can you start within the next month?
What are your salary expectations?
Have you held the title of a data engineer? If not, what experience do you have as a data engineer? Please be very specific.
Have you released Python code to a production environment in the last year?
Experience:
Data analytics: 2 years (Required)
Work Location: Remote","$119,526 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"DiNi Communications, Inc.
3.0",3.0,"New York, NY",Data Center Engineer,"DiNi Communications, Inc. is seeking a Data Center Engineer. This person needs to be a team player with exceptional communication skills. Maintaining the confidentiality of the client's information is of utmost importance.
Required Education and Experience
· At least 8 years of experience as a large data center technician
· Knowledge of Networking,
· Ability to lift equipment at least (50lbs) Rack and Stack (Network Switches, Servers, Storage,
Appliances)
· Knowledge of structured Cabling in a Data Center.
· General knowledge of Operating Systems, (Windows, Unix).
· General knowledge of networking in N Tier Environment (App, Web, DMZ,).
· Level 1 and Level 2 troubleshooting in a N Tier Environment.
· Ability to follow and execute commands to start up and shutdown systems.
· Physically travel to and be available at site B to perform technical changes that require a
person at Site B.
· Performing backup and restore activities.
· Asset Management.
· Maintaining document.
· Work with vendor on circuit issues, (WAN and Internet).
· Cable Patch the equipment in the environment.
· Participate in Site B fall-over testing
Preferred Network Engineering Experience
· Knowledge of N Tier networking.
· Certification in Networking (CISCO).
· Certification in various Operating systems (Linux and Windows), Storage, Messaging
(Microsoft).
· Certification in IT Security.
· Knowledge of Databases.
· Experience working with CISCO, EMC, Symantec, HP, Oracle, Bluecoat, MacAfee, and other vendors' equipment and software in a networked environment.
Responsibilities include but are not limited to:
Adding equipment to support agency applications in site B Environment,
Coordinating Site B testing with Site A technical resources.
Monitoring the B environment.
Rack and Stack new and replacement equipment
Troubleshooting and resolving incidents and problems in the
Physically travel to and be available at Site B to perform technical changes that require a person at Site B.
Upgrading and expanding the environment
Performing backup and restore activities
Manage Site B assets
Maintaining document
Cable Patch the equipment in the environment
Participate in Site B fall-over testing.
Job Type: Full-time
Pay: $60.00 - $70.00 per hour
Schedule:
Monday to Friday
Experience:
large data center technician: 8 years (Required)
Networking in N Tier Environment (App, Web, DMZ,): 3 years (Required)
Ability to lift equipment at least (50lbs): 1 year (Required)
structured Cabling in a Data Center: 3 years (Required)
Level 1 and 2 troubleshooting in a N Tier Environment: 3 years (Required)
Performing backup and restore activities: 3 years (Required)
Cable Patch the equipment: 3 years (Required)
Certification in Networking (CISCO): 1 year (Preferred)
Certification in IT Security: 3 years (Preferred)
CISCO, EMC, Symantec, HP, Oracle, Bluecoat, MacAfee: 3 years (Preferred)
Work Location: In person",$65.00 /hr (est.),Unknown,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"Quadrant Resource
4.1",4.1,Remote,Data Engineer,"Job Description:
4-6 years of experience as a Data Engineer in the Snowflake-Fivetran-dbt stack
• Possess a solid understanding of end to end Data ingestion, ETL, data analysis, Reporting processes
• Expert in ETL development using Fivetran and experienced in implementing data pipelines for Snowflake Cloud DWH
•Experienced in using dbt for data transformation
•Expertise on Tableau/Metabase for building reports/dashboards
•Expert in troubleshooting and resolving ETL issues, data load failures/problems and transformation translation problems
•Excellent communicator with the ability to work effectively with distributed teams
Job Type: Full-time
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Data Engineer: 9 years (Required)
Fivetran: 8 years (Required)
DBT: 7 years (Required)
Snowflake: 6 years (Required)
Work Location: Remote",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,2004,Unknown / Non-Applicable
"ECI - Sacramento
4.4",4.4,Remote,Healthcare Data Quality/Engineer/SAS SME (FT),"Healthcare Data Quality / Data Engineer / SAS SME

Senior Healthcare SAS Data Quality Management SME
Location: remote
24-months.
Must have 10-years working in Information Technology or related field, and those years must include the following experiences:
Minimum of three (3) years of experience interpreting, analyzing and applying HIPAA health care transaction requirements.
Minimum of three (3) years of functional experience within an organization with a large, complex data set.
Minimum of three (3) years of experience gathering, documenting, and designing and applying data quality requirements and data standards.
Minimum of three (3) years of experience with implementing electronic tools/software to support data quality requirements.
Minimum of three (3) years of experience establishing a framework for enterprise data quality.

Others:
Experience working on Medi-Cal, or another Medicaid, as a Data Quality Analyst, Business Analyst, or Business Intelligence Analyst.
Experience with tools focused on business intelligence dashboards and reports, such as Statistical Analysis Software (SAS), Microsoft PowerBI or Tableau.
Experience applying data quality management concepts and the Data Management Body of Knowledge (DMBOK) or similar methodology.
Experience in the creation and implementation of data quality service level agreements.
A bachelor's degree from an accredited college or university.



About ECI - Sacramento:

Estrada Consulting, Inc. (ECI) delivers technology-enabled services and solutions to clients all over the USA and British Columbia. We provide system integration, custom application development, data warehouse and business intelligence, project management, custom reporting solutions and consulting services to mid-size and large enterprises in all major industries. The Company headquarters is in Sacramento, California, and was established in year 2000. Visit http://www.estradaci.com/ to learn about our projects, managed services, awards and certifications delivering value for a range of businesses and government agencies.",$70.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2000,Unknown / Non-Applicable
"Quirch Foods
3.0",3.0,"Coral Gables, FL",Data Engineer,"About Quirch Foods
In business for over 50 years, Quirch Foods is a global distributor and exporter of fresh and frozen food products with a unique focus on ethnic cultures. We service our customers from 5 distribution centers strategically located in the United States. We have consistently been ranked among the top 50 exporters in the US and are one of the largest importers of seafood. Our customers include independent grocers, chain supermarkets, foodservice distributors, cruise lines, restaurants and food processors/manufacturers. We have an extensive product list with over 8,000 SKU's and carry niche brands such as Panamei, Mambo and Chiquita along with national brands like Tyson, IBP, Excel, JBS, Iowa Premium, Smithfield, Dietz & Watson, along with many others. We are headquartered in Coral Gables, FL.
Quirch is a portfolio company of Palladium Equity Partners, a leading mid-market private equity fund based in New York City, which partnered with the Executive Team to capitalize on Quirch Foods’ record of excellence and to grow it decisively beyond its current business.
Position Summary
At Quirch Foods we believe in leveraging technology as a source of competitive advantage. This includes customized ERP Solutions, Business Intelligence, EDI, Web and Mobile Solutions. We are currently looking for a Data Engineer to join our development efforts.
Essential Duties and Responsibilities:
Develop, test and maintain custom business solutions by leveraging state of the art software development practices using an agile methodology
Evaluate business requirement and help define problems and develop solutions
Participate in design and architecture discussions with business leaders, end users and IT team members
Document business requirements and solution, document code and provide support for creation of end user documentation
Update business knowledge, technical skills and soft skills, leverage educational opportunities, participate in professional organizations
Experience:
MS SQL Programming
Visual Studio\C#\WPS Experience
Microsoft SSAS, SSIS, SSRS desired but not required
Biztalk experience desired but not required
Skills and Requirements:
Bachelor’s degree in Computer Science, Engineering, Math or equivalent and\or related experience and training preferred
Strong analytical and problem-solving skills
Strong collaborator and team player
Great organizational skills, attention to detail and follow thru
Effective oral and written communication skills
Extensive familiarity with data management principles
Benefits:
Professional growth and developmental opportunities to grow your skills using state of the art technology
Comprehensive benefits package that includes: Medical, Dental, Prescription Drug Plan, Disability Plan, Life insurance Plan
401K savings Plan
Paid Holidays
Personal Time off
Employee Discounts
Quirch Foods is an Equal Opportunity Employer (EOE). Qualified applicants are considered for employment without regard to age, race, color, religion, sex, national origin, sexual orientation, disability, or veteran status. All applicants must be eligible to work in the United States.
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Vision insurance
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Coral Gables, FL 33134: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
What is your desired compensation?
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Data Engineering: 1 year (Preferred)
Work Location: In person","$89,340 /yr (est.)",201 to 500 Employees,Company - Private,Retail & Wholesale,Wholesale,1967,$500 million to $1 billion (USD)
"DataArt
4.6",4.6,Remote,Data Engineer,"CLIENT
Our client is a pioneer in US schools’ education since 2000, it is leading the way in next-generation curriculum and formative assessment. They develop a number of solutions and interactive web products for teachers, students and their parents.

The company is technology-driven with a huge number of software engineers involved in product development. They have a very solid approach to technology, employ best practices and processes, with a focus on cutting-edge frameworks, languages and tools.

We invite to the company, not a project
TEAM
There are multiple autonomous and empowered teams who work with their own part of the product and choose their own way of doing things and they expect people to be proactive and good team players.
Responsibilities
Building well-tested and optimized ETL data pipelines for both full and delta extraction
Collaborating with data analysts and learning scientists to store, aggregate, and calculate captured students’ work
Contributing to leading industry data standards, such as Caliper Analytics or xAPI
Improving our deployment and testing automation data pipelines
Requirements
3+ years of professional software development or data engineering experience
Strong CS and data engineering fundamentals
Proven fluency in SQL and a development language such as Python
Experience with the following storages: Snowflake, AWS Storage Services (S3, RDS, Glacier, DynamoDB)
Experience with ETL/BI: Airflow, DBT, Matillion, Looker
Experience with Cloud Infrastructure: AWS Kinesis, Lambda, API Gateway, Terraform
Understanding of ETL/ELT pipelines and Data Warehousing design, tooling, and support
Understanding of different data formatting (JSON, CSV, XML) and data storage techniques (3NF, EAV Model, Star Schema, Data Lake)
BS in Computer Science, Data Science, or equivalent
Experience in education or ed-tech
Proven passion and talent for teaching fellow engineers and non-engineers
Proven passion for building and learning: open source contributions, pet projects, self-education, Stack Overflow
Strong communication skills in writing, conversation",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,1997,$100 to $500 million (USD)
"Titan America
3.8",3.8,"Roanoke, VA",Data Engineer,"Data Engineer
We are seeking a motivated and detail-oriented Data Engineer to join our team based in Roanoke, Virginia. As a Data Engineer, you will play a crucial role in designing, developing and maintaining scalable data pipelines and transformations for business-critical use cases. In addition, you may take on various Software Development and Data Science responsibilities as we frequently share responsibilities and knowledge across our growing team.
Responsibilities:
Design, develop and maintain scalable data pipelines for extracting, transforming, and loading data from various sources
Collaborate with cross-functional teams to identify data needs and determine the best data solutions
Develop and implement data models to support business requirements and ensure data quality
Ensure the security and privacy of sensitive data by implementing appropriate access controls
Monitor and optimize data pipeline performance to ensure timely and accurate data delivery
Document data pipeline processes, data dictionaries, and data storage solutions
Requirements:
Bachelor's degree in Computer Science, Computer Engineering, or a related technical field
Minimum of five years of professional experience working as a Data Engineer or Software Developer
Strong hands-on experience with data warehouse and transformation solutions, i.e. Domo, Snowflake or similar
Proficient in at least one scripting language such as Python, JavaScript, or R
Understanding of data modeling, data integration and data quality processes
Familiarity with cloud platforms such as AWS, Azure, or Google Cloud Platform
Strong analytical and problem solving skills
Full Stack Software Development experience in a professional setting is highly desired, but not required
This is an excellent opportunity for a driven and collaborative individual to make a significant impact in a dynamic and growing team. If you have a passion for data and a desire to work in a fast-paced and dynamic environment, we want to hear from you!
Success begins with hiring the right people to partner with us as we grow and develop our businesses. People are central to everything we do. It is through their efforts and talents that Titan has been successful for over 100 years.
Titan America is an Equal Employment Opportunity (EEO) / Affirmative Action employer and welcomes all qualified applicants. Applicants will receive fair and impartial consideration without regard to race, sex, color, religion, national origin, age, disability, veteran status, genetic data, or other legally protected status.
Job Type: Full-time
Pay: $75,000.00 - $85,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Referral program
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Roanoke, VA 24019: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Do you currently or will you in the future need sponsorship to work in the United States?
This position requires the successful candidate to be in the office in Roanoke, VA at least 3 days a week. Can you meet this requirement?
This position requires the successful candidates to be a full time, W2 employee. Can you meet this requirement?
Education:
Bachelor's (Preferred)
Experience:
Data Engineering: 5 years (Preferred)
Work Location: Hybrid remote in Roanoke, VA 24019","$80,000 /yr (est.)",1001 to 5000 Employees,Company - Public,"Energy, Mining & Utilities",Mining & Metals,1902,Unknown / Non-Applicable
WellTrust Medical Group,#N/A,"Charlotte, NC",Data Engineer,"The Data Engineer will assist the senior analyst in complex modeling and projects to ensure their completion within the allotted timeline. Helps track measures of success and develop reports that measure operational and customer metrics. Demonstrates ability to troubleshoot data consumption and data quality problems.

Responsibilities:
Build and maintain reports using My SQL Server
Identify optimal data sources, structure data validation, and cache optimization.
Performs business problem modeling and quantitative analytics including regression analysis, time-series, hierarchical cluster analysis, and data mining.
Helps track measures of success and develop reports that measure operational and customer metrics.
Demonstrates ability to troubleshoot data consumption and data quality problems.
Evaluates and recommend improvements to current models, metrics, and analytic processes to improve operational efficiency and insight.
Design, document, and communicate effective technical solutions meeting end-user requirements
Perform analyses with large amounts of complex data, and uses data visualization tools to communicate results.
Requirements:

Bachelor's degree or equivalent experience in a quantitative field (Mathematics, Statistics, Computer Science, Economics, Finance, Physics, Engineering)
3+ years of professional working experience as a Data Engineer or Analyst
1-3 years of data warehouse experience
Build and maintain reports using My SQL Server
Strong analytical and creative problem-solving skills
Design, document, and communicate effective technical solutions meeting end-user
SQL and Excel Macro development
Strong understanding of various data environments such as SQL, Oracle, Big Query, etc.
Experience with data visualization tools such as Looker Studio, Tableau, D3.js, etc.
Proficient in R, Python, MATLAB, Java, or another programming language.",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"shaped.ai Inc.
4.0",4.0,"New York, NY",Data Engineer,"Hello! We’re building the future of content discovery infrastructure with AI — come join us!

Shaped (YC W22) is an API for developers to seamlessly add personalized ranking and recommendation to their products. These frictionless discovery experiences help end-users find what they want faster, and, consequently, grow conversion and engagement business metrics. Behind the scenes, we’re building state-of-the-art ML infrastructure for training and deploying large AI models to power our personalization engine.
We’re a seed stage start-up backed by top-tier investors (e.g. Y-Combinator, Susa Ventures, Tribe Capital etc…) and executives (e.g. Google, Amazon, Uber, Dropbox etc…). Our team comes from Meta, Google, Apple and Uber. We’re a remote team but have a small office in Brooklyn, New York.
We’ve been lucky to have a huge amount of success since our initial launch earlier this year and now looking to expand our real-time data engineering capabilities. We are looking for a data engineer to design, build and optimize Shaped's real-time streaming data ingestion endpoints. You will be a founding engineer working on state-of-the-art infrastructure. As one of Shaped’s early employees you will help shape our product, culture and vision.
This role is for a full-time data engineer at $120k - $200k a year and 0.1 - 0.5% equity.

Responsibilities:
Manage the unification of our batch and real-time data processing pipelines.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Requirements:
Bachelor's in computer science, data science or mathematics related field. Master's degree or PhD will be advantageous.
5+ years of data engineering experience, ideally applied to machine learning use-cases.
Experience with AWS or common cloud technologies
Proficient with Python, SQL.
Extensive knowledge of both offline and online large scale data processing frameworks: Spark, Beam, Flink.
Extensive knowledge with stream technologies like Kakfa or Kinesis.
Excellent written and verbal communication skills.
We’re excited to work with you. Come build the future of AI and discovery with us!","$160,000 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Atika Technologies,#N/A,"Pittsburgh, PA",Data Engineer,"Role: Data Engineer/PySpark Developer, Pittsburgh, PA - Day One Onsite
Fulltime and Contract(W2)
LinkedIn Must
USC/GC/EADs
Three round of interview: First two on Zoom Video Call, End Client Round will be Onsite on the Client's Location (Pittsburgh, PA)
Job Description:
Python(PySpark), Jupiter hub, airflow, Java
Database knowledge - Oracle / SQL / Data Analytics
Regulatory reporting knowledge preferred
Good communication skills
Open / Ability to learn new technologies
Very good Analytical Skills
Experience in ETL is preferable
Job Types: Full-time, Contract
Salary: $120,000.00 - $130,000.00 per year
Benefits:
Dental insurance
Health insurance
Life insurance
Compensation package:
Bonus pay
Performance bonus
Yearly pay
Experience level:
10 years
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Pittsburgh, PA 15259: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Required)
Spark: 4 years (Required)
Regulatory reporting: 3 years (Required)
SQL: 4 years (Required)
IT: 10 years (Required)
Airflow: 4 years (Required)
Jupiter: 4 years (Required)
Work Location: One location","$125,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Take Command Health
4.9",4.9,"Dallas, TX",Data Engineer,"Company: Take Command
Position: Data Engineer
Salary: $120,000-$130,000 plus bonus
Location: Dallas or Austin, TX
About Take Command
Take Command is a start-up on a mission to improve the healthcare system, starting with health insurance. Pragmatically speaking, we help employers reimburse employees for individual insurance instead of offering a traditional one-size-fits-all group plan. We believe this model can empower employees (when they have the right support) to be savvy healthcare consumers and have a transformative impact on the entire healthcare system.
Let’s be honest—health insurance is usually a confusing, frustrating, and even emotional experience for people. We want to fix that with a new model, great technology, and a superior user experience. We have made a great start, but we need your help to fully realize our long-term vision.
As a part of the dev team, you'll build the framework for both our clients and our team to succeed. You will be a pillar in our company as we grow and make our product the best product on the market!
In this role, you will:
Take ownership for delivering multiple features, their quality and assist team members in delivering their features
Recommend improvements to data quality, processes and code to make it easier to use, manage and maintain
Experience designing and developing data models and data migrations
Experience managing and maintaining ETL or ELT processes (Hevo, BigQuery, Looker, etc.)
Hands-on development experience with SQL, relational database design (ERDs)
Must be able to write SQL & contribute code
Be product-minded; focus not only on what needs to be built but why something needs to be built
You may be a great fit for this role if you have:
3 or more years of professional experience as a data analyst or data engineer
Cloud-Based Data Tools e.g. Hevo, BigQuery, Looker, etc.
Database technologies: SQL, RDBMS / Relational databases (Postgres, Amazon RDS / Aurora, etc.)
Nice to have experience: Python, Java, Typescript or another modern OOP language
Cloud infrastructure: AWS
Source control: Git
Working at Take Command
We’re excited to build a team and culture that reflects our values! We offer competitive pay, health benefits, and equity options to share with this position. We want you to be an owner along with us in revolutionizing healthcare.
A generously funded ICHRA for medical, dental, and vision premiums and medical expenses. You get to use our own product and we think that’s so exciting and rare!
Unlimited personal vacation in addition to regular company holidays.
401(k): 90-day eligibility for 2% match that vests immediately!
Access to LinkedIn Learning.
We have two beautiful offices conveniently located near The Hill (Dallas) and Holly Commons (Austin). The kitchen is well-stocked and we've designed the space to have lots of different areas to work!
Paid parental leave for new parents.
Flexible on where you work – we hope to see you around the office on average 3 days a week especially when white-boarding or building relationships with your colleagues, but you also have the ability to work from home or wherever you'd like when you need to get focus work done.
More About Us
We secured our Series A funding in 2021 and are thrilled to be able to expand our team. Despite being a small startup in a land of health insurance giants, we’re the recognized industry leader for what we do (health insurance reimbursements) and passionate about bringing it to market because we know we can help fix a broken system and improve our clients’ wellbeing and health outcomes.
We’ve been featured in The New York Times, The Wall Street Journal, The Dallas Morning News, and other national healthcare publications and are excited about our growth opportunities.
Take Command knows diversity and inclusion among our teammates is integral to our company’s success and growth. Our vision is to recruit, develop, and retain the best team from a diverse candidate pool.
This has mostly been about us, but we’d love to hear from you-we can’t wait to hear your story!
Take Command is an equal opportunity employer! We celebrate diversity and are committed to creating an inclusive environment for all employees.
Xrygn4NXMr","$125,000 /yr (est.)",1 to 50 Employees,Company - Private,Insurance,Insurance Agencies & Brokerages,2014,Unknown / Non-Applicable
"Tipico - North America
4.0",4.0,"Denver, CO",Data Engineer,"Company Description

Tipico is an energetic, passionate, bold tech company, and we love sports! Tipico has recently expanded to the US market, establishing offices in Hoboken, New Jersey and Denver, Colorado. We are driven by our core values: passion, progress and trust. Our goal is to build the best possible mobile sports betting product in the industry, in order to amplify the emotions of millions of sports fans every day. Let’s Make This Interesting ™
Please note: this role is located in our Denver, CO office; we work off a hybrid model and come into the office 2 days per week.

Job Description

The Data Engineer will report directly to the Data Engineering Manager and work together with rest of the data engineers, BI analysts, and product teams. The position holder ensures this is done by doing the following:
Create and maintain both batch (ETL/ELT) and real-time data pipelines and architecture
Ensure all data provided by Data Platform is of the highest quality and is delivered in a timely manner and in line with agreed SLAs
Assemble large and complex data sets that meet functional and non-functional business requirements
Identify, design, and implement internal process improvements, including automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies
Collaborate with stakeholders including data architects, Executive, Product, Data and IT team members from the beginning to the delivery of a project
Build a scalable, modern, and trusted data ecosystem, making sure it is responsive, available and in line with business requirements
Take part in sprints, workloads, and deliverables through agile methodologies
Manage quality assurance, verification of accuracy and consistency of data, specifically KPI measures sent to the business
Thrive working in a fast-paced and challenging environment
Open to continuous learning individually and with the team
Comfortable with being challenged / seeking challenge
Willing to work flexible hours if needed

Qualifications
3+ years in Data Platforms / Datawarehouse / BI experience
Must have experience in successfully implementing data-centric applications, such as data warehouses, operational data stores, and data integration projects
Strong analytical and problem-solving skills that balance with creative approaches
Ability to learn new and complex concepts quickly
Strong experience with large-scale production databases and SQL
Solid understanding of cloud data services: AWS services such as S3, Athena, EC2, RedShift, EMR, EKS, RDS and Lambda
Proficient with one or more big data & new data technologies such as DBT, NIFI, Airflow, Kafka
Experience with data modelling
Experience with ETL development (extractions, data load, aggregation, transformation)
Knowledge in working with time-series/analytics databases such as Elasticsearch
Understanding of containerization and orchestration technologies like Docker/Kubernetes
Must be able to take ownership of projects with a high level of attention to detail and priority
Team player with a hands-on approach and attitude
Must be comfortable with on call rotation
Nice to have:
Familiarity with Marketing tooling and/or CRM systems is preferred
Familiarity with using business intelligence tools such as Domo, Tableau or Looker
Knowledge about and / or experience with sports betting
Background in digital companies, online, gambling, gaming, e-commerce, marketing, or mobile apps
Familiarity with one scripting language, preferably Python

Additional Information

What's in it for you:
Work in an environment where you, your work and ideas matter and have an impact.
You will be a be part of the newest and up and coming USA online iGaming and sports betting companies in the market.
Work in a new and young business with high growth potential.
Build your own success story together with us.
Work with self-organized, self-responsible and entrepreneurial employees.
Start-up feeling, backed-up by a leading European sports betting house. We are a high-volume business and are taking off in the US!
Competitive salary, Medical, Vision, Dental Benefits, Unlimited PTO, and more
Salary for this position is $120k+ & annual performance bonus","$120,000 /yr (est.)",201 to 500 Employees,Company - Private,"Arts, Entertainment & Recreation",Gambling,2004,Unknown / Non-Applicable
"MyCare Medical Group
4.1",4.1,"Lutz, FL",Data Engineer,"Job Summary
The Data Engineer will design, develop, optimize, and maintain data architecture and pipelines that adhere to ELT principles and business goals.
Job Responsibilities:
Designs, develops, optimizes, and maintains data architecture and pipelines that adhere to ELT principles and business goals.
Solves complex data problems to deliver insights that helps business achieve its goals.
Creates data products for engineer, analyst, and data scientist team members to accelerate their productivity.
Engineers effective features for modelling in close collaboration with business analysts and stakeholders.
Leads the evaluation, implementation and deployment of emerging tools and process for analytics data engineering in order to improve productivity as a team and quality.
Partners with engineers, BI, and solutions architects to develop technical architectures for strategic enterprise projects and initiatives.
Fosters a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions.
Advises, consults, mentors, and coaches other data and analytic professionals on data standards and practices.
Develops and delivers communication and education plans on analytic data engineering capabilities, standards, and processes.
Job Qualifications:
Education and Work Experience: Bachelor’s degree in computer science, information systems, engineering, or a related field; Masters preferred.
3+ years of experience in data engineering role.
Expertise in ELT and data analysis, SQL and at least one programming language (e.g., SQL, Python).
Conceptual knowledge of data and analytics, such as dimensional modelling, reporting tools, data governance, and structured and unstructured data.
Skills: Strong understanding of agile methodologies and experience as a Data Engineer on a cross-functional agile team preferred.
Experience with designing and maintaining data warehouses and/or data lakes with technologies, such as Hadoop, Spark, or distributed databases, like Redshift and Snowflake, and experience with housing, accessing, and transforming data in a variety of relational databases.
Experience with cloud database technologies (e.g., Azure) and developing solutions on cloud computing services and infrastructure in the data and analytics space.
Experience in building data pipelines and deploying/maintaining them with best-in-class tools (e.g., Git and Jenkins).
Familiarity with the Linux operating system.
Basic familiarity with BI tools (e.g., Tableau, Power BI, Looker).
Experience working in data engineering or data architect role.
Candidate Qualities: Focuses on business agility and agile delivery with a “fail-fast, succeed early” mindset and emphasis on measurable outcomes.
Brings a high-energy and passionate outlook to the job and can influence those around them.
Able to build a sense of trust and rapport that creates a comfortable & effective workplace.
Passion for innovation and “can do” attitude.
BENEFITS
Comprehensive benefits package, including Health, Vision, Dental, and Life insurancesFSA and Life Assistance Program (EAP)
401(k) Retirement Plan
Health Advocacy, Travel Assistance, and My Secure Advantage
PTO Accrual and Holidays
#ZIP
Job Type: Full-time
Pay: $125,000.00 - $150,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Employee discount
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Referral program
Relocation assistance
Retirement plan
Tuition reimbursement
Vision insurance
Compensation package:
Performance bonus
Yearly bonus
Experience level:
3 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Lutz, FL 33558: Reliably commute or planning to relocate before starting work (Preferred)
Experience:
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person","$137,500 /yr (est.)",501 to 1000 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2018,Unknown / Non-Applicable
"Merkle
3.7",3.7,,Data Engineer,"Company Description

We are Dentsu International. We innovate the way brands are built. That means we do things differently so they’re better than before. In this way, we make our clients’ most important marketing assets—their brands—win in a changing world.
Dentsu International is a modern marketing solutions company. Our mission is to help clients navigate, progress, and thrive in a world of change. Businesses rely on our integrated network of agencies and specialized practices to champion meaningful progress through creative, media, commerce, data, and technology. Part of Dentsu Group, our global network comprises 66,000 diverse people in 143 countries, who are dedicated to teaming for growth and good. Follow us on Twitter @DentsuUSA and visit dentsu.com/us.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.

Job Description
Develop onboarding process to receive and process information related to onboarding a new subscriber
Convert Snowflake Stored Procedures to Python Scripts directly in Airflow
Develop metadata driven solutions for managing data processing and cadence
End to end automation development and testing
Building and maintaining stored procedures and scripts in Snowflake Data Cloud
Building and maintaining complex SQL queries for ETL/ELT processing
Develop Dashboard for data processed for audit and governance needs
Follow Agile methodologies to deliver iterative feature sets rapidly
Maintain codebase using source control methodologies, Git
Collaborate with business analysts, data analysts, and other developers to deliver powerful new features
Coordinate with teams as part of a larger data-sharing system
Contribute to a collaborative, positive, and enjoyable environment for your operations team
Performing quality assurance and testing
Writing user and technical documentation such as runbooks

Qualifications
Strong communication skills both verbal and written
Strong understanding of conditional programming logic
Knowledge of software engineering and development methodologies, techniques, and tools; includes Issue Tracking (JIRA) Software Development Lifecycle SDLC), and Continuous Integration / Continuous Development (CI/CD)
Experience working with large databases and data warehouses utilizing both relational and dimensional design concepts
Eagerness to learn new technologies, programming languages and platforms to solve complex data challenges
Experience with Python required
Experience with SQL required
Experience with Snowflake Data Cloud is highly desirable
Experience with Amazon Web Services (AWS) a plus
Experience with Airflow (or other orchestration) a plus
Experience in database marketing is a plus

Additional Information

Perks:
Great compensation package
Comprehensive healthcare plans
401(k) with employer match
Flexible time-off
16 weeks paid parental leave
Additional Information
The anticipated salary range for this position is $94,000-$146,000. Salary is based on a range of factors that include relevant experience, knowledge, skills, other job-related qualifications, and geography. A range of medical, dental, vision, 401(k) matching, paid time off, and/or other benefits also are available. For more information regarding dentsu benefits, please visit dentsubenefitsplus.com
Employees from diverse or underrepresented backgrounds encouraged to apply.
Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact recruiting@dentsuaegis.com if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.
About dentsu
Dentsu is the network designed for what’s next, helping clients predict and plan for disruptive future opportunities in the sustainable economy. Taking a people-centered approach to business transformation, dentsu combines Japanese innovation with a diverse, global perspective to drive client growth and to shape society www.dentsu.com.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.
Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact your recruiter if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.","$120,000 /yr (est.)",5001 to 10000 Employees,Company - Private,Media & Communication,Advertising & Public Relations,1971,$500 million to $1 billion (USD)
"Intersec
4.5",4.5,Remote,Senior Data Engineer,"Architect and implement highly scalable data pipeline to streamline migration of data from source data repositories to the centralized data platform
Implement agreed upon data models (e.g., data marts) to enable faster and efficient consumption of data by downstream consumers
Conduct performance tuning to optimize data migration techniques and ensure adherence to identified service-level agreements (SLAs)
Define and implement automated validation checks to ensure data integrity between source and target
Assist in onboarding consumers with existing technology to connect to the platform
Setup predictive alert mechanism for pipeline degradation for IMF pipelines.
Enhance data quality framework for data pipelines and derived datasets
Enhance Data Audit Balance and Control mechanism for data pipelines
Enhance pipelines to be certified copies of authoritative source
Populate metadata for the Enterprise with particular focus on the Data Sources being onboarded to EDP (Technical and Business Glossaries) related to IMF Modernization Programs
Validate correct lineage for datasets
Required:
*8+ years of Databricks, Data Modeling, and/or Metadata management experience
* Familiarity with architecting and developing real and near-real-time solutions
* Bachelor's Degree in Engineering, Mathematics, computer Science, Statistics or other quantitative discipline, or 4 years equivalent professional experience
* US Citizenship and ability to gain a public trust
Preferred:
* Familiarity with IRS systems and ETL pipelines
* Ability to work independently and manage multiple tasks
* IRS MBI Clearance with IRS Laptop and Informatica Installed
Job Type: Full-time
Salary: $70.00 - $75.00 per hour
Schedule:
Monday to Friday
Application Question(s):
Are you a US/GC card holder?
How many years of Databricks, Data Modeling, and/or Metadata management experience do you have?
Work Location: Remote",$72.50 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,Unknown / Non-Applicable
Fracsys Inc,#N/A,"Washington, DC",Data Engineer,"Fracsys Inc is hiring a Data Engineer position. The ideal candidate must have at least 12+ years of industry experience. He or she must be responsible for successful technical delivery of Data Warehousing and Business Intelligence projects using a formal project management methodology.
Responsibilities
Directs, motivates and develops key technical personnel
Provides technical direction across the full lifecycle system development process
Provides strategic planning and comprehensive senior level technical consulting to IT senior management and senior technical staff
Working independently within guidelines
Provides detailed inputs for plans and schedules, including goals, risks, and resource allocation
Conduct diagnostic, troubleshooting steps and present conclusions to a varied audience
comfortable slinging some code to solve a problem
Build Data Marts and Data warehouses to facilitate analytical capabilities across the varied sources of data
Design and implement data patterns on cloud using cloud Data warehouse and data virtualization tools
Evaluate new tools and technologies for target state on cloud
Learn and implement new ETL, BI tools and data warehousing technologies
Qualifications
Minimum twelve (12) years in a Technical Lead or Consulting role leading Information Management related initiatives (system integration, data warehouse build, data mart build, or similar)
MUST have proven leadership experience for team of at least 3 technical resources.
Experience creating Data Marts, Data warehouses on-premise and on cloud for real time and batch processing frameworks
Experience building and optimizing big-data data pipelines and data sets including Postgres, AWS Relational Data Service
extensive hands-on experience building and optimizing data structures for data analytics, data science and business intelligence
Experience building self-service data consumption patterns and knowledge of cloud-based data Lake platforms
Experience wrangling data (structured and unstructured), in-depth knowledge of database architecture
Experience utilizing or leading implementations leveraging ETL tools (Informatica / Talend), BI Reporting tools such as MicroStrategy, Microsoft Power BI, data modeling tools such as Erwin, Oracle, SQL server, NoSQL, JDBC, UNIX shell scripting, PERL and JAVA, XML/JSON files, SAS , Python, AWS cloud-native technologies, S3, Athena, Redshift
Familiarity with the following technologies: Hadoop, Kafka, Airflow, Hive, Presto, Athena, S3, Aurora, EMR, Spark
Ability to drive, contribute to, and communicate solutions to technical product challenges
Ability to roll-up your sleeves and work in process or resource gaps and fill it in yourself
Excellent written and oral communication skills in English
Education
Bachelor’s degree in computer science, Data Analytics, Information Systems, or a related degree or equivalent experience. Master’s degree is preferred.
Job Type: Full-time
Pay: From $96,336.72 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Experience level:
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Washington, DC: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person","$96,337 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Hiscox
4.1",4.1,"Atlanta, GA",Senior Data Engineer,"job description:
You will be a senior member of our US IT Team data engineering practice. You will be an early member of this team and will work closely with the Director of Data Engineering to help establish it. We will have our work cut out for us delivering planned work, building the practice and evolving our technical landscape. This is an exciting opportunity for the right individual to join at the very beginning and grow with the new team.
As a senior you will be expected to lead, mentor others and deputise for the director of data engineering when required. Yoi will also continue to contribute as a hands-on engineer. You will have the knowledge, best earned through 5+ years of experience, to truly inhabit this senior role in an ambitious team.
What you’ll be doing:
Working with business stakeholders and our delivery team to understand high value business problems that can be solved through the application of data processing and analytical systems.
Developing, expanding and evolving our existing databases and ETL pipelines.
Working with architects to design, build and support a transformational cloud-based data platform for the business.
Being a core and professional member of the new data engineering practice
Understand business requirements and help refine into development tasks and estimate their complexity.
Research, evaluate and adopt new technologies with a right tool for the job mentality.
Focus on both speed of delivery and quality, with suitable pragmatism – ensuring your solutions are always appropriate and not too complex or over-engineered.
Quick progression of projects from proof-of-concept to post-production stage.
Communication and presentation of ideas to colleagues in all parts of the wider tech team.
Participating in code reviews for the data engineering practice.
Act as a servant leader in our practice – ensuring team members have
person specification:
Knowledge of, or ability to rapidly adopt our core languages for data engineering – SQL and/or Python.
Experience working using a modern DevOps approach. You should not just have written data code but also the CI/CD pipelines necessary to test and deploy that code in a professional environment.
A robust understanding of core data engineering topics – ETL vs ELT, structured and unstructured data, data quality and data governance.
Ability to contribute to all aspects of a solution – design, infrastructure, development, testing and maintenance.
The ability to design and advocate for technical solutions to business problems.
Effective collaboration with technical and non-technical team members through agile ceremonies – roadmap planning, feature workshops, backlog elaboration, code review.
Understanding of cloud technology and significant hands-on experience working with it. You must understand how cloud-native solutions must be built differently from traditional ones.
Track record of taking initiative and delivering projects end-to-end; clear evidence of being self-driven and motivated
Immense curiosity, high energy and desire to go the extra mile to make a difference.

Desirable Skills
Beyond the required skills we are open to individuals of diverse talents. Experience with additional technologies, data science knowledge or a business background are all valued. We want to know how your unique abilities can contribute to our team.
Our Technology
We operate in a diverse technical landscape and are looking for flexible engineers who can adapt to and use many different tools. We would not expect any engineer to be familiar with the entire tech stack – no engineer can. Instead, we seek people with a good understanding of data structures and algorithms and the ability to apply this knowledge in learning new tools.
We have an existing data landscape involving both MS SQL Server and Oracle databases. We use SSIS for ETL. We will also need to source data from our integration layer – webMethods.
If some of that sounds like it needs bringing up to date, we know! We hope to start building a modern cloud data platform to realise the full value of our data. The technologies we use for that platform are still to be decided. Likely it will be based on Snowflake with Azure Data Factory for ELT. Other parts of our business have had success using Prefect for orchestration and dbt for transformation and we will reuse where it is sensible.
We prepare data for use by analysts working with a variety of tools – Tableau, PowerBI, and even Excel.
We take a DevOps approach and strive for continuous integration / continuous deployment. We use Azure Pipelines to deploy our code. We deploy infrastructure the same way using Terraform and Docker.
Our technology landscape is not fixed. Our engineering and architecture teams drive the technology we adopt.


Salary range $140,000 - $170,000","$155,000 /yr (est.)",1001 to 5000 Employees,Company - Public,Insurance,Insurance Carriers,1901,$1 to $5 billion (USD)
Redzara.us,#N/A,"Atlanta, GA",Azure Data Engineer,"Responsible for developing new and modernizing existing applications to Azure Cloud Platform.
The ideal candidate must demonstrate in-depth knowledge and understanding of Database concepts.
Requirements:
7+ years of experience in Data Warehousing and Business Intelligence
5+ Years in BI development leveraging the Microsoft SQL Server. Needs to be an expert in T-SQL
5+ Years in SSIS and use of Stored Procedures.
1-2 Years in Azure Data Factory, SQL VMs, build and maintain ETL
Ingest and aggregate data from both internal and external data sources to build our data lake
Strong understanding data modeling (i.e., conceptual, logical, and physical model design, experience with Operation Data Stores, Enterprise Data warehouses and Data Marts).
Job Type: Contract
Pay: $43.49 - $57.00 per hour
Compensation package:
1099 contract
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Atlanta, GA 30334: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure Data Factory: 3 years (Required)
Azure: 3 years (Required)
Data warehouse: 1 year (Preferred)
Work Location: Hybrid remote in Atlanta, GA 30334",$50.25 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Apolis
3.9",3.9,"Saint Louis, MO",Data Engineer (Python Focused),"Role: Data Engineer (Python Focused) (W2 ONLY)
Location : St. Louis, MO 63102
Duration: 18 Months
The primary responsibilities of this role will be to work as a part of the data and analytics team of the client. The ideal candidate would have experience using ETL and other data engineering tools in a cloud environment. Additionally, this candidate would have coding and scripting experience creating code to move and manipulate data using tools such in Python such as Spark or Pandas. Microsoft Azure experience is a plus but relevant experience with other cloud-based environments will be considered.
This team uses Microsoft Azure to build and productionize data sets and processes to the product owners and product users and this role will be expected to contribute to those objectives. As a member of this team, you will work in conjunction with other data engineers, platform engineers, solution developers and others to execute and deliver working solutions to the end users.
Technical Skills
High focus on Python
More Programmer/Coder experience, less Cloud experience needed.
Must have experience with data analytics/engineering and cloud technologies and an in-depth working knowledge and understanding of data engineering, design, and integration.
Intermediate or higher level of proficiency with Python Must have the proven ability to learn quickly and apply cloud data engineering principles and skills to new tools and use cases.
Ability to define and take technical ownership of detailed requirements and collaborate effectively with various team members such as Business Analysts and Project Managers and end users.
Excellent communication (verbal and written) and problem-solving skills are essential for this position.
Initiative, Proactive Cooperation, and curiosity are mandatory behavioral skills required for this position.
Experience with several of the following technologies is preferred:
Experience with Microsoft Azure Platform. In particular, the data processing toolsets including Azure Data Factory Pipelines, Azure Databricks, Azure Data Lake Storage.
Experience using Notebooks (Such as Jupyter, DataBricks) also desired.
Experience using code languages such as Python and C#
Experience using query languages such as SQL, MDX and DAX
Experience using scripting languages such as PowerShell, M-Query (Power Query), and Windows batch commands.
Understanding of Agile and DevOps principles for software development and ownership.
Familiar with compliance and controls for software development and deployment.
Education Required
A bachelor's in computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field [or equivalent work experience] is not required but is preferred.
Job Type: Contract
Salary: $60.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Vision insurance
Compensation package:
Overtime pay
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
St. Louis, MO: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Preferred)
ETL: 5 years (Preferred)
Cloud infrastructure: 4 years (Preferred)
Work Location: One location",$65.00 /hr (est.),501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,1996,$25 to $100 million (USD)
Atalan Tech,#N/A,Remote,Data Engineer,"Atalan Tech is a start-up with a mission to mitigate the impact of clinician burnout and turnover in our nation’s health system. We are expanding our team and looking to hire the critical role of Data Engineer. The Data Engineer will work closely with the Chief Technology Officer, Chief Scientific Officer (UCLA Ph.D.), Chief Product Officer (Oxford Ph.D.), and Chief Executive Officer to deliver actionable insights from data in a variety of situations.
We are a small, close-knit team. This is an excellent opportunity for someone to join a mission-oriented, early-stage company at the ground floor and grow with us. There will be plenty of room for growth and creativity, and opportunities to tackle challenges that present one of the toughest problems in our society.
Requirements
Strong hands-on experience of designing, developing and managing data pipelines
Strong coding skills in Python (required) and SQL (required) and R (strongly preferred)
Experience building and maintaining data pipelines on AWS using EC2, EBS, Redshift, Postgres, S3, Data Pipeline, Kinesis, Lambda, etc.
Thorough understanding and experience with task management tools (Airflow, NiFi, etc.)
Knowledge of data management best practices for ETL, data governance, data warehousing, data modeling and data science
Basic knowledge of Linux (Redhat preferred) administration
Minimum B.S in Computer Science, software engineering or related field (or equivalent in related professional experience)
Strong communication skills
Strong problem-solving/critical thinking skills
Ability to work in a fast-paced, dynamic start-up environment
Some knowledge of healthcare, health tech, or healthcare data analytics (preferred)
Typical Tasks and Responsibilities include:
Working closely with CTO and CSO to build data pipelines for machine learning
Assisting the CPO in managing and supporting analysis of enterprise data to drive better products and solutions
Managing company’s data infrastructure on AWS (Postgres/RDS, Redshift, S3, etc.)
Synthesizing multiple data sources, such as internal medical records and HR data
Integrating with client systems including EHR (Epic, Cerner), HRIS, flat files, etc.
Working closely with other key members on other projects
Developing and designing data security standards and policies in line with industry best practices
About Us:
Atalan Tech’s mission is to improve the wellness of clinicians and their patients. We blend labor economics, machine learning, and clinical psychology with a team from Harvard, Yale, and UCLA. Atalan Tech is looking to contribute innovative and sustainable solutions to one of the most pressing problems in the healthcare industry. Building upon our proof-of-concept success, we have recently secured multiple engagements with nationwide healthcare organizations and have successfully raised institutional funding to expand our reach.
Contact Us:
If you’re interested in this position, please send us your CV/resume and cover letter.
Job Type: Full-time
Pay: $108,448.20 - $130,604.27 per year
Experience level:
2 years
Schedule:
Monday to Friday
Application Question(s):
Do you know the role is full time?
Can you start within the next month?
What are your salary expectations?
Have you held the title of a data engineer? If not, what experience do you have as a data engineer? Please be very specific.
Have you released Python code to a production environment in the last year?
Experience:
Data analytics: 2 years (Required)
Work Location: Remote","$119,526 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"DiNi Communications, Inc.
3.0",3.0,"New York, NY",Data Center Engineer,"DiNi Communications, Inc. is seeking a Data Center Engineer. This person needs to be a team player with exceptional communication skills. Maintaining the confidentiality of the client's information is of utmost importance.
Required Education and Experience
· At least 8 years of experience as a large data center technician
· Knowledge of Networking,
· Ability to lift equipment at least (50lbs) Rack and Stack (Network Switches, Servers, Storage,
Appliances)
· Knowledge of structured Cabling in a Data Center.
· General knowledge of Operating Systems, (Windows, Unix).
· General knowledge of networking in N Tier Environment (App, Web, DMZ,).
· Level 1 and Level 2 troubleshooting in a N Tier Environment.
· Ability to follow and execute commands to start up and shutdown systems.
· Physically travel to and be available at site B to perform technical changes that require a
person at Site B.
· Performing backup and restore activities.
· Asset Management.
· Maintaining document.
· Work with vendor on circuit issues, (WAN and Internet).
· Cable Patch the equipment in the environment.
· Participate in Site B fall-over testing
Preferred Network Engineering Experience
· Knowledge of N Tier networking.
· Certification in Networking (CISCO).
· Certification in various Operating systems (Linux and Windows), Storage, Messaging
(Microsoft).
· Certification in IT Security.
· Knowledge of Databases.
· Experience working with CISCO, EMC, Symantec, HP, Oracle, Bluecoat, MacAfee, and other vendors' equipment and software in a networked environment.
Responsibilities include but are not limited to:
Adding equipment to support agency applications in site B Environment,
Coordinating Site B testing with Site A technical resources.
Monitoring the B environment.
Rack and Stack new and replacement equipment
Troubleshooting and resolving incidents and problems in the
Physically travel to and be available at Site B to perform technical changes that require a person at Site B.
Upgrading and expanding the environment
Performing backup and restore activities
Manage Site B assets
Maintaining document
Cable Patch the equipment in the environment
Participate in Site B fall-over testing.
Job Type: Full-time
Pay: $60.00 - $70.00 per hour
Schedule:
Monday to Friday
Experience:
large data center technician: 8 years (Required)
Networking in N Tier Environment (App, Web, DMZ,): 3 years (Required)
Ability to lift equipment at least (50lbs): 1 year (Required)
structured Cabling in a Data Center: 3 years (Required)
Level 1 and 2 troubleshooting in a N Tier Environment: 3 years (Required)
Performing backup and restore activities: 3 years (Required)
Cable Patch the equipment: 3 years (Required)
Certification in Networking (CISCO): 1 year (Preferred)
Certification in IT Security: 3 years (Preferred)
CISCO, EMC, Symantec, HP, Oracle, Bluecoat, MacAfee: 3 years (Preferred)
Work Location: In person",$65.00 /hr (est.),Unknown,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"Contact Government Services, LLC
4.7",4.7,"Washington, DC",Data Engineer,"CGS is seeking a passionate and driven Data Engineer to support a rapidly growing Data Analytics and Business Intelligence platform focused on providing solutions that empower our Federal customers with the tools and capabilities needed to turn data into actionable insights. The ideal candidate is a critical thinker and perpetual learner; excited to gain exposure and build skillsets across a range of technologies while solving some of our clients’ toughest challenges. As a Data Engineer, you will be integral to data operations for the development and integration of multiple data types across a range of data sets and sources.

You will be responsible for the day-to-day operations of systems that depend on data, ensuring data is properly processed and securely transferred to its appropriate location, in a timely manner. Processing data will include managing, manipulating, storing and parsing data in a data pipeline for variety of target sources. You will also support maintenance of applications and tools that reside on these systems such as upgrades, patches, configuration changes, etc. The work is performed in a multidisciplinary team environment using agile methodologies. The candidate we seek must be highly motivated and enthusiastic about implementing new technologies and learning about new data in a small team environment where deadlines are important.
Responsibilities:
Complete development efforts across data pipeline to store, manage, store, and provision to data consumers
Being an active and collaborating member of an Agile/Scrum team and following all Agile/Scrum best practices
Write code to ensure the performance and reliability of data extraction and processing
Support continuous process automation for data ingest
Achieve technical excellence by advocating for and adhering to lean-agile engineering principles and practices such as API-first design, simple design, continuous integration, version control, and automated testing
Work with program management and engineers to implement and document complex and evolving requirements
Help cultivate an environment that promotes customer service excellence, innovation, collaboration, and teamwork
Collaborate with others as part of a cross-functional team that includes user experience researchers and designers, product managers, engineers, and other functional specialists
Required Skills:
Must be a US Citizen
Must be able to obtain a Public Trust Clearance
7+ years of IT experience including experience in design, management, and solutioning of large, complex data sets and models.
Experience with developing data pipelines from many sources from structured and unstructured data sets in a variety of formats
Proficiency developing ETL processes, and performing test and validation steps
Proficiency to manipulate data (Python, R, SQL, SAS)
Strong knowledge of big data analysis and storage tools and technologies
Strong understanding of the agile principles and ability to apply them
Strong understanding of the CI/CD pipelines and ability to apply them
Experience with relational database, such as, PostgreSQL
Work comfortably in version control systems, such as, Git Repositories
Contact Government Services strives to simplify and enhance government bureaucracy through the optimization of human, technical, and financial resources. We combine cutting edge technology with world class personnel to deliver customized solutions that fit specific needs and solve the most challenging, dynamic problems. Our team emphasizes technical expertise and an empathy to public service to provide unique top tier support for our government engagements.
SOME BENEFITS OF THIS POSITION INCLUDE:
Health, Dental, & Vision insurance
Life Insurance
401k
Flexible Spending Account (Health, Dependent Care, and Commuter)
Paid Time Off & Observance of State/Federal Holidays
For more information about CGS please visit: https://www.cgsfederal.com
39U0b7kJu8","$103,213 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Barracuda Networks Inc.
3.6",3.6,"Alpharetta, GA",Data Engineer,"Job ID: 23-579

Come Join Our Passionate Team! At Barracuda, we make the world a safer place. We believe every business deserves access to cloud-enabled, enterprise-grade security solutions that are easy to buy, deploy, and use. We protect email, networks, data and applications with innovative solutions that grow and adapt with our customers’ journey. More than 200,000 organizations worldwide trust Barracuda to protect them — in ways they may not even know they are at risk — so they can focus on taking their business to the next level.
We know a diverse workforce adds to our collective value and strength as an organization. Barracuda Networks is proud to be an Equal Opportunity Employer, committed to equal employment opportunity and equitable compensation regardless of race, gender, religion, sex, sexual orientation, national origin, or disability.
Envision yourself at Barracuda
We currently have an exciting new position open for a Data Engineer. This person will assist in driving technology innovation for engineering projects and work on architecture of AWS, Elastic and Kubernetes. They will also work closely with the Technology Architect to create and drive the Architecture Review Board.
What you’ll be working on
Determine internal / external customer data needs and identify system specifications
Analyze the needs of large systems and breaking them down into smaller manageable parts
Responsible for designing the journey of data from various sources to the database and everything in between
Plan and design the structure of data, brainstorm with team
Analyze best technology products and systems for internal teams
Help provide build vs buy decisions on data architecture
Communicate data requirements to engineers; explain data architecture to them and provide assistance throughout SDLC
Choose / maintain suitable software, hardware and integration methods
Help resolve technical problems as and when they arise
Review testing procedures to ensure data performant and error free
Ensure that database satisfy quality standards and procedures. Setup such standards.
Work with senior IT personnel to devise plans for future IT requirements of the organization
Produce progress reports on key infrastructures - Elastic and Data Pipelines
What you bring to the role
Bachelor Degree or Post Graduate in Computer Science, or relevant work experience
5+ years of experience in an Engineering / Operations heavy organization
1+ years of experience as a Data Analysis and Data Design responsible for data architecture of a SaaS application
Experience in understanding and designing data sharding in a distributed database
Some Hands-on experience in software development and system administration
Knowledge of strategic IT solutions
Familiarity with programming languages like JavaScript or Java or Python
Familiarity with Linux OS and Windows alike
Experience in cloud computing and cloud Technologies (AWS, GCP, Azure)
Excellent communication skills – you should be adept at listening to, understanding, and explaining key concepts to managerial and technical resources.
Good Anticipation and problem-solving skills
Ability to prioritize and manage time – responsible for multiple streams of work at the same time
Relationship building - Ability to form a good rapport with internal and external stakeholders
In-depth knowledge about quality standards and security best practices

Preferred Skills:
Expert level Understanding of Elastic eco system - Elasticsearch, Logstash, Kibana, Filebeats
Understanding of secure tunnel communication using VPN
Experience designing data at scale
AWS experience is preferred in Cloud Technologies
What you’ll get from us
A team where you can voice your opinion, make an impact, and where you and your experience are valued. Internal mobility – there are opportunities for cross training and the ability to attain your next career step within Barracuda.
Equity, in the form of non-qualified options
High-quality health benefits
Retirement Plan with employer match
Career-growth opportunities
Flexible Time Off and Paid Time Off benefits
Volunteer opportunities
#LI-Remote","$98,161 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,$100 to $500 million (USD)
Infosoft,#N/A,"Peachtree Corners, GA",Data Pipeline Engineer - 2173,"Here are the job position details for your review:
Job Title: Data Pipeline Engineer (GStreamer/ DeepStream)
Pay Rate: $90 to 120/Hr.
Duration: 3 Months
Location: Peachtree Corners, GA
This position is for the Building Automation division.
We are looking for Data Pipeline Engineer (GStreamer) to participate in the development of Streaming data pipelines using NVIDIA Deepstream, GStreamer, Python programming, messaging queues, and related tools.
Preference is for candidates located in Atlanta, GA metro area. However, ok with non-local candidates who are open to 75% travel to Atlanta, GA.
Understanding the basics of our architecture/setup to be able to do setup/support for demos, testing, etc.
Basic computer hardware assembly and set-up
Possible set-up/installation of the application(s)
Possible development tasks depending on what the rest of the development team has for them
Develops business software applications, system software, and/or IT solutions on the basis of functional specifications.
Scope of position embraces both the customization or reuse/modification of system software and/or solutions as well as their development from scratch.
Consults with customers in creating functional specifications and work estimates, and supports Make-or-Buy decisions.
Analyses performance gaps of existing software environments and proposes measures to address efficiency problems.
Is also responsible for the implementation of data-driven components supporting data modeling and engineering in the context of data analytics (Data Stewardship).
Skills:
Python
Linux
Docker and Docker Swarm
Zero MQ / a good understanding of messaging
Desired Skills:
NVIDIA Deepstream SDK
Web Sockets
Flask
Protobuf
Possible:
GStreamer (if different than RTSP / H264 / MJPEG)
Job Type: Contract
Salary: $90.00 - $120.00 per hour
Ability to commute/relocate:
Peachtree Corners, GA: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
""Please share your Email ID""
Work Location: One location",$105.00 /hr (est.),Unknown,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Petrolink International
2.9",2.9,"Houston, TX",Data Analysis Engineer,"Location:
Houston, Texas, U.S.A.
Job Type:
Full-time
Job Description:
Petrolink is a global company that specializes in user-driven technology for the Energy Industry.
We are looking for a Data Analysis Engineer to assist with creating and delivering algorithms primarily used by our engineering solutions.
This position is based in our U.S. office in Houston, Texas.
Duties & Responsibilities:
Research and provide logic for engineering solutions algorithms.
Collaborate with Subject Matter Experts (Drilling Engineers, Mud loggers, Pore Pressure Analysts, and other specialists from the Drilling Industry) to understand different challenges in the Drilling Industry and the practices in place to address/solve these challenges.
Code the algorithms using C# to run on Petrolink’s proprietary analytics engine.
Develop, test, and maintain algorithms.
Support escalations related to issues in algorithms.
Learn existing algorithms to provide support when required.
Prepare training material and sessions for stakeholders as part of the production release.
Skills & Qualifications:
Proficient in C#.
Experience with Python is an advantage.
Knowledge in drilling practices, and ability to speak to other drilling experts and capture requirements and logic.
Knowledge in drilling performance KPIs is an advantage.
Prior experience with WITSML is an advantage.
Function and communicate well in a team having diverse expertise.
Bi-lingual (English and Spanish) preferred.
At least 2 years’ relevant experience.
Benefits:
Health insurance
Dental insurance
401(k) with matching
Health savings account
Life insurance
Long-term disability
This job description is intended to explain the general nature and level of work being performed. Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.
Applicants must be legally authorized to work in the United States for any employer.
To apply for this position, applicants must meet the above criteria.
Interested applicants fulfilling the above criteria, please send your resume to careers.usa@petrolink.com.","$81,133 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"Trella Health
4.8",4.8,"Philadelphia, PA",Senior Data Engineer,"At Trella Health, we are passionate and committed to our mission – empowering meaningful change in healthcare. Since our founding in 2015, we continue to grow our team, enhance our solution and services offerings, accelerate into new markets, and expand our customer base. We are rapidly growing and are looking for new Trellavators to join our team!
""What is a Trellavator?"" you ask. Innovate and elevate is the name of our game! We go above and beyond to collaborate with and support each other – we believe that when a colleague or customers succeeds, we succeed. By learning from others, building on our successes, and taking risks, we constantly raise the bar – continuous improvement is in our DNA. Our word is paramount, we keep our commitments, and we always follow through. We have a strong, reliable support system that fuels growth, collaboration, and passion – and together, we create a positive environment where everyone at Trella Health, including the customers we support, can thrive. Are you ready to learn more about the opportunities with our team? Trell-yeah you are!
Position Overview:
The Senior Data Engineer will be responsible for designing, implementing, and maintaining an effective data pipeline for Trella Health's applications. This position is responsible for turning data concepts into effective data models that are efficient and scalable. The Senior Data Engineer will ensure a smooth data release and deployment process throughout the data development lifecycle.
Successful candidate will be a highly organized, self-starter that can roll up their sleeves and solve problems efficiently. Ability to document procedures, manage time, and have a knack for optimizing data flows.
Location: Atlanta HQ; Raleigh, NC; Nashville, TN; Philadelphia, PA; Remote considered
Reports to: Manager, Data Engineering
As a member of the Data team at Trella, you will:
Champion a comprehensive data vision and operating principles that improve data quality, consistency, accessibility, decision making, use, and stewardship.
Manage data pipeline by developing processes for data input, transformation, and storage
Act as an interface between the Data Science team and Application Developers
Architect and implement front-end (application layer) and back-end (ETL layer) database technologies in a highly customized data environment
Develop automated processes for moving data between systems
Implement ETL processes that transform source data into functional data stores consumed by front-end applications
Scale data pipeline by utilizing the most efficient tools and technologies
This job might be a fit for you if you have:
Bachelor's degree in Computer Science, Engineering, Mathematics, or other technical discipline (proven professional experience will be considered in lieu of a technical degree)
2+ full years of recent advanced Snowflake development experience
Experience manipulating healthcare data
In-depth knowledge of Amazon Web Services (AWS) and Platform as a Service (PAAS)
Demonstrable experience with Snowflake utilities including SnowSQL & Snowflake Javascript
Analyze all aspects of the Snowflake Query Engine to improve bottlenecks that may exist
Ability to automate data ingestion from Amazon S3, FTP, and other technologies
Experience with scripting languages, extensive Python experience, Node.js experience a plus
Working knowledge of Bitbucket and Git
Experience managing and loading data in AWS RDS (MySQL/MariaDb)
Ability to optimize MySQL query execution plans for robust performance
Technical Job Requirements (Nice to Have)
Experience manipulating healthcare data including claims, ADT data
Foundational knowledge of CI/CD platforms, preferably TeamCity
Development and consumption of API services
Experience with data science tools (Jupyter, SageMaker, R)
Understanding of Atlassian suite of tools, including Jira and Confluence
Soft Skill Job Requirements (Nice to Have)
Strong analytical skills and ability to rapidly switch contexts
Excellent communication and problem-solving skills
Sole contributor that is willing to independently uncover and improve processes that are inefficient or sub-optimal
Track record of implementing creative solutions to complex problems
Experience working in a nimble, fast-paced, start-up environment
Knowledge of HIPPA, PHI, PII policies
About Trella Health:
Trella Health provides unmatched, actionable market intelligence to post-acute care and value-based care providers of all sizes. Our industry leading analytics paired with CRM and EHR integration workflows positions us as the most advanced sales enablement platform for the post-acute care market. One of only a few companies to be deemed both a Qualified Entity by CMS and an Innovator under its Virtual Research Data Center Program, Trella has elite access to billions of performance metrics and referral data to enable competitive positioning across the market. Its standardized insights, representing 90% of all lives 65+ U.S. population, help customers identify, engage, and manage critical relationships and advance their organizations with certainty.
Since launching our first product in 2016, we have experienced tremendous growth and have highlighted a few key data points to get an at-a-glance view of Trella:
Over 700 customers including 10 of the top 10 post-acute providers and 4 of the top 10 health systems
Over 120 team members and growing
Recognized as one of Modern Healthcare's ""Best Places to Work"" in consecutive years and were ranked in the Top 50 startups in the US by LinkedIn
Headquartered in Atlanta, GA with remote hubs in Nashville, Philadelphia, Raleigh, and the Philippines
The Trellavator Experience:
At Trella Health, you can expect to join a welcoming team that truly embodies our core values. Our collaborative culture is anchored by trust, transparency, and inclusion — and we also have a lot of fun. No matter where you work, at Trella Health you can expect an awesome team, frequent virtual gatherings, engaging events, casual attire, and more.
We offer competitive salaries with a comprehensive benefits package to all employees and provide an environment that fosters work-life harmonization with unlimited PTO, company-observed holidays, and Summer Fridays. As we continue to see exponential growth, our goal is to continue to put team members first and strive to offer our team members the best culture and benefits possible. Some of the benefits we provide are:
Health, Dental, Vision & Voluntary Benefits
Competitive Salary & Bonus Plans
401k Retirement Savings
Unlimited PTO & 10 Paid Holidays
Flexible Work Hours
Equity Shares
Paid Leave Programs
Marketplace for discounted retail and entertainment
Equal Opportunity Employer
Trella Health is an equal opportunity employer. All persons will receive consideration for employment without regard to race, color, gender, sexual orientation, gender identity or expression, religion, national origin, marital status, age, disability, handicap, veteran status, genetic information, or any other protected status as recognized by federal, state, or local laws.","$113,846 /yr (est.)",51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2015,Unknown / Non-Applicable
Siri info solutions inc,#N/A,"Beaverton, OR",Sr. Data Engineer,"Must Have Skills
PySpark
Python on AWS
Responsibilities:
Understanding of advanced data warehousing concepts, data modeling and extract, transform and load development
Advanced SQL skills with experience querying large datasets from multiple sources and developing automated reporting
Python skills for scripting, data manipulation, custom extract, transform and loads and statistical/regression analysis particularly, as they apply to sales and marketing operations and performance
Job Type: Full-time
Salary: $47.00 - $50.00 per hour
Experience level:
7 years
Work Location: On the road",$48.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"GOBankingRates
3.3",3.3,"North, SC",Staff Data Engineer,"GOBankingRates™ is unique in the digital marketing and media industry - we combine marketing, digital, content and fintech. Our performance based approach increases brand awareness and generates targeted audience engagement on our internal web properties and partner sites.
Learn More About What We Do

What's interesting about this role?
GOBankingRates has big growth plans ahead and is looking for a strong Staff Data Engineer experienced in Data warehousing and Python to join our Data Engineering team. The GOBankingRates Data Engineering team is responsible for designing and developing the Data lake, enterprise database, data warehouse, reporting solutions, and pipelines for data processing. If you are a critical thinker with a solid track record of developing data solutions and solving complex problems with SQL and Python, we want you to join our team! You will play a vital role in designing and developing our next generation data pipelines and data platform. Join our team and prototype new data product ideas and concepts!
How will you make an impact?
Build and maintain multiple data pipelines to ingest new data sources (API and file-based) and support products used by external users and internal teams.
Optimize by building tools to evaluate and automatically monitor data quality and develop automated scheduling, testing, and distribution of feeds.
Work with data engineers, data scientists, and product managers to design, rapid prototype, and productize new data product ideas and capabilities.
Design and build cloud-based data lakes and data warehouses.
Conquer complex problems by finding new ways to solve them with simple, efficient approaches focusing on our platforms' reliability, scalability, quality, and cost.
Collaborate with the team to perform root cause analysis and audit internal and external data and processes to help answer specific business questions.
What will you bring to us?
Master's Degree (or a B.S. degree with relevant industry experience) in math, statistics, computer science, or equivalent technical field
Experience with dimensional data modeling and schema design in a database or data warehouse
Expertise with scripting languages such as Python and writing efficient and optimized SQL.
Working experience in building data warehouses and data lakes.
Experience working directly with data analytics to bridge business requirements with data engineering.
Experience with AWS infrastructure
Ability to operate in an agile, entrepreneurial start-up environment and prioritize
Excellent communication and teamwork, and a passion for learning
Curiosity and passion for data, visualization, and solving problems
Willingness to question the validity, accuracy of data, and assumptions
Preferred Qualifications:
Experience building data warehouse, data lake, and data pipeline using Snowflake/Redshift and other AWS Technologies.
Experience with large-scale distributed systems with large datasets.
Experience with event streams and stream processing (e.g., Kafka, Spark, Kinesis)
Hands-on experience with event streaming with modern event streaming tools like Pulsar, Kafka, and Kinesis. Understanding when streaming vs. batch processing is appropriate, and tradeoffs in a given context
Knowledge of advertising platforms.

Benefits
Competitive salary with excellent growth opportunity; we pride ourselves in having a team that exudes leadership, high initiative, creativity and passion.
Awesome medical, dental and vision plans with heavy employer contribution
Paid maternity leave and paternity leave programs
Paid vacation, sick days and holidays
Company funding for outside classes and conferences to help you improve your skills
Contribution to student loan debt payments after the first year of employment
401(k) - employees can start contributing immediately. After the first year, GOBankingRates matches your contribution up to 4% of your salary
A note about our response to COVID -19 and our new norm: The world has changed and we know it's important to adapt and to do our part to take care of our teams in this global pandemic. Our number one priority is to have our team feel safe, balanced and connected. We're committed to providing our teams with the best resources and tools to navigate this new virtual world that we're living in. We've also reinvented the ways in which we recognize, celebrate, and engage with each other to keep our culture strong!
Here's a peek into our world at GOBankingRates -
Our teams are working remotely 100% for the foreseeable future and have flex time. We're in the digital media space so we're mobile and flexible!
Option to work from an office (if you need to get away!)
Tools & resources are available to keep our team connected across North America. (JIRA, Trello, Airtable, Slack, Zoom and so much more!)
To keep our community engaged and connected, virtual team building events are held weekly and monthly.
For wellness and balance, weekly virtual fitness classes such as yoga are available.
To care for the local communities that we're a part of across the U.S our team members host socially distanced philanthropic events every quarter.
And most importantly, we've committed to consistent and transparent communication to help us all stay informed, engaged and to keep us on our path to success and #greatness.
We are an equal-opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.","$134,519 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Internet & Web Services,#N/A,$25 to $100 million (USD)
"Quirch Foods
3.0",3.0,"Coral Gables, FL",Data Engineer,"About Quirch Foods
In business for over 50 years, Quirch Foods is a global distributor and exporter of fresh and frozen food products with a unique focus on ethnic cultures. We service our customers from 5 distribution centers strategically located in the United States. We have consistently been ranked among the top 50 exporters in the US and are one of the largest importers of seafood. Our customers include independent grocers, chain supermarkets, foodservice distributors, cruise lines, restaurants and food processors/manufacturers. We have an extensive product list with over 8,000 SKU's and carry niche brands such as Panamei, Mambo and Chiquita along with national brands like Tyson, IBP, Excel, JBS, Iowa Premium, Smithfield, Dietz & Watson, along with many others. We are headquartered in Coral Gables, FL.
Quirch is a portfolio company of Palladium Equity Partners, a leading mid-market private equity fund based in New York City, which partnered with the Executive Team to capitalize on Quirch Foods’ record of excellence and to grow it decisively beyond its current business.
Position Summary
At Quirch Foods we believe in leveraging technology as a source of competitive advantage. This includes customized ERP Solutions, Business Intelligence, EDI, Web and Mobile Solutions. We are currently looking for a Data Engineer to join our development efforts.
Essential Duties and Responsibilities:
Develop, test and maintain custom business solutions by leveraging state of the art software development practices using an agile methodology
Evaluate business requirement and help define problems and develop solutions
Participate in design and architecture discussions with business leaders, end users and IT team members
Document business requirements and solution, document code and provide support for creation of end user documentation
Update business knowledge, technical skills and soft skills, leverage educational opportunities, participate in professional organizations
Experience:
MS SQL Programming
Visual Studio\C#\WPS Experience
Microsoft SSAS, SSIS, SSRS desired but not required
Biztalk experience desired but not required
Skills and Requirements:
Bachelor’s degree in Computer Science, Engineering, Math or equivalent and\or related experience and training preferred
Strong analytical and problem-solving skills
Strong collaborator and team player
Great organizational skills, attention to detail and follow thru
Effective oral and written communication skills
Extensive familiarity with data management principles
Benefits:
Professional growth and developmental opportunities to grow your skills using state of the art technology
Comprehensive benefits package that includes: Medical, Dental, Prescription Drug Plan, Disability Plan, Life insurance Plan
401K savings Plan
Paid Holidays
Personal Time off
Employee Discounts
Quirch Foods is an Equal Opportunity Employer (EOE). Qualified applicants are considered for employment without regard to age, race, color, religion, sex, national origin, sexual orientation, disability, or veteran status. All applicants must be eligible to work in the United States.
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Vision insurance
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Coral Gables, FL 33134: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
What is your desired compensation?
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Data Engineering: 1 year (Preferred)
Work Location: In person","$89,340 /yr (est.)",201 to 500 Employees,Company - Private,Retail & Wholesale,Wholesale,1967,$500 million to $1 billion (USD)
Proits Hub LLC,#N/A,Remote,Data Analytics Engineer,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote
Speak with the employer
+91 9417599933",$60.00 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Radiant System,#N/A,"Pleasanton, CA",Data Engineer,"First Priority: Pleasanton, CA –
Second Priority for a superstar: Plano, TX
Experience in developing and deploying data pipelines, preferably in the Cloud
Experience working with Spark, Databricks, and other relevant technologies in Microsoft Azure cloud
Experience in productionizing and deploying Big Data platforms and applications
Expert in writing SQL statements
Job Type: Contract
Application Question(s):
Are you willing to work on W2?
Work Location: In person",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"CDNetworks Inc.
2.9",2.9,"Monrovia, CA",Jr. Data Center Engineer,"CDNetworks is one of the top leading CDN & Edge Service Providers with global offices in Korea, Japan, Singapore, Malaysia, China, Russia, London, and Canada. We focus on delivering integrated cloud and edge computing solutions with unparalleled speed, ultra-low latency, rigorous security, and reliability so that our clients can focus on what’s most important – growing their business.
The Jr. Data Center Engineer is a Full-Time position based in Monrovia, CA. Job is performed indoors in a Data Center or Warehouse environment. 75% Travel required for this position.
Job Responsibilities
Ensure all incidents are logged and resolved, gather all relevant data, and ensure all incidents and tasks follow the appropriate procedures.
Support data center activities and work closely with our system and network team to complete tasks/projects.
First responder to all alerts and problem reports while managing communications between departments and handling crisis documentation and dissemination after the fact.
Utilize internal systems such as JIRA/Wiki to manage project plans and progress.
Performing general system administration duties including OS patching and upgrades, batch job monitoring, system and hardware diagnostics, and other activities to ensure optimal health and performance of all systems as required.
Resolve complex problems related to Server and H/W areas.
Assisting/working closely with Network, System Engineers to configure customer requirements.
Physical deployment of network devices, servers, cables, etc.
Assembling/dissembling server hardware for deployment and OS installation and network equipment testing.
Maintain existing department and system documentation (update workflow, process, training documentation).
Other duties as assigned.
Abilities Required
Good verbal and written communication skills, and ability to work independently with minimum instruction.
Basic degree of mentorship, training, and direction team members skills.
Knowledge of IDC industry.
Intermediate degree of analytical and project management skills.
Other Features of Job
Job is performed indoors in a Data Center or Warehouse environment.
Language Skills
Excellent communication skills (English) – written and verbal. Bilingual Chinese or Korean is a plus!!
Job Type: Full-time
Salary: $20.00 - $25.00 per hour
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Health insurance
Life insurance
Paid time off
Professional development assistance
Referral program
Vision insurance
Schedule:
10 hour shift
8 hour shift
Evening shift
Monday to Friday
Night shift
On call
Overtime
Supplemental pay types:
Bonus pay
Ability to commute/relocate:
Monrovia, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Computer networking: 1 year (Preferred)
Shift availability:
Overnight Shift (Required)
Night Shift (Required)
Day Shift (Required)
Willingness to travel:
75% (Preferred)
Work Location: In person",$22.50 /hr (est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2000,$10+ billion (USD)
"JTEC Consulting
5.0",5.0,"Crystal City, VA",Data Engineer,"Data Engineer
Crystal City, VA 22202

JTEC Consulting LLC focuses on successfully delivering solutions to meet our clients’ most critical needs. Our founding members have decades of experience delivering a wide range of solutions to Air Force and DOD clients. We are a Veteran-Owned Small Business.

Security Clearance Requirement: Current TS/SCI
Location Note: Must reside in Wash DC Metro area and provide on-site support, relocation will be considered



Position Description:
JTEC Consulting is hiring a Mid-Senior level Data Engineer to work in support of a large-scale data analytics platform. This position requires excellent analytical skills and the ability to work directly with the government customer in a dynamic mission driven environment. Qualified candidates will have experience developing solutions for high volume, low latency applications and abilities to operate in a fast-paced environment.

Duties and responsibilities may include (but are not limited to):
Work as part of an enterprise-wide team collaborating with other data engineers, data scientists and product leads to develop innovative solutions for data requirements.
Build pipelines to collect data from disparate external sources.
Implement rules and perform validation and analytics to ensure expected data is received, cleansed, transformed, and in an optimized output format.
Support automation and performance optimization.
Monitor pipeline status and performance.
Support troubleshooting and issue resolution.
Other duties as assigned.
Education and qualifications include (but are not limited to):
U.S. citizenship
Current TS/SCI security clearance
Understanding of distributed computer systems
Experience working in big data environments and understanding of data challenges (DoD command experience preferred)
Experience working in cloud environments
4+ years of experience in the following areas:
Experience with modern programming languages such as Java and Spark
Experience using SQL and ETL
Experience working in a distributed environment using Apache Kafka and Apache Airflow
Experience with stream processing using Apache Flink or StreamSets
Experience working in an Agile development environment is preferred
Proficiency working across MS Office Suite including advanced Excel skills
Experience working with data visualization tools and dashboards is a plus
Effective written and verbal communication skills to work in a collaborative environment with government and contractor teams
Availability to work on-site
JTEC Consulting LLC provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.

Salary:
JTEC offers competitive compensation and benefits to all employees. The estimated salary range displayed in this posting is dependent upon position requirements, individual qualifications, education, experience, location, and other job related factors. Our recruiting team will review candidates' related experience and salary targets during the evaluation process. All qualified candidates will be considered.","$145,000 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"DSMH LLC
5.0",5.0,Remote,"Data Engineer (AWS, Python)","Title: Data Engineer
Duration: 12+ Months
Technical Skills
(Required)
Four years or more of experience in data engineering and/or software
development.
Three years or more of experience with development, operations, or
architecture in AWS using Python.
Experience with development and delivery of microservices using serverless
AWS services (S3, RDS, Aurora, DynamoDB, Lambda, SNS, SQS, Kinesis,
IAM)
Strong competency in testing, including unit testing to coverage standards and
integration testing.
(Desired)
Software development experience with object-oriented development and design
patterns
Experience with serverless design in AWS
AWS technical certifications (Developer Associate, Solution Architect Associate)
Familiarity with machine learning and data design to support machine learning
Experience with productionizing machine learning workloads
Experience with ElasticSearch/ELK, Kibana, Grafana, and/or Prometheus
Experience with OpenTelemetry
Soft Skills
(Required)
Verbal and written communication skills, problem solving skills, customer
service and interpersonal skills.
Basic ability to work independently and manage one’s time.
(Desired)
Ability to work collaboratively in a complex, rapidly changing, and culturally
diverse environment.
Ability to clearly communicate complex technical ideas.
Comfortable working in a dynamic environment where digital is still evolving as
a core offering.
Job Type: Contract
Pay: $50.00 - $55.00 per hour
Experience level:
6 years
Schedule:
Monday to Friday
Experience:
AWS: 5 years (Required)
Python: 5 years (Required)
Work Location: Remote",$52.50 /hr (est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,$1 to $5 million (USD)
Vibrant Planet,#N/A,California,Data Engineer,"About Vibrant Planet
We are a team of leaders in science, forestry, policy, and tech, building a cloud-based, data-driven platform to increase the pace and scale of forest restoration and reduce catastrophic wildfire, tree mortality, forest degradation, and deforestation. Our current software modernizes land management planning and monitoring with AI-driven data development, user friendly scenario building and decision support, and forest resilience trends and treatment outcome detection. The system quantifies potential and actual treatment benefits, helping to grow markets for ecosystem services (carbon, water, biodiversity, sustainable forest-derived products).
Our initial platform (launched in September 2021) is focused on temperate, “fire adapted” landscapes in California and the Western US. Our mission is global; our roadmap expands into other geographies and forest types accordingly.
Driven by our sense of urgency to protect vital forests and the services they provide, we use our expertise to build sophisticated, AI/ML-driven, user friendly products to democratize the use of data and accelerate the process of stabilizing and restoring our forests.
Vibrant Planet is backed by climate and ecosystem resilience solutions leaders, including Grantham Foundation, Earthshot, Elemental Excelerator, Ecosystem Integrity Fund, Chris Cox (CPO at FB), Neil Hunt (ex CPO of Netflix), Cisco, Valia Ventures, and Halogen Ventures.
For further information please visit: VibrantPlanet.net
Equal Opportunity Employer: Vibrant Planet is committed to diversity. We encourage applicants from all cultures, races, colors, religions, sexes, national or regional origins, ages, disability status, sexual orientation, gender identity, military, or other status protected by law to apply.
We are most interested in finding the best candidate for the job, and that candidate may come from a less traditional background, but have capacity to grow into and thrive in the position after some mentoring. We do not require that you have experience with every job description task. We will consider any equivalent combination of knowledge, skills, education, and experience to meet minimum qualifications. We encourage each candidate to think broadly about their unique background and skill set and how it may relate to the role. This is important to us. We aren’t just saying this, we mean it.
For further information please visit: https://vibrantplanet.net
About the Role
We are looking for a versatile, hands-on data focused engineer to help us scale and build our geospatial pipelines and tooling. You will be part of a small team of engineers and scientists, tackling some of the most important climate change related issues facing the world today. This is a remote role (and company) so being comfortable and effective working in a distributed team is crucial.
Key Responsibilities:
Design, develop, scale, and maintain data pipelines that ingest, transform, and store large volumes of geospatial data from multiple sources.
Optimize data processing and storage performance and cost efficiency by leveraging cloud-based technologies and services.
Collaborate with engineers, scientists and other stakeholders to share knowledge and build expertise.
Lead and participate in development life cycle activities like design, coding, testing, and production release.
Contribute to our evolving engineering culture, standards, tooling, and processes.
Mentor and support other engineers and deeply review code.
Technical Qualifications
Strong software engineering skill set.
2+ years experience in data engineering or a related field.
Experience with distributed data processing frameworks and tools (e.g., Airflow, Hadoop, Spark).
Proficiency with Python or an equivalent language. Ability to write clean, high-quality code and tests to keep our system fast, reliable, and monitorable.
Bonus Qualifications - Absolutely not required, but nice to have
Airflow
Pandas / Geopandas
Jupyter notebooks
Geospatial processing
Lidar data
Satellite imagery
Experience with ArcGIS/QGIS
Other Expectations:
Strong communication skills; discussing complex technical concepts to engineers and non-engineers is no problem to you.
Collaborative and supportive team player, with a desire to enrich our engineering culture.
Eagerness to learn, think creatively, and share knowledge with others.
Ability to write understandable, testable code with an eye towards maintainability.
Proactive and empathetic mindset - you love to roll up your sleeves to fix problems
Location - We are a remote first company with the majority of the team in the US west coast time zone. We also have a small and growing presence in New Zealand. Location can be flexible, but we are primarily targeting those two time zones and anything in between.",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"LatentView Analytics
4.0",4.0,"Dallas, TX",Data Engineer,"Role : Data Engineer
Experience : 6 - 8+ Years
Location : Dallas,Tx (Onsite)
Position: FullTime Only
Skills :Python,SQL Server ,Scala, Hadoop, HPCC, Storm, Cloudera, Cassandra,Excel, R,Docker,Kubernetes,Snowflake,Azure,Kafka,Redshift,Hadoop,AWS.
Job Type: Full-time
Pay: $80,000.00 - $1,100,000.00 per year
Compensation package:
Yearly pay
Experience level:
6 years
Schedule:
8 hour shift
Experience:
data engineer: 6 years (Preferred)
Work Location: In person
Speak with the employer
+91 9876543210","$109,012 /yr (est.)",1001 to 5000 Employees,Company - Public,Management & Consulting,Business Consulting,2006,$25 to $100 million (USD)
"Titan America
3.8",3.8,"Roanoke, VA",Data Engineer,"Data Engineer
We are seeking a motivated and detail-oriented Data Engineer to join our team based in Roanoke, Virginia. As a Data Engineer, you will play a crucial role in designing, developing and maintaining scalable data pipelines and transformations for business-critical use cases. In addition, you may take on various Software Development and Data Science responsibilities as we frequently share responsibilities and knowledge across our growing team.
Responsibilities:
Design, develop and maintain scalable data pipelines for extracting, transforming, and loading data from various sources
Collaborate with cross-functional teams to identify data needs and determine the best data solutions
Develop and implement data models to support business requirements and ensure data quality
Ensure the security and privacy of sensitive data by implementing appropriate access controls
Monitor and optimize data pipeline performance to ensure timely and accurate data delivery
Document data pipeline processes, data dictionaries, and data storage solutions
Requirements:
Bachelor's degree in Computer Science, Computer Engineering, or a related technical field
Minimum of five years of professional experience working as a Data Engineer or Software Developer
Strong hands-on experience with data warehouse and transformation solutions, i.e. Domo, Snowflake or similar
Proficient in at least one scripting language such as Python, JavaScript, or R
Understanding of data modeling, data integration and data quality processes
Familiarity with cloud platforms such as AWS, Azure, or Google Cloud Platform
Strong analytical and problem solving skills
Full Stack Software Development experience in a professional setting is highly desired, but not required
This is an excellent opportunity for a driven and collaborative individual to make a significant impact in a dynamic and growing team. If you have a passion for data and a desire to work in a fast-paced and dynamic environment, we want to hear from you!
Success begins with hiring the right people to partner with us as we grow and develop our businesses. People are central to everything we do. It is through their efforts and talents that Titan has been successful for over 100 years.
Titan America is an Equal Employment Opportunity (EEO) / Affirmative Action employer and welcomes all qualified applicants. Applicants will receive fair and impartial consideration without regard to race, sex, color, religion, national origin, age, disability, veteran status, genetic data, or other legally protected status.
Job Type: Full-time
Pay: $75,000.00 - $85,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Referral program
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Roanoke, VA 24019: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Do you currently or will you in the future need sponsorship to work in the United States?
This position requires the successful candidate to be in the office in Roanoke, VA at least 3 days a week. Can you meet this requirement?
This position requires the successful candidates to be a full time, W2 employee. Can you meet this requirement?
Education:
Bachelor's (Preferred)
Experience:
Data Engineering: 5 years (Preferred)
Work Location: Hybrid remote in Roanoke, VA 24019","$80,000 /yr (est.)",1001 to 5000 Employees,Company - Public,"Energy, Mining & Utilities",Mining & Metals,1902,Unknown / Non-Applicable
"shaped.ai Inc.
4.0",4.0,"New York, NY",Data Engineer,"Hello! We’re building the future of content discovery infrastructure with AI — come join us!

Shaped (YC W22) is an API for developers to seamlessly add personalized ranking and recommendation to their products. These frictionless discovery experiences help end-users find what they want faster, and, consequently, grow conversion and engagement business metrics. Behind the scenes, we’re building state-of-the-art ML infrastructure for training and deploying large AI models to power our personalization engine.
We’re a seed stage start-up backed by top-tier investors (e.g. Y-Combinator, Susa Ventures, Tribe Capital etc…) and executives (e.g. Google, Amazon, Uber, Dropbox etc…). Our team comes from Meta, Google, Apple and Uber. We’re a remote team but have a small office in Brooklyn, New York.
We’ve been lucky to have a huge amount of success since our initial launch earlier this year and now looking to expand our real-time data engineering capabilities. We are looking for a data engineer to design, build and optimize Shaped's real-time streaming data ingestion endpoints. You will be a founding engineer working on state-of-the-art infrastructure. As one of Shaped’s early employees you will help shape our product, culture and vision.
This role is for a full-time data engineer at $120k - $200k a year and 0.1 - 0.5% equity.

Responsibilities:
Manage the unification of our batch and real-time data processing pipelines.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Requirements:
Bachelor's in computer science, data science or mathematics related field. Master's degree or PhD will be advantageous.
5+ years of data engineering experience, ideally applied to machine learning use-cases.
Experience with AWS or common cloud technologies
Proficient with Python, SQL.
Extensive knowledge of both offline and online large scale data processing frameworks: Spark, Beam, Flink.
Extensive knowledge with stream technologies like Kakfa or Kinesis.
Excellent written and verbal communication skills.
We’re excited to work with you. Come build the future of AI and discovery with us!","$160,000 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Atika Technologies,#N/A,"Pittsburgh, PA",Data Engineer,"Role: Data Engineer/PySpark Developer, Pittsburgh, PA - Day One Onsite
Fulltime and Contract(W2)
LinkedIn Must
USC/GC/EADs
Three round of interview: First two on Zoom Video Call, End Client Round will be Onsite on the Client's Location (Pittsburgh, PA)
Job Description:
Python(PySpark), Jupiter hub, airflow, Java
Database knowledge - Oracle / SQL / Data Analytics
Regulatory reporting knowledge preferred
Good communication skills
Open / Ability to learn new technologies
Very good Analytical Skills
Experience in ETL is preferable
Job Types: Full-time, Contract
Salary: $120,000.00 - $130,000.00 per year
Benefits:
Dental insurance
Health insurance
Life insurance
Compensation package:
Bonus pay
Performance bonus
Yearly pay
Experience level:
10 years
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Pittsburgh, PA 15259: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Required)
Spark: 4 years (Required)
Regulatory reporting: 3 years (Required)
SQL: 4 years (Required)
IT: 10 years (Required)
Airflow: 4 years (Required)
Jupiter: 4 years (Required)
Work Location: One location","$125,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Apolis
3.9",3.9,"Saint Louis, MO",Data Engineer (Python Focused),"Role: Data Engineer (Python Focused) (W2 ONLY)
Location : St. Louis, MO 63102
Duration: 18 Months
The primary responsibilities of this role will be to work as a part of the data and analytics team of the client. The ideal candidate would have experience using ETL and other data engineering tools in a cloud environment. Additionally, this candidate would have coding and scripting experience creating code to move and manipulate data using tools such in Python such as Spark or Pandas. Microsoft Azure experience is a plus but relevant experience with other cloud-based environments will be considered.
This team uses Microsoft Azure to build and productionize data sets and processes to the product owners and product users and this role will be expected to contribute to those objectives. As a member of this team, you will work in conjunction with other data engineers, platform engineers, solution developers and others to execute and deliver working solutions to the end users.
Technical Skills
High focus on Python
More Programmer/Coder experience, less Cloud experience needed.
Must have experience with data analytics/engineering and cloud technologies and an in-depth working knowledge and understanding of data engineering, design, and integration.
Intermediate or higher level of proficiency with Python Must have the proven ability to learn quickly and apply cloud data engineering principles and skills to new tools and use cases.
Ability to define and take technical ownership of detailed requirements and collaborate effectively with various team members such as Business Analysts and Project Managers and end users.
Excellent communication (verbal and written) and problem-solving skills are essential for this position.
Initiative, Proactive Cooperation, and curiosity are mandatory behavioral skills required for this position.
Experience with several of the following technologies is preferred:
Experience with Microsoft Azure Platform. In particular, the data processing toolsets including Azure Data Factory Pipelines, Azure Databricks, Azure Data Lake Storage.
Experience using Notebooks (Such as Jupyter, DataBricks) also desired.
Experience using code languages such as Python and C#
Experience using query languages such as SQL, MDX and DAX
Experience using scripting languages such as PowerShell, M-Query (Power Query), and Windows batch commands.
Understanding of Agile and DevOps principles for software development and ownership.
Familiar with compliance and controls for software development and deployment.
Education Required
A bachelor's in computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field [or equivalent work experience] is not required but is preferred.
Job Type: Contract
Salary: $60.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Vision insurance
Compensation package:
Overtime pay
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
St. Louis, MO: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Preferred)
ETL: 5 years (Preferred)
Cloud infrastructure: 4 years (Preferred)
Work Location: One location",$65.00 /hr (est.),501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,1996,$25 to $100 million (USD)
"Take Command Health
4.9",4.9,"Dallas, TX",Data Engineer,"Company: Take Command
Position: Data Engineer
Salary: $120,000-$130,000 plus bonus
Location: Dallas or Austin, TX
About Take Command
Take Command is a start-up on a mission to improve the healthcare system, starting with health insurance. Pragmatically speaking, we help employers reimburse employees for individual insurance instead of offering a traditional one-size-fits-all group plan. We believe this model can empower employees (when they have the right support) to be savvy healthcare consumers and have a transformative impact on the entire healthcare system.
Let’s be honest—health insurance is usually a confusing, frustrating, and even emotional experience for people. We want to fix that with a new model, great technology, and a superior user experience. We have made a great start, but we need your help to fully realize our long-term vision.
As a part of the dev team, you'll build the framework for both our clients and our team to succeed. You will be a pillar in our company as we grow and make our product the best product on the market!
In this role, you will:
Take ownership for delivering multiple features, their quality and assist team members in delivering their features
Recommend improvements to data quality, processes and code to make it easier to use, manage and maintain
Experience designing and developing data models and data migrations
Experience managing and maintaining ETL or ELT processes (Hevo, BigQuery, Looker, etc.)
Hands-on development experience with SQL, relational database design (ERDs)
Must be able to write SQL & contribute code
Be product-minded; focus not only on what needs to be built but why something needs to be built
You may be a great fit for this role if you have:
3 or more years of professional experience as a data analyst or data engineer
Cloud-Based Data Tools e.g. Hevo, BigQuery, Looker, etc.
Database technologies: SQL, RDBMS / Relational databases (Postgres, Amazon RDS / Aurora, etc.)
Nice to have experience: Python, Java, Typescript or another modern OOP language
Cloud infrastructure: AWS
Source control: Git
Working at Take Command
We’re excited to build a team and culture that reflects our values! We offer competitive pay, health benefits, and equity options to share with this position. We want you to be an owner along with us in revolutionizing healthcare.
A generously funded ICHRA for medical, dental, and vision premiums and medical expenses. You get to use our own product and we think that’s so exciting and rare!
Unlimited personal vacation in addition to regular company holidays.
401(k): 90-day eligibility for 2% match that vests immediately!
Access to LinkedIn Learning.
We have two beautiful offices conveniently located near The Hill (Dallas) and Holly Commons (Austin). The kitchen is well-stocked and we've designed the space to have lots of different areas to work!
Paid parental leave for new parents.
Flexible on where you work – we hope to see you around the office on average 3 days a week especially when white-boarding or building relationships with your colleagues, but you also have the ability to work from home or wherever you'd like when you need to get focus work done.
More About Us
We secured our Series A funding in 2021 and are thrilled to be able to expand our team. Despite being a small startup in a land of health insurance giants, we’re the recognized industry leader for what we do (health insurance reimbursements) and passionate about bringing it to market because we know we can help fix a broken system and improve our clients’ wellbeing and health outcomes.
We’ve been featured in The New York Times, The Wall Street Journal, The Dallas Morning News, and other national healthcare publications and are excited about our growth opportunities.
Take Command knows diversity and inclusion among our teammates is integral to our company’s success and growth. Our vision is to recruit, develop, and retain the best team from a diverse candidate pool.
This has mostly been about us, but we’d love to hear from you-we can’t wait to hear your story!
Take Command is an equal opportunity employer! We celebrate diversity and are committed to creating an inclusive environment for all employees.
Xrygn4NXMr","$125,000 /yr (est.)",1 to 50 Employees,Company - Private,Insurance,Insurance Agencies & Brokerages,2014,Unknown / Non-Applicable
"Tipico - North America
4.0",4.0,"Denver, CO",Data Engineer,"Company Description

Tipico is an energetic, passionate, bold tech company, and we love sports! Tipico has recently expanded to the US market, establishing offices in Hoboken, New Jersey and Denver, Colorado. We are driven by our core values: passion, progress and trust. Our goal is to build the best possible mobile sports betting product in the industry, in order to amplify the emotions of millions of sports fans every day. Let’s Make This Interesting ™
Please note: this role is located in our Denver, CO office; we work off a hybrid model and come into the office 2 days per week.

Job Description

The Data Engineer will report directly to the Data Engineering Manager and work together with rest of the data engineers, BI analysts, and product teams. The position holder ensures this is done by doing the following:
Create and maintain both batch (ETL/ELT) and real-time data pipelines and architecture
Ensure all data provided by Data Platform is of the highest quality and is delivered in a timely manner and in line with agreed SLAs
Assemble large and complex data sets that meet functional and non-functional business requirements
Identify, design, and implement internal process improvements, including automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies
Collaborate with stakeholders including data architects, Executive, Product, Data and IT team members from the beginning to the delivery of a project
Build a scalable, modern, and trusted data ecosystem, making sure it is responsive, available and in line with business requirements
Take part in sprints, workloads, and deliverables through agile methodologies
Manage quality assurance, verification of accuracy and consistency of data, specifically KPI measures sent to the business
Thrive working in a fast-paced and challenging environment
Open to continuous learning individually and with the team
Comfortable with being challenged / seeking challenge
Willing to work flexible hours if needed

Qualifications
3+ years in Data Platforms / Datawarehouse / BI experience
Must have experience in successfully implementing data-centric applications, such as data warehouses, operational data stores, and data integration projects
Strong analytical and problem-solving skills that balance with creative approaches
Ability to learn new and complex concepts quickly
Strong experience with large-scale production databases and SQL
Solid understanding of cloud data services: AWS services such as S3, Athena, EC2, RedShift, EMR, EKS, RDS and Lambda
Proficient with one or more big data & new data technologies such as DBT, NIFI, Airflow, Kafka
Experience with data modelling
Experience with ETL development (extractions, data load, aggregation, transformation)
Knowledge in working with time-series/analytics databases such as Elasticsearch
Understanding of containerization and orchestration technologies like Docker/Kubernetes
Must be able to take ownership of projects with a high level of attention to detail and priority
Team player with a hands-on approach and attitude
Must be comfortable with on call rotation
Nice to have:
Familiarity with Marketing tooling and/or CRM systems is preferred
Familiarity with using business intelligence tools such as Domo, Tableau or Looker
Knowledge about and / or experience with sports betting
Background in digital companies, online, gambling, gaming, e-commerce, marketing, or mobile apps
Familiarity with one scripting language, preferably Python

Additional Information

What's in it for you:
Work in an environment where you, your work and ideas matter and have an impact.
You will be a be part of the newest and up and coming USA online iGaming and sports betting companies in the market.
Work in a new and young business with high growth potential.
Build your own success story together with us.
Work with self-organized, self-responsible and entrepreneurial employees.
Start-up feeling, backed-up by a leading European sports betting house. We are a high-volume business and are taking off in the US!
Competitive salary, Medical, Vision, Dental Benefits, Unlimited PTO, and more
Salary for this position is $120k+ & annual performance bonus","$120,000 /yr (est.)",201 to 500 Employees,Company - Private,"Arts, Entertainment & Recreation",Gambling,2004,Unknown / Non-Applicable
"MyCare Medical Group
4.1",4.1,"Lutz, FL",Data Engineer,"Job Summary
The Data Engineer will design, develop, optimize, and maintain data architecture and pipelines that adhere to ELT principles and business goals.
Job Responsibilities:
Designs, develops, optimizes, and maintains data architecture and pipelines that adhere to ELT principles and business goals.
Solves complex data problems to deliver insights that helps business achieve its goals.
Creates data products for engineer, analyst, and data scientist team members to accelerate their productivity.
Engineers effective features for modelling in close collaboration with business analysts and stakeholders.
Leads the evaluation, implementation and deployment of emerging tools and process for analytics data engineering in order to improve productivity as a team and quality.
Partners with engineers, BI, and solutions architects to develop technical architectures for strategic enterprise projects and initiatives.
Fosters a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions.
Advises, consults, mentors, and coaches other data and analytic professionals on data standards and practices.
Develops and delivers communication and education plans on analytic data engineering capabilities, standards, and processes.
Job Qualifications:
Education and Work Experience: Bachelor’s degree in computer science, information systems, engineering, or a related field; Masters preferred.
3+ years of experience in data engineering role.
Expertise in ELT and data analysis, SQL and at least one programming language (e.g., SQL, Python).
Conceptual knowledge of data and analytics, such as dimensional modelling, reporting tools, data governance, and structured and unstructured data.
Skills: Strong understanding of agile methodologies and experience as a Data Engineer on a cross-functional agile team preferred.
Experience with designing and maintaining data warehouses and/or data lakes with technologies, such as Hadoop, Spark, or distributed databases, like Redshift and Snowflake, and experience with housing, accessing, and transforming data in a variety of relational databases.
Experience with cloud database technologies (e.g., Azure) and developing solutions on cloud computing services and infrastructure in the data and analytics space.
Experience in building data pipelines and deploying/maintaining them with best-in-class tools (e.g., Git and Jenkins).
Familiarity with the Linux operating system.
Basic familiarity with BI tools (e.g., Tableau, Power BI, Looker).
Experience working in data engineering or data architect role.
Candidate Qualities: Focuses on business agility and agile delivery with a “fail-fast, succeed early” mindset and emphasis on measurable outcomes.
Brings a high-energy and passionate outlook to the job and can influence those around them.
Able to build a sense of trust and rapport that creates a comfortable & effective workplace.
Passion for innovation and “can do” attitude.
BENEFITS
Comprehensive benefits package, including Health, Vision, Dental, and Life insurancesFSA and Life Assistance Program (EAP)
401(k) Retirement Plan
Health Advocacy, Travel Assistance, and My Secure Advantage
PTO Accrual and Holidays
#ZIP
Job Type: Full-time
Pay: $125,000.00 - $150,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Employee discount
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Referral program
Relocation assistance
Retirement plan
Tuition reimbursement
Vision insurance
Compensation package:
Performance bonus
Yearly bonus
Experience level:
3 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Lutz, FL 33558: Reliably commute or planning to relocate before starting work (Preferred)
Experience:
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person","$137,500 /yr (est.)",501 to 1000 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2018,Unknown / Non-Applicable
WellTrust Medical Group,#N/A,"Charlotte, NC",Data Engineer,"The Data Engineer will assist the senior analyst in complex modeling and projects to ensure their completion within the allotted timeline. Helps track measures of success and develop reports that measure operational and customer metrics. Demonstrates ability to troubleshoot data consumption and data quality problems.

Responsibilities:
Build and maintain reports using My SQL Server
Identify optimal data sources, structure data validation, and cache optimization.
Performs business problem modeling and quantitative analytics including regression analysis, time-series, hierarchical cluster analysis, and data mining.
Helps track measures of success and develop reports that measure operational and customer metrics.
Demonstrates ability to troubleshoot data consumption and data quality problems.
Evaluates and recommend improvements to current models, metrics, and analytic processes to improve operational efficiency and insight.
Design, document, and communicate effective technical solutions meeting end-user requirements
Perform analyses with large amounts of complex data, and uses data visualization tools to communicate results.
Requirements:

Bachelor's degree or equivalent experience in a quantitative field (Mathematics, Statistics, Computer Science, Economics, Finance, Physics, Engineering)
3+ years of professional working experience as a Data Engineer or Analyst
1-3 years of data warehouse experience
Build and maintain reports using My SQL Server
Strong analytical and creative problem-solving skills
Design, document, and communicate effective technical solutions meeting end-user
SQL and Excel Macro development
Strong understanding of various data environments such as SQL, Oracle, Big Query, etc.
Experience with data visualization tools such as Looker Studio, Tableau, D3.js, etc.
Proficient in R, Python, MATLAB, Java, or another programming language.",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Hiscox
4.1",4.1,"Atlanta, GA",Senior Data Engineer,"job description:
You will be a senior member of our US IT Team data engineering practice. You will be an early member of this team and will work closely with the Director of Data Engineering to help establish it. We will have our work cut out for us delivering planned work, building the practice and evolving our technical landscape. This is an exciting opportunity for the right individual to join at the very beginning and grow with the new team.
As a senior you will be expected to lead, mentor others and deputise for the director of data engineering when required. Yoi will also continue to contribute as a hands-on engineer. You will have the knowledge, best earned through 5+ years of experience, to truly inhabit this senior role in an ambitious team.
What you’ll be doing:
Working with business stakeholders and our delivery team to understand high value business problems that can be solved through the application of data processing and analytical systems.
Developing, expanding and evolving our existing databases and ETL pipelines.
Working with architects to design, build and support a transformational cloud-based data platform for the business.
Being a core and professional member of the new data engineering practice
Understand business requirements and help refine into development tasks and estimate their complexity.
Research, evaluate and adopt new technologies with a right tool for the job mentality.
Focus on both speed of delivery and quality, with suitable pragmatism – ensuring your solutions are always appropriate and not too complex or over-engineered.
Quick progression of projects from proof-of-concept to post-production stage.
Communication and presentation of ideas to colleagues in all parts of the wider tech team.
Participating in code reviews for the data engineering practice.
Act as a servant leader in our practice – ensuring team members have
person specification:
Knowledge of, or ability to rapidly adopt our core languages for data engineering – SQL and/or Python.
Experience working using a modern DevOps approach. You should not just have written data code but also the CI/CD pipelines necessary to test and deploy that code in a professional environment.
A robust understanding of core data engineering topics – ETL vs ELT, structured and unstructured data, data quality and data governance.
Ability to contribute to all aspects of a solution – design, infrastructure, development, testing and maintenance.
The ability to design and advocate for technical solutions to business problems.
Effective collaboration with technical and non-technical team members through agile ceremonies – roadmap planning, feature workshops, backlog elaboration, code review.
Understanding of cloud technology and significant hands-on experience working with it. You must understand how cloud-native solutions must be built differently from traditional ones.
Track record of taking initiative and delivering projects end-to-end; clear evidence of being self-driven and motivated
Immense curiosity, high energy and desire to go the extra mile to make a difference.

Desirable Skills
Beyond the required skills we are open to individuals of diverse talents. Experience with additional technologies, data science knowledge or a business background are all valued. We want to know how your unique abilities can contribute to our team.
Our Technology
We operate in a diverse technical landscape and are looking for flexible engineers who can adapt to and use many different tools. We would not expect any engineer to be familiar with the entire tech stack – no engineer can. Instead, we seek people with a good understanding of data structures and algorithms and the ability to apply this knowledge in learning new tools.
We have an existing data landscape involving both MS SQL Server and Oracle databases. We use SSIS for ETL. We will also need to source data from our integration layer – webMethods.
If some of that sounds like it needs bringing up to date, we know! We hope to start building a modern cloud data platform to realise the full value of our data. The technologies we use for that platform are still to be decided. Likely it will be based on Snowflake with Azure Data Factory for ELT. Other parts of our business have had success using Prefect for orchestration and dbt for transformation and we will reuse where it is sensible.
We prepare data for use by analysts working with a variety of tools – Tableau, PowerBI, and even Excel.
We take a DevOps approach and strive for continuous integration / continuous deployment. We use Azure Pipelines to deploy our code. We deploy infrastructure the same way using Terraform and Docker.
Our technology landscape is not fixed. Our engineering and architecture teams drive the technology we adopt.


Salary range $140,000 - $170,000","$155,000 /yr (est.)",1001 to 5000 Employees,Company - Public,Insurance,Insurance Carriers,1901,$1 to $5 billion (USD)
"Trella Health
4.8",4.8,"Philadelphia, PA",Senior Data Engineer,"At Trella Health, we are passionate and committed to our mission – empowering meaningful change in healthcare. Since our founding in 2015, we continue to grow our team, enhance our solution and services offerings, accelerate into new markets, and expand our customer base. We are rapidly growing and are looking for new Trellavators to join our team!
""What is a Trellavator?"" you ask. Innovate and elevate is the name of our game! We go above and beyond to collaborate with and support each other – we believe that when a colleague or customers succeeds, we succeed. By learning from others, building on our successes, and taking risks, we constantly raise the bar – continuous improvement is in our DNA. Our word is paramount, we keep our commitments, and we always follow through. We have a strong, reliable support system that fuels growth, collaboration, and passion – and together, we create a positive environment where everyone at Trella Health, including the customers we support, can thrive. Are you ready to learn more about the opportunities with our team? Trell-yeah you are!
Position Overview:
The Senior Data Engineer will be responsible for designing, implementing, and maintaining an effective data pipeline for Trella Health's applications. This position is responsible for turning data concepts into effective data models that are efficient and scalable. The Senior Data Engineer will ensure a smooth data release and deployment process throughout the data development lifecycle.
Successful candidate will be a highly organized, self-starter that can roll up their sleeves and solve problems efficiently. Ability to document procedures, manage time, and have a knack for optimizing data flows.
Location: Atlanta HQ; Raleigh, NC; Nashville, TN; Philadelphia, PA; Remote considered
Reports to: Manager, Data Engineering
As a member of the Data team at Trella, you will:
Champion a comprehensive data vision and operating principles that improve data quality, consistency, accessibility, decision making, use, and stewardship.
Manage data pipeline by developing processes for data input, transformation, and storage
Act as an interface between the Data Science team and Application Developers
Architect and implement front-end (application layer) and back-end (ETL layer) database technologies in a highly customized data environment
Develop automated processes for moving data between systems
Implement ETL processes that transform source data into functional data stores consumed by front-end applications
Scale data pipeline by utilizing the most efficient tools and technologies
This job might be a fit for you if you have:
Bachelor's degree in Computer Science, Engineering, Mathematics, or other technical discipline (proven professional experience will be considered in lieu of a technical degree)
2+ full years of recent advanced Snowflake development experience
Experience manipulating healthcare data
In-depth knowledge of Amazon Web Services (AWS) and Platform as a Service (PAAS)
Demonstrable experience with Snowflake utilities including SnowSQL & Snowflake Javascript
Analyze all aspects of the Snowflake Query Engine to improve bottlenecks that may exist
Ability to automate data ingestion from Amazon S3, FTP, and other technologies
Experience with scripting languages, extensive Python experience, Node.js experience a plus
Working knowledge of Bitbucket and Git
Experience managing and loading data in AWS RDS (MySQL/MariaDb)
Ability to optimize MySQL query execution plans for robust performance
Technical Job Requirements (Nice to Have)
Experience manipulating healthcare data including claims, ADT data
Foundational knowledge of CI/CD platforms, preferably TeamCity
Development and consumption of API services
Experience with data science tools (Jupyter, SageMaker, R)
Understanding of Atlassian suite of tools, including Jira and Confluence
Soft Skill Job Requirements (Nice to Have)
Strong analytical skills and ability to rapidly switch contexts
Excellent communication and problem-solving skills
Sole contributor that is willing to independently uncover and improve processes that are inefficient or sub-optimal
Track record of implementing creative solutions to complex problems
Experience working in a nimble, fast-paced, start-up environment
Knowledge of HIPPA, PHI, PII policies
About Trella Health:
Trella Health provides unmatched, actionable market intelligence to post-acute care and value-based care providers of all sizes. Our industry leading analytics paired with CRM and EHR integration workflows positions us as the most advanced sales enablement platform for the post-acute care market. One of only a few companies to be deemed both a Qualified Entity by CMS and an Innovator under its Virtual Research Data Center Program, Trella has elite access to billions of performance metrics and referral data to enable competitive positioning across the market. Its standardized insights, representing 90% of all lives 65+ U.S. population, help customers identify, engage, and manage critical relationships and advance their organizations with certainty.
Since launching our first product in 2016, we have experienced tremendous growth and have highlighted a few key data points to get an at-a-glance view of Trella:
Over 700 customers including 10 of the top 10 post-acute providers and 4 of the top 10 health systems
Over 120 team members and growing
Recognized as one of Modern Healthcare's ""Best Places to Work"" in consecutive years and were ranked in the Top 50 startups in the US by LinkedIn
Headquartered in Atlanta, GA with remote hubs in Nashville, Philadelphia, Raleigh, and the Philippines
The Trellavator Experience:
At Trella Health, you can expect to join a welcoming team that truly embodies our core values. Our collaborative culture is anchored by trust, transparency, and inclusion — and we also have a lot of fun. No matter where you work, at Trella Health you can expect an awesome team, frequent virtual gatherings, engaging events, casual attire, and more.
We offer competitive salaries with a comprehensive benefits package to all employees and provide an environment that fosters work-life harmonization with unlimited PTO, company-observed holidays, and Summer Fridays. As we continue to see exponential growth, our goal is to continue to put team members first and strive to offer our team members the best culture and benefits possible. Some of the benefits we provide are:
Health, Dental, Vision & Voluntary Benefits
Competitive Salary & Bonus Plans
401k Retirement Savings
Unlimited PTO & 10 Paid Holidays
Flexible Work Hours
Equity Shares
Paid Leave Programs
Marketplace for discounted retail and entertainment
Equal Opportunity Employer
Trella Health is an equal opportunity employer. All persons will receive consideration for employment without regard to race, color, gender, sexual orientation, gender identity or expression, religion, national origin, marital status, age, disability, handicap, veteran status, genetic information, or any other protected status as recognized by federal, state, or local laws.","$113,846 /yr (est.)",51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2015,Unknown / Non-Applicable
"Contact Government Services, LLC
4.7",4.7,"Washington, DC",Data Engineer,"CGS is seeking a passionate and driven Data Engineer to support a rapidly growing Data Analytics and Business Intelligence platform focused on providing solutions that empower our Federal customers with the tools and capabilities needed to turn data into actionable insights. The ideal candidate is a critical thinker and perpetual learner; excited to gain exposure and build skillsets across a range of technologies while solving some of our clients’ toughest challenges. As a Data Engineer, you will be integral to data operations for the development and integration of multiple data types across a range of data sets and sources.

You will be responsible for the day-to-day operations of systems that depend on data, ensuring data is properly processed and securely transferred to its appropriate location, in a timely manner. Processing data will include managing, manipulating, storing and parsing data in a data pipeline for variety of target sources. You will also support maintenance of applications and tools that reside on these systems such as upgrades, patches, configuration changes, etc. The work is performed in a multidisciplinary team environment using agile methodologies. The candidate we seek must be highly motivated and enthusiastic about implementing new technologies and learning about new data in a small team environment where deadlines are important.
Responsibilities:
Complete development efforts across data pipeline to store, manage, store, and provision to data consumers
Being an active and collaborating member of an Agile/Scrum team and following all Agile/Scrum best practices
Write code to ensure the performance and reliability of data extraction and processing
Support continuous process automation for data ingest
Achieve technical excellence by advocating for and adhering to lean-agile engineering principles and practices such as API-first design, simple design, continuous integration, version control, and automated testing
Work with program management and engineers to implement and document complex and evolving requirements
Help cultivate an environment that promotes customer service excellence, innovation, collaboration, and teamwork
Collaborate with others as part of a cross-functional team that includes user experience researchers and designers, product managers, engineers, and other functional specialists
Required Skills:
Must be a US Citizen
Must be able to obtain a Public Trust Clearance
7+ years of IT experience including experience in design, management, and solutioning of large, complex data sets and models.
Experience with developing data pipelines from many sources from structured and unstructured data sets in a variety of formats
Proficiency developing ETL processes, and performing test and validation steps
Proficiency to manipulate data (Python, R, SQL, SAS)
Strong knowledge of big data analysis and storage tools and technologies
Strong understanding of the agile principles and ability to apply them
Strong understanding of the CI/CD pipelines and ability to apply them
Experience with relational database, such as, PostgreSQL
Work comfortably in version control systems, such as, Git Repositories
Contact Government Services strives to simplify and enhance government bureaucracy through the optimization of human, technical, and financial resources. We combine cutting edge technology with world class personnel to deliver customized solutions that fit specific needs and solve the most challenging, dynamic problems. Our team emphasizes technical expertise and an empathy to public service to provide unique top tier support for our government engagements.
SOME BENEFITS OF THIS POSITION INCLUDE:
Health, Dental, & Vision insurance
Life Insurance
401k
Flexible Spending Account (Health, Dependent Care, and Commuter)
Paid Time Off & Observance of State/Federal Holidays
For more information about CGS please visit: https://www.cgsfederal.com
39U0b7kJu8","$103,213 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Atalan Tech,#N/A,Remote,Data Engineer,"Atalan Tech is a start-up with a mission to mitigate the impact of clinician burnout and turnover in our nation’s health system. We are expanding our team and looking to hire the critical role of Data Engineer. The Data Engineer will work closely with the Chief Technology Officer, Chief Scientific Officer (UCLA Ph.D.), Chief Product Officer (Oxford Ph.D.), and Chief Executive Officer to deliver actionable insights from data in a variety of situations.
We are a small, close-knit team. This is an excellent opportunity for someone to join a mission-oriented, early-stage company at the ground floor and grow with us. There will be plenty of room for growth and creativity, and opportunities to tackle challenges that present one of the toughest problems in our society.
Requirements
Strong hands-on experience of designing, developing and managing data pipelines
Strong coding skills in Python (required) and SQL (required) and R (strongly preferred)
Experience building and maintaining data pipelines on AWS using EC2, EBS, Redshift, Postgres, S3, Data Pipeline, Kinesis, Lambda, etc.
Thorough understanding and experience with task management tools (Airflow, NiFi, etc.)
Knowledge of data management best practices for ETL, data governance, data warehousing, data modeling and data science
Basic knowledge of Linux (Redhat preferred) administration
Minimum B.S in Computer Science, software engineering or related field (or equivalent in related professional experience)
Strong communication skills
Strong problem-solving/critical thinking skills
Ability to work in a fast-paced, dynamic start-up environment
Some knowledge of healthcare, health tech, or healthcare data analytics (preferred)
Typical Tasks and Responsibilities include:
Working closely with CTO and CSO to build data pipelines for machine learning
Assisting the CPO in managing and supporting analysis of enterprise data to drive better products and solutions
Managing company’s data infrastructure on AWS (Postgres/RDS, Redshift, S3, etc.)
Synthesizing multiple data sources, such as internal medical records and HR data
Integrating with client systems including EHR (Epic, Cerner), HRIS, flat files, etc.
Working closely with other key members on other projects
Developing and designing data security standards and policies in line with industry best practices
About Us:
Atalan Tech’s mission is to improve the wellness of clinicians and their patients. We blend labor economics, machine learning, and clinical psychology with a team from Harvard, Yale, and UCLA. Atalan Tech is looking to contribute innovative and sustainable solutions to one of the most pressing problems in the healthcare industry. Building upon our proof-of-concept success, we have recently secured multiple engagements with nationwide healthcare organizations and have successfully raised institutional funding to expand our reach.
Contact Us:
If you’re interested in this position, please send us your CV/resume and cover letter.
Job Type: Full-time
Pay: $108,448.20 - $130,604.27 per year
Experience level:
2 years
Schedule:
Monday to Friday
Application Question(s):
Do you know the role is full time?
Can you start within the next month?
What are your salary expectations?
Have you held the title of a data engineer? If not, what experience do you have as a data engineer? Please be very specific.
Have you released Python code to a production environment in the last year?
Experience:
Data analytics: 2 years (Required)
Work Location: Remote","$119,526 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Merkle
3.7",3.7,"Norristown, PA",Data Engineer,"Company Description

We are Dentsu International. We innovate the way brands are built. That means we do things differently so they’re better than before. In this way, we make our clients’ most important marketing assets—their brands—win in a changing world.
Dentsu International is a modern marketing solutions company. Our mission is to help clients navigate, progress, and thrive in a world of change. Businesses rely on our integrated network of agencies and specialized practices to champion meaningful progress through creative, media, commerce, data, and technology. Part of Dentsu Group, our global network comprises 66,000 diverse people in 143 countries, who are dedicated to teaming for growth and good. Follow us on Twitter @DentsuUSA and visit dentsu.com/us.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.

Job Description
Develop onboarding process to receive and process information related to onboarding a new subscriber
Convert Snowflake Stored Procedures to Python Scripts directly in Airflow
Develop metadata driven solutions for managing data processing and cadence
End to end automation development and testing
Building and maintaining stored procedures and scripts in Snowflake Data Cloud
Building and maintaining complex SQL queries for ETL/ELT processing
Develop Dashboard for data processed for audit and governance needs
Follow Agile methodologies to deliver iterative feature sets rapidly
Maintain codebase using source control methodologies, Git
Collaborate with business analysts, data analysts, and other developers to deliver powerful new features
Coordinate with teams as part of a larger data-sharing system
Contribute to a collaborative, positive, and enjoyable environment for your operations team
Performing quality assurance and testing
Writing user and technical documentation such as runbooks

Qualifications
Strong communication skills both verbal and written
Strong understanding of conditional programming logic
Knowledge of software engineering and development methodologies, techniques, and tools; includes Issue Tracking (JIRA) Software Development Lifecycle SDLC), and Continuous Integration / Continuous Development (CI/CD)
Experience working with large databases and data warehouses utilizing both relational and dimensional design concepts
Eagerness to learn new technologies, programming languages and platforms to solve complex data challenges
Experience with Python required
Experience with SQL required
Experience with Snowflake Data Cloud is highly desirable
Experience with Amazon Web Services (AWS) a plus
Experience with Airflow (or other orchestration) a plus
Experience in database marketing is a plus

Additional Information

Perks:
Great compensation package
Comprehensive healthcare plans
401(k) with employer match
Flexible time-off
16 weeks paid parental leave
Additional Information
The anticipated salary range for this position is $94,000-$146,000. Salary is based on a range of factors that include relevant experience, knowledge, skills, other job-related qualifications, and geography. A range of medical, dental, vision, 401(k) matching, paid time off, and/or other benefits also are available. For more information regarding dentsu benefits, please visit dentsubenefitsplus.com
Employees from diverse or underrepresented backgrounds encouraged to apply.
Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact recruiting@dentsuaegis.com if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.
About dentsu
Dentsu is the network designed for what’s next, helping clients predict and plan for disruptive future opportunities in the sustainable economy. Taking a people-centered approach to business transformation, dentsu combines Japanese innovation with a diverse, global perspective to drive client growth and to shape society www.dentsu.com.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.
Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact your recruiter if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.","$120,000 /yr (est.)",5001 to 10000 Employees,Company - Private,Media & Communication,Advertising & Public Relations,1971,$500 million to $1 billion (USD)
Fracsys Inc,#N/A,"Washington, DC",Data Engineer,"Fracsys Inc is hiring a Data Engineer position. The ideal candidate must have at least 12+ years of industry experience. He or she must be responsible for successful technical delivery of Data Warehousing and Business Intelligence projects using a formal project management methodology.
Responsibilities
Directs, motivates and develops key technical personnel
Provides technical direction across the full lifecycle system development process
Provides strategic planning and comprehensive senior level technical consulting to IT senior management and senior technical staff
Working independently within guidelines
Provides detailed inputs for plans and schedules, including goals, risks, and resource allocation
Conduct diagnostic, troubleshooting steps and present conclusions to a varied audience
comfortable slinging some code to solve a problem
Build Data Marts and Data warehouses to facilitate analytical capabilities across the varied sources of data
Design and implement data patterns on cloud using cloud Data warehouse and data virtualization tools
Evaluate new tools and technologies for target state on cloud
Learn and implement new ETL, BI tools and data warehousing technologies
Qualifications
Minimum twelve (12) years in a Technical Lead or Consulting role leading Information Management related initiatives (system integration, data warehouse build, data mart build, or similar)
MUST have proven leadership experience for team of at least 3 technical resources.
Experience creating Data Marts, Data warehouses on-premise and on cloud for real time and batch processing frameworks
Experience building and optimizing big-data data pipelines and data sets including Postgres, AWS Relational Data Service
extensive hands-on experience building and optimizing data structures for data analytics, data science and business intelligence
Experience building self-service data consumption patterns and knowledge of cloud-based data Lake platforms
Experience wrangling data (structured and unstructured), in-depth knowledge of database architecture
Experience utilizing or leading implementations leveraging ETL tools (Informatica / Talend), BI Reporting tools such as MicroStrategy, Microsoft Power BI, data modeling tools such as Erwin, Oracle, SQL server, NoSQL, JDBC, UNIX shell scripting, PERL and JAVA, XML/JSON files, SAS , Python, AWS cloud-native technologies, S3, Athena, Redshift
Familiarity with the following technologies: Hadoop, Kafka, Airflow, Hive, Presto, Athena, S3, Aurora, EMR, Spark
Ability to drive, contribute to, and communicate solutions to technical product challenges
Ability to roll-up your sleeves and work in process or resource gaps and fill it in yourself
Excellent written and oral communication skills in English
Education
Bachelor’s degree in computer science, Data Analytics, Information Systems, or a related degree or equivalent experience. Master’s degree is preferred.
Job Type: Full-time
Pay: From $96,336.72 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Experience level:
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Washington, DC: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person","$96,337 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Barracuda Networks Inc.
3.6",3.6,"Alpharetta, GA",Data Engineer,"Job ID: 23-579

Come Join Our Passionate Team! At Barracuda, we make the world a safer place. We believe every business deserves access to cloud-enabled, enterprise-grade security solutions that are easy to buy, deploy, and use. We protect email, networks, data and applications with innovative solutions that grow and adapt with our customers’ journey. More than 200,000 organizations worldwide trust Barracuda to protect them — in ways they may not even know they are at risk — so they can focus on taking their business to the next level.
We know a diverse workforce adds to our collective value and strength as an organization. Barracuda Networks is proud to be an Equal Opportunity Employer, committed to equal employment opportunity and equitable compensation regardless of race, gender, religion, sex, sexual orientation, national origin, or disability.
Envision yourself at Barracuda
We currently have an exciting new position open for a Data Engineer. This person will assist in driving technology innovation for engineering projects and work on architecture of AWS, Elastic and Kubernetes. They will also work closely with the Technology Architect to create and drive the Architecture Review Board.
What you’ll be working on
Determine internal / external customer data needs and identify system specifications
Analyze the needs of large systems and breaking them down into smaller manageable parts
Responsible for designing the journey of data from various sources to the database and everything in between
Plan and design the structure of data, brainstorm with team
Analyze best technology products and systems for internal teams
Help provide build vs buy decisions on data architecture
Communicate data requirements to engineers; explain data architecture to them and provide assistance throughout SDLC
Choose / maintain suitable software, hardware and integration methods
Help resolve technical problems as and when they arise
Review testing procedures to ensure data performant and error free
Ensure that database satisfy quality standards and procedures. Setup such standards.
Work with senior IT personnel to devise plans for future IT requirements of the organization
Produce progress reports on key infrastructures - Elastic and Data Pipelines
What you bring to the role
Bachelor Degree or Post Graduate in Computer Science, or relevant work experience
5+ years of experience in an Engineering / Operations heavy organization
1+ years of experience as a Data Analysis and Data Design responsible for data architecture of a SaaS application
Experience in understanding and designing data sharding in a distributed database
Some Hands-on experience in software development and system administration
Knowledge of strategic IT solutions
Familiarity with programming languages like JavaScript or Java or Python
Familiarity with Linux OS and Windows alike
Experience in cloud computing and cloud Technologies (AWS, GCP, Azure)
Excellent communication skills – you should be adept at listening to, understanding, and explaining key concepts to managerial and technical resources.
Good Anticipation and problem-solving skills
Ability to prioritize and manage time – responsible for multiple streams of work at the same time
Relationship building - Ability to form a good rapport with internal and external stakeholders
In-depth knowledge about quality standards and security best practices

Preferred Skills:
Expert level Understanding of Elastic eco system - Elasticsearch, Logstash, Kibana, Filebeats
Understanding of secure tunnel communication using VPN
Experience designing data at scale
AWS experience is preferred in Cloud Technologies
What you’ll get from us
A team where you can voice your opinion, make an impact, and where you and your experience are valued. Internal mobility – there are opportunities for cross training and the ability to attain your next career step within Barracuda.
Equity, in the form of non-qualified options
High-quality health benefits
Retirement Plan with employer match
Career-growth opportunities
Flexible Time Off and Paid Time Off benefits
Volunteer opportunities
#LI-Remote","$98,161 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,$100 to $500 million (USD)
"Quadrant Resource
4.1",4.1,Remote,Data Engineer,"Job Description:
4-6 years of experience as a Data Engineer in the Snowflake-Fivetran-dbt stack
• Possess a solid understanding of end to end Data ingestion, ETL, data analysis, Reporting processes
• Expert in ETL development using Fivetran and experienced in implementing data pipelines for Snowflake Cloud DWH
•Experienced in using dbt for data transformation
•Expertise on Tableau/Metabase for building reports/dashboards
•Expert in troubleshooting and resolving ETL issues, data load failures/problems and transformation translation problems
•Excellent communicator with the ability to work effectively with distributed teams
Job Type: Full-time
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Data Engineer: 9 years (Required)
Fivetran: 8 years (Required)
DBT: 7 years (Required)
Snowflake: 6 years (Required)
Work Location: Remote",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,2004,Unknown / Non-Applicable
Redzara.us,#N/A,"Atlanta, GA",Azure Data Engineer,"Responsible for developing new and modernizing existing applications to Azure Cloud Platform.
The ideal candidate must demonstrate in-depth knowledge and understanding of Database concepts.
Requirements:
7+ years of experience in Data Warehousing and Business Intelligence
5+ Years in BI development leveraging the Microsoft SQL Server. Needs to be an expert in T-SQL
5+ Years in SSIS and use of Stored Procedures.
1-2 Years in Azure Data Factory, SQL VMs, build and maintain ETL
Ingest and aggregate data from both internal and external data sources to build our data lake
Strong understanding data modeling (i.e., conceptual, logical, and physical model design, experience with Operation Data Stores, Enterprise Data warehouses and Data Marts).
Job Type: Contract
Pay: $43.49 - $57.00 per hour
Compensation package:
1099 contract
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Atlanta, GA 30334: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure Data Factory: 3 years (Required)
Azure: 3 years (Required)
Data warehouse: 1 year (Preferred)
Work Location: Hybrid remote in Atlanta, GA 30334",$50.25 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Loyal
3.9",3.9,"Atlanta, GA",Senior Cloud Data Operations Engineer,"Loyal is an organization centered on experience and building a platform that allows consumers to make meaningful decisions when it comes to healthcare. We deeply understand providers, locations, services, appointments, business rules, and moreover, we understand patients - who they are, the preferred method of communication, upcoming appointments, lapsed appointments, outstanding bills, health risks, and more. With this intelligence, our platform fuels highly relevant and personalized experiences across all mediums (website, email, voice…) allowing patients to get healthy, stay healthy, and have a better relationship with the health care provider.
**This is a remote role**
Summary
The Senior Cloud Data Operations Engineer is responsible for delivering Loyal's cloud-based data warehouse. They interface with groups across the organization to ensure we are collecting and storing data in a way that makes it easy to create and consume. They identify best practices for big data management, select the right tooling, create a workable data architecture, and drive an implementation roadmap others can depend on. The Senior Cloud Data Operations Engineer also works closely with our security group to ensure data security and privacy. They also mentor others on the team, providing technical leadership to those they work with.
You will
Create data streams and consolidate data sources into a single cloud-based data warehouse
Architect and catalog data warehouse storage schema
Work with product teams to enable easy data aggregation
Work with data consumers (analysts, data scientists, and AI/ML engineers) to ensure data is easily accessible and consumable
Create and execute a roadmap that meets data storage and availability goals
Ensure data is secure through data analysis, access control, and encryption
You have
Bachelor's degree in computer science, information systems, applied math or related field, or equivalent work experience
Minimum 5 years of experience working with cloud data processing services (AWS, Azure, or GCP).
Minimum 3 year experience with Big Data technologies, such as Databricks, Snowflake, Spark, Hadoop, etc.
Strong experience in creating data architecture and storage schemas required
Significant working knowledge of SQL and other query languages and tools
Significant experience with building relational database systems strongly preferred
Experience with ETL tools a plus
Knowledge of Python preferred
Software development, data analytics, or data science experience is a plus.
Experience working within a software as a service (SaaS) company preferred.
Experience working within a high growth and/or ambiguous environment, with proven experience to be dynamic preferred.
#LI-REMOTE
We know that potential candidates are often less likely to apply to a position if they don't match 100% of the job qualifications. Don't let that be why you miss out on this opportunity! We encourage you to apply if you can demonstrate many of these skills and competencies.
Loyal to our employees
We are a remote-friendly company! We encourage you to apply from anywhere in the United States. We also believe in a work/life balance that fulfills you while you're here and supports you when you're not. We built our benefits package to prove that we're committed to you having everything you need (including a little fun). Here is what we offer full-time employees:
Flexible paid time off, sick and personal days
At least one holiday per month (sometimes, more!)
Full health, dental, and vision insurance - Loyal pays the premium for all employees!
One Time Home Office Setup Stipend For Remote & Hybrid Roles
Monthly Internet Stipend for Remote & Hybrid Roles
Long term & short term disability
401[k] plan
16 Weeks Paid Parental Leave
2 Volunteer days per year
Matching Gift Program
Participation Grant Program
Annual Travel/Team Events up to twice per year (post-COVID)
Our Commitment
We believe that the key to Loyal's success is you. Your unique background, life experience, knowledge, self-expression, and talent make you uniquely you. Who you are, what you have experienced, and how you think inspires us to be innovative and bold.
Loyal is an equal opportunity employer. We hire great people from a wide variety of backgrounds, not just because it's the right thing to do, but because it makes our company stronger. We welcome the unique contributions that you can bring in terms of your education, opinions, culture, ethnicity, race, ancestry, sex, gender identity and expression, national origin, citizenship, marital status, age, languages spoken, veteran status, color, religion, disability, sexual orientation, and beliefs.
We consider qualified applicants regardless of criminal histories, consistent with legal requirements.
Further, consistent with applicable federal and state law, Loyal provides reasonable accommodations when requested by qualified applicants or employees with disabilities, unless doing so would cause an undue hardship. Loyal's policy regarding requests for reasonable accommodation applies to all aspects of employment, including the application process. If you require a reasonable accommodation in connection with the application process, please contact the Talent Acquisition Department at talentacquisition@loyalhealth.com.
E-Verify
This employer participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S. If E-Verify cannot confirm that you are authorized to work, this employer is required to give you written instructions and an opportunity to contact Department of Homeland Security (DHS) or Social Security Administration (SSA) so you can begin to resolve the issue before the employer can take any action against you, including terminating your employment. Employers can only use E-Verify once you have accepted a job offer and completed the I-9 Form.
COVID-19 Vaccinations
Consistent with Loyal policy, candidates performing in-person work will be required to show proof of being fully vaccinated against COVID-19 upon commencing employment. Reasonable accommodations will be considered on a case-by-case basis for exemptions to this requirement in accordance with applicable law. If you require a reasonable accommodation to Loyal's vaccination requirement, please contact the People Department at people@loyalhealth.com.",#N/A,51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2015,$5 to $25 million (USD)
Radiant System,#N/A,"Pleasanton, CA",Data Engineer,"First Priority: Pleasanton, CA –
Second Priority for a superstar: Plano, TX
Experience in developing and deploying data pipelines, preferably in the Cloud
Experience working with Spark, Databricks, and other relevant technologies in Microsoft Azure cloud
Experience in productionizing and deploying Big Data platforms and applications
Expert in writing SQL statements
Job Type: Contract
Application Question(s):
Are you willing to work on W2?
Work Location: In person",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"CDNetworks Inc.
2.9",2.9,"Monrovia, CA",Jr. Data Center Engineer,"CDNetworks is one of the top leading CDN & Edge Service Providers with global offices in Korea, Japan, Singapore, Malaysia, China, Russia, London, and Canada. We focus on delivering integrated cloud and edge computing solutions with unparalleled speed, ultra-low latency, rigorous security, and reliability so that our clients can focus on what’s most important – growing their business.
The Jr. Data Center Engineer is a Full-Time position based in Monrovia, CA. Job is performed indoors in a Data Center or Warehouse environment. 75% Travel required for this position.
Job Responsibilities
Ensure all incidents are logged and resolved, gather all relevant data, and ensure all incidents and tasks follow the appropriate procedures.
Support data center activities and work closely with our system and network team to complete tasks/projects.
First responder to all alerts and problem reports while managing communications between departments and handling crisis documentation and dissemination after the fact.
Utilize internal systems such as JIRA/Wiki to manage project plans and progress.
Performing general system administration duties including OS patching and upgrades, batch job monitoring, system and hardware diagnostics, and other activities to ensure optimal health and performance of all systems as required.
Resolve complex problems related to Server and H/W areas.
Assisting/working closely with Network, System Engineers to configure customer requirements.
Physical deployment of network devices, servers, cables, etc.
Assembling/dissembling server hardware for deployment and OS installation and network equipment testing.
Maintain existing department and system documentation (update workflow, process, training documentation).
Other duties as assigned.
Abilities Required
Good verbal and written communication skills, and ability to work independently with minimum instruction.
Basic degree of mentorship, training, and direction team members skills.
Knowledge of IDC industry.
Intermediate degree of analytical and project management skills.
Other Features of Job
Job is performed indoors in a Data Center or Warehouse environment.
Language Skills
Excellent communication skills (English) – written and verbal. Bilingual Chinese or Korean is a plus!!
Job Type: Full-time
Salary: $20.00 - $25.00 per hour
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Health insurance
Life insurance
Paid time off
Professional development assistance
Referral program
Vision insurance
Schedule:
10 hour shift
8 hour shift
Evening shift
Monday to Friday
Night shift
On call
Overtime
Supplemental pay types:
Bonus pay
Ability to commute/relocate:
Monrovia, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Computer networking: 1 year (Preferred)
Shift availability:
Overnight Shift (Required)
Night Shift (Required)
Day Shift (Required)
Willingness to travel:
75% (Preferred)
Work Location: In person",$22.50 /hr (est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2000,$10+ billion (USD)
"DiNi Communications, Inc.
3.0",3.0,"New York, NY",Data Center Engineer,"DiNi Communications, Inc. is seeking a Data Center Engineer. This person needs to be a team player with exceptional communication skills. Maintaining the confidentiality of the client's information is of utmost importance.
Required Education and Experience
· At least 8 years of experience as a large data center technician
· Knowledge of Networking,
· Ability to lift equipment at least (50lbs) Rack and Stack (Network Switches, Servers, Storage,
Appliances)
· Knowledge of structured Cabling in a Data Center.
· General knowledge of Operating Systems, (Windows, Unix).
· General knowledge of networking in N Tier Environment (App, Web, DMZ,).
· Level 1 and Level 2 troubleshooting in a N Tier Environment.
· Ability to follow and execute commands to start up and shutdown systems.
· Physically travel to and be available at site B to perform technical changes that require a
person at Site B.
· Performing backup and restore activities.
· Asset Management.
· Maintaining document.
· Work with vendor on circuit issues, (WAN and Internet).
· Cable Patch the equipment in the environment.
· Participate in Site B fall-over testing
Preferred Network Engineering Experience
· Knowledge of N Tier networking.
· Certification in Networking (CISCO).
· Certification in various Operating systems (Linux and Windows), Storage, Messaging
(Microsoft).
· Certification in IT Security.
· Knowledge of Databases.
· Experience working with CISCO, EMC, Symantec, HP, Oracle, Bluecoat, MacAfee, and other vendors' equipment and software in a networked environment.
Responsibilities include but are not limited to:
Adding equipment to support agency applications in site B Environment,
Coordinating Site B testing with Site A technical resources.
Monitoring the B environment.
Rack and Stack new and replacement equipment
Troubleshooting and resolving incidents and problems in the
Physically travel to and be available at Site B to perform technical changes that require a person at Site B.
Upgrading and expanding the environment
Performing backup and restore activities
Manage Site B assets
Maintaining document
Cable Patch the equipment in the environment
Participate in Site B fall-over testing.
Job Type: Full-time
Pay: $60.00 - $70.00 per hour
Schedule:
Monday to Friday
Experience:
large data center technician: 8 years (Required)
Networking in N Tier Environment (App, Web, DMZ,): 3 years (Required)
Ability to lift equipment at least (50lbs): 1 year (Required)
structured Cabling in a Data Center: 3 years (Required)
Level 1 and 2 troubleshooting in a N Tier Environment: 3 years (Required)
Performing backup and restore activities: 3 years (Required)
Cable Patch the equipment: 3 years (Required)
Certification in Networking (CISCO): 1 year (Preferred)
Certification in IT Security: 3 years (Preferred)
CISCO, EMC, Symantec, HP, Oracle, Bluecoat, MacAfee: 3 years (Preferred)
Work Location: In person",$65.00 /hr (est.),Unknown,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
Infosoft,#N/A,"Peachtree Corners, GA",Data Pipeline Engineer - 2173,"Here are the job position details for your review:
Job Title: Data Pipeline Engineer (GStreamer/ DeepStream)
Pay Rate: $90 to 120/Hr.
Duration: 3 Months
Location: Peachtree Corners, GA
This position is for the Building Automation division.
We are looking for Data Pipeline Engineer (GStreamer) to participate in the development of Streaming data pipelines using NVIDIA Deepstream, GStreamer, Python programming, messaging queues, and related tools.
Preference is for candidates located in Atlanta, GA metro area. However, ok with non-local candidates who are open to 75% travel to Atlanta, GA.
Understanding the basics of our architecture/setup to be able to do setup/support for demos, testing, etc.
Basic computer hardware assembly and set-up
Possible set-up/installation of the application(s)
Possible development tasks depending on what the rest of the development team has for them
Develops business software applications, system software, and/or IT solutions on the basis of functional specifications.
Scope of position embraces both the customization or reuse/modification of system software and/or solutions as well as their development from scratch.
Consults with customers in creating functional specifications and work estimates, and supports Make-or-Buy decisions.
Analyses performance gaps of existing software environments and proposes measures to address efficiency problems.
Is also responsible for the implementation of data-driven components supporting data modeling and engineering in the context of data analytics (Data Stewardship).
Skills:
Python
Linux
Docker and Docker Swarm
Zero MQ / a good understanding of messaging
Desired Skills:
NVIDIA Deepstream SDK
Web Sockets
Flask
Protobuf
Possible:
GStreamer (if different than RTSP / H264 / MJPEG)
Job Type: Contract
Salary: $90.00 - $120.00 per hour
Ability to commute/relocate:
Peachtree Corners, GA: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
""Please share your Email ID""
Work Location: One location",$105.00 /hr (est.),Unknown,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"ECI - Sacramento
4.4",4.4,"Sacramento, CA",Lead Data Quality Engineer (C),"Senior Lead Data Quality Engineer with Snowflake experience:
12-24 months.
Public sector experience is preferred.
Must have these specific experiences in data management, data integration, data qualify and Lead experience.
Must have Snowflake platform experience.
Project will start end of June.

Mandatory Qualifications:
Must have a bachelor's degree in computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field. Please attach degree(s).
Must have 10 or more years of experience working in Information Technology.
Must have at least seven (7) years or more of work experience in data management disciplines including data integration, modeling, optimization, and data quality, and/or other areas directly relevant to data engineering responsibilities and tasks.
Must have at least five (5) years of experience working in cross-functional teams and collaborating with business stakeholders in support of a departmental and/or multi-departmental data management and analytics initiative.
Must have at least three (3) years of experience in data integration, data warehouse, big-data-related initiatives, development, and implementation.
Must have at least three (3) years of experience in architecture patterns and data integration design principles.
Must have at least three (3) years of experience with database technologies (e.g., SQL, NoSQL, SQL Server).
Must have at least three (3) years of experience with the Snowflake Data Warehouse Platform.
Must have at least three (3) years of experience with extract, transform, and load (ETL) and other business intelligence tools (e.g., SSIS, Azure Data Factory, Power BI, Tableau).

Here are the desired which is best to get as many as possible:
Desired Qualifications:
Possess an advanced degree in computer science (MS), statistics, applied mathematics (Ph.D.), information science (MIS), data management, information systems, information science (post-graduation diploma or related) or a related quantitative field.
Possess Azure Cloud and Snowflake certifications.
Three (3) years working experience in the Azure Cloud platform using Synapse, Azure BLOB Storage/ADLS, Azure Data Factory, and Purview.
One (1) year experience with Azure DevOps and CI/CD pipelines.



About ECI - Sacramento:

Estrada Consulting, Inc. (ECI) delivers technology-enabled services and solutions to clients all over the USA and British Columbia. We provide system integration, custom application development, data warehouse and business intelligence, project management, custom reporting solutions and consulting services to mid-size and large enterprises in all major industries. The Company headquarters is in Sacramento, California, and was established in year 2000. Visit http://www.estradaci.com/ to learn about our projects, managed services, awards and certifications delivering value for a range of businesses and government agencies.",$75.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2000,Unknown / Non-Applicable
United Digestive,#N/A,"Atlanta, GA",Data Engineer,"GENERAL SUMMARY OF DUTIES: United Digestive’s Data Engineer will be responsible for developing and maintaining data pipelines, data warehouses, and data models to support the company's Business Intelligence reporting initiatives.
REPORTS TO: Director of Business Intelligence & Analytics
RESPONSIBILITIES:
Duties may include but are not limited to:
Implement, maintain, and improve data integration processes to collect, process, and manage large datasets
Collaborate with stakeholders to understand business requirements and design data solutions that meet their needs
Implement and maintain data quality checks and validations to ensure data accuracy and completeness
Optimize data processing and storage to ensure high availability, scalability, and performance
Develop and maintain automated data workflows and scheduling tools
Code ETL process using Azure Data Factory workflows and other technologies as appropriate
Troubleshoot and resolve data-related issues and incidents
Stay up to date with the latest technologies and trends in data engineering, specifically the Azure suite, and recommend improvements and optimizations to the data architecture and infrastructure
Ability to develop solutions and manage existing Azure DevOps pipelines
Maintains a high level of confidentiality with sensitive information
Performs any other duties and/or special tasks as assigned by United Digestive leadership
REQUIRED EDUCATION, SKILLS & EXPERIENCE:
Bachelor’s degree in Computer Science, Software Engineering, Information Technology, or in a related field required; healthcare-related Data Analytics experience preferred; 2+ years of data pipeline management required. Proficiency in SQL and experience with relational databases (e.g., Oracle, SQL Server, PostgreSQL); experience with data modeling, data warehousing, and ETL processes, proficiency in various programming language (e.g., Python, SQL, Java), experience with cloud platforms (e.g., AWS, Azure, GCP) and big data technologies (e.g., Databricks, Hadoop, Spark, Kafka, Cassandra). Must have knowledge of data visualization and BI tools (e.g., Tableau, Power BI) and how to support these tools in an efficient manner.
ADDITIONAL SKILLS AND EXPERIENCE:
Data Engineer must have:
Excellent verbal and written communication skills.
Excellent organizational skills and attention to detail.
Ability to multi-task and prioritize job needs
Ability to deal with patients, visitors, co-workers, and physicians with courtesy and respect
Work under pressure; assess, respond to, and communicate issues in a timely manner.
Self-motivation and be a proactive individual with a professional can-do attitude
PHYSICAL/MENTAL/ENVIRONMENTAL DEMANDS
Requires sitting and standing associated with a normal office environment. Travel as business needs dictate.
DRUG FREE WORKPLACE
United Digestive is a drug free workplace. All offers of employment are contingent upon passing a pre-employment drug screening.
EQUAL OPPORTUNITY EMPLOYER
United Digestive is an Equal Opportunity Employer and does not discriminate on the basis of race, religion, gender, color, or national origin in its employment practices.
Location: 1355 Peachtree Street NE STE 1600, Atlanta, GA 30309
Job Type: Full-time",#N/A,Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Siri info solutions inc,#N/A,"Beaverton, OR",Sr. Data Engineer,"Must Have Skills
PySpark
Python on AWS
Responsibilities:
Understanding of advanced data warehousing concepts, data modeling and extract, transform and load development
Advanced SQL skills with experience querying large datasets from multiple sources and developing automated reporting
Python skills for scripting, data manipulation, custom extract, transform and loads and statistical/regression analysis particularly, as they apply to sales and marketing operations and performance
Job Type: Full-time
Salary: $47.00 - $50.00 per hour
Experience level:
7 years
Work Location: On the road",$48.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Gold Coast Health Plan
3.8",3.8,"Camarillo, CA",Sr. ETL DEV/Data Engineer,"Data Engineers will be responsible for transformation and modernization of enterprise data solutions on Cloud Platforms integrating Azure services and 3rd party data technologies. Data Engineer will work closely with a multidisciplinary agile team to build high quality data pipelines driving analytic solutions.
ESSENTIAL FUNCTIONS
Reasonable Accommodations Statement
To accomplish this job successfully, an individual must be able to perform, with or without reasonable accommodation, each essential function satisfactorily. Reasonable accommodations may be made to help enable qualified individuals with disabilities to perform the essential functions.
Essential Functions Statement
As a Data Engineer, you will be responsible for assisting our clients envision, design, and deploy data engineering workloads as part of our solutions. As part of a small, dynamic team, you will have the opportunity to contribute to multiple phases of the solution life cycle including designing and implementing models and processes for large-scale datasets used for descriptive, diagnostic, predictive, and prescriptive purposes
Work with the team to evaluate business needs and priorities, liaise with key business partners and address team needs related to data systems and management.
Translate business requirements into technical specifications; establish and define details, definitions, and requirements of applications, components and enhancements.
Generate design, development, test plans, detailed functional specifications documents, user interface design, and process flow charts for execution of programming.
Develop data pipelines / APIs using Python, SQL, potentially Spark and AWS, Azure or GCP Methods.
Build large-scale batch and real-time data pipelines with data processing frameworks in Azure cloud platform.
Assist in the migration from on-prem SQL Server data analytics platform to MS Azure cloud platform.
Work as part of a team to build upon ingestion framework to intake new data sources.
Analyze, design, code and test multiple components of application code across one or more clients.
Perform maintenance, enhancements and/or development work

Qualifications
BA/BS in computer science, mathematics, information management, business, or equivalent experience
6+ years of experience in SQL
4+ years of experience in Cloud Platforms: Azure or AWS or GCP
4+ years of experience in Python and Pyspark
4+ years of experience in Synapse highly preferred
Experience using SQL, dB Visualizer, AWS, Azure, Cloud technologies
Experience with Power BI or similar data visualization tools
knowledge of HL7 v2, HL7 CDA and FHIR interface mapping highly preferred
Exposure to non-relational databases and tools, such as Cassandra, JSON, JAVA, Python, and Spark
In-depth knowledge of healthcare interoperability and patient data aggregation
Ability to effectively communicate, at times in a non-technical language, with customers at all levels of the organization.","$127,500 /yr (est.)",Unknown,Self-employed,Insurance,Insurance Agencies & Brokerages,#N/A,Unknown / Non-Applicable
"Intersec
4.5",4.5,Remote,Senior Data Engineer,"Architect and implement highly scalable data pipeline to streamline migration of data from source data repositories to the centralized data platform
Implement agreed upon data models (e.g., data marts) to enable faster and efficient consumption of data by downstream consumers
Conduct performance tuning to optimize data migration techniques and ensure adherence to identified service-level agreements (SLAs)
Define and implement automated validation checks to ensure data integrity between source and target
Assist in onboarding consumers with existing technology to connect to the platform
Setup predictive alert mechanism for pipeline degradation for IMF pipelines.
Enhance data quality framework for data pipelines and derived datasets
Enhance Data Audit Balance and Control mechanism for data pipelines
Enhance pipelines to be certified copies of authoritative source
Populate metadata for the Enterprise with particular focus on the Data Sources being onboarded to EDP (Technical and Business Glossaries) related to IMF Modernization Programs
Validate correct lineage for datasets
Required:
*8+ years of Databricks, Data Modeling, and/or Metadata management experience
* Familiarity with architecting and developing real and near-real-time solutions
* Bachelor's Degree in Engineering, Mathematics, computer Science, Statistics or other quantitative discipline, or 4 years equivalent professional experience
* US Citizenship and ability to gain a public trust
Preferred:
* Familiarity with IRS systems and ETL pipelines
* Ability to work independently and manage multiple tasks
* IRS MBI Clearance with IRS Laptop and Informatica Installed
Job Type: Full-time
Salary: $70.00 - $75.00 per hour
Schedule:
Monday to Friday
Application Question(s):
Are you a US/GC card holder?
How many years of Databricks, Data Modeling, and/or Metadata management experience do you have?
Work Location: Remote",$72.50 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,Unknown / Non-Applicable
"Syngenta Group
4.0",4.0,"Downers Grove, IL",Senior Master Data Engineer,"Company Description

Syngenta Group is one of the world’s leading sustainable agriculture innovation companies, with roots going back more than 250 years. Our 53,000 people across more than 100 countries strive every day to transform agriculture through tailor-made solutions for the benefit of farmers, society and our planet –making us the world’s most local agricultural technology and innovation partner.
Syngenta Group is committed to operating at the highest standards of ethics and integrity. This is a commitment that we are making to investors, customers, society and employees. Syngenta Group is also committed to maintaining a workplace environment free from discrimination and harassment.

Job Description

The Senior Master Data Engineer will be responsible for ensuring Syngenta has the right identity data capabilities to support the current and future Syngenta production and commercial needs.
We have the responsibility to think beyond our past needs and help unlock future opportunities with one of our most valuable data assets. Robust, accurate and trusted identity data will unlock opportunities to improve customer experience, simplify vendor interactions and allow us to explore new ways of marketing our products and services.
The investment in the MDG platform has been the first phase or our journey to support our current and future business needs. The Senior Master Data Engineer will be responsible to build on this first phase and help to define our vision, strategy, operating model, and roadmap for the future of Party data capabilities. This may include supplementing the MDG platform with additional technology and services
Responsibilities
Contributes to creating a breakthrough transformation that shapes the Party data (including Identity and Reference Data) capability in line with Syngenta's strategic vision
Help define & deliver the strategy and roadmap for Identity data and reference data (including operating model, data products, technology, data quality & process analytics/health) ensuring solutions and technologies are maintainable and scalable
Enroll and align with stakeholders to experiment and leverage the Identity data capabilities within relevant domains (Employee, Sales/Customers, Legal Entities, Intercompany, Vendors). Including simplification, automation, rationalization, and harmonization.
Create and advocate Identity data offers (data as a product) that add value and solve business problems that improve integration and adoption.
Provide technical leadership in leveraging and experimenting with appropriate technologies including existing platforms as well as new opportunities (e.g. Microservices, API’s, AI, ML, etc.) to create Identity data products and services to meet business needs.
Seamlessly embed themselves in a cross-functional teams as a subject matter expert and participate in Identity data design authority.
Contribute to the creation and implementation of a reference data capability the compliments Identity master data.
The preferred candidate will be near an established Syngenta location The right candidate will be considered for a remote setting IN THE UNITED STATES.
We are unable to provide Visa Sponsorship for this position at this time.

Qualifications

Required Skills / Experience
Bachelor’s degree with 8 or more years of relevant experience
MUST HAVE hands on experience with a popular MDM tool
Extensive experience in customer master data management
Must have thought leadership and the ability to influence the business with best practices
Experience in reference data principles and practices.
Understand relevant technologies (Master Data Management tools, Microservices, etc)
Stakeholder management / influencing: able to engage with different functions, leadership levels and cultures

Additional Information

What we Offer
A culture that celebrates diversity & inclusion, promotes professional development, and strives for a work-life balance that supports the team members
We offer flexible work options to support your work and personal needs
Full Benefit Package (Medical, Dental & Vision) that starts your first day
401k plan with company match, Profit Sharing & Retirement Savings Contribution
Paid Vacation, 9 Paid Holidays, Maternity and Paternity Leave, Education Assistance, Wellness Programs, Corporate Discounts, among other benefits
Syngenta is an Equal Opportunity Employer and does not discriminate in recruitment, hiring, training, promotion or any other employment practices for reasons of race, color, religion, gender, national origin, age, sexual orientation, marital or veteran status, disability, or any other legally protected status
Family and Medical Leave Act (FMLA)
(http://www.dol.gov/whd/regs/compliance/posters/fmla.htm)
Equal Employment Opportunity Commission's (EEOC)
(http://webapps.dol.gov/elaws/firststep/poster_direct.htm)
Employee Polygraph Protection Act (EPPA)
(http://www.dol.gov/whd/regs/compliance/posters/eppa.htm)

#LI-SB2","$100,820 /yr (est.)",10000+ Employees,Company - Private,Agriculture,Crop Production,2000,$100 to $500 million (USD)
Cox powered by Atrium,#N/A,"Atlanta, GA",AWS Data Engineer- Hybrid,"Minimum Qualifications:
Bachelor’s degree or equivalent work experience
A minimum of 3+ years’ experience in Microsoft Windows/ SQL Server Technologies, .Net development, AWS Administration.
Experience working on 24x7 environments oriented towards a zero downtime target.
Working knowledge or previous administration of SQL 2016-SQL 2022 and Windows Server 2012+ preferred.
Ability to work with minimal direction, in a team environment.
Performance tuning for AWS/DataLake systems.
Some Experience with SQL in virtual, physical and cloud-based environments.
Experience with Athena and data modeling for cloud technologies.
Proven ability to quickly learn and implement new technologies.
Experience with Administration, Security/Identity Management and Terraforms in AWS.
Preferred Qualifications:
Experience with SentryOne, a plus.
Ability to code Powershell commands and maintain code in GitHub, a plus.
Some Experience with Metabase and Collibra, a plus.
Experience with ETL in AWS, a plus.
Pay Range:
$60-$68/hr
Requisition Disclaimer:
This job posting is for a temporary role as an employee of Atrium on assignment at Cox. The individual selected for this role will be offered the role as an employee of Atrium; compensation, medical benefits, fringe benefits and other terms and conditions of employment shall be presented by Atrium upon offer. The pay rate range provided is a reasonable estimate of the anticipated compensation range for this job at the time of posting. The actual pay rate will be based on a number of factors, including skills, competencies, experience, location and/or being pursued and other job-related factors permitted by law. In addition, this role will be eligible for overtime pay, in accordance with federal and state requirements

By applying for this position you agree to the Atrium Terms and Conditions. Agreeing to these terms, includes permission to use the email address and mobile phone number you provide during the application process or throughout the duration of your prospective or actual employees to notify you of job openings, profiles, articles, news, and other employment-related information, as well as to notify you of special promotions or additional products and services offered by us or our affiliates and partners (collectively, “Atrium Alerts”). Atrium Alerts may be sent by email, phone or text message. Your personal information will be safely stored in our database. Atrium does not sell your personal information to third parties. Text message and data rates may apply. To OPT OUT of text messaging or to modify your communication preferences for Atrium Alerts at any time, please contact us at privacyadministrator@atriumstaff.com.

If you do not agree with the Atrium Terms and Conditions, you can still complete your application for this position by emailing your resume to our team at coxrecruit@atriumworks.com. Please include the job title in the subject of your email.

As a woman-owned firm, Atrium values diversity. We are an equal opportunity employer and will consider all applications without regard to race, sex, age, color, religion, national origin, veteran status, disability, genetic information or any other characteristic protected by law. We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.
Posting: #zip",$64.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Proits Hub LLC,#N/A,Remote,Data Analytics Engineer,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote
Speak with the employer
+91 9417599933",$60.00 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
