company,company_rating,location,job_title,job_description,salary_estimate,company_size,company_type,company_sector,company_industry,company_founded,company_revenue
"Wallero
4.4",4.4,"Seattle, WA",Data Engineer,"Hi,
Pretty much interested in your resume, we have an urgent opening for Data Engineer role with one of our clients based in Seattle WA.
Position: Data Engineer
Location: Seattle WA - Remote (100%)
Description:
Good Experience in reporting space including PowerBI, ADF, and possibly standing up a data warehouse/data mart.
The main scope centering is around reporting.
Experience with SQL, and agile framework including requirement gathering and story/feature writing skills.
Need someone strong who is comfortable leading and guiding teams in that capacity.
Migrating data from third party to Azure Cloud
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Seattle, WA 98101: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",$75.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2015,$1 to $5 million (USD)
"Codinix Technologies
5.0",5.0,"Boston, MA",Cloud Data Engineer - AWS,"Only US Citizen
Strong in warehouse technologies (Hadoop, Spark, AWS, etc.)
Bachelor's degree in Computer Science, Information Systems, or another related field
5+ years of data engineering, and cloud engineering experience
Experience in Agile project methodologies and DevOps
Experience with AWS (certification preferred; including Athena, Redshift, Glue), Cloudera
Experience building a CI/CD pipeline to deploy applications on AWS (GitLab, Terraform)
Advanced experience programming and scripting on Unix/Linux (i.e. Python, Bash)
Experience with CTRL-M, Cron (other) scheduling of batch jobs
Experience migrating legacy systems to cloud-ready architectures
Job Type: Contract
Pay: Up to $75.00 per hour
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Boston, MA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",$75.00 /hr (est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Synovize
5.0",5.0,Remote,Big Data Engineer,"Synovize is a cutting-edge technology company that specializes in providing advanced data analytics and intelligence solutions. Our mission is to empower businesses with the tools and insights they need to make data-driven decisions and drive innovation. We are committed to delivering exceptional results by combining our expertise in big data technologies with our passion for creating impactful solutions.
As a Big Data Engineer at Synovize, you will have the opportunity to work on exciting projects that involve designing and implementing scalable data solutions. You will collaborate with a talented team of professionals, including software developers, data scientists, and domain experts, to tackle complex data engineering challenges and deliver high-quality solutions to our clients.
Job Title:
Big Data Engineer
Company: Synovize
Number of Positions: 2
Location: Remote
Length: 12 Months
Tax Terms: 1099/C2C or W-2
Rate/Range: $70-$90/hr
Work Authorization: US Citizen
Security Clearance: None
Skills Needed:
Proficiency in Java and Kotlin programming languages.
Strong knowledge of AWS (Amazon Web Services) and its big data offerings.
Experience with BigData technologies, frameworks, and tools.
Familiarity with Kafka for streaming data processing.
Proficiency in Python programming language.
Responsibilities:
Design and develop scalable data pipelines to process, integrate, and analyze large volumes of structured and unstructured data.
Implement data ingestion, transformation, and storage mechanisms using big data technologies and frameworks.
Collaborate with data scientists and analysts to understand data requirements and optimize data models for analytics and machine learning.
Utilize AWS services and tools to build and maintain data infrastructure, ensuring data quality, integrity, and security.
Troubleshoot and resolve data-related issues, optimizing performance and efficiency.
Stay up-to-date with emerging trends and advancements in big data technologies, recommending and implementing improvements to existing systems and processes.
Requirements:
Bachelor's or master's degree in computer science, engineering, or a related field.
Proven experience as a Big Data Engineer or similar role, with a strong understanding of big data technologies and architectures.
Proficiency in Java and Kotlin programming languages.
Strong knowledge of AWS and its big data offerings.
Experience with BigData technologies, frameworks, and tools.
Familiarity with Kafka for streaming data processing.
Proficiency in Python programming language.
Strong problem-solving skills and ability to work in a fast-paced, collaborative environment.
Excellent communication and interpersonal skills.
Join Synovize and be part of a dynamic team that is at the forefront of data innovation. Together, we will drive meaningful change and unlock the full potential of data-driven decision-making.
Job Types: Full-time, Contract
Pay: $70.00 - $90.00 per hour
Benefits:
401(k)
Dental insurance
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Vision insurance
Schedule:
10 hour shift
4 hour shift
8 hour shift
Choose your own hours
Application Question(s):
Are you a US Citizen?
Years of experience in Java?
Years of experience in Kotlin?
Years of experience in big data?
Years of experience in AWS?
Security Clearance
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$80.00 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Maven Workforce
4.1",4.1,"McLean, VA",Data Engineer,"MUST HAVES: Hands-on experience with Informatica PowerCenter and/or Informatica Intelligent Cloud Services (IICS), Cloud Data Integration (CDI) , Cloud AWS,UNIX scripting, and any other ETL tools.
Job responsibilities: –
The candidates will be having the IICS module of Informatica more than any other platform modules.
Strong hands-on experience building ETL pipelines and Data Replication
3+ yrs of experience with IICS is required.
Must be hand Ons with AWS services like ECS,S3,EKS etc.
Cloud Data Integration is also required.
Data Engineering Background experience with Spark, Hive is also required.
Preferred:
SQL is preferred, but basic knowledge is required
Job Type: Contract
Salary: $60.00 per hour
Ability to commute/relocate:
Mclean, VA 22101: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica PowerCenter: 7 years (Required)
Informatica Intelligent Cloud Services (IICS): 7 years (Required)
Cloud Data Integration (CDI): 7 years (Required)
UNIX scripting: 7 years (Required)
ETL tools: 7 years (Required)
Work Location: One location
Speak with the employer
+91 7328456015",$60.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2008,Unknown / Non-Applicable
"Kaizen Analytix
3.9",3.9,"Dallas, TX",Cloud Data Engineer: AWS,"Cloud Data Engineer: AWS
Kaizen Analytix LLC, an analytics services company, is seeking a qualified Cloud Data Engineer Contractor to support data science or data analytics projects. Responsibilities, capabilities for success, and job requirements are below. This role requires an estimated 40 hours/week commitment for approximately 24 weeks.
Responsibilities:
Candidate will be responsible for supporting a data science or data analytics project.
Work with client IT to understand access and use of source-of-truth data systems and downstream data sources.
Work with data scientist or visualization experts to determine data requirements such as structure, latency, availability.
Work with IT Security to assess risk of using sensitive data and how these data can be used throughout the project life cycle.
Develop an as-is and to-be state for data architecture.
Capabilities for Success:
In addition to key responsibilities, Candidate should demonstrate an ability to…
Work with the project manager and the business stakeholders to define upstream requirements for data.
Contribute to estimating input and time required for data engineering development tasks.
Contribute to client demonstrations of solution or presentations on architecture.
Proactively raise risks and issues to internal project manager
Work with data scientist or analysts to proactively assist in data concerns
Job Requirements:
Education: Bachelor's degree required
Extensive Experience in Data Engineering – consuming, wrangling, validating, developing pipelines for data.
Extensive Experience with AWS
Must have Solutions Architect Certification
Strong knowledge of databases, ETL concepts, SQL, and NoSQL sources.
Experience in: Python (data packages such as Pandas, PySpark, SQL Alchemy, pydantic, etc.) and SQL.
Position Type: 1099 Contractor",$57.50 /hr (est.),1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,#N/A,Unknown / Non-Applicable
Kanini,#N/A,Remote,Data Engineer,"About Kanini
Kanini provides Agile Software Development, Cloud Computing, Data Science, and Location Intelligence services to public and private organizations. We have successfully served our clients in government, finance, transportation, utility, and software industries since 2003.
Why you should join
Working at Kanini is flexible and personal. We are a highly motivated, collaborative team experimenting with the latest technologies. We are committed to everyone having a healthy work/life balance, and we provide extensive mentorship and training resources to help you succeed.
Kanini is looking for a Big Data Engineer who has a deep experience in Data Engineering, AWS, Python, Data Lakes.
Required Skills
At least 10 years software development experience
At least 5 years leading at least one Scrum team of data engineers building data-intensive products with a modern tech stack.
Significant experience with big data ETL pipeline development with Spark, Hive, and related technologies
Significant experience with a general-purpose programming language such as Python, Scala, or Java
Experience with Spark framework and related tools (PySpark, Scala, SparkR, Spark SQL, Spark UI)
Experience with Hadoop ecosystem using HDFS, ADLS Gen2, or AWS S3
Experience with data visualization development using Python, Tableau, or PowerBI
Experience with Azure, AWS or GCP
Solid understanding of performance tuning concepts for relational and distributed database systems
Familiarity with distributed programming, big data concepts, and cloud computing
Education Qualifications
Bachelor’s degree in computer science/Engineering or Technology related field or possess equivalent work experience.
Preferred Qualifications
Cloud certifications from Azure, AWS or GCP
Big data, data engineering or data science certifications from recognized vendors such as Databricks & Cloudera
Kanini Software Solutions, Inc. does not discriminate in employment matters based on race, gender, religion, age, national origin, citizenship, veteran status, family status, disability status, or any other protected class. We support workplace diversity. If you have a disability, please let us know if there is anything we can do to improve the interview process for you; we’re happy to accommodate.
Kanini Software Solutions, Inc., 25 Century Blvd., Ste. 602, Nashville, TN 37214
Job Types: Full-time, Contract
Pay: $60.00 - $70.00 per hour
Work Location: Remote",$65.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Provista Software Corporation,#N/A,Remote,Data Engineer,"As a successful candidate, you’ll play a pivotal role in the data conversion and business intelligence solution efforts during project implementation for our state and local clients. We are looking for you to join our software implementation team as a Senior Data Engineer.
Qualification:
5+ years of experience working independently as ETL developer on data migration projects that covered diverse database platforms and complex data conversions.
Must have experience across leading Database Platforms especially Oracle, Azure SQL, Snowflake.
Expert level experience with Extraction Transform Load (ETL) plan, protocols, and tools.
Expert level experience with programming languages SQL, Phyton, Java, C#.
Good at communication with clients and internally.
Expert at problem solving and troubleshooting skills.
Bachelor's degree, preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience
Must be a U.S. Citizen or Green Card Holder.
Functional Responsibility:
Data Conversion -
In collaboration with Data Stewards, discover inventory of legacy data sources and all data the client expects to be migrated to destination Modified Off the Shelf (MOTS) solution.
Complete gap assessment between legacy data and MOTS data requirements, and options to address the gaps.
Develop data migration map to minimally include data dictionary, detailed data map of all elements of the current database, data transformation rules, relationship rules, validation rules, process of migrating images, and process for recovering missing or erroneous data.
Identify issues, risks, and/or barriers that may interfere with data migration and recommendations to resolve or mitigate.
Conduct testing and quality assurance to ensure the data is migrated accurately.
Migrate data from legacy to destination MOTS solution progressively using a sample set of data and the entirety of the data prior to deployment into relevant go-live environments (e.g., staging, production, etc.)
Prepare data migration reports that includes – record counts of source vs migrated vs modified; field level validation of data at row and column; discovered exceptions.
Business Intelligence -
Develop ETL pipelines in and out of Snowflake data warehouse using combination of Python, SnowSQL, and SQL.
In collaboration with Data Stewards, create Snowflake data mart as curated subset of data to support the analytics and business intelligence needs of subgroup of users.
Translate requirements for business intelligence and reporting to database design and reporting design.
Conduct discovery sessions with users to capture business intelligence reports.
Develop Tableau self-service ad hoc reports to support operational users.
Develop Tableau Dashboards to support executive, management, and operational users.
About us:
Provista Software Corporation is fast growing Software Solution provider for US State and Local Government Clients.
Job Type: Full-time
Pay: $67,107.98 - $134,285.66 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Life insurance
Paid time off
Parental leave
Professional development assistance
Vision insurance
Compensation package:
Yearly pay
Experience level:
5 years
Schedule:
8 hour shift
Application Question(s):
What is your most preferred work time zone ?
Experience:
ETL: 5 years (Required)
Data warehouse: 1 year (Preferred)
Tableau: 1 year (Preferred)
Talend: 1 year (Preferred)
Microsoft SQL Server: 5 years (Required)
Work Location: Remote","$100,697 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Spar Information Systems
3.7",3.7,Remote,Sr Data Engineer- (Visa Sponsorship Not Available),"Sr Data Engineer- (Visa Sponsorship Not Available)
Location: Remote
Duration: 3 Months Contract to hire
Required Skills:
10+ years of experience in data software development, programming languages and developing with big data technologies
2+ years of experience designing and building on existing and new data applications
2+ years of experience in Cloud DevOps concepts, Cloud Services and Architecture, and Azure/AWS/GCP DevOps Operational Framework
1+ years of experience in open-source data tools and frameworks, or one of the following: .net Core, asp.Net, Angular, or Express.
Experience in data software development, using data technologies such as Relational & NoSQL databases, open data formats, and programming languages such as Python, Scala, and/or other frameworks, building data pipelines (ETL and ELT) with batch or streaming ingestion, error handling, loading, and transforming data, and developing with big data technologies such as Spark, Hadoop, and MapReduce. Experience with analytics solutions.
Experience in development using Python or PySpark, Spark, Scala.
Advanced understanding of designing and building for data quality assurance, reliability, availability, and scalability, on existing and new data applications.
Advanced understanding of DevOps Concepts, Cloud Architecture, and Azure DevOps Operational Framework, Pipelines, Kubernetes.
Advanced understanding of designing and building solutions for data quality and observability, metadata management, data lineage, and data discovery.
Advanced understanding of building products of micro-services oriented architecture and extensible REST APIs.
Advanced understanding of open-source frameworks.
Experience with continuous delivery and infrastructure as code.
Experience in existing Monitoring Portals: Splunk or Application Insights.
Advanced understanding of Security Protocols & Products: Understanding of Active Directory, Windows Authentication, SAML, OAuth.
Advanced understanding of Azure Network (Subscription, Security zoning, etc) & tools like Genesis.
Advanced understanding of existing Operational Portals such as Azure Portal.
Knowledge of CS data structures and algorithms.
Knowledge of developer tooling across the software development life cycle (task management, source code, building, deployment, operations, real-time communication).
Practical knowledge of working in an Agile environment (Scrum/Kanban/SAFe).
Strong problem-solving ability.
Ability to excel in a fast-paced, startup-like environment.
Bachelor’s degree in Computer Science, Information Systems, or equivalent education or work experience.
Job Types: Full-time, Contract
Pay: $60.00 - $70.00 per hour
Schedule:
8 hour shift
Experience:
Data Engineer: 10 years (Preferred)
Cloud: 3 years (Required)
Work Location: Remote",$65.00 /hr (est.),Unknown,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Zenotis Technologies inc,#N/A,"Addison, TX",Big Data Engineer,"Role: Big Data Engineer
Location – Addison, TX/ Charlotte, NC / Chandler, AZ (Hybrid – 3 days onsite 2 days remote)
Position Summary:
Looking for a big data engineer who loves solving complex problems across a full spectrum of technologies. The ideal candidate is excited by experimentation and looking for a new challenge that stretches their talents. The big data engineer will help ensure that our technological infrastructure operates seamlessly in support of business objectives.
The engineer must be able to:
· Translate complex technical and functional requirements into detailed designs
· Guide the development and testing teams in developing and implement pipelines that extract, transform, and load data into an information product that helps the organization reach its strategic goals
· Focus on ingesting, storing, processing, and analyzing large datasets
· Deep understanding on how to create scalable, high-performance web services for tracking data
· Investigate alternatives for data storing and processing to ensure the implementation of the most streamlined solutions
Primary Skill
Hadoop
Secondary Skill
Kafka
Tertiary Skill
Oracle Exadata
Required Skills
· Translate complex technical and functional requirements into detailed designs
· Design data pipelines using design tools and provide solution intent in provided templates
· Work closely with data science team to implement data analytics pipelines
· Analyze vast number of data stores to uncover insights
· Define data governance policies and support data-versioning processes
· Maintain security and data privacy, working closely with data protection guidelines.
· Desired Skills
· Experience with Python, Spark, and Hive
· Understanding of data-warehousing and data-modeling techniques
· Knowledge of industry-wide visualization and analytics tools (ex: Tableau, R)
· Strong data engineering skills with Hadoop platform
· Experience with streaming frameworks such as Kafka
· Knowledge of Core Java, Linux, SQL, and any scripting language
· Good interpersonal skills and positive attitude
Job Type: Full-time
Salary: Up to $70.00 per hour
Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
8 hour shift
Work Location: On the road",$70.00 /hr (est.),201 to 500 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"PRIMUS Global Services, Inc
4.1",4.1,"Minneapolis, MN","Data Engineer – Hadoop, Python – REMOTE WORK 43198","We have an immediate long-term opportunity with one of our key clients for a position of Data Engineer, to work on a remote basis.

The ideal applicant must have extensive experience with Python, Hadoop, Hive and SQL.

**ALL successful candidates for this position are required to work directly for PRIMUS. No agencies please**

For immediate consideration, please contact:

Tejaswini
PRIMUS Global Services
Direct: 972-798-2662
Desk: 972-753-6500 Ext: 204
Email: jobs@primusglobal.com","$91,633 /yr (est.)",501 to 1000 Employees,Private Practice / Firm,Information Technology,Information Technology Support Services,2002,$5 to $25 million (USD)
IT Customer Service - POST,#N/A,"Newark, NJ",Data Center Plant Engineer,"Job Title: ISP-OSP Data Center Plant Engineer
Location: Newark NJ, (M-F)
Travel Required: As Needed
Overview
UC-ISP/OSP engineer is responsible for cable management including structured cabling, carrier data and voice circuit cables/equipment, rack-and-stack, and oversight of power, backup power, network equipment, and environmental needs to cabinets. Leveraging Clients Data Center Infrastructure Management System, (DCIM) tool to maintain, report and monitor.
Conducting capacity assessments of existing infrastructure to ensure that it can support future growth to ensure compliance with federal regulations of the Data Center infrastructure Standards. Projects include design and build outs of the physical plant within the typical WAN/LAN space within data centers, telco rooms, MDFs and IDFs as needed.
Functions:
Perform initial engineering surveys based on project needs.
Document engineering surveys to determine feasibility and the estimated cost.
Obtain authorization to proceed and confirm funding.
Create detailed engineering drawings of build, floor plans, equipment elevation drawings, structured cabling diagrams, power, and environmental needs.
Designs must conform to state, local, national, and Client standards and codes, and the NJ historical and or environmental commissions approvals if needed.
Based on the engineering documents create a SOW (Scope of Work) and BOM (Bill of Materials)
Submit to Client manager to approve and proceed when funding is validated.
Create a construction package and submit to contractors or in house for RFQ.
Review quotes and choose both low bid and qualified contractor.
Work with all stakeholders to date and time work schedules.
Confirm workers are safety trained and conform to Client, FRA, and OSHA requirements.
Supervise and manage installations and conduct inspections.
Review work to ensure contractor met obligations under terms of contract.
Inform Client managers of status to approve invoices.
Coordinate with Client network engineering to connect, configure, and test equipment.
Review and confirm a successful installation were completed and working as designed prior to cutting over to production.
Assist with Break fix of IT infrastructure troubles or repairs.
Daily travel to location depending on scheduled or unscheduled work
Job Types: Full-time, Contract
Experience level:
5 years
Schedule:
8 hour shift
Work setting:
In-person
Experience:
Data Center: 5 years (Required)
WAN/LAN: 3 years (Required)
cabling: 3 years (Required)
Work Location: One location",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Southern Glazer’s Wine and Spirits
3.6",3.6,"Dallas, TX",Data Engineer,"What You Need To Know
Open the door to a groundbreaking tech career with an industry leader. Southern Glazer’s Wine & Spirits is North America’s preeminent wine and spirits distributor, as well as a family-owned, privately held company with a 50+ year legacy of success. To create a new era in alcohol beverage sales and service, we’re heavily invested in the most transformative new technologies – and the most brilliant tech professionals. Southern Glazer’s was named by Newsweek as a Most Loved Workplace and is included on the Forbes lists for Largest Private Companies and Best Employers for Diversity.
As a full-time employee, you can choose from a full menu of our Top Shelf Benefits, including comprehensive medical and prescription drug coverage, dental and vision plans, tax-saving Flexible Spending Accounts, disability coverage, life insurance plans, and a 401(k) plan. We also offer tuition reimbursement, a wellness program, parental leave, vacation accrual, paid sick leave, and more.
We offer continuous learning and career growth in a fast-paced environment where you are respected, your voice is heard, and technology is part of our strategy for success. If you’re looking to fill your glass with opportunity, come join our FAMILY.
Overview
The Data Engineer's role is to design, develop, maintain and enhance interfaces and connectivity to the Data Warehouse ecosystem by coding with a technical language to meet business requirements and business objectives. This can include taking technical specifications and developing an application or integration of data between applications, testing, as well as, completing the appropriate technical documentation. The Data Engineer will use best practices in software development and adhere to SGWS development standards, as well as, focus on quality and innovation. The Data Engineer may also be responsible for delivering support to end users in the organization for specific code, including troubleshooting code.
Specialized Skills and Technologies
Strong PL/SQL skills
Experience in ETL Tools (Preferrable Informatica)
Data Warehouse techniques will be a plus
Experience in cloud platforms like Azure or AWS will be a plus
Knowledge of UNIX/Linux, shell scripting, Python will be a plus
Experience developing Application Programming Interfaces (API's) will be a plus
Experience in Hadoop will be a plus
Primary Responsibilities
Design, develop, implement, and support software applications
Drive technical validity of solution.
Develop user documentation as well as in-code documentation to explain designs and participate/support user training
Structure requirements to facilitate automation of acceptance tests
In conjunction with Data Management Group, develop routines and procedures that provide data quality checks and balances on data delivery/ingestion
Collaborate across the BI / Analytics, Data Management Group, Enterprise Insights and Analytics teams to establish standards, reusable data models and best practices for delivery/ingestion of data from/to Data Warehouse - This includes Publish/Subscription and API options
Obtain any certifications needed to effectively support applications in scope
Support the development of business and technical process documentation and training materials
Structure requirements to facilitate automation of acceptance tests
Provide support for software applications under area of responsibility
Drive Behavior-Driven-Design (BDD) process
Perform other job-related duties as assigned
Minimum Qualifications
Bachelor’s Degree or a combination of work experience and education
Knowledge in application and software development
Knowledge of software design and programming principles
Proficient oral and written communication skills, ability to influence outcomes, and strong attention to detail
Strong analytical, mathematic, and problem-solving skills
Strong team player with ability to demonstrate Agile delivery values working both within a team and working independently
Strategic thinker – can develop a plan to meet a long-term objective
Agile Delivery Values
Openness – Team and stakeholders agree to be open about all work and challenges
Commitment – Personally commit to achieving the goals of the team
Respect – Respect your team members to be capable and independent
Courage – You have courage to do the right thing and work on tough problems
Focus – Everyone focus on the work in the sprint and the goal of the scrum team. Rise and fall as a team
Physical Demands
Physical demands include a considerable amount of time sitting and typing/keyboarding, using a computer (e.g., keyboard, mouse, and monitor), or mobile device
Physical demands with activity or condition may occasionally include walking, bending, reaching, standing, squatting, and stooping
May require occasional lifting/lowering, pushing, carrying, or pulling up to 20lbs
EEO Statement
Southern Glazer's Wine and Spirits, an Affirmative Action/EEO employer, prohibits discrimination and harassment of any type and provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. Southern Glazer's Wine and Spirits provides competitive compensation based on estimated performance level consistent with the past relevant experience, knowledge, skills, abilities and education of employees. Unless otherwise expressly stated, any pay ranges posted here are estimates from outside of Southern Glazer's Wine and Spirits and do not reflect Southern Glazer's pay bands or ranges.","$97,837 /yr (est.)",10000+ Employees,Company - Private,Retail & Wholesale,Wholesale,1968,$10+ billion (USD)
Breadboard,#N/A,"New York, NY",Data Engineer,"About Us
We’re on a mission to revolutionize the global electronics supply chain.
During the pandemic we saw how the inefficiencies in electronics manufacturing can have massive global impact. From over $200B lost in car manufacturing to delays on laptops, gaming systems and much more.
At Breadboard we are rethinking the electronics supply chain down to the finest detail. We’re designing the new operating system in which this entire system will run. We’ve already secured seed funding and are now looking to expand our engineering team to help us set the foundation for the future.
Why work with us?
As a part of our team, you will have the unique opportunity to design and implement the data platform for Breadboard, a critical component in our business.
As one of the first data engineers to join the Breadboard team you will be partnering with the CTO, our founding engineers, and will set the foundation for the design of the operating system for the electronics supply chain.
You will be involved in all aspects of the software development process, architectural decisions, recruiting, and building the organization in general.
We’re a tight-knit, intense team and you will have a ton of end to end ownership.
Work with seasoned founders who have had successful exits.
This is not only a unique opportunity for someone who wants to get their hands dirty, but also take that step back and collaborate with the executive team on setting a strategic future.
You’ll be responsible for
Data Management: Develop and automate robust data acquisition, processing, and normalization pipelines from various sources to ensure data quality and consistency.
Database and ETL Architecture: Design, implement, and optimize a database system for efficient data storage, retrieval, and analysis.
Collaboration on Machine Learning: The prospects of working with data scientists to leverage extracted data features, contributing to the development and refinement of a machine learning model for component matching.
API and UI Integration: Collaborate with the software development team to facilitate seamless integration between the database, machine learning models, and user interface.
Continuous Improvement and Compliance: Monitor system performance and user feedback for continuous improvements, while ensuring adherence to data privacy and security standards.

Requirements
Experience: A minimum of 3-5 years of experience in a data engineering role, preferably in the electronics, manufacturing industry or supply chain industry.
Technical Skills: Proficiency in SQL and experience with database design, data modeling, and performance optimization. Strong coding skills in a major programming language, such as Python or TypeScript. Experience with data processing tools and libraries (e.g., pandas, Spark). Experience with pipeline and workflow orchestration tools, such Airflow. Experience with Snowflake or similar cloud-based data warehousing solutions.
Data Acquisition and Processing: Experience with data acquisition methods, such as APIs and web scraping. Familiarity with data cleaning, normalization, and feature extraction techniques.
Machine Learning: Familiarity with machine learning concepts and algorithms, and experience collaborating with data scientists or machine learning engineers.
API Development: Experience with API development and integration, preferably in a microservices architecture.
Data Governance: Knowledge of data privacy and security regulations, and experience implementing data governance measures.
Bonus Points
Entrepreneurial mindset (we encourage all employees to be future founders and this can be a great stepping stone towards that goal).
Customer-centric and passionate about helping small businesses grow.
Previous experience at a high-growth, fast-paced startup.
Experience with our stack (and some others that may be included shortly):
React, Material UI, Typescript, CSS
NestJS, GraphQL, TypeORM, Knex.js, Objection.js, PostgreSQL, gRPC, Kafka
AWS, Docker, Kubernetes, Helm, Github Actions, CI/CD pipelines.
Compensation + Benefits
Competitive cash compensation
Stock ownership at a fast growing company
Health/dental/vision insurance—100% coverage option
The ability to push for other benefits you think are important
Our Engineering Culture
Openness to diverse opinions and backgrounds - Testing our assumptions
Supported career growth with a high emphasis on learning and mentorship
Lean development
End to end ownership and accountability. We’re all going to make mistakes. It’s about how you own up to them and learn from them.

Equal Opportunity Employer
Breadboard Software does not discriminate in employment on the basis of race, color, religion, sex (including pregnancy and gender identity), national origin, political affiliation, sexual orientation, marital status, disability, genetic information, age, membership in an employee organization, retaliation, parental status, military service, or other non-merit factor.",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Pallidus, Inc
3.7",3.7,Remote,Data Engineer,"Pallidus, Inc. is a fast-growing producer of large diameter Silicon Carbide (SiC) wafers for high power and high frequency semiconductor applications in transportation, green energy, telecommunications, and Industrial markets. Pallidus’ M-SiC™ technology delivers step change improvements in SiC quality and cost effectiveness to drive wafer to device yield performance. Pallidus is a high energy, technology driven and innovative startup company looking for like-minded people to join our team.

SCOPE
Pallidus is looking for a Data Engineer to deliver business value through the design and integration of information models that originate from multiple data sources. You will be responsible for expanding and optimizing our data and data pipeline architecture and data flow and collection for cross-functional teams. The Data Engineer will be part of the growing the data science team and be responsible for the day-to-day activities related to the implementation of new services and support for existing services.

RESPONSIBILITIES
Partner with key business stakeholders to create blueprints for data solutions & services that align with overall corporate objectives and support enterprise scale reporting capabilities
Translate simple to complex user requirements into functional and actionable analytics
Provide guidance on architecture, data-store selection, data modeling, and query optimization for existing and future applications
Execute the design, development and support of data solutions including configuration, administration, monitoring, performance tuning, and debugging for efficient data handling & analytical reporting
Ensure an appropriate infrastructure for optimal extraction, transformation, and loading of data from a wide variety of data sources into cloud computing platforms such as Snowflake and Databricks
Support data quality issue resolution while managing SLAs for services owned
Participate in the development lifecycle using Agile / DevOps methodologies and support testing & deployment as part of the full release cycle.
Become an internal subject matter expert on various datasets and support other departments on usage of those datasets
Collaborate with applications teams and business partners to assist in developing data governance framework & data retention policies
REQUIREMENTS
Bachelor's degree in computer science, engineering, or related field of study
Minimum of 5 years of experience in a data engineering role working with diverse data sets ideally with modern data warehousing/data lake technologies
A successful history of ETL manipulating/processing/extracting value from large disconnected and diverse datasets
Comfortable building pipelines with industry best practices to load huge volumes of data into a Data Warehouse
Strong analytic skills related to working with unstructured datasets
Experience with relational SQL and NoSQL databases, and capable of writing complex queries in SQL, spark, pandas, etc.
Knowledge of data modeling, star schema, incremental load
Ability to quickly build effective working relationships across the organization
Solid software engineering skills and advanced knowledge of at least one language, preferably Python. Appreciation of and dedication to clean coding principles is important
Effective communication skills, including the ability to articulate complex technical requirements to business and end-users in a clear and non-technical manner
Must be a US Citizen or be a lawful permanent resident of the U.S. to meet Export Control requirements","$135,000 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Wisetek Providers, Inc
4.2",4.2,Remote,Principle Data Engineer,"** We do transfer / Sponsor visa
Role: Principle Data Engineer Duration: 6 months.
Looking for Senior candidates with 10+ years of experience. .
Required 3-5 years of experience with the following technologies:
o Hadoop, Python, Hive, SQL, Shell scripting.
o Apache Spark.
o NoSQL and relational databases.
Required 1-3 years of experience with the following technologies:
o Scheduling tools like Airflow / Tivoli Work scheduler.
o Working in Agile/Scrum environment.
o Jenkins or similar CICD tool, GitHub.
Preferred 2-5 years of experience with the following technologies:
o Apache Kafka.
o API’s.
o Kubernetes.
Job Types: Full-time, Contract
Pay: $80,000.00 - $105,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Monday to Friday
COVID-19 considerations:
Yes
Experience:
Hadoop / Python: 5 years (Required)
Jenkins / CICD: 1 year (Required)
SQL: 5 years (Required)
Apache/ Spark: 5 years (Required)
Work Location: Remote","$92,500 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$1 to $5 million (USD)
"WorkCog
4.3",4.3,"Atlanta, GA",Sr. AWS Data Engineer,"Position: AWS Data Engineer
Location: Atlanta ,GA
Duration: 12+ Months
Interview: Video
JD
Responsibilities
Bachelor’s degree in Computer Science, Software Engineering, or other related field, or equivalent work experience.
Extensive experience in data modeling, writing & performance tuning complex SQL queries & user-defined functions
Hands-on experience in design, implementation and orchestration of data pipelines and ETL batch Jobs.
Hands-on experience on all aspects of data handling: cleansing and standardization, normalization, merging, change data capture, data integration, reconciliation, and transformations
Strong experience in working with Big Data technologies such as Hive/Spark, Data lake and Data warehouse using GCP and/or AWS Services
Ability to write robust code in scripting languages like Python, Scala, PySpark or similar.
Good to Have
Associate Architect or equivalent cloud certification
Experience with processing large datasets using AWS and GCP UDFs (User defined functions)
Experience in ML probability & statistics algorithms, frameworks like Keras or PyTorch and tools like Jupyter notebooks.
Job Type: Contract
Pay: $45.83 - $52.42 per hour
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Atlanta, GA: Reliably commute or planning to relocate before starting work (Preferred)
Experience:
SQL (Preferred)
Informatica (Preferred)
Data warehouse (Preferred)
Work Location: In person",$49.13 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable
"FACEBOOK APP
5.0",5.0,Remote,Azure Data Engineer,"Responsibilities:
Create ER diagrams and write relational database queries
Create database objects and maintain referential integrity
Configure, deploy and maintain database
Participate in development and maintenance of Data warehouses
Design, develop and deploy SSIS packages
Creating and deploying reports
Provide technical design, coding assistance to the team to accomplish the project deliverables as planned/scoped.
Ability to talk to client and get the Business Requirements
Skills:
Azure Data Factory
Azure Devops
Azure Storage/ Data Lake
Extraction, Transformation and Loading
Analytics development
Report Development
Relational database and SQL language
Other Requirements:
· Should be well versed with Data Structures & algorithms
· Understanding of software development lifecycle
· Excellent analytical and problem-solving skills.
· Ability to work independently as a self-starter, and within a team environment.
· Good Communication skills- Written and Verbal
Job Type: Full-time
Salary: $84,454.31 - $190,806.62 per year
Benefits:
Flexible schedule
Health insurance
Compensation package:
1099 contract
Yearly pay
Experience level:
10 years
9 years
Schedule:
Day shift
Experience:
Azure Data engineer: 9 years (Preferred)
SQL: 9 years (Preferred)
Data warehouse: 10 years (Preferred)
Work Location: Remote","$137,630 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Urbane Systems LLC,#N/A,"Washington, DC",Senior Cloud Data Engineer,"Seeking a Cloud Data Engineer that will assist in maintaining and monitoring infrastructure as well as build or assist in building data transformation pipelines.
Candidate must have prior experience with AWS or Azure and extensive knowledge of python for ETL development. Additional Cloud-based tools experience is important (see skills section)
Additional desired skills include experience with the following:
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Extensive experience leveraging Python to build data transformation pipelines (ETL) and experience with libraries such as pandas and NumPy.
Experience with pyspark.
Experience using AWS Glue and EMR to construct data pipelines
Experience building and optimizing ‘big data’ data pipelines, architectures, and datasets.
Job Type: Contract
Pay: $80.00 - $90.00 per hour
Schedule:
Day shift
Work Location: Hybrid remote in Washington, DC 20001",$85.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Ripple Effect Consulting
5.0",5.0,"Denver, CO",Data Engineer - 004/005,"Data Engineer
Hybrid-remote based in Denver, CO | $80-$160k
Job Overview:
We are looking for a savvy Data Engineer to join our growing team of analytics experts. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
Responsibilities for Data Engineer
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Qualifications for Data Engineer
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:
1) Experience with big data tools
2) Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
3) Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
4) Experience with cloud services
5) Experience with stream-processing systems: Storm, Spark-Streaming, etc.
6) Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Job Type: Full-time
Pay: $80,000.00 - $160,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Denver, CO: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person","$120,000 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"DiamondPick
4.5",4.5,"Edison, NJ",Senior Data Engineer,"Hi ,
Greetings from Diamond pick inc.
We are currently looking for the below position for one of our clients... Please let me know your interest along with your updated resume ASAP..
Role:Data engineer
Location: Berkley Heights, NJ(Onsite)(locals only)
9+ years of experience is must
Description
Skills: strong Java,Azure,Spark & sql
Company Description:
Apexon is a digital-first technology services firm backed by Goldman Sachs Asset Management and Everstone Capital. We specialize in accelerating business transformation and delivering human-centric digital experiences. For over 17 years, Apexon has been meeting customers wherever they are in the digital lifecycle and helping them outperform their competition through speed and innovation.
RESPONSIBILITIES
Basic Qualifications for consideration:
5+ Overall industry experience
3+ years' experience with building large scale big data applications development
Bachelors in Computer Science or related field
Provide technical leadership in developing data solutions and building frameworks
Experience building Data Lake, data applications and micro-services using Azure, AWS and Hands-on experience in cloud Data stack (preference is Azure)
Hands-on experience with Springboot framework, MVC architecture, Hibernate, API creation, Restful services, SOAP using Java
Java experience with OOPS concepts, multithreading
Experience deploying code on containers
Hands on experience with leveraging CI/CD to rapidly build & test application code
Conduct code reviews and strive for improvement in software engineering quality
Review data architecture and develop detailed implementation design
Hands-on experience in production rollout and infrastructure configuration
Demonstrable experience of successfully delivering big data projects using Kafka, Spark
Experience working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search
Experience working with PCI Data and working with data scientists is a plus
In depth knowledge of design principles and patterns
Able to tune big data solutions to improve performance
QualificationsBachelor's Degree in Computer Science or Computer Engineering is required
Job Type: Contract
Salary: $43.96 - $70.65 per hour
Ability to commute/relocate:
Edison, NJ 08817: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL (Required)
Java (Required)
Azure (Required)
Work Location: One location",$57.30 /hr (est.),1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,#N/A,Unknown / Non-Applicable
LandGate Corp,#N/A,Remote,Data Engineer,"LandGate is a fast growing renewable energy and carbon data & analytics platform. We are looking for a Database Engineer with experience using Python and SQL to process data. You will be expected to be a savvy developer who can work tasks from start to finish, applying your in depth experience of data processing using Python and SQL, and be prepared to hit the ground running. Must have excellent attention to detail, communication skills, and willingness to go the extra mile.
Compensation
LandGate offers a very competitive salary (commensurate with experience), commission, bonus, and shares providing ownership in the company. Benefits include 5 weeks PTO, work from home, platinum health/vision/dental insurance 100% paid for employees and dependents, disability/life insurance 100% paid, phone/internet 100% paid, and other benefits.
About LandGate
LandGate is the leading provider of data solutions, and an online marketplace for US commercial land and its resources: solar, wind, carbon, minerals, and water. The company helps investors, developers, real estate agents, and landowners understand energy & environmental resource values and connect on its online marketplace for land-related transactions.
LandGate enables energy and carbon professionals to run economic engineering studies in minutes; access land leads and MLS listings; and manage their leads in a land CRM web app connecting their team. The company opens energy and carbon commission opportunities to real estate agents. LandGate applies its technology to provide the most advanced analytics for renewable energy M&A deals, market & price trends, operators’ benchmark and performance indicators.
Founded in 2016 in Denver, Colorado, LandGate received Series A funding in 2019 from Rice Investment Group, a widely-respected energy technology investor, and Series B funding in 2022 from Nextera Energy, the world’s largest generator of renewable energy, and from Kimmeridge, a leading carbon solutions private equity firm. The company has partnered with the Realtors Land Institute (RLI), the American Association of Professional Landmen (AAPL), and the Texas Engineering Executive Education (TxEEE) from the University of Texas at Austin.
Skills
Experience with Python and SQL (2+ years)
Proficiency in working with PostgreSQL, writing SQL (queries, functions, stored procedures)
Strong math background with ability to apply scientific formulas to large datasets
Experienced using Linux command line
Proven work experience as a Back-end developer
Familiarity with testing and debugging
In-depth understanding of the entire development process (design, development and deployment)
An ability to perform well in a fast-paced environment
Excellent analytical and multitasking skills
BSc degree in Computer Science or relevant field
Job Type: Full-time
Pay: $100,000.00 - $180,000.00 per year
Benefits:
Dental insurance
Health insurance
Life insurance
Paid time off
Retirement plan
Vision insurance
Compensation package:
Performance bonus
Experience level:
4 years
Schedule:
Monday to Friday
Experience:
SQL: 2 years (Required)
Python: 2 years (Required)
Work Location: Remote","$140,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"ProIT Inc.
5.0",5.0,"Bellevue, WA",Azure Data Engineer,"Data Engineering experience primarily on Spark. Someone who has worked on Azure cloud with knowledge on Azure DEVOPS (CI/CD & Infrastructure as a code), ADF, ADW & Power BI. Apache Nifi will be good to have.Power BI: Understand business requirements to set functional specifications for reporting applications Build automated reports and dashboards with the help of Power BI reporting tool Be experienced in tools and systems on MS SQL Server BI Stack, including SSRS and TSQL, Power Query, MDX, PowerBI, and DAX Be able to quickly shape data into reporting and analytics solutions Have knowledge of database fundamentals such as multidimensional database design, relational database design, and more Create functional reporting Study, analyze and understand business requirements in context to business intelligence. Design and map data models to shift raw data into meaningful insights. Utilize Power BI to build interactive and visually appealing dashboards and reports. Spot key performance indicators with apt objectives Run DAX queries and functions in Power BI Should have an edge over making DAX queries in Power BI desktop. Developing visual reports, KPI scorecards, and dashboards using Power BI desktop. Connecting data sources, importing data, and transforming data for Business intelligence.
Spark ,Azure DEVOPS (CI/CD & Infrastructure as a code), ADF, ADW & Power
Job Type: Full-time
Pay: $100,154.64 - $104,132.47 per year
Ability to commute/relocate:
Bellevue, WA 98004: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Hybrid remote in Bellevue, WA 98004","$102,144 /yr (est.)",51 to 200 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Purple Drive Technologies
4.2",4.2,"Cupertino, CA",Data Engineer,"Terraform along with synapse Azure cloud
Deep expertise in Data Engineering and Data Warehousing (minimum 4+ years)
Azure synapse , Azure Data Factory , Spark Pool , SQL , SQL Pool (minimum 4+ yrs)
CI/CD and Python/Java programming experience (minimum 3+ years)
Ideal to have a 24 x 7 development i.e offshore presence or different time zones
Job Type: Full-time
Salary: $115,871.32 - $207,675.38 per year
Ability to commute/relocate:
Cupertino, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$161,773 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"AbleTo
3.7",3.7,"New York, NY",Data Engineer,"About AbleTo
Join our mission-driven organization, where your work matters and a diversity of ideas and backgrounds are welcomed. AbleTo is the leading provider of high quality, technology-enabled behavioral health care. We believe that everyone deserves access to high-quality care and offer a suite of technology-enabled services to empower people to lead better lives through better mental health. A proprietary platform connects individuals with AbleTo highly trained licensed providers who deliver weekly sessions by phone or video supported by an integrated digital experience. Members also have access to mental health coaches, and digital support programs. Our outcomes-focused approach is proven to improve both behavioral and physical health and lower medical costs.

Overview
Data is integral to nearly everything that we do at AbleTo, and as a result the data team plays a key role at our company. As a Data Engineer at AbleTo, you will grow and develop your skills as a data engineer to build a world class data platform to support our mental health care mission.

What You Will Do
Build and maintain batch and real-time ETL pipelines in a Google Cloud Platform architecture (BigQuery, Dataproc, Firestore, Informatica, Python, etc.)
Identify code quality issues and implement tests to improve future processes.
Document orchestration work-flows and create run books
Implement data integrity tests to ensure we are ingesting accurate data.
Implement high-quality test-driven code.
Participate in daily team stand ups and other Agile ceremonies

Key Initiatives over the next 12 months
Data Mart client migration
Data Mart enhancements
Automation of manual day to day operational tasks
Decommission legacy pipelines

Who You Are:
1+ years of experience coding in Python.
Experience working with SQL (structured query language).
Familiarity with structuring and writing ETLs.
Experience working with Orchestration tools (Airflow) and Cloud Databases (BigQuery) is a plus.
Experience working in an Agile Development environment is a plus.
BS in Information Systems, Computer Science or related field is preferred.
You are excited to work with data!
Curiosity to dive deeply into issues and feel empowered to make a meaningful impact at a mission-driven company.
Committed to agile development and value delivery and solid engineering principles, as well as continuously improving our craft.
You are a collaborator. You build and maintain strong, productive working relationships with your customers and stakeholders.
You are empathetic and seek to understand each individual's diversity of background and experience contributes to making up a great team.
You drive to improve yourself and others.
You are excited about learning new languages, tools, techniques and technologies.

Why You Should Join Our Team:
We're proud to be a Great Place to Work-Certified™ company. We want you to show up and feel your best at work, and that means respecting your time outside of work. Our inclusive, flexible workspace prioritizes a work/life balance. We offer competitive salaries, comprehensive health benefits (for full-time employees), and professional perks such as 401K matching, fully funded HRA, and generous time off, including mental health days because your well-being is important to us.
At AbleTo, we're empowering people to get better and stay better. Want to join us? Take the next step in your career by applying for this role today.
The salary range is $40,000 to $90,000. Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, AbleTo offers benefits such as a comprehensive benefits package, incentive and recognition programs, and 401k contribution (all benefits are subject to eligibility requirements).

#LI-Remote
Follow AbleTo on LinkedIn, Twitter, and Instagram!
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.
AbleTo is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or veteran status. AbleTo is an E-Verify company.","$65,000 /yr (est.)",501 to 1000 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2008,$25 to $100 million (USD)
"Fathom Management LLC
2.0",2.0,Remote,Sr. Data Engineer Remote Opportunity,"Sr. Data Engineer

seeking a Senior Data Engineer who possesses expert level knowledge of appropriate data sources to address the specific requirements of projects for data modeling. Understand business requirements and translating into technical work. Design and implement features in collaboration with team engineers, product owners, data analysts, business partners using Agile/SCRUM Methodology.

This is a full- time position / 100% Remote.
The salary range of $140,000 - $160,000 will be based on technical experience and technical interview.

Responsibilities:

Ability to build programs or systems that can take data and turn it into meaningful information that can be studied.
Build ETL/ELT jobs and workflows to combine data from disparate sources.
Install continuous pipelines of huge pools of filtered information so that data analyst/scientists can pull relevant data sets for their analyses.
Build data workflows using SQL Server Integration Services (SSIS)
Build data workflows using Microsoft Azure (Azure Data Factory, Storage Accounts, Synapse)
Build data workflows using Databricks
Collaborate with business stakeholders, business operations, and product engineering teams in analyzing business problems, in building and testing solutions and data models.
Experience implementing and operating analytic models and services.
Document the current-state and target-state software architecture and create roadmap plans for success on various software components.
Assist in the design, implementation, and maintenance of complex solutions.
Build systems that collect, manage, and convert raw data into usable information for business analysts to interpret.
Make data accessible for evaluation and optimization
Collaborate with business stakeholders, business operations, and product engineering teams.
Coordinate activities with other technical personnel as appropriate.
Works with back-end data and develops tables using SQL scripts, SSIS, and SSMS.
Experience with Azure cloud platforms and Data bricks

Required Experience and Education:
Master's degree in computer science, systems engineering, or related technical discipline is preferred with 7-10 years of experience as a Data Engineer/Administrator or similar role. OR , B.S. in Computer Science with 15 years of relevant experience.

Benefits Overview: Full-time employees are offered comprehensive and competitive benefits package including paid vacation, sick leave, holidays, health insurance, life insurance, military leave, training, tuition reimbursement, a wellness program, short- and long-term disability, 401(k) retirement plan with company matches/immediate vesting, commuter benefits, and more.
EEO Policy: It is our policy to promote equal employment opportunities. All personnel decisions, including, but not limited to, recruiting, hiring, training, promotion, compensation, benefits, and termination, are made without regard to race, creed, color, religion, national origin, sex, age, marital status, sexual orientation, gender identity, citizenship status, veteran status, disability, or any other characteristic protected by applicable federal, state or local law.","$150,000 /yr (est.)",1 to 50 Employees,Self-employed,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Go Intellects Inc,#N/A,"Washington, DC",Data Engineer,"Work Location: REMOTE (1 – 2 Days On-site/Week may require)
Required Skills:
· Collect, manage, and convert raw data accurately and reliably
· Organize data systems for subgroup access and analyses
· Configure and sustain data cloud structures
· Must have expertise in Data Visualization Tools (Tableau)
· Data Modeling/Science as Python/SAS
· Should have AWS cloud native services, security, data pipeline
· Able to work with structured and unstructured data.
· Validate outputs of data pipelines
· Degree in Data Engineering preferred.
Job Type: Full-time
Pay: Up to $150,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Application Question(s):
Do you have, ""Active Secret (or) Top Secret Security Clearance""?
Work Location: In person","$150,000 /yr (est.)",Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"CONN-SELMER INC
3.3",3.3,Remote,Data Engineer,"Reports to: Director of IT
**Remote position**
Perform data analysis and provide Business Intelligence support to the organization
Responsibilities:
Perform data analysis on ERP systems including Infor XA and D365.
Perform data analysis on financial systems and custom developed applications.
Develop PowerBI reports for financial analysis and management reporting.
Create SQL-statement-based queries to facilitate data analysis.
Support data models to enhance BI reporting.
Engage and interact with business users to identify requirements and develop documents for the reporting needs of the business.
Engage and interact with business users to troubleshoot and resolve reporting issues.
Work with users to address ad-hoc data analysis requests.
Prioritize tasks and responsibilities to properly manage one’s own workload.
Troubleshoot data and BI application issues.
Fully analyze the impact and determine the strategy for all report requests.
Escalate top priority or production-critical issues to the appropriate support staff.
Make recommendations of perceived solutions to potential problem areas and methods for improvement.
Communicate effectively.
Other duties as assigned by supervisor
Qualifications and Requirements:
Bachelor of Science in Information Systems/MIS, computer science, business or related field or equivalent experience
4 years Business Intelligence experience preferred
Microsoft Dynamics 365 experience preferred
Courses are desired and/or will be required within the first six months of employment: PL300, DP500, DP203
Microsoft PowerApps and PowerPortal applications preferred
Infor XA or any IBM (AS/400 or iSeries) based ERP preferred
Azure Synapse Experience preferred
Power BI required
SQL required",#N/A,501 to 1000 Employees,Subsidiary or Business Segment,Manufacturing,Consumer Product Manufacturing,1875,Unknown / Non-Applicable
BIGCLFY,#N/A,"Dallas, TX","W2/ 1099, No C2C - SSIS ETL Data Engineer (Minneapolis, MN/ Dallas-Irving)","No C2C. It's W2/1099 Requirement
Only taking USC, GC or H4 EAD, H1 Transfer, L2 Visa or TN Visa - they will not take OPT EAD or CPT
Need to have confirmation in writing that you will be onsite by Day 1
ALL ROLES ARE HYBRID IN THE OFFICE (2-3 DAYS/WEEK) UNLESS REMOTE IS NOTED
Please submit qualified candidates and include FULL LEGAL NAME as it appears on the passport
If your candidate is relocating, need to have confirmation in writing that they will be onsite by Day 1
Senior SSIS/ETL Data Engineer
12 months – 3 days in office
Minneapolis, MN or Dallas-Irving, TX
5+ years of experience with SQL Server 2016 (or newer) and SSIS
7+ years of data engineering
Must be capable of leading teams
2+ years of experience working with large scale SQL implementations
Experience designing, developing, and maintaining complex stored procedures, functions, and views using T-SQL
Experience designing, developing, and maintaining ETL processes using SSIS
Should understand how to unit test their product
Hands on experience with Visual Studio to document
Experience in financial or regulatory environments highly preferred
Ideal candidates will have experience creating conceptual, logical, and physical database designs
Preference given to candidates with extensive knowledge and experience with performance monitoring, database and SQL tuning
Bachelor’s or Master's in computer science or related field or equivalent work experience
Job Type: Contract
Pay: From $73.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Chandler, AZ 85225: Reliably commute or planning to relocate before starting work (Preferred)
Application Question(s):
Expected Payrate on W2/1099 Tax Term? Please mention. Thanks.
Experience:
Risk Analytics: 5 years (Preferred)
Work Location: In person",$73.00 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
ZENOTIS TECHNOLOGY PVT LTD,#N/A,Remote,IT Data Governance Engineer,"Duties:
· Lead technical analysis, design, code, and automation in support of program and project work you're doing using data governance technologies (data catalogs, data dictionary, data quality, business glossary etc.).
· Lead in design sessions, implementation plans, and resolve technical issues as needed.
· Ensure that platform is configured and extended with Enterprise Architecture Guidelines (performance, extensible, API enabled etc.).
· Coordinates, schedules, installs, and tests hardware and software changes.
· Develops testing and implementation plans.
· Works with other engineers to plan installations and upgrades and ensure subsequent maintenance in accordance with established IT policies and procedures.
· Monitor’s system to achieve optimum performance levels.
· Collaborate with business teams by providing technical input to Data Governance policies, standards and processes related to data, access, and security (privacy & protection) of sensitive data.
· Work closely with application/platform/governance teams to create onboarding and intake documentation and definitions as repeatable processes.
Job Type: Contract
Salary: $60.00 - $65.00 per hour
Work Location: Remote
Speak with the employer
+91 9046903290",$62.50 /hr (est.),201 to 500 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"IT Engagements,Inc.",#N/A,"Dallas, TX",Big Data Engineer,"Greeting from IT Engagements.
IT Engagements is a global staff augmentation firm providing a wide-range of talent on-demand and total workforce solutions. We have an immediate opening for the below position with one of our premium clients.
Job Title: Big Data Engineer
Location: Minneapolis, MN (Hybrid 3 days onsite) 2nd location Dallas TX.
Duration: 12 Months (Potential FTE)
Required Qualifications
4+ years of Software Engineering experience
4+ years of Data engineering experience
4+ years of experience in any or all Big-Data stack such as Hadoop, Hive, Spark, python
4+ years of Relational data base experience
4 + years of ETL (Extract, Transform, Load) development experience using any Big-Data technology
2+ years of Agile software development experience
Hands-on experience with cloud-based data environments
Technical Documentation
Thanks and Regards
Divya Kumari
Technical Recruiter
divya(at)itengagements(dot)com
Job Type: Contract
Pay: From $65.00 per hour
Experience level:
4 years
Schedule:
8 hour shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Preferred)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",$65.00 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Inovalon
3.1",3.1,United States,Software Data Engineer,"Inovalon was founded in 1998 on the belief that technology, and data specifically, would empower the transformation of the entire healthcare ecosystem for the better, improving both outcomes and economics. At Inovalon, we believe that when our customers are successful in their missions, healthcare improves. Therefore, we focus on empowering them with data-driven solutions. And the momentum is building.
Together, as ONE Inovalon, we are a united force delivering solutions that address healthcare's greatest needs. Through our mission-based culture of inclusion and innovation, our organization brings value not just to our customers, but to the millions of patients and members they serve.
Overview: The Software Data Engineer is responsible for contributing to data pipelines, ETL, data warehouse, jobs, data operations. They will be part of the team which is building next generation data & reporting platform. This platform will cater internal business stakeholders and external customers to provide insights & forecasting to understand current state of business, improve decision-making for their tactical and strategic goals & KPIs. This position may require independent work, sharing information and assisting others with work request.
Duties and Responsibilities:
Work with the agile team to participate in agile ceremonies like grooming, planning, standup, retrospective, demos
Actively contribute to grooming, and standup, create & update tasks, estimate and status
Work with data architects and business analysts to create a logical data model and create DDL scripts for physical database creation
Work on large data to ensure ingestion of data, dynamic rule & validation of data, cleansing, transforming, and loading into the data warehouse
Write complex queries, stored procedures, functions, SSIS Packages for various job execution
Develop modern ETL framework utilizing tools like ADF (Azure Data Factory), MS-SSIS etc
Develop STAR or SNOWFLAKE database schema utilizing industry best practices to build Data warehouse, data marts, views, and data sets/products
Develop ETL pipelines, using SQL, Stored procedures/functions to extract data from various sources and load into warehouse
Develop Symantec layer and data export frameworks to extract the data from the warehouse, transform, pre-aggregate, perform calculations and load into various data marts for Analytics use
Develop configurable export framework to extract data from Data warehouse and data marts to generate reports for internal and external customers in .csv, flat files and
Design and implement data validation and quality checks to ensure the accuracy and completeness of the data in the data warehouse
Perform performance of queries and data processing, identify and resolve any issues
Work and communicate in a cross-functional geographically dispersed team environment comprised of software engineers and product managers; and
Ensure compliance to company procedures when making changes and implementing code.
Maintain compliance with Inovalon's policies, procedures and mission statement;
Adhere to all confidentiality and HIPAA requirements as outlined within Inovalon's Operating Policies and Procedures in all ways and at all times with respect to any aspect of the data handled or services rendered in the undertaking of the position; and
Fulfill those responsibilities and/or duties that may be reasonably provided by Inovalon for the purpose of achieving operational and financial success of the Employer.
Job Requirements:
Minimum two (2) years related experience required; healthcare industry experience preferred.
Strong understanding to develop SQL queries for data analysis.
Experience working on Azure Cloud is preferred
3+ experience in MS SQL, T-SQL, ETL Jobs
3+ experience in Microsoft tools like SSMS, SSIS, SQL Server
Strong understanding of database concepts and schema (like star, snowflake schema)
Ability to learn quickly and independently
Ability to effectively communicate with internal and external customers
Experience with test driven development methodologies.
Education:
Bachelor's degree in Computer Science, Software Engineering, or Information Technology.
Physical Demands and Work Environment:
Sedentary work (i.e., sitting for long periods of time);
Exerting up to 10 pounds of force occasionally and/or negligible amount of force
Frequently or constantly to lift, carry push, pull or otherwise move objects and repetitive motions
Subject to inside environmental conditions; and
Studies have shown that women and people of color are less likely to apply for jobs unless they believe they meet every one of the qualifications listed in a job description. If you don't meet every qualification listed but are excited about our mission and the work described, we encourage you to apply regardless. Inovalon is most interested in finding the best candidate for the job and you may be just the right person for this or other roles.
By embracing diversity, equity and inclusion we enhance our work environment and drive business success. Inovalon strives to reflect the diversity of the communities where we operate and of our clients and everyone whom we serve. We endeavor to create a culture of inclusion in which our associates feel empowered to bring their full, authentic selves to work and pursue their professional goals in an equitable setting. We understand that by fostering this type of culture, and welcoming different perspectives, we generate innovation and growth.
Inovalon is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirement.
The Company maintains a drug free work environment for all of its associates, which includes employees, contractors and vendors. It is unlawful for associates to manufacture, sell, distribute, dispense, possess or use any controlled substance or marijuana in the workplace and doing so will result in disciplinary action, up to and including termination of employment or the contracted relationship.",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Software Development,1998,$500 million to $1 billion (USD)
"Gleecus TechLabs Pvt. Ltd
4.5",4.5,"Glen Allen, VA",Lead Azure Data Engineer,"Position Title: Lead Azure Data Engineer - Glen Allen VA (Hybrid)
Tech Skills:
· Concepts to be covered ? (if any).
· Databricks(Python, SQL).
· Hands on Coding.
· Databricks optimization/Pyspark Transformations/PySpark Architecture.
· Azure Data Factory.
· Scenario-Based Questions and Practical Application.
· Azure Data Factory activities/Use of ADF Monitor/CI CD concepts.
· Azure ML, Databricks ML, Azure SDK.
· Scenario-Based Questions and Practical Application.
· Optimized Apache Spark environment, Machine Learning Integration, Experiment tracking, Model training, Feature development, Model serving.
· Synapse Analytics, Parque and Delta tables.
· Scenario-Based Questions and Practical Application.
· Synapse Analytics related concepts.
Job Type: Contract
Salary: $130,000.00 - $150,000.00 per year
Benefits:
Flexible schedule
Health insurance
Paid time off
Schedule:
Day shift
Monday to Friday
Ability to commute/relocate:
Glen Allen, VA 23058: Reliably commute or planning to relocate before starting work (Required)
Experience:
ADF: 4 years (Required)
Azure Data Lake: 4 years (Required)
Databricks: 5 years (Required)
Work Location: One location","$140,000 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2015,Unknown / Non-Applicable
Schober Consulting LLC,#N/A,"Minneapolis, MN",Data Engineer (Junior),"We are seeking a talented and experienced Data Engineer to join our team. The ideal candidate will have 2-3 years of professional experience in data engineering, with a bonus if they have worked in the real estate industry. Additionally, an interest in machine learning is highly desirable.
Responsibilities:
Design, develop, and maintain scalable and efficient data pipelines and ETL processes
Build and optimize data models and data storage solutions for large-scale datasets
Collaborate with cross-functional teams, including data scientists and analysts, to gather requirements and design data-driven solutions
Develop and maintain data infrastructure to support advanced analytics and machine learning initiatives
Ensure data quality and integrity by implementing data validation and cleansing processes
Continuously monitor and optimize data pipelines and systems for performance and reliability
Stay updated with the latest industry trends and technologies in data engineering and machine learning
Qualifications:
Bachelor's or Master's degree in Computer Science, Data Engineering, or a related field
2-3 years of professional experience as a Data Engineer, preferably with exposure to the real estate industry
Strong understanding of data engineering principles, ETL processes, and data warehousing concepts
Proficiency in SQL and experience with relational databases
Solid programming skills in Python, Java, or other relevant languages
Familiarity with cloud computing platforms, preferably AWS or Azure
Experience with data modeling and designing efficient database schemas
Knowledge of machine learning concepts and a strong interest in applying machine learning to data engineering tasks
Excellent problem-solving and analytical abilities
Strong communication and collaboration skills
If you are a self-motivated individual with a passion for data engineering and an interest in the real estate industry and machine learning, we encourage you to apply. We offer a competitive salary, comprehensive benefits package, and a collaborative work environment that fosters professional growth and development.
Job Type: Full-time
Pay: $70,000.00 - $100,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Compensation package:
Employee stock ownership plan
Experience level:
2 years
Schedule:
Monday to Friday
Ability to commute/relocate:
Minneapolis, MN 55402: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
SQL (Preferred)
Data warehouse (Preferred)
Python (Required)
Work Location: Hybrid remote in Minneapolis, MN 55402","$85,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Innova IT Services LLC,#N/A,"Raleigh, NC",Network Data Engineer,"W2 only
==========
HP ARUBA experience is a must.
Develop and install network infrastructure, configurations and equipment such as routers and switches
Monitor networks and troubleshoot issues or outages.
LAN, WAN, SWITCH & ROUTING experience is needed.
Consult with clients to suggest network solutions
Manage junior employees and provide training resources for team members
Test and install new computer systems, hardware, software and applications
Develop engineering design packages to integrate new processes into existing ones
Collaborate with clients, other tech support services and network providers to ensure the quality of networks
Team player and support on-call responsibilities as HP ARUBA NMS team member
Coordinate upgrades/updates and maintenance of HP ARUBA NMS tool solutions
Collaborate across multiple technical teams and lines of business with focus on implementing enterprise NMS solutions.
Strong problem-solving and troubleshooting skills
Excellent communication and interpersonal abilities
Ability to work independently and as part of a team
Ability to multi-task, prioritize, and manage time efficiently
Highly organized and detail-oriented
Certifications:-
CCNA minimum CCNP preferred.
-7+ years of experience with network planning, design, and implementing LAN, WAN, network security, and wireless network infrastructures.
Job Type: Contract
Pay: $40.00 - $45.00 per hour
Experience level:
7 years
Schedule:
8 hour shift
Ability to commute/relocate:
Raleigh, NC 27606: Reliably commute or planning to relocate before starting work (Required)
Experience:
Network engineering: 4 years (Required)
Routing protocols: 4 years (Required)
Data network design: 2 years (Required)
Work Location: In person",$42.50 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Radiant System,#N/A,Remote,Snowflake Data Engineer,"REQUIRED SKILLS
- Strong/ Proven Data Engineering experience with Snowflake
-Strong Azure Experience is required
-Experience configuring private networks on the cloud
-Experience moving data from Snowflake to Azure
Job Type: Contract
Application Question(s):
Are you willing to work on W2?
Experience:
Snowflake: 1 year (Required)
Work Location: Remote",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Global Enterprise Services, LLC
4.0",4.0,"Atlanta, GA",Data Harmonization Engineer,"Data Harmonization Engineer

Collects data models for disparate datasets within a domain. Collaborates with CDC & STLT stakeholders to develop a conceptual model that defines the atomic concepts for that domain.
Constructs semantic mappings from the data elements in the source datasets to atomic or compound concepts in the conceptual model.
Develops sample transformation code to translate among representations.
Configures and manages vocabulary services to allow data modernization users to improve understanding of the relationships among data representations.

Data Harmonization Engineer - Junior [YoE, Edu, Certs: 0-3 yrs & BA/BS] [Salary Range: $81k - $91k]
Data Harmonization Engineer - Journeyman [YoE, Edu, Certs: 4-9 yrs & BA/BS] [Salary Range: $95k - $105k]
Data Harmonization Engineer - Senior [YoE, Edu, Certs: 10+ yrs & MA/MS] [Salary Range: $108k - $108k]","$100,000 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Dave & Buster's
3.6",3.6,"Dallas, TX",Data Lakehouse Engineer,"THE JIST:
The Data Lakehouse Engineer will be tasked with managing data movement and data storage for Dave & Buster’s, from a variety of data sources to a consolidated data lake/ODS/warehouse, intended to support a variety of business intelligence offerings. Works closely with members of the Information Technology departments to support, design and implement data solutions through an iterative process with end users.

THE ESSENTIALS:
Works across Technology, Vendor, and D&B operational and executive teams for data-related and insight needs.
Familiarity with data structures, storage systems, cloud infrastructure, and other technical tools.
Ability to work effectively in teams of technical and non-technical individuals.
Ability to continuously learn, work independently, and make decisions with minimal supervision
Demonstrate accountability, prioritize tasks, and consistently meet deadlines.
Create, maintain, and optimize data environments needed to satisfy defined business requirements in on-premise and hosted environments.
Identifying, designing and implementing internal process improvements, including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes.
Familiarity with Agile/SCRUM practices

SCOPE:
Understanding of how technical decisions impact the user of what you’re building
Prioritization of work is typically determined by the team
Will frequently makes recommendations to end users to drive better outcomes
Has some authority to modify processes, interpret policies or recommend programs that will directly impact the functional area
Coordinate communications with IT Support of updates, changes, and issues.
Monitor, maintain, and provide on-call support for production data environments.

CREDENTIALS:
1 -3 years of data engineering or relevant industry experience
Preferred Computer Science, Mathematics, and Software Engineering degree

THE GOODS:
(Qualifications - The minimum level of specific skills or abilities one must possess to be considered for this job, such as computer skills, communication skills, leadership ability, analytical ability, etc.)
Proficiency with programming languages such as Python, SQL, MDX, DAX or similar languages
Proficiency with reporting tools such as Power BI, SSRS, Tableau, QlikView, etc.
Experience with SQL database and design including stored procedures, triggers, and views (SQL Server, Oracle, mysql, Pyspark).
Windows Server knowledge is helpful (powershell, scripting, performance metrics checks, event viewer)
Experience with data warehousing, Azure and the Azure Stack
Exposure to DataBricks

PHYSICAL DEMANDS:
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.

While performing the duties of this job, the employee is regularly required to use hands to finger, handle, or feel and talk or hear. The employee frequently is required to sit and reach with hands and arms. The employee is occasionally required to stand and walk. The employee must occasionally lift and/or move up to 10 pounds. Specific vision abilities required by this job include close vision, distance vision, color vision, peripheral vision, depth perception, and ability to adjust focus.

WORK ENVIRONMENT:
The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.

The noise level in the work environment is usually moderate.","$58,253 /yr (est.)",10000+ Employees,Company - Public,Restaurants & Food Service,Restaurants & Cafes,1982,$1 to $5 billion (USD)
C.G. Consulting Group Inc.,#N/A,Remote,Network Engineer - Data Center,"Datacenter Network Engineer
This is with a global IT services firm for a leading Healthcare / HMO firm in USA.
Candidate should have legal work status – ideal if citizen, Green Card Holder, etc.
Must be located in USA.
Location EST Zone in USA
6 months contract, possible extension.
Market rate
Description
Datacenter Engineering team consists of highly qualified network engineers that support Datacenter Network environments (IT Infrastructure Engineering). The team is staffed globally and provides network analysis, design, engineering, support, and integration services.
Responsibilities
Contribute to and help lead multiple Network Engineering initiatives and projects consisting of design and development for complex products and platforms, including solution design, analysis, testing, and integration
Collaborate on the development of datacenter strategies; evaluate and select hardware and software products, implement, and integrate these products into comprehensive solutions meeting company's business requirements
Performs after-hours systems support, installation, and maintenance as required
Provide specialized technical expertise in researching and resolving complex technology network infrastructure problems which may involve coordination of hardware and software vendor support
Advise team members on procedures, techniques, and requirements to insure maximum performance and availability of the company infrastructure. Recommend areas for improvement in both processes and systems
Conduct implementations, conversions, and upgrades in a manner consistent with Standard Operating Procedures within company maintenance windows; including change management procedures
Communicate effectively with supervisors, peers, project managers, and stakeholders and provide feedback on projects status, milestones and risks.
Author technical documentation such as network diagrams
Work closely with vendors on implementation of products and services
Ensure security, governance and operational standards and procedures are maintained in accordance with UHG IT compliance requirements, HIPPA standards, and industry best practices
Perform risk analysis to identify IT security issues and remediating plans. Identify and/or mitigate operational risks where appropriate
Excellent interpersonal skills including written and verbal skills with the ability to create and maintain strong relationships with members of cross departmental teams and department team members
Provide 24-hour support for network infrastructure as part of an on-call rotation
Skills
Must have
8 Years of experience implementing, maintaining, and troubleshooting converged IP networks
8 Years of knowledge of core networking concepts Vlans, VRFs & VPNs
8 Years with advanced knowledge of routing protocols EIGRP, OSPF & BGP
8 Years of experience in supporting Cisco, Arista or HPE/Aruba Network Infrastructure
4 Years of experience working in a change-controlled environment
CCIE or higher certification
Good English communications
Nice to have
Familiarity with network automation utilizing Ansible, Python, Jinja2, Git & Jenkins
4 Years of experience with SD-WAN Solutions
4 Years of experience with Cisco ACI software-defined-networking solutions
3 Years of Experience in Palo Alto firewall installation and management
4 Years of Experience with F5 Load Balancing Technology
ITIL Foundations v3 certification or higher
Job Type: Contract
Salary: $48.00 - $68.00 per hour
Schedule:
Monday to Friday
Experience:
Computer networking: 8 years (Preferred)
Work Location: Remote",$58.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Kairos Technologies
4.6",4.6,"Irving, TX",Data Quality Engineer,"Role: Data Quality Engineer (ETL/AWS/ Python)
Location: Irving TX ( Locals only)
Experience: 10+
Job Requirements:
Must Haves Skills
ETL/AWS/ Python
Extensive experience with Python scripting and Cloud Technologies
Extensive experience with AWS components like S3, Athena, EMR, Glue, Redshift, Kinesis and Sagemaker
Experience in Automating ETL process with Python and Automation around AWS Data & Infrastructure
Extensive Experience with SQL/Unix/Linux scripting is a must
Extensive experience on Developing/testing Cloud/On Prem ETL (Ab Initio, AWS Glue, Informatica, Alteryx)
Extensive Experience in Data migration is a must (Teradata to Redshift preferred)
Experienced in large-scale application development testing – Cloud/ On Prem Data warehouse, Data Lake, Data Science
Experience in building Data flow CI/CD pipelines in GitLab
Extensive experience in DevOps/Data Ops space.
Experience in Data Science platforms like SageMaker/Machine Learning Studio/ H2O.
Strong experience of Kafka.
Nice to Have Skills
Experience using Jenkins and Gitlab
Experience using both Waterfall and Agile methodologies.
Experience in testing storage tools like S3, HDFS
Experience with one or more industry-standard defect or Test Case management Tools
Soft Skills
Great communication skills (regularly interacts with cross functional team members)
Who takes Ownership to complete the tasks on time with less supervision
Guiding developers and automation teams in case of an issue
Monitoring, reviewing, and managing technical operations
Effective problem-solving expertise, trouble shooting, code debugging, and root cause analysis skills
Job Type: Contract
Pay: $60.00 - $68.00 per hour
Ability to commute/relocate:
Irving, TX 75015: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 1 year (Preferred)
ETL: 1 year (Preferred)
AWS: 1 year (Preferred)
Work Location: In person",$64.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2003,$5 to $25 million (USD)
"PrizePicks
4.8",4.8,"Atlanta, GA",Data Engineer,"At PrizePicks, we are the fastest growing sports company in North America, as recognized by Inc. 5000. As the leading platform for Daily Fantasy Sports, we cover a diverse range of sports leagues, including the NFL, NBA, and Esports titles like League of Legends and CS : GO. Our team of over 200 employees thrives in an inclusive culture that values individuals from diverse backgrounds, regardless of their level of sports fandom. Ready to reimagine the DFS industry together?
Our Analytics Team is responsible for building and maintaining analytics tools and workflows to support the PrizePicks business. By developing, maintaining, and testing data generation infrastructures, you will enable the PrizePicks business to make smarter, better, and faster data-driven decisions.
What you'll do:
Create, maintain and orchestrate optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Improve and streamline data systems to drive innovation within Prizepicks
What you have:
3+ years of experience building frameworks for data ingestion pipelines but real time and batch using data modeling, ETL/ELT processes
Stellar SQL skills, experience building DBT pipelines in production and know your way around structured, semi-structured and unstructured data.
Have built and optimized 'big data' data pipelines, architectures and data sets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Experience with Python and other object oriented scripting languages
Cloud experience: AWS, EC2, EMR, RDS, Redshift, GCP, etc..
Not required but would be awesome if you have experience with any of the following data pipeline and workflow tools: Azkaban, Luigi, Airflow, Prefect, etc.
Where you'll live:
Anywhere in the US is fine (we are based in Atlanta, GA)
Benefits you'll receive:
In addition to your great compensation package, company subsidized medical/dental/vision coverage plans and matching 401(k), we'll shower you with perks including:
Break room with ping pong, endless snacks and in-office lunch once a week
Unlimited PTO to encourage a healthy work/life balance (2 week min required!)
Modern work schedule focused on getting the job done, not hours clocked
Workplace flexibility
Company and team outings, we encourage a tight-knit workplace
Generous Maternity AND Paternity leave (16 weeks!)
Annual bonus & stock options
Wellness program
Company equipment provided (Windows & Mac options)
Annual performance reviews with opportunity for growth and career development
#LI-REMOTE

You must be authorized to work for any employer in the U.S. We are unable to sponsor or take over sponsorship or an employment Visa at this time.
PrizePicks is an Equal Opportunity Employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.","$97,814 /yr (est.)",1 to 50 Employees,Company - Private,"Arts, Entertainment & Recreation",Sports & Recreation,2015,Unknown / Non-Applicable
"Green Worldwide Shipping LLC
3.7",3.7,"Aurora, CO",GEM Data Engineer,"About Green
“Global Reach with a personal touch”, that’s the foundation on which four partners (and long-time friends) agreed to when they started Green Worldwide Shipping back in 2008. This ethos has allowed Green to Grow to a 200+ strong team with 15 offices across the US and growing.At green we are a family of innovative problem-solvers who communicate freely to present creative solutions in an environment of trust, respect, and integrity. We are dedicated to customer service, passionate about logistics, and committed to sustainable growth while inspiring others.
Why this role is special..
GEM (Green Engineered Management) is a technology solutions department that supports various commercial technology products, data management, and development for Green Worldwide Shipping.We are seeking a skilled and experienced Data Engineer to join our dynamic team. As a Data Engineer, you will be responsible for designing, developing, and maintaining our data infrastructure and various technology systems to support efficient data processing and analysis. Your expertise in SQL, CargoWise, database management, Altova Flowforce Server, Altova Mapforce ETL software, Python and C#, along with experience in EDI/API connection protocols integration, will be instrumental in achieving success in this role.
How you will be successful…
Design, develop, and maintain data pipelines and ETL processes to extract, transform, and load data from various sources into our databases using Altova Mapforce ETL software.
Collaborate with cross-functional teams to understand data requirements and provide optimal data solutions.
Develop and optimize SQL queries to retrieve and manipulate data from databases, ensuring efficient data retrieval and storage.
Implement and integrate EDI/API connection protocols to enable seamless data exchange between internal and external systems.
Monitor and maintain the performance, availability, and integrity of the databases and data infrastructure.
Support for reporting, document modification, workflow and database support for internal ERP.
Troubleshoot and resolve data-related issues, including data quality and data integration problems.
Implement data security measures and ensure compliance with data privacy regulations.
Stay updated with industry best practices and emerging technologies in data engineering and incorporate them into the data infrastructure.
Provide technical guidance and support to other team members on data-related projects.
Collaborate with data analysts and data scientists to understand data requirements and support their data exploration and analysis needs.
Create and maintain documentation related to data pipelines, database schemas, and data transformation processes.
What you will bring….
Bachelor's degree in Computer Science, Information Systems, or a related field.
Preferred experience in CargoWise (or related ERP) with reporting, document modification, workflow and database support
Proven experience working with SQL and databases, preferably MySQL.
Prefer strong knowledge and hands-on experience with Altova Flowforce Server and Altova Mapforce ETL software, or related applications.
Experience in integrating and working with EDI/API connection protocols.
Proficiency in Python and/or C# (optional but preferred).
Familiarity with data modeling concepts and techniques.
Experience with data warehousing concepts and technologies.
Excellent problem-solving and analytical skills.
Strong attention to detail and the ability to work in a fast-paced environment.
Effective communication skills to collaborate with cross-functional teams.
Ability to work independently and prioritize tasks effectively.
Preferred Skills:
*
Experience with cloud-based data platforms (e.g., AWS, Azure, Google Cloud Platform).
Knowledge of big data technologies (e.g., Hadoop, Spark) and distributed computing.
Familiarity with data visualization tools (e.g., Tableau, Power BI).
Experience with version control systems (e.g., Git).
Understanding of Agile development methodologies.
What we offer:
Employee High Deductible Health Plan (HDHP) currently paid for 100% by Green.
$1,800 contributed to employee HSA Account, if on HDHP.
Employee Dental provided by Green.
Option to purchase disability insurance.
Paid life insurance and option to buy-up.
401(K) and company match.
Parental Leave Program.
Monthly recognition points to give/receive, which can be converted to gift cards.
Tuition Reimbursement Program.
Weekly fruit baskets in office locations
A tree planted for every new employee to Trees.org
Hybrid work schedule.
Wellbeing Program.
Training and Development.
Access to Mentorship Program.
Green Worldwide Shipping, LLC (Green) is committed to the full inclusion of all qualified individuals. As part of this commitment, Green will ensure that persons with disabilities are provided reasonable accommodations. If reasonable accommodation is needed to participate in the job application or interview process and/or to perform essential job functions, please contact Faye Hughes, Head of People and Culture at faye.hughes@greenworldwide.com.Green is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.
Requirements
Bachelor's degree in Computer Science, Information Systems, or a related field.
Preferred experience in CargoWise (or related ERP) with reporting, document modification, workflow and database support
Proven experience working with SQL and databases, preferably MySQL.
Strong knowledge and hands-on experience with Altova Flowforce Server and Altova Mapforce ETL software, or related applications.
Experience in integrating and working with EDI/API connection protocols.
Proficiency in Python and/or C# (optional but preferred).
Familiarity with data modeling concepts and techniques.
Experience with data warehousing concepts and technologies.
Excellent problem-solving and analytical skills.
Strong attention to detail and the ability to work in a fast-paced environment.
Effective communication skills to collaborate with cross-functional teams.
Ability to work independently and prioritize tasks effectively.
Preferred Skills:
Experience with cloud-based data platforms (e.g., AWS, Azure, Google Cloud Platform).
Knowledge of big data technologies (e.g., Hadoop, Spark) and distributed computing.
Familiarity with data visualization tools (e.g., Tableau, Power BI).
Experience with version control systems (e.g., Git).
Understanding of Agile development methodologies.
Summary
GEM (Green Engineered Management) is a technology solutions department that supports various commercial technology products, data management, and development for Green Worldwide Shipping. We are seeking a skilled and experienced Data Engineer to join our dynamic team. As a Data Engineer, you will be responsible for designing, developing, and maintaining our data infrastructure and various technology systems to support efficient data processing and analysis. Your expertise in SQL, CargoWise, database management, Altova Flowforce Server, Altova Mapforce ETL software, Python and C#, along with experience in EDI/API connection protocols integration, will be instrumental in achieving success in this role.
Job Type: Full-time
Pay: $60,000.00 - $75,000.00 per year
Benefits:
401(k) matching
Dental insurance
Employee discount
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Tuition reimbursement
Vision insurance
Experience level:
4 years
Schedule:
Monday to Friday
Ability to commute/relocate:
Aurora, CO 80014: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL/MySQL (Required)
ETL Software (Preferred)
Data warehouse (Preferred)
Python/C# (Preferred)
Work Location: Hybrid remote in Aurora, CO 80014","$67,500 /yr (est.)",51 to 200 Employees,Company - Private,Transportation & Logistics,Shipping & Trucking,2008,$25 to $100 million (USD)
"Evergreen Residential Holdings, LLC
5.0",5.0,"Dallas, TX",Engineer Data & Analytics,"We are Evergreen Residential, a high growth early-stage institutional investment platform in the single-family residential sector. Our team is collaborative, open-minded and curious. Transparency is a core value, we speak our minds, are responsible for our actions and celebrate our wins. We are serious about the business without taking ourselves too seriously. We look for people who thrive in an entrepreneurial and fast paced environment. If you are self-motivated and mission driven with a 'can do' mindset and see solutions where others may see problems, come and grow with us!
We offer a flexible, empowering culture, competitive compensation and benefits, and potential for career growth through working closely with, and learning from, our experienced leadership team.
As a technical/engineering expert, you pride yourself on being able to quickly build strong business relationships both internally and externally e.g., with the leadership team, current and potential investors. With a passion for keeping current with advancements of the field, you deploy technology and data resources to provide innovative solutions to business needs.
The Role: Priorities can often change in a fast-paced environment like ours. Initial focus is to work with external data purchased, and to harvest internal data and work within Snowflake warehouse for use in our 3rd party property mgt system and BI reporting tool. Overall ensure there is one source of truth.

The role includes, but is not limited to, the following responsibilities:
Designing and implementing data pipelines to extract, transform, and load data from various sources into a centralized data repository
Developing and maintaining data processing and storage infrastructure
Establish productive relationships and effective communications with Company leadership to understand business drivers and align on required outcomes
Collaborating with data analysts to ensure that data is readily available for analysis and modeling
Optimizing database performance and troubleshooting issues as they arise
Implementing data security and access controls to protect sensitive data
Highlight key trends derived from data analysis and be a resource for improving data proficiency throughout the organization
Staying up-to-date with emerging trends and technologies in data engineering
Leverage historical data and predictive models to identify key historical factors that impact critical KPIs, and recommend actions to drive future performance
Ensure scientific method and research are key drivers of the product roadmap
What You Will Bring to the Table:
Knowledge of data modeling, database design, and ETL best practices
At least 3-5 years of experience in data engineering or a related field
Proficiency in one or more programming languages such as Python, Java, or Scala
Experience with data processing and storage technologies such as Hadoop, Spark, Kafka, Snowflake, and NoSQL databases
Experience in real estate investment and/or rental sector highly desirable
Prior experience managing a team of direct reports within the Data Science, Data Engineering, Analytics space in the SFR or Multifamily industry
Significant Experience building, motivating, and retaining a high- performing, flexible and collaborative data and analytics function
Proven hands-on technical background in data science, business intelligence or data engineering with demonstrated strategic impact at an executive level
A strong problem solver with experience building technical strategy and understanding technical tradeoffs and risk
Collaborative team player, you are truly a ""do-er"", happy to be a hands-on problem-solver to move the data program forward
Excellent communication skills – verbal and written
About Evergreen Residential
Founded in 2021, Evergreen Residential is a full-service SFR platform leveraging proven operational practices and the latest technological advances to optimize investor returns and achieve positive outcomes for our residents and the communities in which we operate. We offer a full suite of services, including Investment Management, Asset Origination, and Advisory Services. The firm is headquartered in Dallas with offices in New York City.
The leadership team has extensive experience dating back to the early institutionalization of SFR and unrivaled depth of experience in the complete asset life cycle. We are built to withstand changing market conditions, and our business produces resilient, predictable cash flows and margins. We are committed to charting new paths and using data to achieve best-in-class results. Our business is evergreen.
Beyond financial returns, the Company is committed to measurable impact objectives. We believe that inclusive and equitable management, environmentally sustainable long-term strategies, and resident-focused policies are good business - for our residents, our investors, and our team. We are committed to using environmentally sustainable practices and empowering our residents to improve their financial health.
Our cornerstone values - Accountability, Transparency and Partnership - are built on a foundation of Integrity and provide the roadmap for our daily actions, interactions and decisions.

Equal Opportunities and Other Employment Statements
We are deeply committed to building a workplace and community where inclusion is not only valued, but prioritized. We take pride in being an equal opportunity employer and seek to create a welcoming environment based on mutual respect, and to recruit, develop and retain the most talented people from a diverse candidate pool. All employment decisions shall be made without regard to race, color, religion, gender, gender identity or expression, family status, marital status, sexual orientation, national origin, genetics, neuro-diversity, disability, age, or veteran status, or any other basis as protected by federal, state, or local law.","$98,425 /yr (est.)",51 to 200 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Overhaul
3.5",3.5,"Austin, TX",Data Platform Engineer,"Who We Are
Overhaul is a supply chain integrity solutions company that allows shippers to connect disparate sources of data into the first fully transparent situational analysis engine designed for the logistics industry. Data that is transformed into critical insights can instantly trigger corrective actions, impacting everything from temperature control to handling requirements or package-level tracking, ensuring cargo arrives at its destination safely, undamaged, and on time. We are a dynamic, innovative, and fun team who is highly committed to our customers' experiences and our Mission and Vision.
The Role
At Overhaul, we're building the future of supply chain monitoring technology. As a Data Platform Engineer, you'll work with our platform and data analytics teams to understand and improve our data model, research and implement business intelligence tools to allow us to better serve our data internally and externally, and work with other engineers to design better systems for data ingestion and processing. Our company is growing fast, and we need you to help us deliver the best results to our users as we continue that trend.
Responsibilities:
Analyze existing tools, including business intelligence, databases, messaging services, etc. and provide solution recommendations
Deploy and manage data software solutions in a cloud environment (We use AWS and Azure)
Analyze custom data models and provide solutions for data ingestion/ETL
Create custom data software solutions for our internal and external customers using workflow tools such as Argo Workflows
Required Skills and Qualifications:
Experience doing data model analysis
Experience using cloud based technologies for data ingestion, analysis, and extraction (AWS Lambda, Workflow solutions, Snowflake, etc)
Excellent written and oral communication skills
Preferred Qualifications:
Software Engineering experience beyond writing simple scripts, preferably in Python
Experience using workflow solutions such as Argo Workflows, Prefect, Kubeflow, or Jenkins to implement data pipelines
Our Core Values and how they benefit you as an ""Overhauler""
Authenticity, Receptivity and Trust
Extremely competitive base salary package
401(k) with Overhaul match
Flexible working schedules
Remote, hybrid, and/or In-office*
Encouragement and Learning
Progressive advancement opportunity & career mobility
Paid development personal stipend
Monthly lunch and learns
2 Unique learning systems w/Instructor led content
Wellness and Integrity
Rotating Overhaul ""Perks @ work"" (Discounts and Freebies)
Overhaul fully provided healthcare plan
Employee assistance & wellbeing programs
New Parent/Family/Caregiver leave(s)
Daily BAMM time (body and mind movement)
Life by design vacation policy
Diversity and Inclusivity Statement:
Overhaul has always been, and always will be, committed to diversity and inclusion. Our Overhaul Culture Code's top listed commitment is to ""Diversity and Synergy."" All aspects of employment will be based on merit, competence, performance, and business needs. We do not discriminate on the basis of race, color, religion, marital status, age, national origin, ancestry, physical or mental disability, medical condition, pregnancy, genetic information, gender, sexual orientation, gender identity or expression, veteran status, or any other status protected under federal, state, or local law. We strongly encourage people from underrepresented groups to apply!
#BI-Remote","$102,068 /yr (est.)",51 to 200 Employees,Company - Private,Transportation & Logistics,Shipping & Trucking,2016,$25 to $100 million (USD)
"Repstor
4.3",4.3,"Charlotte, NC",Data Engineer,"The Role:
We're hiring a hands-on Data Engineer to be part of Data Solutions organization. The data solutions team builds out tools and infrastructure needed to source, validate, clean, and process the data and build compelling reports for leaders and integrating our systems. Our team is looking for a Data Engineer to help scale our data efforts. If you have passion for data and want to help build Intapp’s next gen data platform that provide actionable insights to drive customer and business outcomes, we’d love to hear from you.
You will contribute to the full Data development and apps integrations life cycle, including design, modeling, data integrations, unit testing, performance tuning, and deployment activities.
What you’ll do:
Work collaboratively with business leaders to identify and define the business requirements, design technical solution, and data architecture for self-serve and provide actionable insights.
You can build highly scalable end-to-end data integrations pipelines using different open source tools and operationalizing them.
Define data integration points, data flow, data strategy and develop functional and technical specifications test plans/scripts to validate the data integration needs, Build and sanity test release libraries.
Build infrastructure for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, cloud based relational or non-relational databases employing ETL tools (Informatica, Talend, Workato) and/or scripting languages like Python and streaming technologies like Kafka, Kinesis that allows users to self-serve themselves.
Apply technologies to solve complex data problems with expert knowledge in programming languages like SQL, Python, Linux, SQL, Hive, Spark and serverless services like Athena, Spectrum
Build and deploy dashboards using one or more visualization tools such as tableau, Power BI
Define optimized DB schemas/semantic layer that power the compelling story-telling dashboards.
Hands on experience with web services (AWS, Azure, etc..).
Experience in REST APIs (i.e. ingesting and creating data through the APIs)
Define SLA and acceptable time lags by data source, define QA process, and socialize resolution process to ensure data accuracy and consistency.
Ideal Candidate Must-Haves
BA/BS Degree in Computer Science, any Engineering discipline, and 2+ years of experience in
relevant Data engineering and Business Intelligence platforms or Master’s degree in Computer
Science, any Engineering discipline / information technology and 2+ years in relevant Data
engineering and Business Intelligence platforms experience
Scripting languages like Python, Go, bash, etc...
Scripting languages packages for data manipulation (i.e., Python’s Pandas, etc.)
Scripting languages with packages for REST data ingestion
Relational SQL and NoSQL databases
Tableau, Power BI, Sisense or other reporting tools
Working knowledge of DW Concepts (Star vs snowflake schema, CDC, SCD, etc...)
ETL Tools such as Informatica, Talend, Dell Boomi, Workato
Serverless query service – Athena, Spectrum
Experience working with AWS components [EC2, S3, DynamoDB]
Data visualization experience.
Preferred Qualifications:
Big data tools Hadoop, Hive, HDFS, Spark etc.
Experience in CI/CD pipelines
Demonstrated proficiency implementing self-service solutions to empower an organization to generate valuable actionable insights.
Setup, maintain, and implement Kafka topics and processes. #LI-JS2
Intapp provides equal employment opportunities to all qualified applicants and will make hiring decisions without regard to race, color, sex, sexual orientation, gender identity or expression, religion, national origin or ancestry, age, disability, marital status, pregnancy, protected veteran status, protected genetic information, political affiliation, or any other characteristic protected by federal, state or local laws. All offers are contingent upon passing a criminal history and other background checks if applicable to the position.
Please note: Intapp will not hire through text message, social media, or email alone. We will never extend a job offer unless you have been contacted directly by an Intapp recruiter and have participated in the interview process which will generally consist of 3 or more virtual or in person meetings. We post all legitimate job openings on the Intapp Career Site at
https://www.intapp.com/working-at-intapp/
.","$100,789 /yr (est.)",501 to 1000 Employees,Company - Public,Information Technology,Enterprise Software & Network Solutions,2000,$100 to $500 million (USD)
"Tietoevry
4.2",4.2,"Hudson, MA",Data Center and Network Engineer,"A leader digital services and software company
Tietoevry creates digital advantage for businesses and society. We are a leading digital services and software company with global presence. Our values - openness, trust, and diversity - and Nordic heritage steer our success.
Headquartered in Finland, Tietoevry employs around 24 000 experts globally. The company serves thousands of enterprises and public sector customers in more than 90 countries. Tietoevry’s annual turnover is approximately EUR 3 billion and its shares are listed on the NASDAQ in Helsinki and Stockholm as well as on the Oslo Børs.
Job description
Tieto U.S. Inc is growing its services to US customers, and we are looking for a skilled Junior-Midlevel Linux admin and Network Engineer for our Data Center R&D IT laboratory automation.
We are not a Cloud Based operation. AWS, Azure, Google cloud experience though a use-full Skill is not a required skill. This position values and requires hands on knowledge and skills with physical hardware and the different Linux OS flavors. Expert skills are not required. What is required is a ""solid foundation"" in Linux OS and a firm understanding of Networking, Hardware, Hardware and OS.
The ideal candidate should have skills and a firm understanding in the foundations of Linux and DevOps and able to handle the lab automation projects consisting of deployments of OS's, access administration, and day-to-day changes that our engineering group and customer needs on Bare Metal Servers. In leu of Devops/Automation we will strongly consider candidates skilled and competent within reason in any of the common and popular programming language such as Java, C++, C#, Python. It is our belief that individuals with these set of skills can easily navigate thru the day to day challenges while growing and incorporating new Automation and CI platforms and challenges into the mix.
Desired skills Example:
- (* a must have) *Linux User administration on Bare Metal. Setting up and tearing down access privileges, Linux deployment, and debugging Bare Metal servers and VM's. Automation skills with the use of Ansible, Jenkins, Puppet Vagrant, will be included in the daily routines.
-In leu of Automation/CI skills, reasonable competence in any of the common and popular programming language such as Java, C++, C#, Python.
- Strong understanding of the nuances of Linux across major Distributions
- Good Linux/Unix Networking skills i.e. config of the NICs via CLI environment or Editable Text Config files,
- Knowledge of firewalls set up, Boot process, LVM, EFI, and Legacy Boot and Hard Drive partitions, creating mount points.
- A Break-Fix (without breaking it) approach to troubleshooting.
- Debugging skills. Maintain an organized and methodical approach to debugging
- Be able to work around inconsistent/intermittent results/outcomes.
- Solid understanding of Hypervisors, Virtualization (VM's) their purpose and use.
- Understanding of Automation, Cloud Technologies, VM Deployment/Orchestration methods, and technologies i.e. PXE, Cloud, Containers.
- Strong scripting experience (enabling quick automation engineering)
- Experience in the Datacenter HW environment and familiarity with Deployment automation tools like MAAS or RedHat Beaker would be an additional asset
Additional Skills Requirements
Organizational Skills
Ability to switch gears while prioritizing tasks
Great with Communication
Ability to complete a task with little or no supervision once directed
Commitment to the Position with our Client
Time Management
Ability to work under timelines
Multi - Task
Methodical
Attention to detail
Willingness to Learn
Ability to think ahead, Planning
Join our growing team, with very generous benefits with health, insurances, 401k, vacation.
Location: Hudson, MA **THIS IS NOT A REMOTE POSITION
The position will be opened on 1/15/2023.
Job Type: Full-time
Pay: $55,000.00 - $70,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Life insurance
Paid time off
Vision insurance
Ability to commute/relocate:
Hudson, MA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Linux Admin: 1 year (Required)
Work Location: In person","$62,500 /yr (est.)",10000+ Employees,Company - Public,Information Technology,Information Technology Support Services,1968,$1 to $5 billion (USD)
1point system,#N/A,Remote,Senior Data Engineer,"REQUIRED Top 3 Technical skills:
1. MS Sql and .Net experience
2. PostGras, Mongo
3. Experience with BI tools – Tableau,
4. Cloud Technologies – Azure and AWS
5. Python, Java – Nice to haves
Typical Day to Day: They need someone who can come in and roll up their sleeves, get in the trenches and getting things done. Need equal parts soft skills like attention to detail, time management, customer service; lots of technical skills. They will be working with the data team to build, design and develop the data infrastructure systems. This person will work with others and to gather data requirements, implement data solutions and ensure performance of the data to the pipelines. They will be working through an Azure Data Lake.
Job Title: Data Engineer
Job Location: Fully Remote – But ideally someone who can work east coast hours.
Roles
and Responsibilities
· Design and develop scalable, efficient, and reliable data pipelines and ETL processes to process large volumes of structured and unstructured data.
· Collaborate with stakeholders to understand data requirements and translate them into technical specifications and data models.
· Architect, implement, and manage data storage and retrieval systems, including databases, data warehouses, and data lakes.
· Optimize data pipelines and processes for performance, scalability, and data quality, ensuring timely and accurate data delivery.
· Perform data profiling, validation, and cleansing to maintain data integrity and consistency.
· Identify and resolve data-related issues, ensuring the accuracy and reliability of data.
· Implement and maintain data governance practices, data security measures, and data access controls.
· Stay up to date with the latest advancements in data engineering technologies and methodologies and evaluate the potential impact on our data infrastructure.
· Mentor and provide guidance to data engineers, fostering their professional growth and development.
· Collaborate with cross-functional teams to drive data-driven decision-making and provide actionable insights.
· Communicate effectively with technical and non-technical audiences.
· Compile and maintain clear and comprehensive documentation for all data products, services, and platforms.
Working Conditions:
· Ability to collaborate in a team working environment.
· Demonstrates strong collaboration and communication skills (written and verbal).
· Active listener able to translate problems into creative solutions.
· Possesses excellent customer service skills.
Qualifications and Requirements:
· Bachelor’s degree in computer science, Information Systems, or a related field.
· Proven experience as a Data Engineer or similar role, with at least 7 years of experience in data engineering.
· Strong programming skills in languages like Python, Java, .NET, with experience in data manipulation and transformation.
· Proficiency in SQL and experience with relational databases (e.g., MS SQL, MySQL, PostgreSQL) and NoSQL databases (e.g., MongoDB) and common data formats (e.g., JSON, XML, CSV).
· In-depth knowledge of data modeling, data warehousing, and data integration techniques and best practices.
· Hands-on experience with big data technologies and distributed computing frameworks is highly desirable.
· Familiarity with cloud-based data platforms (e.g., AWS, Azure, Google Cloud) and services.
· Strong understanding of data quality, data governance, and data security principles.
· Experience with data visualization tools (e.g., Tableau, Power BI) is a plus.
· Excellent problem-solving and analytical skills, with a strong attention to detail.
· Effective communication skills, with the ability to convey complex technical concepts to non-technical stakeholders.
· Proven ability to work collaboratively in a team environment and lead technical initiatives.
Job Type: Contract
Salary: $84,454.31 - $190,806.62 per year
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote","$137,630 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Charles Schwab
4.1",4.1,"Westlake, TX",Associate - Data Engineer,"Your Opportunity

This full-time role is part of a nine-month NERD (New Employee Recruitment and Development) program that blends on-the-job experience with an extensive training curriculum that covers tools, technologies, processes, and soft skills required to be successful in Schwab Technology Services. By pairing the curriculum, on-the-job experience, and a web-of-support from others, NERDs are well prepared to
succeed in their role which sets the table for future opportunities at Schwab.

What you’ll do
As a member of the NERD program, you will be working in our Data and Rep Technology (DaRT) organization that governs the strategy and implementation of the enterprise data warehouse and emerging data platforms. You will collaborate with partners across the firm to build the next generation analytics platform and capabilities for Charles Schwab. DaRT supports executive leadership, Sales, Marketing, and Finance teams by integrating and analyzing data to help them make data-based decisions through four primary opportunities:

Data Science Frameworks
This team partners with our data scientists and business stakeholders to design intuitive data solutions and create the best-suited serving pipelines for their modeling and analysis needs. You will help drive the deployment of cross-functional applications, determine platform design and architecture, and set the vision to build and scale our machine learning and experimentation platforms.

Data Engineering
This team designs, develops, and implements enterprise data integration solutions. You will have the opportunity to partner with other developers to set the future of the Data Warehouse through exciting and challenging projects and learning and using emerging technologies.

Data Analytics
This team performs data analysis of enterprise integration solutions to transform data into actionable insights leveraging best-in-class technologies, analytics, and visualization tools.

Platform and Production Support
This team builds next-gen enterprise data platforms, manages currency upkeep of existing platform assets, and matures those investments. The core functions include platform engineering, tenant-focused governance, capacity management, software upgrades, performance optimization, administration, lifecycle management, and maintenance.

What you will get out of the program
Mentorship
Challenging Career Opportunities
Continuous Training and Development
Certification Opportunities
Hands-on Technology Experience
Knowledge Sharing and Presentation Opportunities
Exposure to Leadership
Community of Dedicated and Welcoming Peers

Workplace Flexibility Program
We're proud to support our employees in a working approach that allows you to bring your best self to work – whether that’s in the office or remote.
Employees will have the flexibility of a hybrid work environment, spending some time working remotely and sometimes in the office.
Employees and managers can discuss and decide what works best for them, with flexibility available based on their role, business needs, and individual circumstances.
What you are good at

You enjoy problem solving
You have a hunger for knowledge
You work well with others
You are a good communicator
What you have

Undergraduate or graduate degree in Computer Science, Management Information Systems, or related discipline with a graduation date of August 2023 or earlier and/or
Workforce training certifications through coding bootcamps with a graduation date of August 2023 or earlier
Ability to start full-time with the program on September 18, 2023
Basic understanding of data modeling
Understanding of SQL development principles
Experience with algorithm design
Basic understanding of object-oriented analysis and design
Familiarity with data structures
Experience with data engineering, Extract, Transform, Load (ETL), Hadoop, MongoDB, Unix, Sqoop, HiveQL, or Pig Scripting is a plus
Inventiveness and eagerness to work with experimental technology
Demonstrated leadership potential
Passion and interest in solving problems applying innovation and experimentation","$88,000 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Investment & Asset Management,1973,$10+ billion (USD)
"Core4ce
4.3",4.3,"Herndon, VA",Data Engineer,"The Data Engineer will provide the engineering support to data science and software engineering team members.
Design and optimize Data Pipelines using Spark, Hudi, EMR cloud services, and Kubernetes containers
Make sure pedigree and provenance of the data is maintained such that the access to data is protected
Clean and preprocess data to enable analytic access
Architects complex, repeatable ETL processes
Provide Advanced Database Administration support in Oracle, MySQL, MariaDB, MongoDB, Elastic and others
Supports Experience with Targeting using Sponsor Tools, Reverse Engineering
Develop API connectors to enable ingest of new data catalog entries from databases and files
Ensure that data mappings will provide the best performance for expected user experience
Augmenting technical support in the area of data science, data engineering, and systems engineering, and reviewing and providing technical assessments.
Provides current system architecture documentation, engineering/web development programming support for program/project requirements defined tasks, data science/data engineering related technical assessments, effective communication with users and managers, and server administration, to include hardware and software support to existing servers.
Support ad-hoc data analysis requirements defined by the client's Leadership.
Reporting solutions will encompass multiple technology platforms used by the client and will drive business process re-design and raining/change management initiatives.
Will work on Data cleaning and transformation efforts in delivery of CDRLs.
Supports Deliverables and Reports

Requirements
5+ years experience working with Data Sets
Experienced in extracting and aggregating structured and unstructured data.
Experienced in data programming languages and tools such as Python and R.
Experience with SQL or similar database language.
Experience designing and implementing data models to enable, sustain and enhance the value of information they contain
Knowledge of business intelligence reporting tools and data visualization software including Tableau.
Strong analytical and critical thinking skills.
Ability to work collaboratively and effectively in a team environment.
TS/SCI with Full Scope Poly Required

All qualified applicants will receive consideration for employment without regard to race, color, sex, sexual orientation, gender identity, religion, national origin, disability, veteran status, age, marital status, pregnancy, genetic information, or other legally protected status

Required Skills

Required Experience","$103,496 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2018,$100 to $500 million (USD)
Next Gen It,#N/A,"Charlotte, NC","Senior Data Integration Engineer – Charlotte, NC/New York/Chicago, IL","Major Responsibilities:
Experience designing and developing Enterprise Data Warehouse solutions.
Demonstrated proficiency with Data Analytics, Data Insights
Proficient in writing SQL queries and programming including stored procedures and reverse engineering existing process
Leverage SQL, programming language (Python or similar) and/or ETL Tools (Azure Data Factory, Data Bricks, Talend and SnowSQL) to develop data pipeline solutions to ingest and exploit new and existing data sources.•
Perform code reviews to ensure fit to requirements, optimal execution patterns and adherence to established standards.
Skills
10+ years - Enterprise Data Management
10+ years - SQL Server based development of large datasets
5+ years with Data Architecture
3+ years’ experience in Finance / Banking industry – some understanding of Securities and Banking products and their data footprints.
2+ years Python coding experience
Proficient with Data Visualization tools
Hands-on experience with Snowflake utilities such as SnowSQL and SnowPipe
Working knowledge of MS Azure configuration items with respect to Snowflake.
Hands-on experience with Tasks, Streams, Time travel, Optimizer, Metadata Manager, data sharing
Experience in Data warehousing - OLTP, OLAP, Dimensions, Facts, and Data modeling.
Previous experience leading an enterprise-wide Cloud Data Platform migration with strong architectural and design skills
Capable of discussing enterprise level services independent of technology stack
Experience with Cloud based data architectures, messaging, analytics
Superior communication skills
Cloud certification(s)
Any experience with Regulatory Reporting is a Plus
Education
Minimally a BA degree within an engineering and/or computer science discipline
Master’s degree strongly preferred
Job Types: Full-time, Contract
Pay: $75.00 per hour
Experience level:
11+ years
Ability to commute/relocate:
Charlotte, NC: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
2) How many years of TOTAL IT experience do you have? (Text Box with Numeric value)
4) Is the actual candidate applying for this job?
8) Did you include JOB NO in your resume? (Please ensure to include JOB NO in your resume prior to attaching to the job posting)
9) Have you attached your resume to the job posting?
10) Are you the employer/Vendor applying on behalf of the candidate?
11) Do you have an employer supporting your employment currently?
Work Location: In person",$75.00 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Lcp Tracker Inc
4.4",4.4,"New Braunfels, TX",Data Engineer,"Company Summary
LCPtracker, Inc. is a leading software service provider specializing in construction site compliance related software, headquartered in Orange, CA. Our main solution, LCPtracker Pro, is a powerful web-based SaaS solution for collecting, verifying, and managing certified payrolls and other labor compliance related documents. Over 200 government agencies and 100,000 contractors have used LCPtracker for their certified payroll reporting.
In 2023, our growth continues at a rapid pace, making LCPtracker one of the fastest growing small companies in Orange County, California, recognized by the Orange County Business Journal. In 2017, 2018, 2019, 2020, 2021 and 2022, LCPtracker was recognized as an Orange County ""Best Places to Work"" by the Orange County Register.

Position Summary
As a Data Engineer at LCPtracker, you will leverage your experience with SQL, ETL, and reporting to drive the design and development of data-driven solutions. The position is responsible for performing advanced technical and analytical work in the development and support of standardized and customized reports, as well as the testing and maintaining of data integrity.
Other Duties: Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.

Core Competencies
Confidentiality: This role may be privy to confidential and/or sensitive information. Must demonstrate integrity in maintaining confidential and sensitive information and demonstrate strict adherence to organizational policies and procedures.
Communication Proficiency: uses friendly and proficient communication to interact with a wide range of people, frequently exchanging information about office operations.
Time Management: Must manage their own time. They use an electronic calendar in an email program to set meetings, to request others to attend and to coordinate their responses. They respond to requests for attendance at various meetings.
Initiative and Proactivity: Correctly anticipates a need, volunteers readily, and acts without being told to do so. Brings new ideas to the company. Undertakes self-development activities; seeks increased responsibilities; takes calculated risks; looks for and takes advantage of opportunities; asks for and offers help when needed.
Drive for Results: Is goal-oriented; maintains focus on the objective.
Problem Solving, Personal Judgment: Identifies and resolves problems in a timely manner; gathers and analyzes information skillfully; develops alternative solutions; works well in group problem-solving situations; uses reason even when dealing with emotional topics. Solicits and applies feedback.
Quality Management: Looks for ways to improve and promote quality; demonstrates accuracy and thoroughness. Does not cut corners; monitors work to ensure quality; applies feedback to improve performance.
Primary Duties and Responsibilities
Develop, modify, maintain, and support custom reports (MS SQL, SSRS) for both ad-hoc and ongoing business needs.
Develop MS SQL objects (tables, stored procedures, functions, views, etc.) as applicable.
Create and customize weekly, monthly, quarterly, and annual reports using Microsoft Excel or other reporting tools as applicable.
Ensure high data quality through regular quality checks.
Extract, filter, and aggregate data through logical queries and programming.
Maintain a high level of confidentiality and use discretion when needed.
Perform other work including specific tasks or special projects as required.
Promote and maintain positive morale through teamwork.
Other Duties: Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.

Salary Range
Data Engineer rate $100,000 to $140,000 annual salary
Pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, and experience. The total compensation package for this position may also include other elements dependent on the position offered. Details of participation in these benefit plans will be provided if an employee receives an offer of employment.

Benefits
Paid Time Off
9 Paid Holidays
Phantom Stock
401k Plan with up to 4% company match
Medical Benefits (Health, Vision and Dental)
Life Insurance
LTD & STD

Work Environment
This position performs its duties from our New Braunfels, TX office. This position operates in a professional office environment and role routinely uses standard office equipment such as computers, phones, mobile devices, photocopiers, filing cabinets and fax machines.

Physical Requirements
While performing the functions of this job, the employee is regularly required to sit; frequently required to talk and hear, use hands and fingers to type, scroll and use computer equipment. The employee is required to have close visual acuity to perform an activity such as: preparing and analyzing data and figures; transcribing; viewing a computer terminal; extensive reading; visual inspection of text/data in both print and electronic forms.
Ability to lift and move up to 25 pounds.
Position Type and Expected Hours of Work
This is a full-time exempt position reporting to our New Braunfels, TX office M-F 8am – 5pm. Days/hours worked are dependent on the workload at the time. General availability and presence in the office is expected during regular business hours Monday-Friday. However, some flexibility is allowed. Occasional evening and weekend work may be required as job duties demand.

Travel
There is no major travel requirement for this position. However, infrequent travel may be necessary to visit remote office(s), attend conferences/industry events, etc. Attendance at our corporate Staff Retreat is required. This event is a 2-3-day retreat. Attendance at our annual User Conference as assigned.

LCPtracker, Inc. is an equal opportunity employer of all qualified individuals. All applicants will be afforded equal opportunity without discrimination because of race, color, religion, sex, sexual orientation, marital status, order of protection status, national origin or ancestry, citizenship status, age, physical or mental disability unrelated to ability, military status or an unfavorable discharge from military service. LCPtracker, Inc. will consider for employment qualified applicants with criminal histories in a manner consistent with all federal, state, and local ordinances.
LCPtracker is committed to the full inclusion of all qualified individuals. In keeping with our commitment, LCPtracker will take steps to assure that people with disabilities are provided reasonable accommodations. Accordingly, if reasonable accommodation is required to fully participate in the job application or interview process, to perform the essential functions of the position, and/or to receive all other benefits and privileges of employment, please contact the LCPtracker Human Resources Department at HR@lcptracker.com.
Education and Experience
MUST HAVE:
Bachelor’s degree in software engineering disciplines, computer science or other related field and/or the equivalent combination of education and experience.
7+ years of experience with SSRS reporting tools and MS SQL server.
7+ years of experience using ETL tools such as SSIS and/or ADF.
7+ years of experience in Data Warehousing with SSAS or AAS.
Experience in applying security to SSAS or AAS models using authentication frameworks such as AAD or Active Directory.
Adept at queries, report writing and presenting findings.
Ability to comprehend, analyze, and systematically compile technical, statistical, and information into comprehensive reports or other formats.
Effective business writing and composition skills with good command of the English language.
Ability to independently plan, organize, and complete a variety of projects within established standards, objectives and time frames.
Ability to work in fast-paced, multi-tasking environment with shifting priorities and demanding deadlines.
Ability to work independently in finding solutions
Ability to work in an agile work environment
Ability to work in a team environment
Must be detailed-oriented and able to effectively prioritize and organize workload, with efficient time management.
Minimum 1-year experience working on Scrum teams.
Basic understanding of the Agile methodology.
Scrum certifications are a plus.
Strong interpersonal communication skills.","$120,000 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,1992,$1 to $5 million (USD)
Premier Consulting Group,#N/A,"Boca Raton, FL",Senior Data Engineer,"* The right person could be located in Florida or Massachusetts as company has office locations in Boca Roton and Boston.
* Must be a US Citizen
Overview Summary of the Work:
ETL Azure Data Factory work: A Data Engineer who is to be able to import data from a variety of different sources (SFTP, API, Fileshare) and file types (CSV, Excel, JSON). Company also captures files via email now, but they have a pattern set up to be able to do that, so the person would just repeat how they’ve done that. Check existing Azure Data Factory pipelines for failures and troubleshoot. Familiar with dynamic expression and syntax in Azure Data Factory pipelines.
Backlog of stored procedures: Be skilled in stored procedure development and SQL skills.
Data quality and data cleansing: Be able to evaluate data (incoming and existing). Look for issues and be able to resolve them. In general, someone who can look at the data and mentally make the leap of “hey, this looks wrong.” Look at data and determine how it might be better utilized (examples: 2.5% as a string or 0.025 as a number, identify values as empty strings and save as nulls instead, etc).
Technical Experience Required/Preferred:
SQL, Azure SQL, Azure Data Factory, Data warehousing environments (dims and facts), Snowflake (highly preferred), Python (highly preferred).
Job Types: Full-time, Permanent
Pay: $140,000.00 - $160,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Bonus pay
Yearly bonus
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Weekend availability
Work Location: Hybrid remote in Boca Raton, FL 33431","$150,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
TekValue IT Solutions,#N/A,"Houston, TX",Data Engineer with Migration,"Data Engineer Day 1 Onsite Loc Houston Texas Required Skills: Requirements: 8-10 Required Experince on Data Engineer and Revalent Skills Experience with NoSQL (MongoDB) Experience with API'S Good Knowledge on Data Migration like Oracle to Mongo db Experience with Python Programming
Job Type: Contract
Salary: $65.00 - $70.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Houston, TX 77001: Reliably commute or planning to relocate before starting work (Required)
Experience:
MongoDB: 5 years (Preferred)
Data migration: 5 years (Preferred)
NoSQL: 5 years (Preferred)
Python: 3 years (Preferred)
Work Location: One location
Speak with the employer
+91 7328323606",$67.50 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Seamless.AI
3.4",3.4,"Columbus, OH",Data Engineer - Remote US,"The Opportunity
We are seeking an experienced Data Engineer with a minimum of 3 years of experience in building data ingestion pipelines for large datasets. As a key player in our team, you will be responsible for designing, building, and managing our data infrastructure, supporting our data-driven decision-making capability. You'll need to be proficient in Python, AWS, and common frameworks used for data ingestion, transformation, and consolidation. The ideal candidate will be passionate about data, a strong team player, and have a continuous learning mentality.
About Seamless
Seamless delivers the world's best sales leads. Through our product, we help sales teams maximize revenue, increase sales, and easily acquire their total addressable market using artificial intelligence; by development of a robust real-time contact and company search engine as well as a suite of technically-advanced tools to support sales and lead generation. We have been recognized as one of Ohio's fastest growing companies and have been recently ranked No. 7 in LinkedIn's Top 50 Startups of 2022, featured in Forbes as #1 Software company in Ohio in 2022, and on G2's ""Top 100 Highest Satisfaction Products for 2022"" list!
The Seamless Team
We have an amazing culture and work environment that anyone would want to be a part of. We encourage a culture of positivity. We thrive off of continuous feedback and do whatever it takes to help our team and customers be successful. You will grow as an individual, professionally, and be able to see and feel the impact you are making to the growth of Seamless every day.
Role Responsibilities
Design, develop and optimize data ingestion pipelines to handle real-time and batch data streams using a variety of sources.
Utilize your extensive knowledge of Python and AWS to engineer solutions for the transformation, consolidation, and storage of large datasets.
Collaborate with data scientists and other stakeholders to understand data needs and translate them into data systems and pipelines.
Enhance our data ecosystem by leveraging industry best practices for testing, deployment, and runtime environments.
Drive continuous improvements to data reliability, efficiency, and quality.
Document data architectures, procedures, and data flows, maintaining excellent communication with the team and stakeholders.
Monitor data systems performance, troubleshoot data issues, perform root cause analysis, and ensure the implementation of optimal solutions.
Participate in data governance and ensure adherence to data security and privacy standards.
Candidate Requirements
A minimum of 3 years of experience as a Data Engineer or in a similar role.
Strong proficiency with Python, AWS, and common frameworks used in data ingestion and transformation.
Hands-on experience building and optimizing large scale data pipelines, architectures, and data sets.
Knowledge of data warehousing concepts, including data modeling, data cleaning, and ETL processes.
Strong understanding of database design and data management principles.
Experience with AWS cloud services such as EC2, S3, Redshift, DynamoDB, and others.
Strong problem-solving skills, and the ability to analyze data and design solutions to complex data issues.
Excellent communication and teamwork skills, and a passion for data.
Experience with other programming languages (e.g., Java, Scala) is a plus.
Familiarity with big data tools (e.g., Hadoop, Spark) is a plus.
Seamless.AI has been delivering the world's best sales leads since 2015. Our product is the first real time, B2B search engine helping sales teams maximize revenue, increase sales, and easily acquire their total addressable market using artificial intelligence. We have been recognized as one of Ohio's fastest growing companies and won 2020 Best Places to Work and LinkedIn's Top 50 Tech Startups in 2020 and 2022. We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. Visa Sponsorship is not included in our hiring package. Applicants will need to be authorized to work in the U.S.","$84,853 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable
"HCA Healthcare
3.3",3.3,"Nashville, TN",Data Engineer,"Introduction
Last year our HCA Healthcare colleagues invested over 156,000 hours volunteering in our communities. As a Staff Data Engineer with HCA Healthcare you can be a part of an organization that is devoted to giving back!
Benefits
HCA Healthcare, offers a total rewards package that supports the health, life, career and retirement of our colleagues. The available plans and programs include:
Comprehensive medical coverage that covers many common services at no cost or for a low copay. Plans include prescription drug and behavioral health coverage as well as free telemedicine services and free AirMed medical transportation.
Additional options for dental and vision benefits, life and disability coverage, flexible spending accounts, supplemental health protection plans (accident, critical illness, hospital indemnity), auto and home insurance, identity theft protection, legal counseling, long-term care coverage, moving assistance, pet insurance and more.
Free counseling services and resources for emotional, physical and financial wellbeing
401(k) Plan with a 100% match on 3% to 9% of pay (based on years of service)
Employee Stock Purchase Plan with 10% off HCA Healthcare stock
Family support through fertility and family building benefits with Progyny and adoption assistance.
Referral services for child, elder and pet care, home and auto repair, event planning and more
Consumer discounts through Abenity and Consumer Discounts
Retirement readiness, rollover assistance services and preferred banking partnerships
Education assistance (tuition, student loan, certification support, dependent scholarships)
Colleague recognition program
Time Away From Work Program (paid time off, paid family leave, long- and short-term disability coverage and leaves of absence)
Employee Health Assistance Fund that offers free employee-only coverage to full-time and part-time colleagues based on income.
Learn more about Employee Benefits
Note: Eligibility for benefits may vary by location.

Would you like to unlock your potential with a leading healthcare provider dedicated to the growth and development of our colleagues? Join the HCA Healthcare family! We will give you the tools and resources you need to succeed in our organization. We are looking for an enthusiastic Staff Data Engineer to help us reach our goals. Unlock your potential!
Job Summary and Qualifications
Data Engineers within HCA’s Information and Analytics organization are responsible for defining and implementing data management practices across the enterprise. This full-time position will focus primarily on enterprise data management and migrating of data to the cloud. Data Engineers are expected to source and incorporate new data sources into the Enterprise Data Ecosystem. The responsibilities will include writing, testing, and reviewing ETL pipelines for defining and implementing data management practices across the enterprise. Due to the emerging and fast-evolving nature of Cloud technology and practice, the position requires that one stay well-informed of technological advancements and be proficient at putting new innovations into effective practice.
As a Data Engineer, you will work closely with all team members to create a modular, scalable solution that addresses current needs, but will also serve as a foundation for future success. The position will be critical in building the team’s engineering practices in test driven development, continuous integration, and automated deployment and is a hands-on team member who actively coaches the team to solve complex problems. This is a leadership position that assumes the responsibility for project success and the upward development of team members. They are the development team's point of contact that must interface with business partners of varying roles ranging from technical staff to executive leadership.
As a Staff Data Engineer level, the role requires 'self-starters' who are proficient in problem solving and capable of bringing clarity to complex situations. It requires contributing to strategic technical direction and system architecture approaches for individual projects and platform migrations. It also requires working closely with others, frequently in a matrixed environment, and with little supervision. This candidate will have a history of increasing responsibility in a small multi-role team. This position requires a candidate who can analyze business requirements, perform design tasks, construct, test, and implement solutions with minimal supervision.
Our Purpose
Applied to this position, your skills will help transform healthcare through technology and solutions that dramatically improve patient care and business operations.
Core Competencies
At HCA ITG, your deliverables will influence patient care. Every process, technology, and decision matters. This role will provide leadership and deep technical expertise in all aspects of solution design and application development for specific business environments. It will focus on setting technical direction on groups of applications and similar technologies as well as taking responsibility for technically robust solutions encompassing all business, architecture, and technology constraints.
Responsible for building and supporting a Cloud based ecosystem designed for enterprise-wide analysis of structured, semi-structured, and unstructured data. Direct the transformation from HCA Healthcare’s current on premise Teradata platform to Google Cloud Platform to enable analytics and machine learning at scale.
Design the cloud environment from a comprehensive perspective, ensuring that it satisfies all the company’s needs.
Develop, manage, and own full data lifecycle from raw data acquisition through transformation to end user consumption
Share knowledge and experience to contribute to growth of overall team capabilities
Perform activities such as deployment, maintenance, monitoring, and management inside the cloud framework that has been created
Provide guidance on technology choices and design considerations for migrating data to the Cloud
Maintain a holistic view of information assets by creating and maintaining artifacts that illustrate how information is stored, processed, and accessed
Demonstrate deep understanding and act as a leader in the team’s continuous integration and continuous delivery automation pipeline
Collaborate with business analysts, project lead, management, and customers on requirements
Design fit-for-purpose products to ensure products align to the customer's strategic plans and technology road maps
Actively participate in technical group discussions and adopt any new technologies to improve the development and operations.
Assist team members with production issues and offer support, guidance, and assist in communicating issues with appropriate stakeholders when necessary.
Provide leadership on key technology choices for Enterprise Data Ecosystem including data warehouse, analytical and big data platforms.
Ensure architectural, quality, and governance adherence through design reviews.
Education & Experience
Bachelor's degree in computer science or related field - Required
Master's degree in computer science or related field - Preferred
3+ years of experience in Data Engineer/Architect- Required
1+ year(s) of experience in Healthcare - Preferred
8+ years of experience in Information Technology - Required
Knowledge, Skills, Abilities, Behaviors
A successful candidate will have:
Experience developing and supporting data pipelines from various source types (on-prem rdbms, AWS, GCS bucket, flat file) to Big Query utilizing Google Cloud Platform native technologies
Knowledge and experience using the following technologies
o Big Query
o Dataflow, Data Proc, Data Fusion, Cloud Composer
o GSUTIL, GCS, Kafka, Pub/Sub
o Data Catalog/Dataplex
o Python, Unix, Linux
Strong understanding of best practices and standards for cloud application design and implementation.
Extensive experience with relational database management systems; Teradata, Oracle or SQL Server:
o Advanced SQL skills
o Write, tune, and interpret SQL queries
o BTEQs
o Stored procedures
Experience with Unstructured Data
Ability to troubleshoot, maintain, reverse engineer, and optimize existing ETL pipelines.
Requires strong practical experience in agile application development and DevOps discipline, including deployment of CI/CD pipelines in Git
Ability to multitask and to balance competing priorities.
Expertise in planning, implementing, supporting, and tuning Cloud ecosystem environments using a variety of tools and techniques.
Ability to define and utilize best practice techniques and to impose order in a fast-changing environment. Must have strong problem-solving skills.
Strong verbal, written, and interpersonal skills, including a desire to work within a highly-matrixed, team-oriented environment.
A successful candidate may have:
o Experience in Healthcare Domain
o Experience in Patient Data
Certifications (a plus, but not required)
GCP Cloud Professional Data Engineer
HCA Healthcare has been recognized as one of the World’s Most Ethical Companies® by the Ethisphere Institute more than ten times. In recent years, HCA Healthcare spent an estimated $3.7 billion in cost for the delivery of charitable care, uninsured discounts, and other uncompensated expenses.
""There is so much good to do in the world and so many different ways to do it.""- Dr. Thomas Frist, Sr.
HCA Healthcare Co-Founder
Be a part of an organization that invests in you! We are reviewing applications for our Staff Data Engineer opening. Qualified candidates will be contacted for interviews. Submit your application and help us raise the bar in patient care!
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.","$93,734 /yr (est.)",10000+ Employees,Company - Public,Healthcare,Health Care Services & Hospitals,1968,$10+ billion (USD)
NextGenITSupport,#N/A,"Raleigh, NC",Network data engineer - onsite,"Job Title: Network Data Engineer.
Must have Skill: HP ARUBA experience.
Rate: $45/hour
Location: On-Site in Raleigh, North Carolina
Job Description:
Experience with HP ARUBA is a must.
Develop and install network infrastructure, configurations and equipment such as routers and switches
Monitor networks and troubleshoot issues or outages.
LAN, WAN, SWITCH & ROUTING experience is needed.
Consult with clients to suggest network solutions
Manage junior employees and provide training resources for team members
Test and install new computer systems, hardware, software and applications
Develop engineering design packages to integrate new processes into existing ones
Collaborate with clients, other tech support services and network providers to ensure the quality of networks
Team player and support on-call responsibilities as HP ARUBA NMS team member
Coordinate upgrades/updates and maintenance of HP ARUBA NMS tool solutions
Collaborate across multiple technical teams and lines of business with focus on implementing enterprise NMS solutions.
Strong problem-solving and troubleshooting skills
Excellent communication and interpersonal abilities
Ability to work independently and as part of a team
Ability to multi-task, prioritize, and manage time efficiently
Highly organized and detail-oriented
Certifications:-
CCNA minimum CCNP preferred.
· 7+ years experience with network planning, design, and implementing LAN, WAN, network security, and wireless network infrastructures.
Job Type: Contract
Pay: $45.00 - $48.00 per hour
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",$46.50 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"DiNi Communications, Inc.
3.0",3.0,Remote,Senior Data Engineer,"DiNi Communications, Inc. is seeking a Senior Data Engineer. This person needs to be a team player with exceptional communication skills. Maintaining the confidentiality of the client's information is of utmost importance.
Skills and experience:
· 12+ years of experience in data engineering with a focus on designing and building scalable data platforms using cloud technologies.
· Strong experience designing and implementing data models and schemas in Snowflake.
· Define Snowflake databases objects to support efficient data storage and retrieval.
· Expert proficiency in SQL and one or more programming languages such as Python, Java, or Scala.
· Knowledge of cloud-based data platforms such as AWS, Azure, or GCP, including experience with cloud-based storage, compute, and data processing services.
Experience designing and implementing ETL and data integration pipelines, and familiarity with data modeling concepts and database design principles.
· Excellent communication skills and the ability to collaborate effectively with cross-functional teams.
· Experience with Agile methodologies and working in an Agile team environment.
· Experience developing production-grade, large-scale data solutions using cloud technologies.
· Experience managing data orchestration at scale using tools such as Airflow and Dagster.
· Familiarity with version control systems (e.g., Git) and CI/CD principles.
Preferred skills:
· Experience developing dashboards and reports in applications such as Oracle Analytics Server (OAS), Microsoft Power BI, and Google Looker.
· Thorough experience with data integration tools such as Informatica Intelligent Cloud Services and MuleSoft.
· Experience using Azure services for Security, Blob Storage, Data Lake, Databricks, Data Factory etc.
· Experience with Azure Monitoring services
· Microsoft Certified Azure Solutions Architect Expert or a Snowpro Certification or a similar one
Tasks include, but are not limited to:
· Collaborate with stakeholders to define and understand data needs.
· Design and develop efficient data architectures that can support large-scale data processing and storage requirements.
· Develop and maintain data pipelines, data models, and ETL processes that align with business requirements, data quality standards, and industry best practices.
· Work closely with other data engineering teams to build and maintain reusable data pipelines and tools, enabling faster time-to-market for data-driven solutions.
· Monitor, troubleshoot, and optimize data pipelines and processes for performance, reliability, and scalability.
· Ensure the quality and integrity of various datasets across different platforms and data sources.
· Continuously evaluate and recommend emerging technologies and methodologies to improve data engineering processes, workflows, and performance.
· Mentor and guide junior data engineers on technical best practices, code reviews, and design patterns to ensure high-quality, scalable, and maintainable data engineering solutions.
· Perform special technology-related projects, as assigned.
Job Type: Full-time
Pay: $80.00 - $95.00 per hour
Schedule:
Monday to Friday
Education:
Bachelor's (Preferred)
Experience:
(12+) Data engineering: 10 years (Required)
Snowflake: 5 years (Preferred)
Expert proficiency in SQL: 10 years (Preferred)
AWS, Azure, or GCP: 5 years (Preferred)
design and implementing ETL and data integration pipelines: 5 years (Preferred)
Agile methodologies: 5 years (Preferred)
developing production-grade, large-scale data solutions: 5 years (Preferred)
managing data orchestration: 5 years (Preferred)
version control systems and CI/CD principles: 4 years (Preferred)
local, state, or federal government entities: 2 years (Preferred)
leveraging Jira, Trello, ServiceNow or other tools: 3 years (Preferred)
Work Location: Remote",$87.50 /hr (est.),Unknown,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"INTELETECH GLOBAL INC
3.7",3.7,"Altamonte Springs, FL",Data Engineer,"Role: Data Engineer
Location: Florida
Type: Contract ( Only w2 )
Visa: Any
Experience level: Mid-level

The Role: · You'll be developing, deploying, and maintaining our production data pipeline which produces risk scores and patient reports vital to the workflows of doctors and care coordinators.· You'll be ensuring product deliverables are executed reliably and accurately on a regular basis.· You'll be managing and monitoring client interfaces to ensure timely delivery of data.

Qualifications :· Bachelor's Degree in computer science, physics, math, or a related field· Minimum 3+ years experience in data engineering in industrial or clinical settings· Experience in deploying data pipelines to production, including large-scale cloud deployments· Adaptability within a dynamic and collaborative environment· Commitment to improve processes and reduce inefficiencies· Deep curiosity to dive into the details of human research studies

We also appreciate if you have familiarity with the following:

Python
SQL
Cloud platforms (e.g. AWS)
Relational and no-SQL database systems
Multi-modal and sensor data","$83,235 /yr (est.)",1 to 50 Employees,Company - Private,Financial Services,Accounting & Tax,#N/A,Unknown / Non-Applicable
keasis Inc,#N/A,"Wilmington, DE",Big Data Engineer,"Big Data Engineer - (ETL/Data Warehousing/Java)
Wilmington, DE
6+ Month CTH
5 openings
Supporting big data platforms
Going through a modernization and data center migration
Moving from informatica tools to Java/Spark, big data, data modeling etc – Java is a MUST HAVE
Looking for strong sr devs with Java/Spark skillsets
Any modernization or migration experience is a huge plus
Transitioning from ETL skillsets into java/spark
Required Skills:
5+ years of professional experience as a developer in a data warehousing or other data oriented, batch processing environment
5+ years experience in analysis of data or complex processes and systems, demonstrating strong analytical skills
3+ years of hands on experience with Java
2+ years of hands on experience with Spark
3+ years of hands on experience with relational databases such as Teradata or Oracle
3+ years using SQL
2+ years creating complex technical designs which included data mappings
Experience with Unix shell scripting (is a plus)
Experience with Tableau or other BI Reporting tools is a plus
Experience with Ab Initio is a plus
Experience with a scheduling tool, especially Control-M, is a plus.
Experience with issue analysis and resolution including usage of issue resolution processes for application production problems.
Experience working in an Agile setting
Demonstrated ability to work in a team environment with a structured SDLC and interface and coordinate with a variety of business and I/T groups.
Job Type: Contract
Salary: $50.00 - $70.00 per hour
Ability to commute/relocate:
Wilmington, DE 19801: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
How many years of Big Data experience do you have?
Are you ready to work on W2?
Experience:
Informatica: 8 years (Required)
SQL: 5 years (Required)
Data warehouse: 5 years (Required)
Work Location: One location",$60.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Arcon Group Inc,#N/A,"Mooresville, NC",Data Engineer,"Job Title: Dot NET Full Stack
Location: Charlotte, NC (Day 1 Onsite)
Duration: 12+ Months
Client: Wells Fargo
only on W2
Interview Mode: Phone & TEAMS
Note: We are looking only at OPT & H1-B
Minimum 3-5 years of experience with C#, .NET
Familiarity with the ASP.NET framework, SQL Server, and design/architectural patterns (e.g. Model-View-Controller (MVC))Experienced in implementing niche solutions with C# and .NET
Abundant experience in designing and writing reusable code with C# and .NET
Experienced with SQL/Oracle/Linux/Windows Servers
Work experience with Oracle, SQL, MySQL Database
Good to Have
Familiarity with Any Cloud Functions
C++/Java/Perl
Power Shell script
SAFE Agile Development
Job Type: Full-time
Salary: $45.00 - $55.00 per hour
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Mooresville, NC 28115: Reliably commute or planning to relocate before starting work (Required)
Education:
Master's (Preferred)
Experience:
Informatica: 1 year (Preferred)
SQL: 2 years (Preferred)
ASP.NET: 1 year (Preferred)
License/Certification:
Driver's License (Preferred)
Work Location: One location",$50.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Blackspoke
4.9",4.9,"Chantilly, VA",Data Engineer,"Own your opportunity to serve as a critical component of our nation’s safety and security. Make an impact by using your expertise to protect our country from threats and help ensure today is safe and tomorrow is smarter. Our work depends on a Data Engineer joining our team to support data analysts, data scientists, and big data engineers in identifying data sources, performing exploratory data analysis, developing data models, ensuring data cleanliness and accuracy to provide new mission-enhancing insights.

What you will be working on:
As the Data Engineer, you will tailor cutting-edge solutions to the unique requirements of our clients and make systems performance and availability your priority.
Expand our mission capabilities in automating data integration and collection strategies
Coordinate system development to include design, modeling, security, integration, and formal testing
Contribute to completion of engineering programs and projects
Develop solutions to a variety of complex engineering problems
Optimize the data ingestion pipeline architecture, develop strategies for efficient ingestion, processing, storage, structuring, and access
Improve responsiveness and overall performance of the data ingestion pipeline architecture
Prepare and maintain documentation for processes and procedures relate to engineering projects

What you will bring to us (Must):
Active TS/SCI with CI Polygraph
Current DoD IAT II Level certification

Would be nice if you bring the following (Highly Desired):
Bachelor's degree in Engineering, Computer Science, or other related analytical, scientific, or technical discipline
3+ years of related experience
Strong Python skills, Experience with messages systems like Kafka, Working experience with ETL processing, Working experience with data workflow products like NiFi
Working knowledge of entity resolution systems, Experience with Lambda functions, Working experience with Python RESTful API services / JDBC

What you will get:
Joining a team of technology experts and partners in the same mission
A high-growth environment with plenty of opportunities to grow your career as the company grows
Owner and Leadership team that come from technical backgrounds so they understand the day-to-day challenges of the technical consulting world and can offer real-life solutions and guidance
Highly competitive benefits package that shows we want to hire the very best

Equal Opportunity Employer/Veterans/Disabled. Individuals with disabilities, including disabled veterans or veterans with service-connected disabilities, are encouraged to apply. If you need assistance applying outside of the online application, please contact recruiting@blackspoke.com for more information.","$96,671 /yr (est.)",51 to 200 Employees,Company - Private,Government & Public Administration,National Agencies,2011,$5 to $25 million (USD)
"Green Worldwide Shipping LLC
3.7",3.7,"Aurora, CO",GEM Data Engineer,"About Green
“Global Reach with a personal touch”, that’s the foundation on which four partners (and long-time friends) agreed to when they started Green Worldwide Shipping back in 2008. This ethos has allowed Green to Grow to a 200+ strong team with 15 offices across the US and growing.At green we are a family of innovative problem-solvers who communicate freely to present creative solutions in an environment of trust, respect, and integrity. We are dedicated to customer service, passionate about logistics, and committed to sustainable growth while inspiring others.
Why this role is special..
GEM (Green Engineered Management) is a technology solutions department that supports various commercial technology products, data management, and development for Green Worldwide Shipping.We are seeking a skilled and experienced Data Engineer to join our dynamic team. As a Data Engineer, you will be responsible for designing, developing, and maintaining our data infrastructure and various technology systems to support efficient data processing and analysis. Your expertise in SQL, CargoWise, database management, Altova Flowforce Server, Altova Mapforce ETL software, Python and C#, along with experience in EDI/API connection protocols integration, will be instrumental in achieving success in this role.
How you will be successful…
Design, develop, and maintain data pipelines and ETL processes to extract, transform, and load data from various sources into our databases using Altova Mapforce ETL software.
Collaborate with cross-functional teams to understand data requirements and provide optimal data solutions.
Develop and optimize SQL queries to retrieve and manipulate data from databases, ensuring efficient data retrieval and storage.
Implement and integrate EDI/API connection protocols to enable seamless data exchange between internal and external systems.
Monitor and maintain the performance, availability, and integrity of the databases and data infrastructure.
Support for reporting, document modification, workflow and database support for internal ERP.
Troubleshoot and resolve data-related issues, including data quality and data integration problems.
Implement data security measures and ensure compliance with data privacy regulations.
Stay updated with industry best practices and emerging technologies in data engineering and incorporate them into the data infrastructure.
Provide technical guidance and support to other team members on data-related projects.
Collaborate with data analysts and data scientists to understand data requirements and support their data exploration and analysis needs.
Create and maintain documentation related to data pipelines, database schemas, and data transformation processes.
What you will bring….
Bachelor's degree in Computer Science, Information Systems, or a related field.
Preferred experience in CargoWise (or related ERP) with reporting, document modification, workflow and database support
Proven experience working with SQL and databases, preferably MySQL.
Prefer strong knowledge and hands-on experience with Altova Flowforce Server and Altova Mapforce ETL software, or related applications.
Experience in integrating and working with EDI/API connection protocols.
Proficiency in Python and/or C# (optional but preferred).
Familiarity with data modeling concepts and techniques.
Experience with data warehousing concepts and technologies.
Excellent problem-solving and analytical skills.
Strong attention to detail and the ability to work in a fast-paced environment.
Effective communication skills to collaborate with cross-functional teams.
Ability to work independently and prioritize tasks effectively.
Preferred Skills:
*
Experience with cloud-based data platforms (e.g., AWS, Azure, Google Cloud Platform).
Knowledge of big data technologies (e.g., Hadoop, Spark) and distributed computing.
Familiarity with data visualization tools (e.g., Tableau, Power BI).
Experience with version control systems (e.g., Git).
Understanding of Agile development methodologies.
What we offer:
Employee High Deductible Health Plan (HDHP) currently paid for 100% by Green.
$1,800 contributed to employee HSA Account, if on HDHP.
Employee Dental provided by Green.
Option to purchase disability insurance.
Paid life insurance and option to buy-up.
401(K) and company match.
Parental Leave Program.
Monthly recognition points to give/receive, which can be converted to gift cards.
Tuition Reimbursement Program.
Weekly fruit baskets in office locations
A tree planted for every new employee to Trees.org
Hybrid work schedule.
Wellbeing Program.
Training and Development.
Access to Mentorship Program.
Green Worldwide Shipping, LLC (Green) is committed to the full inclusion of all qualified individuals. As part of this commitment, Green will ensure that persons with disabilities are provided reasonable accommodations. If reasonable accommodation is needed to participate in the job application or interview process and/or to perform essential job functions, please contact Faye Hughes, Head of People and Culture at faye.hughes@greenworldwide.com.Green is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.
Requirements
Bachelor's degree in Computer Science, Information Systems, or a related field.
Preferred experience in CargoWise (or related ERP) with reporting, document modification, workflow and database support
Proven experience working with SQL and databases, preferably MySQL.
Strong knowledge and hands-on experience with Altova Flowforce Server and Altova Mapforce ETL software, or related applications.
Experience in integrating and working with EDI/API connection protocols.
Proficiency in Python and/or C# (optional but preferred).
Familiarity with data modeling concepts and techniques.
Experience with data warehousing concepts and technologies.
Excellent problem-solving and analytical skills.
Strong attention to detail and the ability to work in a fast-paced environment.
Effective communication skills to collaborate with cross-functional teams.
Ability to work independently and prioritize tasks effectively.
Preferred Skills:
Experience with cloud-based data platforms (e.g., AWS, Azure, Google Cloud Platform).
Knowledge of big data technologies (e.g., Hadoop, Spark) and distributed computing.
Familiarity with data visualization tools (e.g., Tableau, Power BI).
Experience with version control systems (e.g., Git).
Understanding of Agile development methodologies.
Summary
GEM (Green Engineered Management) is a technology solutions department that supports various commercial technology products, data management, and development for Green Worldwide Shipping. We are seeking a skilled and experienced Data Engineer to join our dynamic team. As a Data Engineer, you will be responsible for designing, developing, and maintaining our data infrastructure and various technology systems to support efficient data processing and analysis. Your expertise in SQL, CargoWise, database management, Altova Flowforce Server, Altova Mapforce ETL software, Python and C#, along with experience in EDI/API connection protocols integration, will be instrumental in achieving success in this role.
Job Type: Full-time
Pay: $60,000.00 - $75,000.00 per year
Benefits:
401(k) matching
Dental insurance
Employee discount
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Tuition reimbursement
Vision insurance
Experience level:
4 years
Schedule:
Monday to Friday
Ability to commute/relocate:
Aurora, CO 80014: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL/MySQL (Required)
ETL Software (Preferred)
Data warehouse (Preferred)
Python/C# (Preferred)
Work Location: Hybrid remote in Aurora, CO 80014","$67,500 /yr (est.)",51 to 200 Employees,Company - Private,Transportation & Logistics,Shipping & Trucking,2008,$25 to $100 million (USD)
1point system,#N/A,Remote,Senior Data Engineer,"REQUIRED Top 3 Technical skills:
1. MS Sql and .Net experience
2. PostGras, Mongo
3. Experience with BI tools – Tableau,
4. Cloud Technologies – Azure and AWS
5. Python, Java – Nice to haves
Typical Day to Day: They need someone who can come in and roll up their sleeves, get in the trenches and getting things done. Need equal parts soft skills like attention to detail, time management, customer service; lots of technical skills. They will be working with the data team to build, design and develop the data infrastructure systems. This person will work with others and to gather data requirements, implement data solutions and ensure performance of the data to the pipelines. They will be working through an Azure Data Lake.
Job Title: Data Engineer
Job Location: Fully Remote – But ideally someone who can work east coast hours.
Roles
and Responsibilities
· Design and develop scalable, efficient, and reliable data pipelines and ETL processes to process large volumes of structured and unstructured data.
· Collaborate with stakeholders to understand data requirements and translate them into technical specifications and data models.
· Architect, implement, and manage data storage and retrieval systems, including databases, data warehouses, and data lakes.
· Optimize data pipelines and processes for performance, scalability, and data quality, ensuring timely and accurate data delivery.
· Perform data profiling, validation, and cleansing to maintain data integrity and consistency.
· Identify and resolve data-related issues, ensuring the accuracy and reliability of data.
· Implement and maintain data governance practices, data security measures, and data access controls.
· Stay up to date with the latest advancements in data engineering technologies and methodologies and evaluate the potential impact on our data infrastructure.
· Mentor and provide guidance to data engineers, fostering their professional growth and development.
· Collaborate with cross-functional teams to drive data-driven decision-making and provide actionable insights.
· Communicate effectively with technical and non-technical audiences.
· Compile and maintain clear and comprehensive documentation for all data products, services, and platforms.
Working Conditions:
· Ability to collaborate in a team working environment.
· Demonstrates strong collaboration and communication skills (written and verbal).
· Active listener able to translate problems into creative solutions.
· Possesses excellent customer service skills.
Qualifications and Requirements:
· Bachelor’s degree in computer science, Information Systems, or a related field.
· Proven experience as a Data Engineer or similar role, with at least 7 years of experience in data engineering.
· Strong programming skills in languages like Python, Java, .NET, with experience in data manipulation and transformation.
· Proficiency in SQL and experience with relational databases (e.g., MS SQL, MySQL, PostgreSQL) and NoSQL databases (e.g., MongoDB) and common data formats (e.g., JSON, XML, CSV).
· In-depth knowledge of data modeling, data warehousing, and data integration techniques and best practices.
· Hands-on experience with big data technologies and distributed computing frameworks is highly desirable.
· Familiarity with cloud-based data platforms (e.g., AWS, Azure, Google Cloud) and services.
· Strong understanding of data quality, data governance, and data security principles.
· Experience with data visualization tools (e.g., Tableau, Power BI) is a plus.
· Excellent problem-solving and analytical skills, with a strong attention to detail.
· Effective communication skills, with the ability to convey complex technical concepts to non-technical stakeholders.
· Proven ability to work collaboratively in a team environment and lead technical initiatives.
Job Type: Contract
Salary: $84,454.31 - $190,806.62 per year
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote","$137,630 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Gleecus TechLabs Pvt. Ltd
4.5",4.5,"Glen Allen, VA",Lead Azure Data Engineer,"Position Title: Lead Azure Data Engineer - Glen Allen VA (Hybrid)
Tech Skills:
· Concepts to be covered ? (if any).
· Databricks(Python, SQL).
· Hands on Coding.
· Databricks optimization/Pyspark Transformations/PySpark Architecture.
· Azure Data Factory.
· Scenario-Based Questions and Practical Application.
· Azure Data Factory activities/Use of ADF Monitor/CI CD concepts.
· Azure ML, Databricks ML, Azure SDK.
· Scenario-Based Questions and Practical Application.
· Optimized Apache Spark environment, Machine Learning Integration, Experiment tracking, Model training, Feature development, Model serving.
· Synapse Analytics, Parque and Delta tables.
· Scenario-Based Questions and Practical Application.
· Synapse Analytics related concepts.
Job Type: Contract
Salary: $130,000.00 - $150,000.00 per year
Benefits:
Flexible schedule
Health insurance
Paid time off
Schedule:
Day shift
Monday to Friday
Ability to commute/relocate:
Glen Allen, VA 23058: Reliably commute or planning to relocate before starting work (Required)
Experience:
ADF: 4 years (Required)
Azure Data Lake: 4 years (Required)
Databricks: 5 years (Required)
Work Location: One location","$140,000 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2015,Unknown / Non-Applicable
Dobbs Defense Solutions,#N/A,"Fort Meade, MD",Data/NiFi Engineer,"Our Mission
At Dobbs Defense, we deliver mission-centric IT, Cyber, and data analytics solutions for our government and commercial clients through the convergence of automation, innovation, training, and education. Delivering high-quality IT, cybersecurity, and data analytics solutions through proven and innovative methods is our vision.

Job Description
Dobbs Defense Solutions is seeking a Senior Data/NiFi Engineer to design, develop, implement, test, secure, and support data transport and brokering for multiple classified networks and between networks.

Duties
Design, develop, implement, test, secure, and support data transport and brokering.
Data flow engineering for one of DISA’s premier boundary defense systems.
Perform system administration of Niagara Files (NiFi) on Linux VMs.
Support backup/failover/load balancing of data brokering to improve availability.
Monitor availability & amp, usage.
Respond to outages, take corrective action, and issue outage reports.
Familiarity with DISA STIGs and SCAP.
Familiarity with Java programing.

Qualifications
Required education and experience:
5+ years of Data Engineering experience.
Significant experience with Niagara Files (NiFi) and MiNiFi or similar data engineering tools.
Excellent written and verbal communication skills.
IAM or IAT Level II Certification (e.g., GSEC, SEC+, SCNP, SSCP, etc.).
Active Secret clearance.

Required Clearance
Secret

Working Environment
Onsite

Our Equal Employment Opportunity Policy
Dobbs Defense Solutions is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, creed, sex (including pregnancy, childbirth, breastfeeding, or medical conditions related to pregnancy, childbirth or breastfeeding), sexual orientation, gender or gender identity (except where gender is a bona fide occupational qualification), gender expression and transgender status, national origin, ancestry, age, disability, military or veteran status, marital or domestic partner status, genetic information, citizenship, low-income status or any other status or characteristic protected by applicable law. We are committed to equal employment opportunity in all decisions related to employment, promotion, wages, benefits, and all other privileges, terms, and conditions of employment.","$83,925 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Rite Pros
3.4",3.4,"Portland, ME",Big Data Engineer,"Big Data Engineer with Bachelor’s degree in Computer Science, Computer Information Systems, Information Technology, or a combination of education and experience equating to the U.S. equivalent of a Bachelor’s degree in one of the aforementioned subjects.
Job Duties:
Define and manage the data architecture framework, standards, and principles, including modeling, metadata, security, reference, and master data.
Define reference architecture, and models to provide a common vocabulary, reusable components, and industry best practices.
Create solution frameworks integrating large or complex data sets.
Lead all data modeling efforts within DataBricks, including the design of data structures and the identification of business transformation logic.
Analyzing and translating business requirements into conceptual and fully detailed logical data models.
Creating logical data models based on existing applications and databases.
Working with business architects, and data stewards to capture business requirements in a Logical Data Model.
Transform the logical representation of the model into a physical representation and work with data engineering team to instantiate and manage the data.
Contribute to assessment of appropriate data platform(s) for solutioning efforts.
Collaborate with the BI and Analytics teams on creating the optimized, reusable semantic model, complete with metadata and lineage information.
Define and govern data modeling and design standards, tools, best practices, and related development for enterprise data models
Maintain the metadata - the ""data about the data"" describing the data model, its structure, and semantics
Recommend data and design patterns for common business functionality
Assist in multi-phase roadmaps within and across functional teams
Provide insight and technology recommendations that result in enterprise solutions and architectural risk mitigation.
Contribute to innovation strategies by exploring, investigating, recommending, benchmarking and implementing new data centric technologies for the platform.
Identify, design, and implement internal process improvements such as automation of manual processes, optimization of data delivery and re-design of existing infrastructure for greater scalability.
Build the infrastructure and processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and/or other data technologies.
Create and maintain data-related documentation (e.g. data dictionaries, data maps, data models etc.)
Work with data engineering and analytics experts to strive for greater functionality in our data systems and incorporation of industry best practices
Work experience / Technologies required for the position :
Experience building and optimizing Big Data, Data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, meta data, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected data sets.
Working knowledge of message queuing, stream processing, and highly scalable Big Data, data stores.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience in a Data Engineer or similar roles, who has attained a Bachelor’s degree in Bachelor’s degree in Engineering, Computer Science, Information Technology, related field or equivalent work experience.
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Work location is Portland, ME with required travel to client locations throughout USA.

Rite Pros is an equal opportunity employer (EOE).

Please Mail Resumes to:
Rite Pros, Inc.
565 Congress St, Suite # 305
Portland, ME 04101.
Email: resumes@ritepros.com","$97,894 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Less than $1 million (USD)
Premier Consulting Group,#N/A,"Boca Raton, FL",Senior Data Engineer,"* The right person could be located in Florida or Massachusetts as company has office locations in Boca Roton and Boston.
* Must be a US Citizen
Overview Summary of the Work:
ETL Azure Data Factory work: A Data Engineer who is to be able to import data from a variety of different sources (SFTP, API, Fileshare) and file types (CSV, Excel, JSON). Company also captures files via email now, but they have a pattern set up to be able to do that, so the person would just repeat how they’ve done that. Check existing Azure Data Factory pipelines for failures and troubleshoot. Familiar with dynamic expression and syntax in Azure Data Factory pipelines.
Backlog of stored procedures: Be skilled in stored procedure development and SQL skills.
Data quality and data cleansing: Be able to evaluate data (incoming and existing). Look for issues and be able to resolve them. In general, someone who can look at the data and mentally make the leap of “hey, this looks wrong.” Look at data and determine how it might be better utilized (examples: 2.5% as a string or 0.025 as a number, identify values as empty strings and save as nulls instead, etc).
Technical Experience Required/Preferred:
SQL, Azure SQL, Azure Data Factory, Data warehousing environments (dims and facts), Snowflake (highly preferred), Python (highly preferred).
Job Types: Full-time, Permanent
Pay: $140,000.00 - $160,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Bonus pay
Yearly bonus
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Weekend availability
Work Location: Hybrid remote in Boca Raton, FL 33431","$150,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"CareATC
3.3",3.3,"Tulsa, OK",Healthcare Data Analytics Engineer,"General Summary:
This exempt position is responsible for conceptualizing, presenting, creating and maintaining healthcare business intelligence reporting and self-service dashboards for business users throughout the CareATC enterprise and certain key employer clients. These reports and dashboards are flexible and configurable by our end users, and they provide actionable insights that support the strategic objectives and clinical mission of our organization, including health care utilization, member risk stratification, healthcare provider capacity, referrals and other key operational data. The ideal candidate is a motivated self-starter with strong expertise in Power BI or Tableau data visualization, data extraction queries, collaboration with clinicians and operations teams and personal insight into relevant opportunities for improvement.
Essential Job Responsibilities:
Creating and maintaining Power BI dashboards and visualizations that allow end users to easily interpret trends and anomalies.
Responsible for the utilization of views and Power BI datasets to create a centralized data model to facilitate a single source of truth for reporting.
Monitor the accuracy, completeness, and validity of data to ensure the integrity of data through ongoing analysis.
Create and maintain documentation of knowledge assets, such as data mapping, data flows, dashboard content, and data dictionaries.
Perform ad hoc analysis using data from different sources (source systems, data warehouse, external websites, etc.).
Evaluation of reporting requirements from clients, clinicians and health center operators to collaborate on the best opportunities for replicable insights and reporting.
Other duties as assigned.
Experience:
High School Diploma or Equivalent. Bachelor of Science preferred
Minimum of 3 years experience with Power BI preferred; Expertise in business intelligence visualization tools.
Knowledge of Power BI Cloud environment and Power BI Data Gateway.
Working knowledge of relational databases and database structures including proficiency developing, testing, and executing advanced SQL queries. Minimum SQL experience of 3 years.
One plus years of experience in SSRS; SSIS experience a plus.
Background in statistical analysis and experience working with clinical healthcare data. Experience working with medical, or prescription drug claims strongly preferred.
Ability and interest in coordinating and presenting directly with health center operators, employer clients and client management personnel.
Other Requirements:
Ability to understand the broad objectives while paying strong attention to detail.
Expert level proficiency with Excel and PowerPoint.
Excellent written, verbal, and interpersonal communications skills.
Ability to prioritize and handle multiple tasks with competing priorities.
Demonstrated ability to identify, analyze, and interpret trends or patterns in complex data sets, especially related to healthcare operation.
Ability to work collaboratively and build solutions based on minimal technical requirements.
Possess strong analytical and problem-solving skills.
Demonstrated ability to explain complex analytical and statistical techniques to a wider audience.","$77,152 /yr (est.)",201 to 500 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2000,$25 to $100 million (USD)
Arcon Group Inc,#N/A,"Mooresville, NC",Data Engineer,"Job Title: Dot NET Full Stack
Location: Charlotte, NC (Day 1 Onsite)
Duration: 12+ Months
Client: Wells Fargo
only on W2
Interview Mode: Phone & TEAMS
Note: We are looking only at OPT & H1-B
Minimum 3-5 years of experience with C#, .NET
Familiarity with the ASP.NET framework, SQL Server, and design/architectural patterns (e.g. Model-View-Controller (MVC))Experienced in implementing niche solutions with C# and .NET
Abundant experience in designing and writing reusable code with C# and .NET
Experienced with SQL/Oracle/Linux/Windows Servers
Work experience with Oracle, SQL, MySQL Database
Good to Have
Familiarity with Any Cloud Functions
C++/Java/Perl
Power Shell script
SAFE Agile Development
Job Type: Full-time
Salary: $45.00 - $55.00 per hour
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Mooresville, NC 28115: Reliably commute or planning to relocate before starting work (Required)
Education:
Master's (Preferred)
Experience:
Informatica: 1 year (Preferred)
SQL: 2 years (Preferred)
ASP.NET: 1 year (Preferred)
License/Certification:
Driver's License (Preferred)
Work Location: One location",$50.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Resolution Life
3.5",3.5,"Sydney, FL",Senior Data Engineer,"At Resolution Life, we’re proud to have evolved into a global business under the Resolution Life name. For customers, advisers, companies and the industry. We’re making an impact worldwide
Resolution Life Group is a global life insurance group focusing on the acquisition and management of in-force life insurance policies. With assets of $31 billion and 1.5 million customers, Resolution Life are providing existing customers with life insurance, super and investments.

Why us?
Our platform vision is to be the leading in-force specialist life-insurer in Australasia by 2024, by being customer-obsessed and data-driven.
We are one of the first life insurers globally to operate in an entirely Enterprise Agile environment. The strategic priorities of the platform are focused on ensuring the business is future-fit and sustainable, and to grow through bolt-on acquisition of in-force portfolios. We are guided by core behaviours that inform the way Resolution Life team members show up each day and interact with others.
The Opportunity
The individual will play a hands-on role and work closely with business and technology stakeholders to deliver an enterprise-wide data solution in our data platform to grow and retain data domain expertise to support our data driven vision and reduce our ongoing operating costs and technical debit.
Your Story
Align Architecture, Solution Design with business requirements.
Design, Build and Maintain Physical data model in data platform layers.
Map business glossary, data definition and data sets to physical data model
Build ETL that collects, manage, and convert raw data into consumable tables applying technical logic based on business rules.
Responsible to defining and building job orchestration.
Build of database objects by applying application best practices
Build of reusable components and frameworks based on application best practices.
Build a robust codebase to improve data reliability, efficiency, latency, and quality.
Build code and maintain version control by following Software development lifecycle.
Responsible for unit testing and system testing and system integration testing.
Build a test-driven development approach to identify data anomalies and data quality issues.
Support SIT, UAT and Performance Testing
Support Code deployment and Code stabilisation in UAT, Pre-Prod and Prod environments
Present details technical design at DnA design forum and obtain approval.
Be a key technical leader across data platforms technologies including Snowflake, SQL Server, Informatica Cloud, H2O, DBT, advanced SQL, Power BI and Azure / AWS Cloud
Critical Skills
At Resolution Life, we have identified the following critical skills which are key to success in our culture:
Customer Focused: Passionate drive to delight our customers and offer unique solutions that deliver on their expectations.
Critical Thinking: Thoughtful process of analysing data and problem-solving data to reach a well-reasoned solution.
Team Mentality: Partnering effectively to drive our culture and execute on our common goals.
Business Acumen: Appreciation and understanding of the financial services industry in order to make sound business decisions.
Learning Agility: Openness to new ways of thinking and acquiring new skills to retain a competitive advantage.
What Will We Do For You:
Our culture underpins our values and guides our decision making. It's also what makes Resolution Life a great place to work.
Resolution Life Australasia supports virtual working, and our enduring primary place of work continues to be “virtual” with the physical office and home office used interchangeably. We recognise that our workers can contribute and connect equally regardless of where they are located, and we have seen and experienced the wellbeing and benefits that come from working at home. This means some of us work at home most of the time, in the office most of the time or a balanced mix.
Every day is an opportunity to grow – and we hope to offer our people a career, not just a job.
The learning and development opportunities we offer include supporting the completion of executive-level short courses, access to leading online learning tools, on the job training, and mentoring by highly experienced business leaders.
Join us
Before commencing employment in this role you will need to provide two references, full working rights and complete police and credit checks through an online provider.
As an equal opportunity employer strongly committed to working in a diverse and inclusive workforce you will be provided with any support or accessibility requirements throughout your interview process. Please feel free to contact our Talent Team directly at talent@resolutionlife.com.au.
Privacy Policy
Please refer to our
Privacy Policy
to learn about how we use the information you give us, alternatively you can view the same information by navigating to the page
https://www.resolutionlife.com.au/privacy
.","$105,758 /yr (est.)",201 to 500 Employees,Company - Private,Insurance,Insurance Carriers,2003,Unknown / Non-Applicable
"Core4ce
4.3",4.3,"Herndon, VA",Data Engineer,"The Data Engineer will provide the engineering support to data science and software engineering team members.
Design and optimize Data Pipelines using Spark, Hudi, EMR cloud services, and Kubernetes containers
Make sure pedigree and provenance of the data is maintained such that the access to data is protected
Clean and preprocess data to enable analytic access
Architects complex, repeatable ETL processes
Provide Advanced Database Administration support in Oracle, MySQL, MariaDB, MongoDB, Elastic and others
Supports Experience with Targeting using Sponsor Tools, Reverse Engineering
Develop API connectors to enable ingest of new data catalog entries from databases and files
Ensure that data mappings will provide the best performance for expected user experience
Augmenting technical support in the area of data science, data engineering, and systems engineering, and reviewing and providing technical assessments.
Provides current system architecture documentation, engineering/web development programming support for program/project requirements defined tasks, data science/data engineering related technical assessments, effective communication with users and managers, and server administration, to include hardware and software support to existing servers.
Support ad-hoc data analysis requirements defined by the client's Leadership.
Reporting solutions will encompass multiple technology platforms used by the client and will drive business process re-design and raining/change management initiatives.
Will work on Data cleaning and transformation efforts in delivery of CDRLs.
Supports Deliverables and Reports

Requirements
5+ years experience working with Data Sets
Experienced in extracting and aggregating structured and unstructured data.
Experienced in data programming languages and tools such as Python and R.
Experience with SQL or similar database language.
Experience designing and implementing data models to enable, sustain and enhance the value of information they contain
Knowledge of business intelligence reporting tools and data visualization software including Tableau.
Strong analytical and critical thinking skills.
Ability to work collaboratively and effectively in a team environment.
TS/SCI with Full Scope Poly Required

All qualified applicants will receive consideration for employment without regard to race, color, sex, sexual orientation, gender identity, religion, national origin, disability, veteran status, age, marital status, pregnancy, genetic information, or other legally protected status

Required Skills

Required Experience","$103,496 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2018,$100 to $500 million (USD)
"ClarisHealth Inc
3.7",3.7,Remote,SENIOR DATA SERVICES ENGINEER,"Job Summary:
The Data Services Engineer is responsible for researching, developing and implementing analytics solutions for our health care clients. Solutions may be in specific areas such as contractual reimbursement, coordination of benefits, behavioral health or third-party liability. Solutions may also be general in nature or focused on a system solution. The Data Services Engineer also provides ETL support to our Operations team by in-taking and loading data.

Company Description:
ClarisHealth is a rapidly growing healthcare technology firm based in Nashville, Tennessee focused on addressing the systematic payment integrity challenges existing between healthcare carriers and providers. The organization provides an extensive array of consulting and related services to carrier and providers; however, the flagship solution is the Pareo platform, the industry’s first comprehensive payment integrity SaaS platform technology. Pareo has gained strong market reception and is being rapidly adopted across the carrier market and as of 2018, formally launched in the provider market.

Duties and Responsibilities / Essential Functions:
The essential functions include, but are not limited to the following:
Work with operations to identify areas of focus for data analysis
Research, develop and test queries and data solutions
Analyze, map and load data to SQL, PostgreSQL or Mongo databases as part of client implementation
Analyze, interpret and summarize large data sets
Identify new areas of focus for payer cost containment
Work with C# consoles to make edits for ETL processes
Create complex SQL statements to find claims identified for refund based on specs
Work with the team to brainstorm on new ideas
Learn medical billing terminology

Measures of Success
A successful candidate will be able to write complex SQL statements, aptitude for learning new ways of coding, and a desire to write optimized SQL for faster run times. A desire to always learn something new and be a team player.

Required Education and Experience:
Bachelor’s degree or equivalent work experience
SQL (T-SQL, MySQL, or Postgres) experience
Healthcare data knowledge (preferred)
Excellent communication and organizational skills
Strong analytical skills",#N/A,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,Unknown / Non-Applicable
"Colorado Rockies Baseball Club
3.8",3.8,"Denver, CO",Senior Data Engineer,"Senior Data Engineer

The Role
The Colorado Rockies Baseball Club is building a platform to house all our baseball data, from scouting reports to baseball statistics and rosters, into an all-encompassing application that will help us more effectively and efficiently make baseball decisions. This role will be responsible for building and maintaining the ingestion, transformation, and cloud storage of baseball data that will drive analysis and decision-making in all facets of Baseball Operations. The Senior Engineer will be a technical leader on the team and help grow less-experienced engineers.

Essential Duties and Responsibilities
Implement ETL pipelines from various external vendors for the use of internal statistical analysis.
Facilitate the aggregation of data for consumption by technical and non-technical users.
Maintain existing pipelines and debug problems that arise.
Work closely with a web development team to deliver the data they need for an internal information application.
Assist less-experienced engineers with their projects to increase their skill sets.

Job Requirements
Bachelor’s degree or completion of an immersive technical program in Computer Science, Information Systems, Computer Engineering, Web Development, or a related field preferred.
At least five years of experience working in data engineering.
Experience and strong understanding of AWS cloud services including S3, Cloudwatch, Glue, and CDK.
Expert-level understanding of relational databases, including SQL Server, MySQL, and Postgres.
In depth experience using a scripting language for data management or analysis, such as Python or R.
Strong understanding of handling and parsing various data formats, including XML, JSON, and CSV.
Relocation and on-site work are required for this position.

Preferred Skills
Strongly prefer experience with commonly used baseball data sources, such as StatCast, EBIS, Trackman and CollegeSplits.
Understanding of modern baseball analysis.
Experience working with Apache Airflow and additional ingestion management tools not mentioned above.

SALARY RANGE:
$115,000 - $135,000 a year. This is a regular status, full-time position eligible for all company benefits including but not limited to Health Insurance (medical, dental, vision), Retirement, and accrued time off (Vacation/ Sick/ Holiday).

EQUAL OPPORTUNITY EMPLOYER:
Rockies baseball is for everyone! We pride ourselves on hiring, developing, and promoting talent as an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, national origin, gender, age, religion, disability, veteran status, or any other category protected by law. In addition, we will endeavor to provide reasonable accommodation to otherwise qualified job applicants and employees with known physical or mental disabilities in compliance with the ADA. All employment and promotion decisions will be decided on the basis of qualifications, merit, and business needs","$125,000 /yr (est.)",501 to 1000 Employees,Company - Private,"Arts, Entertainment & Recreation",Sports & Recreation,1991,Less than $1 million (USD)
"The Bouqs Company
3.1",3.1,"Marina del Rey, CA",Data Analytics Engineer,"The role contributes to The Bouq’s mission of revolutionizing the way we commemorate life’s moments by connecting people to beautifully designed floral experiences and the responsible partners who create them by being a key member of the data team. As an Analytics Engineer, you will work closely with the Product, Engineering, and Data teams to build and maintain the data infrastructure needed to support our data and business needs. You will also be responsible for developing, optimizing, and maintaining the best-in-class data pipelines, data models, and ETL processes to ensure that data is accurate, reliable, and available to stakeholders in a timely manner. The Analytics Engineer will also serve as the liaison between Engineering and Analytics and will serve as an active member in both teams.
Responsibilities:
Lead the transfer of data modeling from legacy systems to DBT
Contribute to building the data modeling layer, which exposes clean, transformed data to the whole company for analytics
Build datasets in DBT (cloud) for data analysts to improve speed and accuracy for the team
Improve current processes, whether that includes modularizing and standardizing a piece of commonly used code
Design and develop new data pipelines and streaming processes that are highly available, scalable, and reliable
Develop review processes for new data models and take charge on implementing SQL standards for the team
Actively strive towards writing performant SQL rather than just SQL that works, while also ensuring the same SQL is easy to understand when new eyes look at it
Optimize data processing and flow within our Snowflake Data Warehouse
Document new datasets and pipelines and the reasoning/story behind their structure
Work closely with engineering to keep track of schema changes in the production database and adjust pipelines, as needed
Support existing data pipelines and systems in production
Apply software engineering best practices to analytics code such as version control and testing
Develop and communicate strong opinions about best practices in analytics
Help explore and evaluate new technologies
Qualifications:
4+ years of experience working within a data team, preferably as an Analyst/Data Engineer
Bachelor's degree in a quantitative field such as statistics, mathematics, economics, or computer science preferred
Strong SQL fluency in both DDL/DML and analytics (Snowflake experience is a plus)
Experience working with JSON, DBT or other data transformation tools
Experience working with an ETL tool such as Fivetran or Stitch
Knowledge of data structures and how to write performant SQL
Experience with ensuring data quality through testing, deltas, lineage, etc
Strong communication and critical thinking skills to deliver solutions that not only solve problems but also serve as tools we didn’t know we needed
Ability to transform raw data into intuitive datasets that serve as building blocks for analytics
Comfortable leading the growth of a data warehouse and maintaining it
Capable of working through uninformative assumptions and built-biases in datasets and are not stalled when data is not perfect/sparse
Compensation & Perks:
Competitive Base Salary Range of $120,000.00 - $180,000.00 USD + Equity Package
Health, Dental & Vision with 100% employee coverage
401k Matching
Three Weeks Paid Vacation
Discounts on The World’s Best Flowers (obviously!)
Work on cutting edge new technologies
About The Bouqs:
Our mission here at The Bouqs is to revolutionize the way we commemorate life’s moments by connecting people to beautifully designed flowers and the responsible partners who create them. Grounded in transparency, responsibility, and simplicity, we create genuine moments of emotional connection for our customers, build meaningful relationships with like-minded farmers and florists while empowering them to thrive, and eliminate unnecessary waste along the way.

Founded in 2012, The Bouqs is a venture-backed online floral retailer that delivers flowers fresh from eco-friendly, sustainable farms to doorsteps nationwide. Headquartered in Marina Del Rey, CA, The Bouqs connects farms and a curated network of artisan florists directly to consumers and disrupts the traditional supply chain by eliminating overhead costs like warehouses, importers, distributors, auctioneers and more. In turn, this model enables a superior product and redefines the experience and economics for both consumers and producers alike.

For more information, visit www.bouqs.com and follow the #BouqLove on Facebook, Instagram and Twitter.
The Bouqs is an Equal Opportunity Employer!","$150,000 /yr (est.)",51 to 200 Employees,Company - Private,Retail & Wholesale,Other Retail Stores,2012,$25 to $100 million (USD)
"Gridiron IT
4.5",4.5,"Springfield, VA",Data/ETL Engineer,"GridIron IT is seeking an ETL Engineer with an active Top Secret/SCI Eligible Clearance.
MUST BE LOCAL TO DC METRO AREA
In this role, the ETL Engineer will work with a large database (Terabyte scale), mission-critical geospatial data, and best-of-breed cloud (AWS) databases and development tools to enable customer mission.
RESPONSIBILITIES:
The primary duties will be developing and maintaining a custom data integration and validation pipeline used by federal mission clients.
Build and support custom Python scripts to transform, standardize, and load an extensive geospatial data archive.
Creating and maintaining a custom data transformation pipeline using Python and Apache NIFI.
Hands-on work with AWS services such as EC2, RDS, Lambda, IAM, and S3
Working and optimizing the performance of an RDS Postgres database
CORE QUALIFICATIONS:
Proactive self-starter demonstrating a positive, willing attitude and excellent oral and written communication skills.
Deep understanding of AWS permission and restrictions
Extensive experience using RDS
Direct experience with Python scripting
Experience using GitLab
Experience with AWS services/technologies (e.g. EC2, Lambda, IAM, S3)
Proficient using Team Collaboration tools (Jira, Confluence, RocketChat)
Must possess an active Top Secret Security Clearance with SCI (TS/SCI)
Must be a U.S. Citizen
Typically requires a BS in Information Systems, Information Technology, Computer Science or closely related discipline and 8-12 years of prior relevant experience or Masters with 6-10 years of prior relevant experience.
PREFERRED QUALIFICATIONS:
AWS certification (e.g. Database, Big Data)
Experience with ArcGIS or geospatial technologies
Gridiron IT Solutions is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status or disability status.
Gridiron IT is a Women Owned Small Business (WOSB) company specializing in IT Infrastructure, Cyber & Cloud Security, Software Development, and Enterprise Support. Gridiron is an Inc. 5000 2022 recipient and Washington Business Journal Fastest Growing Companies in the Greater Washington Area for 2022. Gridiron offers a competitive benefits package to include medical, dental, vision, 401(k), life insurance, disability insurance, and pet insurance.
Job Types: Full-time, Contract
Pay: $140,000.00 - $160,000.00 per year
Benefits:
401(k) matching
Dental insurance
Health insurance
Health savings account
Life insurance
Referral program
Retirement plan
Vision insurance
Compensation package:
Hourly pay
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Overtime
Experience:
ETL: 5 years (Required)
AWS: 5 years (Required)
RDS: 4 years (Required)
Security clearance:
Top Secret (Required)
Work Location: In person","$150,000 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable
Pyramid Global Hospitality Corporate Offices - Boston,#N/A,"Boston, MA",Data Engineer Manager,"About Us:
At Pyramid Global Hospitality, people come first. As a company that values its employees, Pyramid Global Hospitality is dedicated to creating a supportive and inclusive work environment that fosters diversity, growth, development, and wellbeing. Our commitment to a People First culture is reflected in our approach to employee development, employee benefits and our dedication to building meaningful relationships.

Pyramid Global Hospitality offers a range of employment benefits, including comprehensive health insurance, retirement plans, and paid time off, as well as unique perks such as on-site wellness programs, local discounts, and employee rates on hotel stays. In addition, Pyramid Global Hospitality is committed to providing ongoing training and development opportunities to help our people build the skills and knowledge they need to advance their careers.

Whether you are just starting out in the hospitality industry or are a seasoned professional, Pyramid Global Hospitality offers a supportive and collaborative work environment that encourages growth and fosters success, in over 230 properties worldwide. Join their team and experience the benefits of working for a company that values its employees and is committed to creating exceptional guest experiences.

Check out this video for more information on our great company!
Location Description:

Pyramid Global Hospitality (“Pyramid”) is a leading hotel management company, operating in the US, Caribbean, and Western Europe. With portfolio revenues exceeding $3 billion, Pyramid manages 230 hotels, resorts, and conference centers, both branded and independent. The firm maintains offices in Boston (Headquarters), Cincinnati, Houston, and London. Additional information about Pyramid can be found at www.pyramidglobal.com

In 2021, Pyramid and Benchmark Resorts and Hotels merged to add an additional 59 Managed or Asset Managed Resorts and over 10,000 additional team members. The two companies share the same company culture, values and philosophies. We are growing and opportunities abound!

What really sets Pyramid apart from our competitors is our reputation as an employer. Professional growth is not just possible throughout the company but planned and encouraged. The Leadership Team at Pyramid consider team member development its first priority, understanding that success is only achieved in a workplace where every contributor is respected and recognized. This is why we deliver superior results.

There is opportunity to work directly with senior leaders, experience stretch assignments and learn hospitality management from industry giants. You will come to know a distinctive people centric culture that is at the core of all we do. The decisions we make and the paths we take are bound by a commitment to our Owners, Associates, Customers and the Communities where we work. We attract the most talented associates in the industry, and actively encourage candidates with a “hospitality spirit” who may be thinking about a career change to join our team.

Overview:
Our organization is seeking a highly motivated Manager of Data Engineering to oversee and advance our data management capabilities. You will be joining a dynamic team and playing a critical role in managing data pipelines that provide decision-making information used by our field leaders, corporate executives, and property owners. Timely, accurate information is our competitive advantage and this role is pivotal to our success.

The primary focus involves leveraging data management best practices to improve data reliability and quality. This includes ensuring day-to-day operational excellence, enabling new data source integration supporting the company’s aggressive growth plans, and evolving our data capabilities.

This leadership role involves strategic design and operational execution. You will join a team led by the Vice President of Data Sciences and play a crucial part in advancing our data strategy and supporting the company's ever-expanding appetite for information.

Essential Functions:

As the Data Engineering Manager, your responsibilities will include but not be limited to:
Support and improve the data pipeline operations – extract/transform/load data from various sources, manage storage, make data available for analytics
Resolve data integrity issues with measures to permanently address the root cause
Architect data management practices to ensure high-quality data
Drive thought-leadership for optimizing our data pipelines for advanced analytics
Implement best-practice methodologies to accelerate our data velocity
Automate data management processes for more efficient data capture, storage, and availability
Establish a data governance capability to solidify ownership across the data life cycle
Facilitate collaboration with stakeholders from across the company to create improvements in effective data use
Stay current with the latest technologies and methodologies
Other responsibilities as assigned
Qualifications:
A Bachelor's degree in Computer Science, Information Systems, or a related field
Demonstrate data management leadership experience
Microsoft Azure Synapse Analytics ecosystem
Azure SQL Data Warehousing experience
ETL expertise with large, complex, heterogeneous datasets
Accomplished work with PowerBI, star-schemas, and analytics data prep
Solid understanding of data analysis and machine learning
A track record of delivering successful data management improvement projects
Excellent communication and interpersonal skills
Strong problem-solving skills and the ability to work independently
A commitment learning about the latest technologies and methodologies

If you're a highly motivated individual with experience in data management and a desire to drive process improvement, we want to hear from you!
Compensation Range: The compensation for this position is $110,000.00/Yr. - $125,000.00/Yr. based on qualifications and experience.","$117,500 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Blackspoke
4.9",4.9,"Chantilly, VA",Data Engineer,"Own your opportunity to serve as a critical component of our nation’s safety and security. Make an impact by using your expertise to protect our country from threats and help ensure today is safe and tomorrow is smarter. Our work depends on a Data Engineer joining our team to support data analysts, data scientists, and big data engineers in identifying data sources, performing exploratory data analysis, developing data models, ensuring data cleanliness and accuracy to provide new mission-enhancing insights.

What you will be working on:
As the Data Engineer, you will tailor cutting-edge solutions to the unique requirements of our clients and make systems performance and availability your priority.
Expand our mission capabilities in automating data integration and collection strategies
Coordinate system development to include design, modeling, security, integration, and formal testing
Contribute to completion of engineering programs and projects
Develop solutions to a variety of complex engineering problems
Optimize the data ingestion pipeline architecture, develop strategies for efficient ingestion, processing, storage, structuring, and access
Improve responsiveness and overall performance of the data ingestion pipeline architecture
Prepare and maintain documentation for processes and procedures relate to engineering projects

What you will bring to us (Must):
Active TS/SCI with CI Polygraph
Current DoD IAT II Level certification

Would be nice if you bring the following (Highly Desired):
Bachelor's degree in Engineering, Computer Science, or other related analytical, scientific, or technical discipline
3+ years of related experience
Strong Python skills, Experience with messages systems like Kafka, Working experience with ETL processing, Working experience with data workflow products like NiFi
Working knowledge of entity resolution systems, Experience with Lambda functions, Working experience with Python RESTful API services / JDBC

What you will get:
Joining a team of technology experts and partners in the same mission
A high-growth environment with plenty of opportunities to grow your career as the company grows
Owner and Leadership team that come from technical backgrounds so they understand the day-to-day challenges of the technical consulting world and can offer real-life solutions and guidance
Highly competitive benefits package that shows we want to hire the very best

Equal Opportunity Employer/Veterans/Disabled. Individuals with disabilities, including disabled veterans or veterans with service-connected disabilities, are encouraged to apply. If you need assistance applying outside of the online application, please contact recruiting@blackspoke.com for more information.","$96,671 /yr (est.)",51 to 200 Employees,Company - Private,Government & Public Administration,National Agencies,2011,$5 to $25 million (USD)
"FinditParts
4.8",4.8,"Los Angeles, CA",Senior Data Warehouse Engineer,"FinditParts is the nation's largest eCommerce provider of heavy-duty truck and trailer parts. From hard-to-find parts to everyday preventative maintenance items, we offer more than 3 million heavy-duty OEM, branded, and aftermarket parts ready to ship. Each month thousands of repair shops, fleets, and owner-operators rely on FinditParts to streamline their part sourcing efforts and keep their trucks on the road.

As the industry leader in parts discovery through visual identification technology and aces/pies, we simplify the complexity of finding the right part to fit any commercial vehicle, reducing the time and frustration associated with parts sourcing.
Founded in 2010, with offices in Los Angeles and the Philippines, FinditParts is well-funded, having recently raised $30 million in Series A funding, and profitable.
SUMMARY
The Senior Data Warehouse Engineer, reporting to the Director of Analytics, will be responsible for leading the complete end-to-end architectural blueprint and development of our Data Warehouse system. A successful candidate will deliver a thoughtful data storage system that will store large data sets through various data intelligence environments. The Senior Data Warehouse Engineer will have a high level of impact, by providing architectural solutions to the design, development, testing, and deployment, and allowing FinditParts to store large volumes of data efficiently and effectively for the organization as a whole.
RESPONSIBILITIES
Lead the architectural design blueprint and development of our data warehouse
End-to-end ownership during the design, development, testing, and deployment
Evaluate architectural and software solutions that will deliver appropriate solutions
Develop and maintain data pipelines as well as API-based or file-based data flows between source systems and the data warehouse
Translate client user requirements into a technical architecture vision and implementation plan
Architect the data intelligence environments, including data lakes, data marts, and metadata repositories
Consult with leadership to define goals and requirements, develop technical requirements, deliver analysis, and thoughtful conclusions with data to provide actionable insights
Build, review, and audit existing ETL jobs and SQL queries
Identify gaps and develop a plan to integrate current systems with a desired future state
Perform root cause analysis of data failures and update existing processes to prevent re-occurring issues
Collaborate with various teams to gather information and create visually appealing, engaging, and informative reports in Tableau
Work closely with the BI team, Product team, and Analytics teams to ensure the Data Warehouse is structured in a way to meet data needs for all business teams
QUALIFICATIONS
5+ years of industry experience in Data Warehousing and/or Data Engineering
Bachelor's Degree in Computer Science, Engineering, Statistics, Mathematics, or another quantitative field
Proven experience managing and transforming data in a data warehouse
Proven experience in manipulating, processing, and extracting value from large data-sets
Familiarity with data schema designs that best support business needs and reporting
High level of experience extracting/cleansing data and generating insights from large transactional data sets using SQL, R, Python, and/or Spark on the cloud
Experience building and managing data pipelines and repositories in cloud environments such as Google Cloud, Microsoft Azure or AWS
Experience in Looker, Redshift, Apache Spark, Spark Structured Streaming, SQL, etc
Experience with cloud computing with Dataproc, Databricks, or similar technologies
Strong problem-solving/analytical abilities
BENEFITS
100% US-based remote
Competitive salary, bonus, and equity
Flexible PTO policy
Paid Medical, dental, and vision insurance options for Employees
Fun and energizing start-up environment
""The US base salary range for this full-time position is $125,000 - $165,000 + bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about specific salary range for your preferred location during the hiring process. Please note that the compensation details listed in the US role postings reflect the base salary only and do not include bonus, equity, or benefits.""
FinditParts is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran or any other characteristic protected by law. FinditParts conforms to the letter of all applicable laws and regulations.","$130,474 /yr (est.)",1 to 50 Employees,Company - Private,Retail & Wholesale,Automotive Parts & Accessories Stores,2010,Unknown / Non-Applicable
"Lcp Tracker Inc
4.4",4.4,"New Braunfels, TX",Data Engineer,"Company Summary
LCPtracker, Inc. is a leading software service provider specializing in construction site compliance related software, headquartered in Orange, CA. Our main solution, LCPtracker Pro, is a powerful web-based SaaS solution for collecting, verifying, and managing certified payrolls and other labor compliance related documents. Over 200 government agencies and 100,000 contractors have used LCPtracker for their certified payroll reporting.
In 2023, our growth continues at a rapid pace, making LCPtracker one of the fastest growing small companies in Orange County, California, recognized by the Orange County Business Journal. In 2017, 2018, 2019, 2020, 2021 and 2022, LCPtracker was recognized as an Orange County ""Best Places to Work"" by the Orange County Register.

Position Summary
As a Data Engineer at LCPtracker, you will leverage your experience with SQL, ETL, and reporting to drive the design and development of data-driven solutions. The position is responsible for performing advanced technical and analytical work in the development and support of standardized and customized reports, as well as the testing and maintaining of data integrity.
Other Duties: Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.

Core Competencies
Confidentiality: This role may be privy to confidential and/or sensitive information. Must demonstrate integrity in maintaining confidential and sensitive information and demonstrate strict adherence to organizational policies and procedures.
Communication Proficiency: uses friendly and proficient communication to interact with a wide range of people, frequently exchanging information about office operations.
Time Management: Must manage their own time. They use an electronic calendar in an email program to set meetings, to request others to attend and to coordinate their responses. They respond to requests for attendance at various meetings.
Initiative and Proactivity: Correctly anticipates a need, volunteers readily, and acts without being told to do so. Brings new ideas to the company. Undertakes self-development activities; seeks increased responsibilities; takes calculated risks; looks for and takes advantage of opportunities; asks for and offers help when needed.
Drive for Results: Is goal-oriented; maintains focus on the objective.
Problem Solving, Personal Judgment: Identifies and resolves problems in a timely manner; gathers and analyzes information skillfully; develops alternative solutions; works well in group problem-solving situations; uses reason even when dealing with emotional topics. Solicits and applies feedback.
Quality Management: Looks for ways to improve and promote quality; demonstrates accuracy and thoroughness. Does not cut corners; monitors work to ensure quality; applies feedback to improve performance.
Primary Duties and Responsibilities
Develop, modify, maintain, and support custom reports (MS SQL, SSRS) for both ad-hoc and ongoing business needs.
Develop MS SQL objects (tables, stored procedures, functions, views, etc.) as applicable.
Create and customize weekly, monthly, quarterly, and annual reports using Microsoft Excel or other reporting tools as applicable.
Ensure high data quality through regular quality checks.
Extract, filter, and aggregate data through logical queries and programming.
Maintain a high level of confidentiality and use discretion when needed.
Perform other work including specific tasks or special projects as required.
Promote and maintain positive morale through teamwork.
Other Duties: Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.

Salary Range
Data Engineer rate $100,000 to $140,000 annual salary
Pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, and experience. The total compensation package for this position may also include other elements dependent on the position offered. Details of participation in these benefit plans will be provided if an employee receives an offer of employment.

Benefits
Paid Time Off
9 Paid Holidays
Phantom Stock
401k Plan with up to 4% company match
Medical Benefits (Health, Vision and Dental)
Life Insurance
LTD & STD

Work Environment
This position performs its duties from our New Braunfels, TX office. This position operates in a professional office environment and role routinely uses standard office equipment such as computers, phones, mobile devices, photocopiers, filing cabinets and fax machines.

Physical Requirements
While performing the functions of this job, the employee is regularly required to sit; frequently required to talk and hear, use hands and fingers to type, scroll and use computer equipment. The employee is required to have close visual acuity to perform an activity such as: preparing and analyzing data and figures; transcribing; viewing a computer terminal; extensive reading; visual inspection of text/data in both print and electronic forms.
Ability to lift and move up to 25 pounds.
Position Type and Expected Hours of Work
This is a full-time exempt position reporting to our New Braunfels, TX office M-F 8am – 5pm. Days/hours worked are dependent on the workload at the time. General availability and presence in the office is expected during regular business hours Monday-Friday. However, some flexibility is allowed. Occasional evening and weekend work may be required as job duties demand.

Travel
There is no major travel requirement for this position. However, infrequent travel may be necessary to visit remote office(s), attend conferences/industry events, etc. Attendance at our corporate Staff Retreat is required. This event is a 2-3-day retreat. Attendance at our annual User Conference as assigned.

LCPtracker, Inc. is an equal opportunity employer of all qualified individuals. All applicants will be afforded equal opportunity without discrimination because of race, color, religion, sex, sexual orientation, marital status, order of protection status, national origin or ancestry, citizenship status, age, physical or mental disability unrelated to ability, military status or an unfavorable discharge from military service. LCPtracker, Inc. will consider for employment qualified applicants with criminal histories in a manner consistent with all federal, state, and local ordinances.
LCPtracker is committed to the full inclusion of all qualified individuals. In keeping with our commitment, LCPtracker will take steps to assure that people with disabilities are provided reasonable accommodations. Accordingly, if reasonable accommodation is required to fully participate in the job application or interview process, to perform the essential functions of the position, and/or to receive all other benefits and privileges of employment, please contact the LCPtracker Human Resources Department at HR@lcptracker.com.
Education and Experience
MUST HAVE:
Bachelor’s degree in software engineering disciplines, computer science or other related field and/or the equivalent combination of education and experience.
7+ years of experience with SSRS reporting tools and MS SQL server.
7+ years of experience using ETL tools such as SSIS and/or ADF.
7+ years of experience in Data Warehousing with SSAS or AAS.
Experience in applying security to SSAS or AAS models using authentication frameworks such as AAD or Active Directory.
Adept at queries, report writing and presenting findings.
Ability to comprehend, analyze, and systematically compile technical, statistical, and information into comprehensive reports or other formats.
Effective business writing and composition skills with good command of the English language.
Ability to independently plan, organize, and complete a variety of projects within established standards, objectives and time frames.
Ability to work in fast-paced, multi-tasking environment with shifting priorities and demanding deadlines.
Ability to work independently in finding solutions
Ability to work in an agile work environment
Ability to work in a team environment
Must be detailed-oriented and able to effectively prioritize and organize workload, with efficient time management.
Minimum 1-year experience working on Scrum teams.
Basic understanding of the Agile methodology.
Scrum certifications are a plus.
Strong interpersonal communication skills.","$120,000 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,1992,$1 to $5 million (USD)
"HARAMAIN SYSTEMS INC.
5.0",5.0,"New Richmond, WI",ETL-Data Engineer: Direct hire,"Role : Data Engineer
Location : Roseville MN. (Can be mostly remote, but ideally someone could go in occasionally as needed)

Essential Functions and Accountabilities of the Data Engineer:
Design and build data models and data transformations efficiently and reliably to fit the analytical needs of the business
Develop and maintain ETL processes utilizing Azure Cloud tools such as Azure Data Factory, FiveTran, DataBricks as well as efficient cloud data strategies to minimize costs
Engage with teams to push the boundaries of analytical insights, creating new product features using data, and powering data models
Anticipate, Identify and solve issues concerning data management to improve data quality
Understand and use continuous integration, test driven development and production deployment frameworks using DevOps and GIT
Develop and champion modern Data Engineering concepts to technical audience and business stakeholders
Manage and maintain relational databases, including but not limited to SQL Server
Maintain data integrity and transparency to increase data, reporting and dashboard confidence and consistency across all departments
Advocate for new sources of data to create actionable insights and recommendations
Define and communicate business rules and terms governing use of the data including security and data lifecycle management
Communicate technical and non-technical information clearly, concisely, and effectively both orally and in writing. Present findings, recommendations, and specifications in formal reports and/or oral presentations

Knowledge & Education Requirements:
BA/BS degree in Computer Science, Database Administration or similar work experience
3-5+ years of experience in Data Engineering
2 or more years of experience in data modeling, data architecture and ETL
Experience with tools and concepts related to data and analytics, such as dimensional modeling, reporting tools, data governance, data warehousing, structured and unstructured data
Proficient in at least one major programming language (e.g.. Java Script, Python) and comfortable working with SQL
Advanced understanding of analytics tools and methods, best practices and awareness of new techniques of analytics
Experience working with relational databases like SQL, Postgres and Data Bricks
Experience working with ETL tools like Azure Data Factory, Fivetran and SSIS

This is a remote position.","$57,125 /yr (est.)",51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,#N/A,Unknown / Non-Applicable
"Overhaul
3.5",3.5,"Austin, TX",Data Platform Engineer,"Who We Are
Overhaul is a supply chain integrity solutions company that allows shippers to connect disparate sources of data into the first fully transparent situational analysis engine designed for the logistics industry. Data that is transformed into critical insights can instantly trigger corrective actions, impacting everything from temperature control to handling requirements or package-level tracking, ensuring cargo arrives at its destination safely, undamaged, and on time. We are a dynamic, innovative, and fun team who is highly committed to our customers' experiences and our Mission and Vision.
The Role
At Overhaul, we're building the future of supply chain monitoring technology. As a Data Platform Engineer, you'll work with our platform and data analytics teams to understand and improve our data model, research and implement business intelligence tools to allow us to better serve our data internally and externally, and work with other engineers to design better systems for data ingestion and processing. Our company is growing fast, and we need you to help us deliver the best results to our users as we continue that trend.
Responsibilities:
Analyze existing tools, including business intelligence, databases, messaging services, etc. and provide solution recommendations
Deploy and manage data software solutions in a cloud environment (We use AWS and Azure)
Analyze custom data models and provide solutions for data ingestion/ETL
Create custom data software solutions for our internal and external customers using workflow tools such as Argo Workflows
Required Skills and Qualifications:
Experience doing data model analysis
Experience using cloud based technologies for data ingestion, analysis, and extraction (AWS Lambda, Workflow solutions, Snowflake, etc)
Excellent written and oral communication skills
Preferred Qualifications:
Software Engineering experience beyond writing simple scripts, preferably in Python
Experience using workflow solutions such as Argo Workflows, Prefect, Kubeflow, or Jenkins to implement data pipelines
Our Core Values and how they benefit you as an ""Overhauler""
Authenticity, Receptivity and Trust
Extremely competitive base salary package
401(k) with Overhaul match
Flexible working schedules
Remote, hybrid, and/or In-office*
Encouragement and Learning
Progressive advancement opportunity & career mobility
Paid development personal stipend
Monthly lunch and learns
2 Unique learning systems w/Instructor led content
Wellness and Integrity
Rotating Overhaul ""Perks @ work"" (Discounts and Freebies)
Overhaul fully provided healthcare plan
Employee assistance & wellbeing programs
New Parent/Family/Caregiver leave(s)
Daily BAMM time (body and mind movement)
Life by design vacation policy
Diversity and Inclusivity Statement:
Overhaul has always been, and always will be, committed to diversity and inclusion. Our Overhaul Culture Code's top listed commitment is to ""Diversity and Synergy."" All aspects of employment will be based on merit, competence, performance, and business needs. We do not discriminate on the basis of race, color, religion, marital status, age, national origin, ancestry, physical or mental disability, medical condition, pregnancy, genetic information, gender, sexual orientation, gender identity or expression, veteran status, or any other status protected under federal, state, or local law. We strongly encourage people from underrepresented groups to apply!
#BI-Remote","$102,068 /yr (est.)",51 to 200 Employees,Company - Private,Transportation & Logistics,Shipping & Trucking,2016,$25 to $100 million (USD)
"Moser Consulting
4.6",4.6,"Indianapolis, IN",Senior Data Engineer,"About Moser
For more than 25 years we have formed partnerships and grown through open and honest collaboration with our clients, partners, and employees. We are best known for taking great care of our clients, our dedication to creating a work environment where employees do their best work, and our deep commitment to continuous improvement. Our consultants work in a collaborative and fast-paced environment, are self-motivated, and are passionate about evolving technology. It is no accident that we are recognized as one of the Best Places to Work in Indiana for 10 consecutive years.
Internally, we believe in building strong teams from the top down with a focus on values in our Model-Coach-Care philosophy. Our leadership are encouraged and trained to model good practices, mentor other employees and each other, and show empathy and caring in all interactions. This is the base of our core values: Accountability, Balance, Collaboration, Focus, Integrity, Social Responsibility, Support and Transparency.
Moser Consulting believes in equal opportunity for all people and is committed to enabling a diverse, equitable, and inclusive culture. We foster a spirit of unity that respects the remarkable individuality of everyone's culture, history, and service.
Description
We are seeking an experienced Senior Data Engineer to join our team. As a Senior Data Engineer, you will be responsible for designing and implementing complex data pipelines that enable efficient data processing, storage, and retrieval. You will work closely with internal and external stakeholders to ensure that data is available and accessible for business analysis and decision making. You will also be responsible for mentoring and coaching data engineers and collaborating with cross-functional teams to drive data strategy and innovation.
Design, Develop, Deploy, and Monitor complex data pipelines which may contain various required transformations of data using various languages and techniques.
Work with client stakeholders to understand their data needs and design data solutions to meet those needs.
Work with Pre-Sales team to prepare level of effort estimates for incoming projects.
Optimize and tune data processing and storage systems for performance, scalability, and reliability.
Ensure data quality and integrity by implementing data validation, cleansing, and transformation processes.
Stay up-to-date with emerging technologies and trends in data engineering and propose new solutions to improve the data infrastructure.
Identify, design, and implement internal process improvements.
Define and drive data engineering strategy, best practices, and standards across the organization
Requirements
At least 7 years of experience in data engineering.
Strong programming skills in Python, SQL, or other programming languages commonly used in data engineering.
Experience with data warehousing, data modeling, and data architecture.
Strong understanding of ETL processes, data integration, and data transformation.
Experience with cloud-based data solutions such as AWS, Azure, or Google Cloud Platform.
Strong problem-solving and analytical skills.
Strong communication and collaboration skills to work effectively with client stakeholders.
Ability to create functioning ETL prototypes to address quickly changing business needs.
Ability to clearly articulate pros and cons of proposed technologies and solutions.
Familiarity with Azure Data Factory, Azure Data Lake Storage, and/or Snowflake.
Preferred
10+ years of experience in data engineering.
Azure, Databricks, or Snowflake certifications.
Experience with technologies such as Hadoop, Spark, and Kafka.
Understanding of DevOps principles and practices, including Continuous Integration, Continuous Deployment pipelines, and configuration management.
Understanding of Azure services, including Azure Compute, Storage, Network, Security, and Management services. Knowledge of Azure architecture patterns and best practices.
Required Location: Indianapolis, IN
Where You'll Work
Moser has two offices in Indianapolis, IN, and one in Baltimore, MD. This position will require you to be in the Indianapolis, IN area.
Benefits
For more than 25 years, Moser Consulting has been the go-to source for exceptional IT talent with the ability to self-manage. At Moser Consulting, our people are our #1 asset. We hire the best people, welcome them like family, connect them with opportunities, and let them do what they do best: produce innovative solutions to technology problems.
Our culture gives us a competitive advantage by keeping our employees happy, healthy, and by lowering stress levels in a very demanding industry. It is no accident that we are recognized as one of the Best Places to Work in Indiana. We focus on giving employees: an incredible work space; a fun, collaborative, and creative atmosphere; an extremely generous compensation package; and dozens of outstanding and unique perks usually not found at one company.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.","$114,691 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1996,$25 to $100 million (USD)
"Repstor
4.3",4.3,"Charlotte, NC",Data Engineer,"The Role:
We're hiring a hands-on Data Engineer to be part of Data Solutions organization. The data solutions team builds out tools and infrastructure needed to source, validate, clean, and process the data and build compelling reports for leaders and integrating our systems. Our team is looking for a Data Engineer to help scale our data efforts. If you have passion for data and want to help build Intapp’s next gen data platform that provide actionable insights to drive customer and business outcomes, we’d love to hear from you.
You will contribute to the full Data development and apps integrations life cycle, including design, modeling, data integrations, unit testing, performance tuning, and deployment activities.
What you’ll do:
Work collaboratively with business leaders to identify and define the business requirements, design technical solution, and data architecture for self-serve and provide actionable insights.
You can build highly scalable end-to-end data integrations pipelines using different open source tools and operationalizing them.
Define data integration points, data flow, data strategy and develop functional and technical specifications test plans/scripts to validate the data integration needs, Build and sanity test release libraries.
Build infrastructure for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, cloud based relational or non-relational databases employing ETL tools (Informatica, Talend, Workato) and/or scripting languages like Python and streaming technologies like Kafka, Kinesis that allows users to self-serve themselves.
Apply technologies to solve complex data problems with expert knowledge in programming languages like SQL, Python, Linux, SQL, Hive, Spark and serverless services like Athena, Spectrum
Build and deploy dashboards using one or more visualization tools such as tableau, Power BI
Define optimized DB schemas/semantic layer that power the compelling story-telling dashboards.
Hands on experience with web services (AWS, Azure, etc..).
Experience in REST APIs (i.e. ingesting and creating data through the APIs)
Define SLA and acceptable time lags by data source, define QA process, and socialize resolution process to ensure data accuracy and consistency.
Ideal Candidate Must-Haves
BA/BS Degree in Computer Science, any Engineering discipline, and 2+ years of experience in
relevant Data engineering and Business Intelligence platforms or Master’s degree in Computer
Science, any Engineering discipline / information technology and 2+ years in relevant Data
engineering and Business Intelligence platforms experience
Scripting languages like Python, Go, bash, etc...
Scripting languages packages for data manipulation (i.e., Python’s Pandas, etc.)
Scripting languages with packages for REST data ingestion
Relational SQL and NoSQL databases
Tableau, Power BI, Sisense or other reporting tools
Working knowledge of DW Concepts (Star vs snowflake schema, CDC, SCD, etc...)
ETL Tools such as Informatica, Talend, Dell Boomi, Workato
Serverless query service – Athena, Spectrum
Experience working with AWS components [EC2, S3, DynamoDB]
Data visualization experience.
Preferred Qualifications:
Big data tools Hadoop, Hive, HDFS, Spark etc.
Experience in CI/CD pipelines
Demonstrated proficiency implementing self-service solutions to empower an organization to generate valuable actionable insights.
Setup, maintain, and implement Kafka topics and processes. #LI-JS2
Intapp provides equal employment opportunities to all qualified applicants and will make hiring decisions without regard to race, color, sex, sexual orientation, gender identity or expression, religion, national origin or ancestry, age, disability, marital status, pregnancy, protected veteran status, protected genetic information, political affiliation, or any other characteristic protected by federal, state or local laws. All offers are contingent upon passing a criminal history and other background checks if applicable to the position.
Please note: Intapp will not hire through text message, social media, or email alone. We will never extend a job offer unless you have been contacted directly by an Intapp recruiter and have participated in the interview process which will generally consist of 3 or more virtual or in person meetings. We post all legitimate job openings on the Intapp Career Site at
https://www.intapp.com/working-at-intapp/
.","$100,789 /yr (est.)",501 to 1000 Employees,Company - Public,Information Technology,Enterprise Software & Network Solutions,2000,$100 to $500 million (USD)
"Goodship
2.6",2.6,Remote,Senior Infrastructure Engineer - Data,"About GoodShip
GoodShip, the first neutral platform in the $900B US truckload market, offers a comprehensive solution for procurement, performance measurement, and business optimization. Our platform empowers shippers and freight carriers to transition from traditional spreadsheet and email management to a more streamlined, AI-powered system that provides automated reports, insightful analytics, and carrier scorecards. By integrating seamlessly with any shipper TMS, GoodShip enhances collaboration among brokers and carriers.
Backed by leading venture firms such as FUSE, Ironspring Ventures, and Chicago Ventures along with founders of leading freight-tech companies, including Convoy, Stord, Project44, and FreightWaves, GoodShip is positioned for success with significant financial support and strong early traction.
Your Role
As a Senior Data Infrastructure Engineer, you'll join our mission to build a modern freight platform to empower shippers. You will be responsible for building out our data and general infrastructure to support the growing needs of the product and business.
Key Responsibilities:
Own the data ingestion architecture & system from our customers
Design new data pipelines to support the growing needs of the business
Own the strategy for scaling our data infrastructure
Improve our general engineering infrastructure to ensure operational excellence
Establish and promote best engineering practices to facilitate our ongoing growth.
What We're Looking For:
5+ years of industry experience in software development as part of a professional team.
Significant experience maintaining and scaling data infrastructure systems
Strong familiarity with the data technology ecosystem
BS degree or higher in Computer Science/Engineering, or equivalent experience.
A strong sense of ownership and responsibility.
A growth mindset, coupled with the ability to quickly learn new technologies and concepts to meet customer needs.
Proven ability to foster diverse, flexible, and empathetic engineering teams.
Nice To Have
Experience building enterprise SaaS applications
Familiarity with trucking or logistics industry
Experience with AWS, Javascript/Typescript, GraphQL, React, or PostgreSQL
Benefits
Generous equity compensation
100% Employer paid health benefits for employees
Paid time off
Opportunity to work on cutting-edge technology that directly makes an impact on customers and the freight industry",#N/A,1 to 50 Employees,Company - Private,Transportation & Logistics,Shipping & Trucking,#N/A,$5 to $25 million (USD)
"S&P Global
4.1",4.1,"Raleigh, NC",Data Engineer 1,"The Role : Data Engineer 1
The Team: Content Development technology groups develops and support systems which provide high-quality entity, ownership, and compliance data across a wide range of equity and fixed-income asset classes.

The Impact : The candidate will join the Content Development Ownership team which supports mission-critical Fixed Income & Equity data products.
The job will involve hands-on fault diagnosis, resolution, process creation and optimization, knowledge sharing, and delivery in a high-pressure client-focused environment.

What’s in it for you:
Build a career with a global company.
Grow and improve your skills by working on enterprise-level products and new technologies.
Be part of a dynamic and growing organization that values innovation, creativity, and excellence.
Working with a team of highly skilled, ambitious, and result-oriented professionals.
It’s a fast-paced agile environment that deals with a huge volume of data, so you will have the opportunity to sharpen your data skills and work on an emerging technology stack.

Responsibilities:
Content Development team developers use their passion for programming and problem-solving to produce feature-rich applications that deliver value for our clients. With a dedication to continual improvement, our development team challenges themselves each day to expand their knowledge base and produce solutions that are maintainable, extensible, and elegant. We take great pride in our work, share our expertise and ideas to achieve common goals, and aspire to learn more. We have adopted an Agile Development Methodology and are committed to continually improving within this model.
Success in this role does require knowledge of Python, AWS, and Database technologies in the short term; long-term opportunities working with alternative technologies are expected.

What We’re Looking For:
Bachelor’s in computer science, related field, or equivalent experience.
2+ years of experience in Python.
2+ years of experience with relational databases such as SQL Server or Postgres.
1+ years of experience in processing Big Data using any Cloud Native technology.
Hands-on experience with IDEs such as Visual Studio / IntelliJ / PyCharm.
Must possess strong oral and written communication skills.

The following experience would be advantageous:
Familiarity with CI/CD
Familiarity with NoSQL Databases is a plus

Grade/Level (relevant for internal applicants only): 8
The Location: Raleigh, NC. Hybrid 1 day/week.

S&P Global states that the anticipated base salary range for this position is $63,000 to $91,000. Final base salary for this role will be based on the individual’s geographic location, as well as experience level, skill set, training, licenses and certifications.
In addition to base compensation, this role is eligible for an annual incentive plan. This role is not eligible for additional compensation such as an annual incentive bonus or sales commission plan.

This role is eligible to receive additional S&P Global benefits. For more information on the benefits we provide to our employees, please click here .

-----------------------------------------------------------

Equal Opportunity Employer
S&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment.

If you need an accommodation during the application process due to a disability, please send an email to: EEO.Compliance@spglobal.com and your request will be forwarded to the appropriate person.

US Candidates Only: The EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf describes discrimination protections under federal law.

----------------------------------------------------------- 20 - Professional (EEO-2 Job Categories-United States of America), IFTECH203 - Entry Professional (EEO Job Group), SWP Priority – Ratings - (Strategic Workforce Planning)

Job ID: 286434
Posted On: 2023-05-25
Location: Raleigh, North Carolina, United States","$77,000 /yr (est.)",10000+ Employees,Company - Public,Management & Consulting,Research & Development,1860,$10+ billion (USD)
"BrightFarms Inc
3.8",3.8,"Irvington, NY",Data Engineer,"At BrightFarms, we’re on a mission to revolutionize the way leafy greens are grown. But we don’t just want to grow great tasting greens, we want them to do good as well: for the planet, for the health of people, and for the well-being of our employees.
Our passion for building the next generation of farming fuels our passion for nurturing our people. We give BrightFarmers the tools, training, support, and opportunities they need to do better for themselves and the world every day. Because when you do good for your people, they do good for the world.
BrightFarms. The Place to Grow.
Summary:
This is a new position within the existing Business Operations team and you will have the opportunity to establish and shape a data engineering competence within the organization. You will be working closely with our software and IT leads and their expanding teams, and partnering with all functions in the organization as we modernize our approach to data gathering and analysis.
You will be building a Postgres-backed data warehouse for employee and organizational analytics, using BrightFarms' existing data processing frameworks and creating new ones. You will work closely with other stakeholders, developers, data scientists, and analysts to write efficient pipelines aggregating data from various sources. This new position will play a ground-breaking role in driving more informed and objective decisions at BrightFarms by modernizing the way that we collect, manage and consume organizational information.
We’re looking for people who excel at working with others, challenge the status-quo, and are outstanding problem-solvers. We value clear communication, honest feedback, and empathy for the users of our services. This is an exciting time to join BrightFarms, as we embark on next-generation work.
Responsibilities:
Design, model, and implement data warehousing activities to deliver the data foundation that drives impact through informed decision making
Build ETL pipelines, data warehouses, and aggregate tables.
Interface with stakeholders and data analysts to understand data needs
Solve challenging data integration problems utilizing optimal ETL patterns, frameworks, query techniques, and sourcing from structured and unstructured data sources
Optimize pipelines, dashboards, frameworks, and systems to facilitate easier development of data artifacts
Build data expertise and own data quality for allocated areas of ownership and supporting processes running in production
Choose, propose and use the appropriate technology or programming language for the task at hand.
Provide estimates or project ideas that will influence your team’s roadmap
Qualifications and Education:
A degree (preferably an advanced degree) in Computer Science, Engineering or a related field.
4-6 years of industry experience in architecting and developing data warehousing solutions, preferably in an operational business domain.
Familiar with traditional data warehousing methodologies and hands-on experience in extracting data from various data sources including unstructured excel models and building complex data models.
Experience building applications and managing infrastructure using one of the major cloud providers is preferred. (We use AWS and Azure here at Brightfarms).
Expertise in data ETL, schema design, and dimensional modeling.
Strong programming experience in production environments writing complex SQL queries to pull data into visualization tools and utilizing Python, (PHP as a bonus).
Experience working in an agile environment.
Additional Notes:
May require travel to farms as needed","$110,791 /yr (est.)",51 to 200 Employees,Company - Private,Agriculture,Crop Production,2011,$5 to $25 million (USD)
"Jane Technologies
4.8",4.8,"Santa Cruz, CA",Data Migration Engineer,"Company Overview:
Jane is an MIT-founded, high-growth, and quickly-growing technology company in the cannabis industry. We believe in the cannabis industry's ability to bring well-being, health, and love into this world, and it is our mission to bring confidence to the online cannabis shopping experience. As the cannabis industry's first complete real-time marketplace, we aim to provide consumers with a confident, safe, and simple shopping experience.
To learn more about who we are, our culture, and whether this is the right place for you, read our Key Values profile: https://www.keyvalues.com/jane. Check out our product at: https://www.iheartjane.com/.

What You'll do:
Jane is looking for a Data Migration Engineer to help with the development, documentation and implementation of data migration services for Jane Point of Sale. As a Data Migration Engineer you will work within the Operations Department and collaborate with Engineering, Partner Success, Content and Tech Support. The Data Migration Engineer will collaborate with cross-functional teams and directly with our partners during the onboarding process to extract, transform and load (ETL) data from diverse sources into Jane POS. You will help ensure our partners experience a smooth transition from their legacy system to our industry-leading point of sale. Culture is the single most important part of Jane's success to date. You will report to our Program Manager.
Responsibilities
Take lead responsibility for researching, implementing, testing, validating, and documenting data migration services for Jane POS
Take lead responsibility for executing data migration services for each Jane POS onboarding, which may include personal health information (PHI) of dispensary customers
Validate and cleanse data to ensure accuracy and integrity during the migration process
Develop data mapping rules and perform data transformations to align source data with target system requirements
Conduct data profiling and quality assessments to resolve any issues or discrepancies
Help create test plans to validate the accuracy of migrated data
Collaborate with cross-functional teams, including developers, project managers, and data owners, to ensure seamless data migration
Follow data protection and privacy regulations, ensuring the secure handling of sensitive customer information
Document migration processes, procedures, and any encountered issues for future reference
Provide ad-hoc support to Deal Desk team migrating internal data within existing Jane systems
Work with Engineering to identify challenges in the Jane POS data model and import tool
Work with Partner Success and Technical Support to identify and remedy issues that occur during or because of the data migration process
Stay updated on industry best practices and latest trends in data migration techniques and technologies
Qualifications
Familiar with Python, software engineering concepts, and current technologies
Familiarity with working with relational databases and API endpoints
Communicate challenges and progress with team members through presentations, group calls.
Ability to leverage and create technical documentation
Bachelor's degree in software engineering or similar field or 2+ years of experience working in a similar role
Our Benefits:
Great compensation package and equity
Remote friendly work environment with employees throughout the US and Canada
Health, Dental, Vision, 401k, Unlimited PTO, and home office stipend

Jane Technologies is proud to be an equal opportunity employer and committed to providing employment opportunities regardless of race, religious creed, color, national origin, ancestry, physical disability, mental disability, medical condition, genetic information, marital status, sex, gender, gender identity, gender expression, pregnancy, childbirth and breastfeeding, age, sexual orientation, military or veteran status, or any other protected classification, in accordance with applicable federal, state, and local laws. EOE, including disability/vets.","$90,292 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Software Development,2017,Unknown / Non-Applicable
"Acrisure Technology Group
3.9",3.9,"Austin, TX",Data Engineer,"Data Engineer
Hybrid Position (3 days per week average in Downtown Austin, TX or Grand Rapids, MI office)
Note: This is a full-time, in-house position. We do not offer C2C or C2H employment and are not able to sponsor visas for this position.
Acrisure Technology Group (ATG) is a fast-paced, AI-driven team building innovative software to disrupt the $6T+ insurance industry. Our mission is to help the world share its risk more intelligently to power a more vibrant economy. To do this, we are transforming insurance distribution and underwriting into a science.
At the core of our operating model is our technology: we're building the premier AI Factory in the world for risk and applying it at the center of Acrisure, a privately held company recognized as one of the world's top 10 insurance brokerages and the fastest growing insurance brokerage globally. By using the latest technology and advances in AI to push the boundaries of understanding risk, we are systematically converting data into predictions, insights, and choices, and we believe we can remove the constraints associated with scale, scope, and learning that have existed in the insurance industry for centuries.
We are a small team of extremely high-caliber engineers, technologists, and successful startup founders, with diverse backgrounds across industries and technologies. Our engineers have worked at large companies such as Google and Amazon, hedge funds such as Two Sigma and Jump Trading, and a variety of smaller startups that quickly grew such as Indeed, Bazaarvoice, RetailMeNot, and Vrbo.
The Role
The Business Intelligence team's mission is to unify data across the enterprise to optimize business decisions made at the strategic, tactical, and operational levels of the organization. We accomplish this by providing an enterprise data warehouse, data lake, reporting platform, and business processes that provide quality data, in a timely fashion, from any channel of the company and present them in such a manner as to maximize the value of that data for both internal and external customers.
The Data Engineer is responsible for designing and developing moderate to complex ETL processes required to populate a data lake and structured data warehouse which supply data for the machine learning, AI & BI teams. Responsibility includes working with a team of contracted developers as well as coaching and mentoring junior and mid-level developers. Ensuring high quality and best practices are maintained through the development cycle is key to this position.
You will interact with some of the top technologists on the planet. Our technology runs on Google Cloud and is configured with Kubernetes, leveraging various services in that environment. Our data storage layer includes BigQuery, BigTable, and Postgres. We code primarily in Kotlin, Python, Java, and JavaScript and make use of many frameworks, including Dataflow, Cloud AI Platform, KubeFlow, Spring, and React.
Here are some of the ways in which you'll achieve impact
Leverage established guidelines and custom designs to create complex ETL processes to meet the needs of the business
Develop from strategic and non-strategic data sources including data preparation/ETL and modeling for data visualizations in a self-service platform
Contribute to the definition and development of the overall reporting roadmap
Translate reporting requirements into reporting models, visualizations and reports by having a strong understanding of the enterprise architecture
Standardize reporting that helps generate efficiencies, optimization, and end user standards
Integrate dashboards and reports from a variety of sources, ensuring that they adhere to data quality, usability, and business rule standards
Independently determine methods and procedures for new or existing requirements and functionality
Work closely with analysts and data engineers to identify opportunities and assess improvements of our products and services
Contribute to workshops with the business user community to further their knowledge and use of the data ecosystem
Produce and maintain accurate project documentation
Collaborate with various data providers to resolve dashboard, reporting and data related issues
Perform Data Services reporting benchmarking, enhancements, optimizations, and platform analytics
Participate in the research, development, and adoption of trends in reporting and analytics
Mentor BI Developers and BI Analysts
Other projects as assigned in order to support necessary business goals across teams
You may be fit for this role if you have
Minimum 5 years required, particularly in an Azure environment with Azure Data Bricks, Azure Data Factory, Azure Data Lake
Minimum 5 years designing data warehouses, data modeling, and end-to-end ETL processes in a MS-SQL environment
Minimum 2 years developing machine learning models with Azure ML, ML Flow, BQML
Expert working knowledge of SQL, Python and Spark (and ideally PySpark) with a demonstrated ability to create ad-hoc SQL queries to analyze data, create prototypes, etc required.
Successfully delivered 2+ end to end projects – from Inception to Execution - in Data Engineering / Data Science / Data Integration as a Tech Senior/Principal
Ability to Analyze, summarize, and characterize large or small data sets with varying degrees of fidelity or quality, and identify and explain any insights or patterns within them.
Experience with multi-source data warehouses
Strong skills in in data analytics and reporting, particularly with Power BI
Experience with other cloud environments (GCS, AWS) a definite plus
Strong experience creating reports, dashboards, and/or summarizing large amounts of data into actionable intelligence to drive business decisions required
Strong understanding of core principles of data science and machine learning; experience developing solutions using related tools and libraries
Hands on experience building logical data models and physical data models and using tools like ER/Studio/Idera
Write SQL fluently, recognize and correct inefficient or error-prone SQL, and perform test-driven validation of SQL queries and their results
Proficient in writing Spark sql using complex syntax and logic like analytic functions etc.
Well versed in Data Lake & Delta Lake Concepts
Well versed in Databricks usage in dealing with Delta tables (external \ managed)
Well versed with Key Vault \ create & maintenance and usage of secrets in both Databricks & ADF
Should be knowledgeable in Stored procedures \ functions and be able to use them by ADF & Databricks as this is a widely used Practice internally
Familiar with DevOps process for Azure artifacts and database artifacts
Well versed with ADF concepts like chaining pipelines, passing parameters, using APIs for ADF & Databricks to perform various activities.
Experience creating and sharing standards, best practices, documentation, and reference examples for data warehouse, integration/ETL systems, and end user reporting
Apply disciplined approach to testing software and data, identifying data anomalies, and correcting both data errors and their root causes
Academics: Undergraduate degree preferred or equivalent experience along with a demonstrated desire for continuing education and improvement
Location: Austin, TX or Grand Rapids, MI
We are interested in every qualified candidate who is eligible to work in the United States. We are not able to sponsor visas for this position.","$96,659 /yr (est.)",5001 to 10000 Employees,Company - Private,Insurance,Insurance Agencies & Brokerages,2005,$1 to $5 billion (USD)
"Slide Insurance LLC
5.0",5.0,"Tampa, FL",Data Engineer,"Calling all innovators ready to take a proactive approach to Insurance in the digital world!
Slide is a Tampa based insurtech company, recently named a
**2023 BEST PLACES TO WORK** by Tampa Bay Business Journal

Slide is looking for a Data Engineer who will assist in the designing, building, and maintaining large-scale data processing systems.
**This position is located in Tampa at our Corporate Headquarters and candidate must reside already in Tampa region**

We do not offer Sponsorship opportunities for any of our positions.
Duties & Responsibilities:
Design, build, and maintain scalable data pipelines and ETL processes.
Collaborate with cross-functional teams to integrate new data sources and optimize data flows.
Ensure the reliability and availability of our data infrastructure through monitoring and proactive maintenance.
Automate and streamline data processing workflows to increase efficiency and reduce manual effort.
Continuously evaluate and implement new technologies to improve the performance and scalability of our data infrastructure.
Manage data storage and retrieval using cloud-based technologies such as Amazon S3, Azure Blob Storage, or Google Cloud Storage.
Perform other duties as assigned.

Education, Experience & Licensing Requirements:
Bachelors degree or equivalent education and work experience required.
3+ year's experience in data warehouse development required.
3+ year's experience with SQL required.

Qualifications/Skills/Competencies:
Strong knowledge of ETL tools (SSIS, SSAS, SSRS) required.
Knowledge of SQL Code Performance Tuning and optimizing SQL Code.
Familiarity with code repositories such as Github, Gitlab, and Bitbucket.
Cloud Data Warehouse technologies (ADF, Azure SQL, Databricks) experience a plus
Experience with cloud-based data storage and processing technologies like AWS, Azure, or GCP.
Experience with data modeling and schema design.
Comfortable in a fast-paced agile process, embracing TDD and CI/CD practices.
Strong interpersonal skills.
Excellent verbal and written communication skills.
Ability to work independently and with a team and prioritize effectively.
Ability to think critically and objectively.
Desire to live Slide's Core Values.

What's in it for you?? A paycheck of course but really so much more!
*****
The Slide Vibe - An opportunity to be a part of a fun and innovation-driven culture fueled by Passion, Purpose and Technology!
*****
Benefits - We have extensive and cost-effective benefits that cover you and your family from every angle... Physical Health, Emotional Health, Financial Health, Social Health, and Professional Health","$91,541 /yr (est.)",51 to 200 Employees,Company - Private,Insurance,Insurance Carriers,2021,Unknown / Non-Applicable
"Decision Point Healthcare
5.0",5.0,"Boston, MA",Data Pipeline Engineer,"Boston, MA
Full-Time

We need your help.
We’re looking for a self-motivated and detail-oriented data engineer to join our DataOps team. This opportunity will enable you contribute to the operation, support, and enhancement of our mission critical data operations platform and the development of data pipelines. Our data operations platform supports high volume, high velocity data ingestion and curation to support our existing, and rapidly expanding, health plan client base.
The position:
Design and develop scalable data pipeline processes (including ingestion, cleansing, curation, unification, etc.)
Automate the processing of inbound client data feeds
Design and develop tools and processes to support automated data profiling and data quality methodologies
Work with our data science team to assist with the development of feature store data including data prep, enrichment, and feature engineering for AI/ML
Write and maintain documentation on data pipelines
Provide periodic support to our customer success team Skills & Experience
BS / MS in Computer Science, Engineering, or applicable experience
3+ Year using Python (Pandas/NumPy) in a production environment
3+ Year using PowerShell in a production environment
Expertise with ETL/ELT and the development of automated validation and data pipelines
Understand database design and data manipulation and transformation methodologies
Keen understanding of EDW, master data management and other database design principles
Experience designing solutions using a range of AWS Services
Experience with data engineering and workflow management frameworks such as Airflow and dbt
Comfortable working with high volume data in a variety of formats
Experience with CI/CD such as Jenkins
Experience with version control tools: Git preferred
Excellent verbal and written communication
Familiarity with healthcare data is a plus
Familiarity with ML pipelines, principles and libraries is a plus
Experience with REST API is a plus
Why Decision Point?:
We’re as passionate about our people as we are about making our mark on healthcare. Fostering a fun and challenging environment that’s centered around personal and professional growth has brought us to where we are today. We are constantly seeking out new ways to reinvest in our team members because let’s face it, we all do our best work when we feel valued.
Meaningful work
Remote friendly environment
We encourage outside the box ideas
Great healthcare coverage
Competitive compensation","$87,903 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,2013,Less than $1 million (USD)
"DocuSign
3.7",3.7,"San Francisco, CA",Senior Data Engineer,"Company Overview

DocuSign helps organizations connect and automate how they agree. Our flagship product, eSignature, is the world’s #1 way to sign electronically on practically any device, from virtually anywhere, at any time. Today, more than a million customers and a billion users in over 180 countries use DocuSign to accelerate the process of doing business and simplify people’s lives.


What you'll do

DocuSign is seeking a talented and results-oriented Senior Data Engineer to focus on delivering trusted data to the business. The Senior Data Engineer delivers data for analytics using our Enterprise Data Warehouse, enabling the global DocuSign analytics community via curated, governed and cleansed data. As a member of the Global Data and Analytics team, the Data Engineer leverages a variety of technologies to accomplish this goal, ranging tools like Airflow, Matillion, dbt, Snowflake and Fivetran to languages like SQL and Python. The successful candidate will develop solutions with innovative cloud technologies, work on a variety of fast-paced assignments, and partner with world-class technical and business teams to maximize the value of data.

This position is an individual contributor role reporting to the Manager, Data Analytics.

Responsibility
Build data pipelines using Fivetran, dbt/Matillion, Snowflake and Airflow
Develop and maintain data documentation including ERD, data dictionaries, data lineage and metadata
Ensure data quality and integrity by implementing appropriate data validation and cleansing techniques
Monitor and troubleshoot data pipelines to identify and resolve issues in a timely manner
Build POCs to validate new concepts and new technologies
Collaborate with business, engineering, and data science teams to understand their data needs and design efficient solutions to support their requirements


Job Designation

Hybrid:
Employee divides their time between in-office and remote work. Access to an office location is required. (Frequency: Minimum 2 days per week; may vary by team but will be weekly in-office expectation)


What you bring

Basic
Bachelor’s Degree in Computer Science, Data Analytics, Information Systems, relevant field or equivalent work experience
8+ years of relevant experience
Experience with database and data warehouse concepts such as facts and dimensions to design and develop data models that support enterprise reporting and analytics needs
5+ years of dimensional and relational data modeling experience
Experience with modern data integration and transformation tools such as Fivetran, Dbt, and Matillion
Experience with workflow orchestration tools such as Airflow
Experience with MPP databases like Snowflake, Redshift and BigQuery
Experience with cloud platforms like AWS, Azure and GCP
Experience with versioning tools like git
Experience working with tools like Jira and Confluence
Experience with SQL and Python
Experience with document and data debugging

Preferred
Ability to work independently with minimal supervision, as well as in a team environment
Excellent communication skills
Eye for detail, good data intuition, and a passion for data quality
Comfortable working in a rapidly changing environment with ambiguous requirements
Organizational and time management skills, with the ability to prioritize tasks and meet deadlines


Wage Transparency

Based on applicable legislation, the below details pay ranges in the following locations:

California: $130,500 - $208,050 base salary

Illinois and Colorado: $123,700 - $174,700 base salary

Washington and New York (including NYC metro area): $123,700 - $184,275 base salary

This role is also eligible for bonus, equity and
benefits.


Life at DocuSign

Working here
DocuSign is committed to building trust and making the world more agreeable for our employees, customers and the communities in which we live and work. You can count on us to listen, be honest, and try our best to do what’s right, every day. At DocuSign, everything is equal.

We each have a responsibility to ensure every team member has an equal opportunity to succeed, to be heard, to exchange ideas openly, to build lasting relationships, and to do the work of their life. Best of all, you will be able to feel deep pride in the work you do, because your contribution helps us make the world better than we found it. And for that, you’ll be loved by us, our customers, and the world in which we live.

Accommodation
DocuSign provides reasonable accommodations for qualified individuals with disabilities in job application procedures. If you need such an accommodation, including if you need accommodation to properly utilize our online system, you may contact us at accommodations@docusign.com.

If you experience any technical difficulties or issues during the application process, or with our interview tools, please reach out to us at taops@docusign.com for assistance.

Our global benefits
Paid time off
Take time to unwind with earned days off, plus paid company holidays based on your region.
Paid parental leave
Take up to six months off with your child after birth, adoption or foster care placement.
Full health benefits
Options for 100% employer-paid health plans from day one of employment.
Retirement plans
Select retirement and pension programs with potential for employer contributions.
Learning & development
Grow your career with coaching, online courses and education reimbursements.
Compassionate care leave
Paid time off following the loss of a loved one and other life-changing events.","$149,200 /yr (est.)",5001 to 10000 Employees,Company - Public,Information Technology,Information Technology Support Services,2003,$1 to $5 billion (USD)
"Gleecus TechLabs Pvt. Ltd
4.5",4.5,"Glen Allen, VA",Lead Azure Data Engineer,"Position Title: Lead Azure Data Engineer - Glen Allen VA (Hybrid)
Tech Skills:
· Concepts to be covered ? (if any).
· Databricks(Python, SQL).
· Hands on Coding.
· Databricks optimization/Pyspark Transformations/PySpark Architecture.
· Azure Data Factory.
· Scenario-Based Questions and Practical Application.
· Azure Data Factory activities/Use of ADF Monitor/CI CD concepts.
· Azure ML, Databricks ML, Azure SDK.
· Scenario-Based Questions and Practical Application.
· Optimized Apache Spark environment, Machine Learning Integration, Experiment tracking, Model training, Feature development, Model serving.
· Synapse Analytics, Parque and Delta tables.
· Scenario-Based Questions and Practical Application.
· Synapse Analytics related concepts.
Job Type: Contract
Salary: $130,000.00 - $150,000.00 per year
Benefits:
Flexible schedule
Health insurance
Paid time off
Schedule:
Day shift
Monday to Friday
Ability to commute/relocate:
Glen Allen, VA 23058: Reliably commute or planning to relocate before starting work (Required)
Experience:
ADF: 4 years (Required)
Azure Data Lake: 4 years (Required)
Databricks: 5 years (Required)
Work Location: One location","$140,000 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2015,Unknown / Non-Applicable
"Rite Pros
3.4",3.4,"Portland, ME",Big Data Engineer,"Big Data Engineer with Bachelor’s degree in Computer Science, Computer Information Systems, Information Technology, or a combination of education and experience equating to the U.S. equivalent of a Bachelor’s degree in one of the aforementioned subjects.
Job Duties:
Define and manage the data architecture framework, standards, and principles, including modeling, metadata, security, reference, and master data.
Define reference architecture, and models to provide a common vocabulary, reusable components, and industry best practices.
Create solution frameworks integrating large or complex data sets.
Lead all data modeling efforts within DataBricks, including the design of data structures and the identification of business transformation logic.
Analyzing and translating business requirements into conceptual and fully detailed logical data models.
Creating logical data models based on existing applications and databases.
Working with business architects, and data stewards to capture business requirements in a Logical Data Model.
Transform the logical representation of the model into a physical representation and work with data engineering team to instantiate and manage the data.
Contribute to assessment of appropriate data platform(s) for solutioning efforts.
Collaborate with the BI and Analytics teams on creating the optimized, reusable semantic model, complete with metadata and lineage information.
Define and govern data modeling and design standards, tools, best practices, and related development for enterprise data models
Maintain the metadata - the ""data about the data"" describing the data model, its structure, and semantics
Recommend data and design patterns for common business functionality
Assist in multi-phase roadmaps within and across functional teams
Provide insight and technology recommendations that result in enterprise solutions and architectural risk mitigation.
Contribute to innovation strategies by exploring, investigating, recommending, benchmarking and implementing new data centric technologies for the platform.
Identify, design, and implement internal process improvements such as automation of manual processes, optimization of data delivery and re-design of existing infrastructure for greater scalability.
Build the infrastructure and processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and/or other data technologies.
Create and maintain data-related documentation (e.g. data dictionaries, data maps, data models etc.)
Work with data engineering and analytics experts to strive for greater functionality in our data systems and incorporation of industry best practices
Work experience / Technologies required for the position :
Experience building and optimizing Big Data, Data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, meta data, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected data sets.
Working knowledge of message queuing, stream processing, and highly scalable Big Data, data stores.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience in a Data Engineer or similar roles, who has attained a Bachelor’s degree in Bachelor’s degree in Engineering, Computer Science, Information Technology, related field or equivalent work experience.
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Work location is Portland, ME with required travel to client locations throughout USA.

Rite Pros is an equal opportunity employer (EOE).

Please Mail Resumes to:
Rite Pros, Inc.
565 Congress St, Suite # 305
Portland, ME 04101.
Email: resumes@ritepros.com","$97,894 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Less than $1 million (USD)
Premier Consulting Group,#N/A,"Boca Raton, FL",Senior Data Engineer,"* The right person could be located in Florida or Massachusetts as company has office locations in Boca Roton and Boston.
* Must be a US Citizen
Overview Summary of the Work:
ETL Azure Data Factory work: A Data Engineer who is to be able to import data from a variety of different sources (SFTP, API, Fileshare) and file types (CSV, Excel, JSON). Company also captures files via email now, but they have a pattern set up to be able to do that, so the person would just repeat how they’ve done that. Check existing Azure Data Factory pipelines for failures and troubleshoot. Familiar with dynamic expression and syntax in Azure Data Factory pipelines.
Backlog of stored procedures: Be skilled in stored procedure development and SQL skills.
Data quality and data cleansing: Be able to evaluate data (incoming and existing). Look for issues and be able to resolve them. In general, someone who can look at the data and mentally make the leap of “hey, this looks wrong.” Look at data and determine how it might be better utilized (examples: 2.5% as a string or 0.025 as a number, identify values as empty strings and save as nulls instead, etc).
Technical Experience Required/Preferred:
SQL, Azure SQL, Azure Data Factory, Data warehousing environments (dims and facts), Snowflake (highly preferred), Python (highly preferred).
Job Types: Full-time, Permanent
Pay: $140,000.00 - $160,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Bonus pay
Yearly bonus
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Weekend availability
Work Location: Hybrid remote in Boca Raton, FL 33431","$150,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Long Finch Technologies
4.6",4.6,"Bellevue, WA",Data Engineer- AZURE with Strong SPARK,"Job Title: Data Engineer
Location: Bellevue, WA Onsite
Duration: Full Time
Job Description
Data Engineering experience primarily on Spark. Someone who has worked on Azure cloud with knowledge on Azure DEVOPS (CI/CD & Infrastructure as a code), ADF, ADW & Power BI. Apache Nifi will be good to have.Power BI: Understand business requirements to set functional specifications for reporting applications Build automated reports and dashboards with the help of Power BI reporting tool Be experienced in tools and systems on MS SQL Server BI Stack, including SSRS and TSQL, Power Query, MDX, PowerBI, and DAX Be able to quickly shape data into reporting and analytics solutions Have knowledge of database fundamentals such as multidimensional database design, relational database design, and more Create functional reporting Study, analyze and understand business requirements in context to business intelligence. Design and map data models to shift raw data into meaningful insights. Utilize Power BI to build interactive and visually appealing dashboards and reports. Spot key performance indicators with apt objectives Run DAX queries and functions in Power BI Should have an edge over making DAX queries in Power BI desktop. Developing visual reports, KPI scorecards, and dashboards using Power BI desktop. Connecting data sources, importing data, and transforming data for Business intelligence.
Job Type: Full-time
Salary: $100,000.00 - $105,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Vision insurance
Schedule:
8 hour shift
Ability to commute/relocate:
Bellevue, WA 98004: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 5 years (Required)
Azure: 3 years (Required)
Spark: 3 years (Required)
Work Location: One location","$102,500 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2012,$25 to $100 million (USD)
"Oak Street Health
3.7",3.7,United States,"Principal Engineer, Data","Company: Oak Street Health
Title: Principal Engineer, Data
Company Description
Oak Street Health is a rapidly growing company of primary care centers for adults on Medicare in medically-underserved communities where there is little to no quality healthcare. Oak Street’s care is based on an entirely new model that is based on value for its patients, not on volume of services. The company is accountable for its patients’ health, spending more than twice as long with its patients and taking on the risks and costs of their care. For more information, visit http://www.oakstreethealth.com.
Role Description:
The Principal Data Engineer will be responsible for delivering high quality modern data solutions through collaboration with our engineering, analysts, and product teams in a fast-paced, agile environment leveraging cutting-edge technology to reimagine how Healthcare is provided. They will be instrumental in designing, integrating, and implementing solutions as well as supporting migrations of existing workloads to Azure cloud. The Principal Data Engineer is expected to have extensive knowledge of modern programming languages, designing and developing data solutions.
Core Responsibilities:
Develop and automate solutions to consume data from multiple data sources, including external API
Program and modify code in languages like Java, Json, and Python to support and implement Data Warehouse solutions
Design and deploy enterprise-scale cloud infrastructure solutions
Research, analyze, recommend and select technical approaches for solving difficult and meaningful development and integration problems
Work closely with the Data and Engineering teams to design best in class Azure implementations
Participate in efforts to develop and execute testing, training, and documentation across applications
Design, develop and deliver customized ETL and Database solutions
Other duties as assigned
What are we looking for?
Minimum of 8 years of relevant working experience with Azure
Minimum of 5 years of experience working with SQL
Minimum of 5 years of hands-on experience with cloud orchestration and automation tools, CI/CD pipeline creation
Minimum of 5 years of experience in provisioning, configuring, and developing solutions in Azure Data Lake, Azure Data Factory, Azure SQL Data Warehouse, Azure Synapse and Cosmos DB
Hands-on experience working with PaaS/ IaaS/ SaaS products and solutions
Understanding of Distributed Data Processing of big data batch or streaming pipelines
Strong knowledge in DevOps, Python or Java or Json, (HL7/ FHIR is a plus)
A desire to work within a fast-paced, collaborative, and team-based support environment
Ability to work independently as well as function as part of a team
Willingness to identify and implement process improvements, and best practices as well as ability to take ownership
Familiarity with healthcare data and healthcare insurance feeds is a plus
Excellent oral and written communication skills
US work authorization
Someone who embodies being “Oaky”

What does being “Oaky” look like?
Radiating positive energy
Assuming good intentions
Creating an unmatched patient experience
Driving clinical excellence
Taking ownership and delivering results
Being scrappy

Why Oak Street?

Oak Street Health offers our coworkers the opportunity to be at the forefront of a revolution in healthcare, as well as:
Collaborative and energetic culture
Fast-paced and innovative environment
Competitive benefits including paid vacation and sick time, generous 401K match with immediate vesting, and health benefits
Oak Street Health is an equal opportunity employer. We embrace diversity and encourage all interested readers to apply to oakstreethealth.com/careers.",#N/A,1001 to 5000 Employees,Company - Public,Healthcare,Health Care Services & Hospitals,2012,Unknown / Non-Applicable
Dobbs Defense Solutions,#N/A,"Fort Meade, MD",Data/NiFi Engineer,"Our Mission
At Dobbs Defense, we deliver mission-centric IT, Cyber, and data analytics solutions for our government and commercial clients through the convergence of automation, innovation, training, and education. Delivering high-quality IT, cybersecurity, and data analytics solutions through proven and innovative methods is our vision.

Job Description
Dobbs Defense Solutions is seeking a Senior Data/NiFi Engineer to design, develop, implement, test, secure, and support data transport and brokering for multiple classified networks and between networks.

Duties
Design, develop, implement, test, secure, and support data transport and brokering.
Data flow engineering for one of DISA’s premier boundary defense systems.
Perform system administration of Niagara Files (NiFi) on Linux VMs.
Support backup/failover/load balancing of data brokering to improve availability.
Monitor availability & amp, usage.
Respond to outages, take corrective action, and issue outage reports.
Familiarity with DISA STIGs and SCAP.
Familiarity with Java programing.

Qualifications
Required education and experience:
5+ years of Data Engineering experience.
Significant experience with Niagara Files (NiFi) and MiNiFi or similar data engineering tools.
Excellent written and verbal communication skills.
IAM or IAT Level II Certification (e.g., GSEC, SEC+, SCNP, SSCP, etc.).
Active Secret clearance.

Required Clearance
Secret

Working Environment
Onsite

Our Equal Employment Opportunity Policy
Dobbs Defense Solutions is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, creed, sex (including pregnancy, childbirth, breastfeeding, or medical conditions related to pregnancy, childbirth or breastfeeding), sexual orientation, gender or gender identity (except where gender is a bona fide occupational qualification), gender expression and transgender status, national origin, ancestry, age, disability, military or veteran status, marital or domestic partner status, genetic information, citizenship, low-income status or any other status or characteristic protected by applicable law. We are committed to equal employment opportunity in all decisions related to employment, promotion, wages, benefits, and all other privileges, terms, and conditions of employment.","$83,925 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"FinditParts
4.8",4.8,"Los Angeles, CA",Senior Data Warehouse Engineer,"FinditParts is the nation's largest eCommerce provider of heavy-duty truck and trailer parts. From hard-to-find parts to everyday preventative maintenance items, we offer more than 3 million heavy-duty OEM, branded, and aftermarket parts ready to ship. Each month thousands of repair shops, fleets, and owner-operators rely on FinditParts to streamline their part sourcing efforts and keep their trucks on the road.

As the industry leader in parts discovery through visual identification technology and aces/pies, we simplify the complexity of finding the right part to fit any commercial vehicle, reducing the time and frustration associated with parts sourcing.
Founded in 2010, with offices in Los Angeles and the Philippines, FinditParts is well-funded, having recently raised $30 million in Series A funding, and profitable.
SUMMARY
The Senior Data Warehouse Engineer, reporting to the Director of Analytics, will be responsible for leading the complete end-to-end architectural blueprint and development of our Data Warehouse system. A successful candidate will deliver a thoughtful data storage system that will store large data sets through various data intelligence environments. The Senior Data Warehouse Engineer will have a high level of impact, by providing architectural solutions to the design, development, testing, and deployment, and allowing FinditParts to store large volumes of data efficiently and effectively for the organization as a whole.
RESPONSIBILITIES
Lead the architectural design blueprint and development of our data warehouse
End-to-end ownership during the design, development, testing, and deployment
Evaluate architectural and software solutions that will deliver appropriate solutions
Develop and maintain data pipelines as well as API-based or file-based data flows between source systems and the data warehouse
Translate client user requirements into a technical architecture vision and implementation plan
Architect the data intelligence environments, including data lakes, data marts, and metadata repositories
Consult with leadership to define goals and requirements, develop technical requirements, deliver analysis, and thoughtful conclusions with data to provide actionable insights
Build, review, and audit existing ETL jobs and SQL queries
Identify gaps and develop a plan to integrate current systems with a desired future state
Perform root cause analysis of data failures and update existing processes to prevent re-occurring issues
Collaborate with various teams to gather information and create visually appealing, engaging, and informative reports in Tableau
Work closely with the BI team, Product team, and Analytics teams to ensure the Data Warehouse is structured in a way to meet data needs for all business teams
QUALIFICATIONS
5+ years of industry experience in Data Warehousing and/or Data Engineering
Bachelor's Degree in Computer Science, Engineering, Statistics, Mathematics, or another quantitative field
Proven experience managing and transforming data in a data warehouse
Proven experience in manipulating, processing, and extracting value from large data-sets
Familiarity with data schema designs that best support business needs and reporting
High level of experience extracting/cleansing data and generating insights from large transactional data sets using SQL, R, Python, and/or Spark on the cloud
Experience building and managing data pipelines and repositories in cloud environments such as Google Cloud, Microsoft Azure or AWS
Experience in Looker, Redshift, Apache Spark, Spark Structured Streaming, SQL, etc
Experience with cloud computing with Dataproc, Databricks, or similar technologies
Strong problem-solving/analytical abilities
BENEFITS
100% US-based remote
Competitive salary, bonus, and equity
Flexible PTO policy
Paid Medical, dental, and vision insurance options for Employees
Fun and energizing start-up environment
""The US base salary range for this full-time position is $125,000 - $165,000 + bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about specific salary range for your preferred location during the hiring process. Please note that the compensation details listed in the US role postings reflect the base salary only and do not include bonus, equity, or benefits.""
FinditParts is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran or any other characteristic protected by law. FinditParts conforms to the letter of all applicable laws and regulations.","$130,474 /yr (est.)",1 to 50 Employees,Company - Private,Retail & Wholesale,Automotive Parts & Accessories Stores,2010,Unknown / Non-Applicable
"HCA Healthcare
3.3",3.3,"Nashville, TN",Principal Data Engineer,"Introduction
Last year our HCA Healthcare colleagues invested over 156,000 hours volunteering in our communities. As a Principal Data Engineer with HCA Healthcare you can be a part of an organization that is devoted to giving back!
Benefits
HCA Healthcare, offers a total rewards package that supports the health, life, career and retirement of our colleagues. The available plans and programs include:
Comprehensive medical coverage that covers many common services at no cost or for a low copay. Plans include prescription drug and behavioral health coverage as well as free telemedicine services and free AirMed medical transportation.
Additional options for dental and vision benefits, life and disability coverage, flexible spending accounts, supplemental health protection plans (accident, critical illness, hospital indemnity), auto and home insurance, identity theft protection, legal counseling, long-term care coverage, moving assistance, pet insurance and more.
Free counseling services and resources for emotional, physical and financial wellbeing
401(k) Plan with a 100% match on 3% to 9% of pay (based on years of service)
Employee Stock Purchase Plan with 10% off HCA Healthcare stock
Family support through fertility and family building benefits with Progyny and adoption assistance.
Referral services for child, elder and pet care, home and auto repair, event planning and more
Consumer discounts through Abenity and Consumer Discounts
Retirement readiness, rollover assistance services and preferred banking partnerships
Education assistance (tuition, student loan, certification support, dependent scholarships)
Colleague recognition program
Time Away From Work Program (paid time off, paid family leave, long- and short-term disability coverage and leaves of absence)
Employee Health Assistance Fund that offers free employee-only coverage to full-time and part-time colleagues based on income.
Learn more about Employee Benefits
Note: Eligibility for benefits may vary by location.

Would you like to unlock your potential with a leading healthcare provider dedicated to the growth and development of our colleagues? Join the HCA Healthcare family! We will give you the tools and resources you need to succeed in our organization. We are looking for an enthusiastic Principal Data Engineer to help us reach our goals. Unlock your potential!
Job Summary and Qualifications
HCA Healthcare ITG
Job Summary:
The role requires working closely with others, frequently in a matrixed environment, and with little supervision. As a Principal Data Engineer/Architect level, the role requires 'self-starters' who are proficient in problem solving and capable of bringing clarity to complex situations. It requires contributing to strategic technical direction and system architecture approaches for individual projects and platform migrations. The culture of the organization places an emphasis on teamwork, so social and interpersonal skills are equally important as technical capability. Due to the emerging and fast-evolving nature of GCP/Big Data technology and practice, the position requires that one stay well-informed of technological advancements and be proficient at putting new innovations into effective practice.
Responsible for leading GCP development efforts, driving adoption and appropriate use of technology and consulting on internal and external development efforts to ensure code quality and sound architecture. This position that assumes the responsibility for project success and the upward development of team members technical skills. They are the development team's point of contact that must interface with business partners of varying roles ranging from technical staff to executive leadership. In addition, this candidate will have a history of increasing responsibility in a multi-role team. This position requires a candidate who can analyze business requirements, perform design tasks, construct, test, and implement cutting-edge technical data solutions with minimal supervision.
As a Principal Data Engineer/Architect, you will work closely with all team members to create a modular, scalable solution that addresses current needs, but will also serve as a foundation for future success. The position will be critical in building the team’s engineering practices in test driven development, continuous integration, and automated deployment and is a hands-on team member who actively coaches the team to solve complex problems. She / he will be responsible for the design, development, performance and support of the Cloud Platform components.
This candidate will have a record of accomplishment of participation in successful projects in a fast-paced, mixed team (consultant and employee) environment. In addition, the applicant must be willing to train and mentor other developers to prepare them for assuming the responsibilities.
General Responsibilities:
Responsible for building and supporting a GCP/Hadoop-based ecosystem designed for enterprise-wide analysis of structured, semi-structured, and unstructured data.
Bring new data sources into GCP/HDFS, transform and load to databases.
Lead projects in delivering the data and projects on-time
Closely collaborates with team members to successfully execute development initiatives using Agile practices and principles
Leads efforts to design, development, deploy, and support software systems
Experience with HL7, FHIR, and Whistle mapping.
Collaborates with business analysts, project lead, management and customers on requirements
Participates in large-scale development projects involving multiple areas outside of core team
Designs fit-for-purpose products to ensure products align to the customer's strategic plans and technology road maps
Demonstrates deep understanding and coaches’ value-based decision making and Agile principles across teams
Coaches team on clinical data, existing system structure, constraints and deficiencies with product
Shares knowledge and experience to contribute to growth of overall team capabilities
Participates in the deployment, change, configuration, management, administration and maintenance of deployment process and systems
Work closely with management, architects and other teams to develop and implement the projects.
Actively participate in technical group discussions and adopt any new technologies to improve the development and operations.
Focuses on customer satisfaction
Rapidly prototypes and delivers just-in-time solutions
Gather requirements, designs, constructs and delivers solutions with minimal team interaction
Works in an environment with rapidly changing business requirements and priorities
Demonstrates deep understanding and acts as a leader in the team’s continuous integration and continuous delivery automation pipeline
Work collaboratively with Data Scientists, business, and IT leaders throughout the company to understand Cloud/Big Data needs and use cases.
Education, Experience and Certifications:
Bachelor's Degree in computer science or related field – Required
Master's Degree in computer science or related field – Preferred
3+ years of experience in Data Engineer – Required
1+ year(s) of experience in Healthcare – Preferred
10+ years of experience in Information Technology – Required
CCDH (Cloudera Certified Developer for Apache Hadoop) – Preferred
GCP Cloud Professional Data Engineer – Preferred
Other Required Qualifications:
A successful candidate will have:

Strong understanding of best practices and standards for GCP/Hadoop application design and implementation.
Hands on experience with Big Data Technologies and experience with many of the following components:
Hadoop, MapReduce, Spark, Impala, Hive, Solr, YARN
Flume, Spark Streaming, Kafka
HBase or Cassandra
SQL, JSON, Avro, Parquet
Two Year of hands-on experience with GCP platform and experience with many of the following components:
GCS, Cloud Run, Cloud Functions
Bigtable, Cloud SQL
Kafka, Pub/Sub
Python, Spark, Scala or Java
BigQuery, Dataflow, Data Fusion
OpenShift, Docker
Experience with Unstructured Data
Understanding of Lambda Design Architectures and Real-Time Streaming
Ability to multitask and to balance competing priorities.
Requires strong practical experience in agile application development, file systems management, and DevOps discipline and practice using short-cycle iterations to deliver continuous business value.
Expertise in planning, implementing, supporting, and tuning Cloud/Hadoop ecosystem environments using a variety of tools and techniques.
Knowledge of all facets of Cloud/Hadoop ecosystem development including ideation, design, implementation, tuning, and operational support.
A successful candidate may have:
Experience in Healthcare Domain
Experience in Patient Data
Experience with Natural Language Processing (NLP)
Hardware/Operating Systems:
Linux, UNIX
GCP
Distributed, highly-scalable processing environments
Databases:
NoSQL, HBase, Cassandra, MongoDB, Cosmos, In-memory, Columnar, other emerging technologies
Other Languages – Java, Python, Scala, R
Build Systems – TFS, Github
Ability to integrate tools outside of the core Cloud/Hadoop ecosystem
Physical Demands/Working Conditions
Prolonged sitting or standing at computer workstation including use of mouse, keyboard, and monitor.
Requires ability to provide after-hours support.
Occasional Travel: The job may require travel from time- to-time, but not on a regular basis.
HCA Healthcare’s Information Technology Group (ITG) delivers healthcare IT products and services to HCA Healthcare's portfolio of business and partners, including Parallon, HealthTrust and Sarah Cannon.

For decades, ITG has been a pioneer in the industry, leading the transformation of healthcare into a new era of quality and connectivity. ITG relies on the breadth of the organization and depth of technical expertise to advance and enhance today’s healthcare and to enable our physicians and clinicians to provide world-class, innovative care for patients.

ITG employees rally around the noble cause of transforming healthcare through technology and find inspiration in the meaningful work they do—creating a culture that follows our mission statement which begins by saying “above all else we are committed to the care and improvement of human life.”

If you want a career in technology and have a heart for healthcare, apply your expertise to a mission that matters.
HCA Healthcare has been recognized as one of the World’s Most Ethical Companies® by the Ethisphere Institute more than ten times. In recent years, HCA Healthcare spent an estimated $3.7 billion in cost for the delivery of charitable care, uninsured discounts, and other uncompensated expenses.
""There is so much good to do in the world and so many different ways to do it.""- Dr. Thomas Frist, Sr.
HCA Healthcare Co-Founder
Be a part of an organization that invests in you! We are reviewing applications for our Principal Data Engineer opening. Qualified candidates will be contacted for interviews. Submit your application and help us raise the bar in patient care!
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.","$116,594 /yr (est.)",10000+ Employees,Company - Public,Healthcare,Health Care Services & Hospitals,1968,$10+ billion (USD)
"Truelogic Software
4.7",4.7,"San Francisco, CA",Sr Data Engineer (Java/ Python/ React) - Marketing (ID:1432),"Truelogic is a leading provider of nearshore staff augmentation services, located in New York. Our team of 500 tech talents is driving digital disruption from Latin America to the top projects in U.S. companies. Truelogic has been helping companies of all sizes to achieve their digital transformation goals.
Would you like to make innovation happen? Have you ever dreamed of building Products that impact millions of users? Nice! Then we have a seat for you on our team!

What are you going to do?
You will have the opportunity to work as a Data Engineer, and will need to have experience in Java, Python, AWS, Snowflake, Kafka, and Apache Flink real-time streaming applications, who is eager to design and deploy large-scale data applications and data governance solutions. The ideal candidate will have a strong background in software development and a passion for building scalable, high-performance systems.
Occupy a unique position in the market, you will enjoy the benefits of both worlds: the quick expansion and agility of a startup combined with the size and profitability of a huge international corporation.
Design and implement software solutions using Java, Python.
Build and maintain real-time streaming applications using Apache Flink and Kafka.
Contribute to the development of software architectures and designs.
Utilize AWS services to build and deploy scalable, reliable systems.
Review code and mentor junior team members.
Troubleshoot and debug software issues.

Why do we need your skills?
Education in the field of Computer Science or equivalent experience.
+5 years of hands-on Experience with software development using Python or Java.
Hands-on experience with SQL.
Knowledge of Data Governance and Role base access control.
Knowledge of Cloud databases like Snowflake.
Basic knowledge of design techniques involved in building object-oriented software applications.
Good verbal and written communication skills, problem-solving skills, and interpersonal skills.","$129,759 /yr (est.)",501 to 1000 Employees,Company - Public,Information Technology,Enterprise Software & Network Solutions,2003,Unknown / Non-Applicable
"Dealer Tire
3.6",3.6,"Cleveland, OH",Data Engineer,"Who We Are
We’re Dealer Tire, a family-owned, international distributor of tires and parts established in 1918 in Cleveland, OH. We’re laser focused on helping the world’s largest and most trusted auto manufacturers grow their tire business—in fact, we’ve sold more than 60 million tires to date. We’re a thriving company, and we’re looking for driven individuals to join our team. That’s where you come in!
As a Data Engineer at Dealer Tire, you will be part of a highly skilled team of innovative data professionals who are responsible for designing and implementing our enterprise data lake, data model, and the ETL/ELT pipelines that feed our business and analytical systems. You and your teammates will collaborate with internal and external customers and our affiliate companies, empowering them to solve business problems and gain powerful insights using high-performance datasets, enterprise analytics systems, and self-service tools such as Alteryx, R, Python, and Power BI.
As a Data Engineer, your essential job functions will include the following
Collaborate with business and IT teams to implement a comprehensive and easily expandable Enterprise Data Model based on business need.
Be a thought leader on future data usage (Data Futurist) and a catalyst for converting data into a knowledge base.
Contribute, build, and participate in the design and enhancement of our enterprise data model, enterprise analytics systems, data marts, and data warehouse/data lake.
Build robust, scalable, and high-performing ETL/ELT solutions involving structured & unstructured data.
Leverage appropriate design patterns for the problem being solved, such as near-real-time/change data capture, batch processing, streaming data, etc.
Collaborate with internal and external customers, our affiliates, and other IT teams to define and implement a roadmap for the enterprise data model and the systems that support it, including the maintenance of consistent data entity and element definitions across multiple environments.
Participate in design reviews to foster team accountability and maintain a high standard of quality.
Evangelize and democratize our data assets and empower users with self-service tools and training to enable them to leverage those assets effectively.
Engage with multiple concurrent projects both within your team and with other teams.
Articulate ideas and architectural concepts clearly and concisely through verbal and written communication.
Monitor and measure system and process performance and make corrections and enhancements where needed as part of a focus on continual improvement.
Provide on-call/after-hours support for processes and systems owned by the team on a rotating basis.
Other Duties as Assigned
Position Requirements
3-5 years of experience in a Data Engineer or comparable role is required.
Strong skills in querying databases (t-SQL, PL-SQL, etc.), data modeling (including data warehouse design concepts such as star schema design, etc.), data engineering, and data reverse engineering skills are required.
Strong verbal and written communication skills are required.
Ability to work cross-functionally in a fast-paced, high growth environment and manage multiple concurrent workstreams and priorities is a must.
A bachelor’s degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field is a plus.
Experience with any of the following technologies is a plus: SAP Data Integrator, Syniti Data Replication, Microsoft SQL Server, SQL Server Integration Services (SSIS), Oracle Business Intelligence/Oracle Analytics Server, JD Edwards, DB2, Python, Power Shell, R, Scala, Go, Amazon AWS (EC2 instances, Redshift, lambda functions, S3, etc.), Google Analytics, Google Big Query, Snowflake, Alteryx, Power BI, and Windows and Linux servers.
Experience performing root cause analysis on datasets and processes to answer specific business questions, identify opportunities for improvement, or resolve system or process issues is a plus.
General expertise in enterprise IT architecture (databases, ERP, middleware, UI, networking, infrastructure) is a plus.
Experience implementing high availability (HA) and disaster recovery (DR) solutions is a plus.
Competencies Required
Results Orientation
Agility
Initiative
Influence
Customer Focus
Business Acumen
Strategic Thinking
Organizational Agility
Relationship Building
Physical Job Requirements
Continuous viewing from and inputting data to a computer screen
Sitting for long periods of time
Travel as necessary
Drug Policy:
Dealer Tire is a drug-free environment. All applicants being considered for employment must pass a pre-employment drug screen before beginning work.
Why Dealer Tire: An amazing opportunity to join a growing organization, built on the efforts of hard working, innovative, and team-oriented people. We offer a competitive salary + bonus, and a comprehensive benefit package including: paid time off, medical, dental, vision, and 401k matching (50% on the dollar up to 7% of employee contribution).
EOE Statement: Dealer Tire is an Equal Employment Opportunity (EEO) employer and does not discriminate on the basis of race, color, national origin, religion, gender, age, veteran status, political affiliation, sexual orientation, marital status or disability (in compliance with the Americans with Disabilities Act*), or any other legally protected status, with respect to employment opportunities.
ADA Disclosure: Any candidate who feels that they may need an accommodation to complete this application, or any portions of same, based on the impact of a disability should contact Dealer Tire’s Human Resources Department to discuss your specific needs. Please feel free to contact us at ADAAAccommodation@dealertire.com or via phone at 833-483-8232.","$92,386 /yr (est.)",501 to 1000 Employees,Company - Private,Retail & Wholesale,Wholesale,2001,$1 to $5 billion (USD)
"Core4ce
4.3",4.3,"Herndon, VA",Data Engineer,"The Data Engineer will provide the engineering support to data science and software engineering team members.
Design and optimize Data Pipelines using Spark, Hudi, EMR cloud services, and Kubernetes containers
Make sure pedigree and provenance of the data is maintained such that the access to data is protected
Clean and preprocess data to enable analytic access
Architects complex, repeatable ETL processes
Provide Advanced Database Administration support in Oracle, MySQL, MariaDB, MongoDB, Elastic and others
Supports Experience with Targeting using Sponsor Tools, Reverse Engineering
Develop API connectors to enable ingest of new data catalog entries from databases and files
Ensure that data mappings will provide the best performance for expected user experience
Augmenting technical support in the area of data science, data engineering, and systems engineering, and reviewing and providing technical assessments.
Provides current system architecture documentation, engineering/web development programming support for program/project requirements defined tasks, data science/data engineering related technical assessments, effective communication with users and managers, and server administration, to include hardware and software support to existing servers.
Support ad-hoc data analysis requirements defined by the client's Leadership.
Reporting solutions will encompass multiple technology platforms used by the client and will drive business process re-design and raining/change management initiatives.
Will work on Data cleaning and transformation efforts in delivery of CDRLs.
Supports Deliverables and Reports

Requirements
5+ years experience working with Data Sets
Experienced in extracting and aggregating structured and unstructured data.
Experienced in data programming languages and tools such as Python and R.
Experience with SQL or similar database language.
Experience designing and implementing data models to enable, sustain and enhance the value of information they contain
Knowledge of business intelligence reporting tools and data visualization software including Tableau.
Strong analytical and critical thinking skills.
Ability to work collaboratively and effectively in a team environment.
TS/SCI with Full Scope Poly Required

All qualified applicants will receive consideration for employment without regard to race, color, sex, sexual orientation, gender identity, religion, national origin, disability, veteran status, age, marital status, pregnancy, genetic information, or other legally protected status

Required Skills

Required Experience","$103,496 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2018,$100 to $500 million (USD)
Pyramid Global Hospitality Corporate Offices - Boston,#N/A,"Boston, MA",Data Engineer Manager,"About Us:
At Pyramid Global Hospitality, people come first. As a company that values its employees, Pyramid Global Hospitality is dedicated to creating a supportive and inclusive work environment that fosters diversity, growth, development, and wellbeing. Our commitment to a People First culture is reflected in our approach to employee development, employee benefits and our dedication to building meaningful relationships.

Pyramid Global Hospitality offers a range of employment benefits, including comprehensive health insurance, retirement plans, and paid time off, as well as unique perks such as on-site wellness programs, local discounts, and employee rates on hotel stays. In addition, Pyramid Global Hospitality is committed to providing ongoing training and development opportunities to help our people build the skills and knowledge they need to advance their careers.

Whether you are just starting out in the hospitality industry or are a seasoned professional, Pyramid Global Hospitality offers a supportive and collaborative work environment that encourages growth and fosters success, in over 230 properties worldwide. Join their team and experience the benefits of working for a company that values its employees and is committed to creating exceptional guest experiences.

Check out this video for more information on our great company!
Location Description:

Pyramid Global Hospitality (“Pyramid”) is a leading hotel management company, operating in the US, Caribbean, and Western Europe. With portfolio revenues exceeding $3 billion, Pyramid manages 230 hotels, resorts, and conference centers, both branded and independent. The firm maintains offices in Boston (Headquarters), Cincinnati, Houston, and London. Additional information about Pyramid can be found at www.pyramidglobal.com

In 2021, Pyramid and Benchmark Resorts and Hotels merged to add an additional 59 Managed or Asset Managed Resorts and over 10,000 additional team members. The two companies share the same company culture, values and philosophies. We are growing and opportunities abound!

What really sets Pyramid apart from our competitors is our reputation as an employer. Professional growth is not just possible throughout the company but planned and encouraged. The Leadership Team at Pyramid consider team member development its first priority, understanding that success is only achieved in a workplace where every contributor is respected and recognized. This is why we deliver superior results.

There is opportunity to work directly with senior leaders, experience stretch assignments and learn hospitality management from industry giants. You will come to know a distinctive people centric culture that is at the core of all we do. The decisions we make and the paths we take are bound by a commitment to our Owners, Associates, Customers and the Communities where we work. We attract the most talented associates in the industry, and actively encourage candidates with a “hospitality spirit” who may be thinking about a career change to join our team.

Overview:
Our organization is seeking a highly motivated Manager of Data Engineering to oversee and advance our data management capabilities. You will be joining a dynamic team and playing a critical role in managing data pipelines that provide decision-making information used by our field leaders, corporate executives, and property owners. Timely, accurate information is our competitive advantage and this role is pivotal to our success.

The primary focus involves leveraging data management best practices to improve data reliability and quality. This includes ensuring day-to-day operational excellence, enabling new data source integration supporting the company’s aggressive growth plans, and evolving our data capabilities.

This leadership role involves strategic design and operational execution. You will join a team led by the Vice President of Data Sciences and play a crucial part in advancing our data strategy and supporting the company's ever-expanding appetite for information.

Essential Functions:

As the Data Engineering Manager, your responsibilities will include but not be limited to:
Support and improve the data pipeline operations – extract/transform/load data from various sources, manage storage, make data available for analytics
Resolve data integrity issues with measures to permanently address the root cause
Architect data management practices to ensure high-quality data
Drive thought-leadership for optimizing our data pipelines for advanced analytics
Implement best-practice methodologies to accelerate our data velocity
Automate data management processes for more efficient data capture, storage, and availability
Establish a data governance capability to solidify ownership across the data life cycle
Facilitate collaboration with stakeholders from across the company to create improvements in effective data use
Stay current with the latest technologies and methodologies
Other responsibilities as assigned
Qualifications:
A Bachelor's degree in Computer Science, Information Systems, or a related field
Demonstrate data management leadership experience
Microsoft Azure Synapse Analytics ecosystem
Azure SQL Data Warehousing experience
ETL expertise with large, complex, heterogeneous datasets
Accomplished work with PowerBI, star-schemas, and analytics data prep
Solid understanding of data analysis and machine learning
A track record of delivering successful data management improvement projects
Excellent communication and interpersonal skills
Strong problem-solving skills and the ability to work independently
A commitment learning about the latest technologies and methodologies

If you're a highly motivated individual with experience in data management and a desire to drive process improvement, we want to hear from you!
Compensation Range: The compensation for this position is $110,000.00/Yr. - $125,000.00/Yr. based on qualifications and experience.","$117,500 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Decision Point Healthcare
5.0",5.0,"Boston, MA",Data Engineer,"Boston, MA
Full-Time

We need your help.
We’re looking for a self-motivated and detail-oriented data engineer to join our DataOps team. This opportunity will enable you contribute to the operation, support, and enhancement of our mission critical data operations platform and the development of data pipelines. Our data operations platform supports high volume, high velocity data ingestion and curation to support our existing, and rapidly expanding, health plan client base.
What you’ll do:
As a data engineer, you will be responsible for the execution and management of inbound client and internal service-based data pipelines. This encompasses the development, operation, and management of our client data hubs, including data intake, data quality assessment/evaluation and data curation and enrichment/preparation processes. Our client data hubs consist of various health plan data sources to support Decision Point services including our AI/ML platform, analytics platform and OPUS application. If this interests you, read on.
The position:
Design and develop scalable data integration (ETL/ELT) processes (including ingestion, cleansing, curation, unification, etc.)
Automate the processing of inbound client data feeds
Design and develop tools to support data profiling and data quality methodologies
Engage with our software engineering team to ensure precise data points per application specification
Provide periodic support to our customer success team
Skills & experience:
BS / MS in Computer Science, Engineering or applicable experience
3+ years of experience with ETL/ELT and data pipeline principles
3+ years of experience with Python, JavaScript, and/or PowerShell
3+ years of SQL experience; Microsoft SQL Server or PostgreSQL preferred
Knowledge of data manipulation methodologies
Excellent verbal and written communication
Strong data profiling skills; Ability to discover and highlight unique patterns/trends within data to identify and solve complex problems
Keen understanding of EDW and other database design principles
Comfortable working with very large data sets and VLDB environment
Experience with version control tools: Git preferred
Experience with CI/CD such as Jenkins
Ability to write and interpret complex queries towards data-driven business solutions
Understanding of data science and machine learning concepts preferred
Experience working within hybrid cloud environment; AWS experience is a plus
Familiarity with data visualization tools such as Tableau or QuickSight is a plus
Experience with Python and PowerShell
Ability to write and interpret complex queries towards data-driven business solutions
Familiarity with healthcare data is a plus
Why Decision Point?:
We’re as passionate about our people as we are about making our mark on healthcare. Fostering a fun and challenging environment that’s centered around personal and professional growth has brought us to where we are today. We are constantly seeking out new ways to reinvest in our team members because let’s face it, we all do our best work when we feel valued.
Meaningful work
Remote friendly environment
We encourage outside the box ideas
Great healthcare coverage
Competitive compensation","$98,161 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,2013,Less than $1 million (USD)
#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Tetra Tech
3.9",3.9,"Arlington, VA",Data Engineer/Data Analyst,"Position Summary:
Segue Technologies, A Tetra Tech Company seeks a Data Engineer/Data Analyst to support our Federal Government clients. This position will leverage data to improve Business Intelligence (BI) in order to provide more value to end users.
This is a full-time remote position. Occasional travel expected to the Indian Affairs Office in Albuquerque, NM or nearest Indian Affairs office (less than 5%)

Job Duties and Responsibilities include but are not limited to:
Assess data quality, identifying suspect data via value distributions, cross field correlations, etc.
Formulate Extraction, Transformation, and Data Loading strategies to deal with anomalous data in a manner where the impact on the business domain is understood
Model, map and design data structures and data architecture to implement data mart or warehouse
Identify patterns and relationships within data that have explanatory power within a business domain, using appropriate methodologies to provide different types of information, such as Descriptive, Inferential, and Predictive
Design and develop data products that fulfill the business needs, such as reports, visualizations, dashboards, analyses, etc.
Perform and interpret data studies and product experiments concerning new data sources or new uses for existing data sources
Develop prototypes, proof of concepts, predictive models, and custom analysis
Design and build new data set processes for modeling and data mining
Search data for relevant patterns and significant relationships
Recognize non-uniformity of data and resolve inconsistencies
Devise tools to help business users gain new insights and improve overall business performance
Additional tasks as required

Required Skills:
Bachelor’s degree in Data Sciences, Statistics, Computer Science, IT Systems, or another technical field, or related major. Relevant technical experience may be substituted for the bachelor’s degree
Must have expert level Tableau server and cloud skillset and experience
Must have expert level proficiency in using Microsoft Excel with strong emphasis on data analysis
Experience developing data pipelines and ETL for dashboards, reports and for data warehousing
Ability to organize, interpret and present data
Experience identifying patterns and relationships within data
Relational database design, modeling (theory) experience
Data warehousing design, modeling (theory) experience
Data quality assessment and impact management experience
Experience communicating and working with stakeholders across the enterprise
Experience architecting and developing data pipeline and related processes
Experience working with data modeling/design tools such as SQL Developer Designer, etc.
Experience with SQL, SQL Server RDBMS, NoSQL databases, TOAD, and Data modeling tools
Excellent problem solving, debugging, and troubleshooting skills
Ability to prepare and interpret complex reports and dashboards
Ability to use independent judgment and to know when to escalate issues
Strong communication skills in multiple formats, such as writing, explaining, listening, asking, teaching, mentoring, etc.
Must hold or be able to pass a Federal Background investigation to obtain a T1 (also known as Public Trust or NACI)

Desired Skills:
Experience using AWS Cloud resources and data services
Experience working with IBM Maximo and Facilities related data
Segue Technologies is a wholly owned subsidiary of Tetra Tech, Inc. Segue is based out of Arlington, VA, with a presence in 14 states and DC. We support Federal and DoD organizations to develop and enhance mission-critical business systems. We provide custom software applications, solve data management problems, and support the evolution of the mobile workforce.

At Segue Technologies health and safety play a vital role in our success. Segue’s employees work together to comply with all applicable health & safety practices and protocols, including health orders and regulations related to COVID-19 that are mandated by local, state and federal authorities.

Our compensation package includes: Competitive Annual Salaries, Rewards and Recognition Program, Employee Stock Purchase Plan, Paid Time Off that Increases with Seniority, Paid Holidays, Life and Disability Insurance, 401K Retirement Plan with Employer Contribution, Dental, Vision, and Health Insurance, Flexible Spending Account, Tuition and Training Reimbursement.
Segue Technologies, A Tetra Tech Company is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by law. Please visit our website, www.seguetech.com/careers, to submit an application.","$85,833 /yr (est.)",10000+ Employees,Company - Public,"Construction, Repair & Maintenance Services",Architectural & Engineering Services,1966,$1 to $5 billion (USD)
"GEICO
3.0",3.0,"Chevy Chase, MD",Principal Engineer - Data,"Benefits:
At GEICO, we make sure you have the support and resources to leverage and develop your skills, secure your financial future, and take care of your health and well-being. GEICO continually seeks to provide a workplace where everyone can be their authentic self. To help achieve this goal, we support associate-led Employee Resource Groups that foster a true sense of community. Through GEICO’s competitive benefits offerings and various training and development opportunities, we have you covered with our
Total Rewards Program
that includes:
Premier Medical, Dental and Vision Insurance with no waiting period**
Paid Vacation, Sick and Parental Leave
401(k) Plan
Tuition Reimbursement
Paid Training and Licensures
Benefits may be different by location. Benefit eligibility requirements vary and may include length of service.
**Coverage begins on the date of hire. Must enroll in New Hire Benefits within 30 days of the date of hire for coverage to take effect.
GEICO is proud to be an equal opportunity employer. We are committed to cultivating an environment where equal employment opportunities are available to all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. GEICO celebrates diversity and believes it is critical to our success. As such, we are committed to recruit, develop and retain the most talented individuals to join our team.
#LI-SS3
Annual Salary
$82,000.00 - $204,500.00
The above annual salary range is a general guideline. Multiple factors are taken into consideration to arrive at the final hourly rate/ annual salary to be offered to the selected candidate. Factors include, but are not limited to, the scope and responsibilities of the role, the selected candidate’s work experience, education and training, the work location as well as market and business considerations.","$143,250 /yr (est.)",10000+ Employees,Subsidiary or Business Segment,Insurance,Insurance Carriers,1936,$10+ billion (USD)
"Pacific Northwest National Laboratory
4.0",4.0,United States,Data Engineer 3,"Overview:
The Pacific Northwest National Laboratory (PNNL) and the Environmental Molecular Sciences Laboratory (EMSL) seek a junior-level Data Engineer to further EMSL’s data management and integrated research goals. The candidate should have experience in software development and managing scientific data using FAIR principles.
Rockstar Rewards:
Employees and their families are offered medical insurance, dental insurance, vision insurance, health savings account, flexible spending accounts, basic life insurance, disability insurance*, employee assistance program, business travel insurance, tuition assistance, supplemental parental bonding leave**, surrogacy and adoption assistance, and fertility support. Employees are automatically enrolled in our company funded pension plan* and may enroll in our 401k savings plan. Employees may accrue up to 120 vacation hours per year and may receive ten paid holidays per year.
Research Associates excluded.
**Once eligibility requirements are met.

Click Here For Rockstar Rewards
Responsibilities:
Developing and optimizing capabilities at the division level and identify mission challenges and formulate engineering solutions methodically.
Generates new ideas for proposals and business development opportunities of small to medium proposals. Stay current about data management and database technology developments
Collaborate with researchers on the data development and applying analysis techniques to a rich variety of scientific instrument data form experiment
Contribute to innovative application of advanced simulation and modeling to research problems relevant to DOE-BER missions where computational approaches are significant.
Design, build, deploy & maintain data pipelines, ETL workflows, data, and computing environments; or implement and configure third-party solutions, including participation in software engineering work in Python or other programming languages.
Work on data migrations and ingestion projects with users to design large-scale data processing systems, develop data pipelines optimized for scaling, and troubleshoot potential data platform challenges.
Ensure quality, availability, and integrity of data, solutions, and respective systems and integrate proactive strategies and best practices regarding data integrity, security, and scalability.
Partner cross-functionally to build and improve new/existing constructs and solve data engineering problems. Supports scoping, scheduling, and budgeting at a project or major task level.
Qualifications:
Minimum Qualifications:
BS/BA and 5+ years of relevant work experience -OR-
MS/MA and 3+ years of relevant work experience -OR-
PhD with 1+ year of relevant experience
Preferred Qualifications:
ETL, SQL, NoSQL, and Data Management frameworks
Backend languages and tools such as Python, PostgreSQL, and database management
Experience with Linux computing environments.
Hazardous Working Conditions/Environment:
Not applicable.
Additional Information:

Not applicable.

“Referral Eligible”
Testing Designated Position (TDP):
This is not a Testing Designated Position (TDP).
About PNNL:
Pacific Northwest National Laboratory (PNNL) is a world-class research institution powered by a highly educated, diverse workforce committed to the values of Integrity, Creativity, Collaboration, Impact, and Courage. Every year, scores of dynamic, driven people come to PNNL to work with renowned researchers on meaningful science, innovations and outcomes for the U.S. Department of Energy and other sponsors; here is your chance to be one of them!

At PNNL, you will find an exciting research environment and excellent benefits including health insurance, flexible work schedules and telework options. PNNL is located in eastern Washington State—the dry side of Washington known for its stellar outdoor recreation and affordable cost of living. The Lab’s campus is only a 45-minute flight (or ~3-hour drive) from Seattle or Portland, and is serviced by the convenient PSC airport, connected to 8 major hubs.
Commitment to Excellence, Diversity, Equity, Inclusion, and Equal Employment Opportunity:
Our laboratory is committed to a diverse and inclusive work environment dedicated to solving critical challenges in fundamental sciences, national security, and energy resiliency. We are proud to be an Equal Employment Opportunity and Affirmative Action employer. In support of this commitment, we encourage people of all racial/ethnic identities, women, veterans, and individuals with disabilities to apply for employment.

Pacific Northwest National Laboratory considers all applicants for employment without regard to race, religion, color, sex (including pregnancy, sexual orientation, and gender identity), national origin, age, disability, genetic information (including family medical history), protected veteran status, and any other status or characteristic protected by federal, state, and/or local laws.

We are committed to providing reasonable accommodations for individuals with disabilities and disabled veterans in our job application procedures and in employment. If you need assistance or an accommodation due to a disability, contact us at careers@pnnl.gov.
Drug Free Workplace:
PNNL is committed to a drug-free workplace supported by Workplace Substance Abuse Program (WSAP) and complies with federal laws prohibiting the possession and use of illegal drugs.

If you are offered employment at PNNL, you must pass a drug test prior to commencing employment. PNNL complies with federal law regarding illegal drug use. Under federal law, marijuana remains an illegal drug. If you test positive for any illegal controlled substance, including marijuana, your offer of employment will be withdrawn.",#N/A,5001 to 10000 Employees,Company - Private,"Energy, Mining & Utilities",Energy & Utilities,1965,$1 to $5 billion (USD)
"BlueLabs Analytics, Inc.
4.2",4.2,"Washington, DC",Data Engineer II,"About BlueLabs
BlueLabs is a leading provider of analytics services and technology dedicated to helping our partners do the most good with their data. Our team of analysts, scientists, engineers, and strategists hail from diverse backgrounds yet share a passion for using data to solve the world's greatest social and analytical challenges. Since our inception we've worked with more than 400 organizations ranging from government agencies, advocacy groups, unions, political campaigns, and international groups. In addition, we service an ever-expanding portfolio of commercial clients in the automotive, travel, CPG, entertainment, healthcare, media, and telecom industries. Along the way, we've developed some of the most innovative tools available in analytics, media optimization, reporting, and influencer outreach.
About the team
The BlueLabs Civic Tech practice revolutionizes the way government agencies use data to reduce the friction between residents and the essential services they use. Our team develops deep expertise within the areas our clients care about most, then builds data programs that create impact at scale. We work closely with internal government innovation groups, and build on the analytics methodology pioneered in e-commerce, advocacy, politics, and consumer finance.
About the role:
As a Data Engineer, you will play a critical role in our database upgrade roadmap, in support of our work for a large federal healthcare program. The Data Engineer will work with complex and nuanced data pipelines and will be responsible for the continuous development, review, and documentation of data wrangling and pipeline solutions. You should have experience working in a rapid development team in the past in which you were responsible for significant contributions to client data pipeline solutions.
In this position you will:
Develop, test, and operationalize the data pipelines that power our analytics.
Provide visibility into data transformations by designing and implementing data tests throughout existing pipelines.
Extract business logic (ETL/ELT, metrics, metadata) from current data systems into portable cloud-agnostic layers.
Analyze, build, and deploy data models, including relational models for data warehousing.
Plan and maintain data architectures that are aligned with business requirements.
Produce scalable, replicable code and engineering solutions that help automate repetitive data management tasks.
Work closely with data analysts to understand, identify and effectively respond to their specific needs.
Partner with other engineers to deploy and troubleshoot pipeline-related tools, automations, and integrations.
Work as part of a team to maintain a well documented, consistent codebase.
What we are seeking:
3+ years of experience as a contributor to technical projects, such as working with complex data pipelines or software applications.
Experience working with production grade data warehouses using SQL queries and scripting languages (e.g., Python, R).
Experience with SQL transformation tools like dbt.
Strong background in database design and data modeling.
Experience with Snowflake, implementing a modern data stack, and contributing to large scale data migration efforts.
Effective communication skills when working with team members of varied backgrounds, roles, and functions.
Ability to manage your individual priorities and comfortably context-switch between active development, client discussion, and issue response.
Experience delivering on client priorities that operate on a regular deployment schedule.
Passion in applying your skills to our social mission to problem-solve and collaborate within a cross-functional, client-facing team environment.
The ability to successfully attain and maintain a Federal Public Trust background investigation that our government clients require; this includes a requirement that the individual has U.S. Citizenship or U.S. residency for three of the past five years.
What We Offer:
BlueLabs offers a friendly work environment and competitive compensation and benefits package including:
Salary range: $85,000 - $95,000 annually
Premier health insurance plan
401K matching
Unlimited vacation leave
Paid sick, personal, and volunteer leave
13 paid holidays
15 weeks paid parental leave
Professional development stipend & tuition reimbursement
Employee Assistance Program (EAP)
Supportive & collaborative culture
Flexible working hours
Remote friendly (within the U.S.)
And more!
The salary range for candidates who meet the minimum posted qualifications reflects the Company's good faith understanding and belief as to the wage range, and is accurate as of the date of this job posting.

To protect the health and safety of our workforce, as a company policy, BlueLabs strongly encourages all employees to be fully vaccinated against COVID-19 prior to beginning employment. BlueLabs adheres to all federal, state and local COVID-19 vaccination regulations. Except where prohibited by law, applicants who receive a conditional offer of employment will be required to produce proof of vaccination status prior to their first day of employment; if not the offer may be rescinded or employment terminated. BlueLabs will evaluate requests for reasonable accommodations for applicants unable to be vaccinated due to a religious belief, disability, pregnancy, or on an individualized basis in accordance with applicable laws.
At BlueLabs, we celebrate, support and thrive on differences. Not only do they benefit our services, products, and community, but most importantly, they are to the benefit of our team. Qualified people of all races, ethnicities, ages, sex, genders, sexual orientations, national origins, gender identities, marital status, religions, veterans statuses, disabilities and any other protected classes are strongly encouraged to apply. As an equal opportunity workplace and an affirmative action employer, BlueLabs is committed to creating an inclusive environment for all employees. BlueLabs endeavors to make reasonable accommodations to the known physical or mental limitations of qualified applicants with a disability unless the accommodation would impose an undue hardship on the operation of our business. If an applicant believes they require such assistance to complete the application or to participate in an interview, or has any questions or concerns, they should contact the Director, People Operations. BlueLabs participates in E-verify. EEO is the Law.
Collection of Personal Information Notice:
As you are likely aware, by submitting your job application, you are submitting personal information to our company. We collect various categories of personal information, including identifiers, protected classifications, professional or employment related information and sensitive personal information. We may retain and use this information for up to three years, in order to come to a decision on whether or not you are a good fit for our company. We may also retain or use some of this information to comply with any requirements under law, or for purposes of defending ourselves in any litigation. We do not use this information for any other purpose, or share it with third parties, unless you become an employee. To learn more, or to see our fully Notice to Job Applicants, please click here.","$90,000 /yr (est.)",51 to 200 Employees,Company - Private,Management & Consulting,Business Consulting,2013,Unknown / Non-Applicable
"Colorado Rockies Baseball Club
3.8",3.8,"Denver, CO",Senior Data Engineer,"Senior Data Engineer

The Role
The Colorado Rockies Baseball Club is building a platform to house all our baseball data, from scouting reports to baseball statistics and rosters, into an all-encompassing application that will help us more effectively and efficiently make baseball decisions. This role will be responsible for building and maintaining the ingestion, transformation, and cloud storage of baseball data that will drive analysis and decision-making in all facets of Baseball Operations. The Senior Engineer will be a technical leader on the team and help grow less-experienced engineers.

Essential Duties and Responsibilities
Implement ETL pipelines from various external vendors for the use of internal statistical analysis.
Facilitate the aggregation of data for consumption by technical and non-technical users.
Maintain existing pipelines and debug problems that arise.
Work closely with a web development team to deliver the data they need for an internal information application.
Assist less-experienced engineers with their projects to increase their skill sets.

Job Requirements
Bachelor’s degree or completion of an immersive technical program in Computer Science, Information Systems, Computer Engineering, Web Development, or a related field preferred.
At least five years of experience working in data engineering.
Experience and strong understanding of AWS cloud services including S3, Cloudwatch, Glue, and CDK.
Expert-level understanding of relational databases, including SQL Server, MySQL, and Postgres.
In depth experience using a scripting language for data management or analysis, such as Python or R.
Strong understanding of handling and parsing various data formats, including XML, JSON, and CSV.
Relocation and on-site work are required for this position.

Preferred Skills
Strongly prefer experience with commonly used baseball data sources, such as StatCast, EBIS, Trackman and CollegeSplits.
Understanding of modern baseball analysis.
Experience working with Apache Airflow and additional ingestion management tools not mentioned above.

SALARY RANGE:
$115,000 - $135,000 a year. This is a regular status, full-time position eligible for all company benefits including but not limited to Health Insurance (medical, dental, vision), Retirement, and accrued time off (Vacation/ Sick/ Holiday).

EQUAL OPPORTUNITY EMPLOYER:
Rockies baseball is for everyone! We pride ourselves on hiring, developing, and promoting talent as an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, national origin, gender, age, religion, disability, veteran status, or any other category protected by law. In addition, we will endeavor to provide reasonable accommodation to otherwise qualified job applicants and employees with known physical or mental disabilities in compliance with the ADA. All employment and promotion decisions will be decided on the basis of qualifications, merit, and business needs","$125,000 /yr (est.)",501 to 1000 Employees,Company - Private,"Arts, Entertainment & Recreation",Sports & Recreation,1991,Less than $1 million (USD)
"Edelman Financial Engines
3.5",3.5,United States,Data Engineer,"Data Engineer (Remote)
At Edelman Financial Engines (EFE), we believe everyone deserves to move their financial life forward!
We know that assets have the power to fund goals. Those numbers represent individual lifetimes filled with hard work and dreams for our clients and generations to follow. Our clients trust us to guide them forward with empathy, integrity and invention. We uphold that same standard of respect and commitment for clients and colleagues alike.
Founded on the idea that financial education is a fundamental right for everyone, Edelman Financial Engines continues to grow and challenge the status quo. We’re moving forward, together. If our purpose-driven commitment inspires you, we invite you to consider joining our team.
This individual is a hardworking, and outcome focused data engineer with experience working with structured and unstructured data including data modeling and developing data pipelines primarily using Python and SQL on AWS cloud platform.
In addition to an earnest desire to help people, we are looking for the ideal candidate to complement the team’s existing talents. For this Data Engineer role, we are seeking a teammate with proven experience in data engineering, data governance and cloud technologies. If you are passionate about data, analytics and helping business leverage data as an asset, this may be an ideal match for you!
Responsibilities:
Continue to scale our cloud data analytics platform to meet the company’s vision and growth
Follow engineering and industry standards
Contribute to the development of data pipelines to publish high-quality data products for data analysts and data scientists
Flawless communication within internal teams and across the organization
Requirements:
2+ years of total experience with a bachelor's degree in computer science and equivalent experience
1-2+ years’ experience working as a developer in Data Engineering with data pipelines and data processing.
Practical hands-on experience with SQL and GitLab or other file control systems
Experience working with data formats such as Apache AVRO, Apache Parquet, and common methods in data transformation
Experience with AWS technology stack and AWS Certification is a strong plus
Dedicated, and ability to work with a geographically distributed diverse team
Excellent analytical, communication and organizational skills
Strong teamwork, coordination, planning and influencing skills
Experience with Agile framework is a plus
Must have desire to continue learning new technologies, ask questions, and improve skills
About Edelman Financial Engines
Since 1986, Edelman Financial Engines has been committed to always acting in the best interests of our clients. We were founded on the belief that all investors – not just the wealthy – deserve access to personal, comprehensive financial planning and investment advice. Today, we are America’s top independent financial planning and investment advisory firm, recognized by Barron’s,1 with 145+ offices2 across the country and entrusted by more than 1.3 million clients to manage more than $242 billion in assets.3 Our unique approach to serving clients combines our advanced methodology and proprietary technology with the attention of a dedicated personal financial planner. Every client’s situation and goals are unique, and the powerful fusion of high-tech and high-touch allows Edelman Financial Engines to deliver the personal plan and financial confidence that everyone deserves.
For more information, please visit EdelmanFinancialEngines.com.
© 2023 Edelman Financial Engines, LLC. Edelman Financial Engines® is a registered trademark of Edelman Financial Engines, LLC. All advisory services provided by Financial Engines Advisors L.L.C., a federally registered investment advisor. Results are not guaranteed. See EdelmanFinancialEngines.com/patent-information for patent information. AM2789819
For California residents, please see the link for the Privacy Notice for Candidates. California law requires that we provide you this notice about the collection and use of your personal information. Please read it carefully.
Edelman Financial Engines encourages success based on our individual merits and abilities without regard to race, color, religion, creed, sex, gender identity or expression, sexual orientation, pregnancy; marital, domestic partner or civil union status; national origin, citizenship, ancestry, ethnic heritage, genetic information, age, legally recognized disability, military service or veteran status.
Accommodations are modifications or adjustments to the hiring process that would enable you to fully participate in that process. If you need assistance to accommodate a disability, you may request one at any time by either contacting your recruiter or HRQ@EdelmanFinancialEngines.com.
1 The Barron’s 2022 Top 100 RIA Firms list, a seven-year ranking of independent advisory firms, is qualitative and quantitative, including assets managed by the firms, technology spending, staff diversity, succession planning and other metrics. Firms elect to participate but do not pay to be included in the ranking. Ranking awarded each September based on data within a 12-month period. Compensation is paid for use and distribution of the rating. Investor experience and returns are not considered. The 2018 ranking refers to Edelman Financial Services, LLC, which combined its advisory business in its entirety with Financial Engines Advisors L.L.C. (FEA) in November 2018. For the same survey, FEA received a precombination ranking of 12th.
2 Edelman Financial Engines data, as of Dec. 31, 2022.
3 Edelman Financial Engines data, as of Dec. 31, 2022.",#N/A,1001 to 5000 Employees,Company - Private,Financial Services,Investment & Asset Management,1987,Unknown / Non-Applicable
"McDonald's Corporation
3.5",3.5,"Chicago, IL","Data Engineer, Global Data, CRM","Company Description

McDonald’s evolving Accelerating the Arches growth strategy puts our customers and people first, and leverages our competitive advantages to strengthen our brand. We are recognized on lists like Fortune’s Most Admired Companies and Fast Company’s Most Innovative Companies.
Doubling Down on the 4Ds (Delivery, Digital, Drive Thru, and Development)
Our growth pillars emphasize the important role technology plays as the leading, global omni-channel restaurant brand. Technology enables the organization through digital technology, and improving the customer, crew and employee experience each and every day.
Global Technology forging the way
Leading the digitization of our business is the Technology organization made up of intrapreneurs who build industry defining tech using the latest innovations and platforms, like AI and edge computing to deliver on the next set of cutting-edge opportunities for the business. At McDonald’s you get to solve technology innovation challenges at an incredible scale, and work across global teams who are always hungry for a challenge. This provides access to exciting career paths for technologists. It’s bonus points when you get to see your family and friends use the tech you build at their favorite McD restaurant.

Job Description

We are seeking a highly motivated individual for the Data Engineering Manager for the Marketer Customer Data Platform team on Global Data and Architecture. This role will manage, develop, and lead the execution and implementation cloud-based data delivery systems for the MCD Customer Data Platform. This role will specialize in leading and managing the engineering teams in both onshore and offshore locations, as well as working with the relevant product management teams on overall product direction and implementation of the product vision.
What you will do:
Lead and manage the day-to-day operations for the CRM Data Engineering team with both, onshore and offshore, resources to with a focus on CDP/ CRM customer data technologies
Independently drive efforts in marketing technology and automation, collaborating with key stakeholders and business units. Must be able to find marketing solutions alongside building a long-term strategic vision of a marketing ecosystem across a highly technical, complex data stack
Participate in activities of the data engineering team on the solution, design, architect, implementation, support, and monitoring of CRM & CDP related data products
Develop and implement monitoring and remediation structure and applications for critical data flows
Continuously develop and deploy solutions to streamline and enhance the processes, logic, and tactics around customer cohort identification, and targeting via corporate CRM, marketing, and advertising systems. Additionally, develop the overall best practice framework for use by
Gather business and functional requirements and translate into robust, scalable, and user-friendly solutions

Qualifications
Bachelor’s degree in Information Technology or related technical fields such as computer science, marketing tech, mathematics, engineering, etc.
Minimum 2-5 years of relevant work experience in data analytics, engineering, computer science, mathematics or similar logic based fields
Minimum 2 years hands on experience with complex marketing automation tech stacks, leveraging CDP tools such as mParticle, Segment, and Tealium
Strong analytics skills, root cause analysis experiences on internal and external data and processes to answer business questions and improve application performance
Comfortable partnering and working with a wide range of stakeholders at various levels of management and leadership
Ability to focus on details as well as big picture
Competence and comfort focusing on specific detail engineering questions, as well as “big picture” questions
Ability to leverage critical thinking, problem-solving, and analytical skills to improve user experience, adoption, and outcomes
Curious, task-oriented self-starter with a strong bias for action; Proven track record of “getting things done”
Self-motivated with ability to set priorities and mentor others in a performance driven environment.
Passion for technology and innovation & uncompromising attention to detail & passion for problem solving
Desired Skills

Extensive experience working with Agile methodologies and expertise in production deployment processes and continuous deployment best practices.
1-2 years of hands-on experience with Braze and/or similar market automation delivery platforms
1-2 years of hands on experience with marketing automation and technology experience, understanding key components of marketing technology stacks, including CDP tools such as mParticle, Tealium, or Segment
Experience with clickstream tracking architecture implementations, and knowledge of Adobe Analytics and/or Google Analytics
Demonstrated ability to problem solve in complex technology solutioning
Experience developing Confluence pages and working with Jira
Demonstrable knowledge of multiple programming languages leveraged in martech/adtech stacks – javascript, ruby (liquid logic), html/css, python etc.

Additional Information

McDonald’s is committed to providing qualified individuals with disabilities reasonable accommodations to perform the essential functions of their jobs. Additionally, if you (or another applicant of whom you are aware) require assistance accessing or reading this job posting or otherwise seek assistance in the application process, please contact recruiting.supportteam@us.mcd.com
McDonald’s provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to sex, sex stereotyping, pregnancy (including pregnancy, childbirth, and medical conditions related to pregnancy, childbirth, or breastfeeding), race, color, religion, ancestry or national origin, age, disability status, medical condition, marital status, sexual orientation, gender, gender identity, gender expression, transgender status, protected military or veteran status, citizenship status, genetic information, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.
Nothing in this job posting or description should be construed as an offer or guarantee of employment.","$104,720 /yr (est.)",10000+ Employees,Company - Public,Restaurants & Food Service,Restaurants & Cafes,1955,$10+ billion (USD)
Blancas Sandoval & Associates. PA,#N/A,"Miami, FL",Data Engineer,"Company Description

Cyncrocity is a startup that tracks global innovation ecosystems and aggregates all upcoming and past conferences, including post-event recordings. Our platform enables users to track and discover events, companies, and thought leaders.

Job Description

We are seeking a Data Engineer to join our team and help us design and implement systems to collect, store, and process large volumes of data scattered across different sources and formats. The ideal candidate should have a strong background in scraping, automation, data entry, and processes. We are looking for a candidate with a wide set of tools and skills who can also lead data entry and curation.

Qualifications

Background in Computer Science, Data Science, or related field

Proficiency in English

High attention to detail & creative problem solver

Additional Qualifications (preferred but not required):
Interest in mapping global innovation ecosystems.

Additional Information

Responsibilities:
Manual data entry to ensure completeness and accuracy of database.

Design and implement systems for large scale data collection from various sources.

Automate data collection and data processing, including scraping and cleaning.

Standardize, combine, and enhance different databases.

Toolbox Requirements:
Strong skills in web scraping tools and techniques

Familiarity with relational databases and Airtable

Strong skills in automation and familiarity with Make (Integromat) or AutoGPT

Google Sheets & Google Alerts,RSS & alternative website watchers

APIs & Integration with social media (Youtube, Twitter, Linkedin)

This is a full-time remote internship contract. To apply, please submit your resume or portfolio. We will contact you if we decide to move forward with your application. Thank you for your interest in Cyncrocity!","$94,721 /yr (est.)",Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Tek Ninjas
4.0",4.0,"Carrollton, TX",CLOUD BIG DATA ENGINEER,"CLOUD BIG DATA ENGINEER

Looking for Cloud Big Data Engineer in Carrollton, TX area,
Candidate should work on Amazon Web Services (AWS) using EC2 for computing and S3 as storage.
ETL process jobs using Ab-Initio in dynamic, high-volume environment.
Design and deploy well-tuned Ab-Initio graphs for ODS and DSS instances using both Linux and UNIX environments.
Test all applications and transport data to target Warehouse tables, schedule and run extraction and load process by using Informatica Workflow Manager.
Should be familiar with Hadoop, Hive, Pig, Sqoop, HBase, Cassandra, Spark, Spark Streaming, Spark SQL, Oozie, Zookeeper, Kafka, Flume, MapReduce framework, Yarn, Scala and related technicalities.
Req: MS/BS in Computer Science/any engineering/closely related field. Ex: MS+1yr/BS+5yrs in MicroStrategy developer/any software-related experience.

Should be WILLING TO TRAVEL/RELOCATE TO CLIENT SITES THROUGHOUT THE US.

Email: hr@tekninjas.com, Tek Ninjas Solutions, LLC, 4425 Plano Pkwy, Ste#1402, Carrollton, TX 75010.","$111,153 /yr (est.)",51 to 200 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Optimal Inc.
3.6",3.6,"Dearborn, MI",Data Engineer-GCP/Alteryx,"Position Description:
Responsible for focusing on data security, data access and related activities to ensure data is efficiently utilized as an enterprise asset.
Define, implement, and maintain security policies for users, ensuring appropriate access based on data classifications and roles.
Provide feedback to enhance the enterprise data governance model as needed.
Partner with data stewards to ensure all information utilized adheres to compliance, access management and control polices.
Support the data security requirements of different functional areas like Ford Credit, MS&S, PD, etc., and all the regional KPI / Metrics initiatives.
Support Access Monitoring, Reviews and Audits
Comply with all Ford, OGC, ISP and ACR practices and procedures.
Skills Required:
Ability to troubleshoot and resolve access-related issues.
Big Data / Hadoop administration - include Ranger Policy development and maintenance.
Big Data / Hadoop development - Hive, MapReduce, Spark, etc.
Strong customer service skills with an ability to communicate complex solution concepts in simple terms while working collaboratively.
Ability to apply data management standards and data governance practices.
Skills Preferred:
GCP - Alteryx - Python
Experience Required:
2+ of experience in cloud platforms like AWS, Azure, and Google Cloud Platform (GCP)
2+ years of experience with automation using Python or similar language.
1+ years of performing Hadoop security administration - include Ranger Policy development and maintenance.
2+ years of experience in Customer Support role, such as call center, Application support or other administration.
Education Required:
Bachelor's degree in computer science, Computer Engineering, Data Analytics, or a closely related field of study","$81,597 /yr (est.)",1 to 50 Employees,Nonprofit Organization,Education,Education & Training Services,2004,Unknown / Non-Applicable
"Blackhawk Mining LLC
2.9",2.9,"South Charleston, WV",IT Network (Data & Voice) Engineer,"IT Network (Data & Voice) Engineer
Classification/Reports to
Exempt/Director of Technology
Summary/Objective
Blackhawk Mining LLC, a leader in the coal industry; with operations in Kentucky and West Virginia, is seeking an IT Network (Data & Voice) Engineer to join the technology team at the operations headquarters in Charleston, WV.
Essential Functions
Assess business and applications requirements to securely manage corporate data and voice networks by designing, planning, and upgrading network installation projects and eliminate single points of failure.
Establish and maintain backup, version-control, and security defense systems.
Proactively monitor (PRTG) and troubleshoot network architecture, making recommendations for system findings and enhancements, reducing operational costs.
Manage third-party service vendors to ensure the network stays operational.
Maintain and update the network documentation (PRTG)
Helpdesk ticketing support (at all levels – from ticket entry, to escalation, to close)
Other tasks as assigned by the Director of Technology.
Competencies
Customer driven attitude, that places emphasis on user support and problem resolution.
Ability to self-manage and achieve team goals.
Ability to work in a dynamic small group, with rapidly changing situations and needs.
Required Education and Experience
At least 5-7 years of experience with network operating systems such as Cisco, Windows Server, and Cisco preferred.
Networking certification from Cisco and Microsoft is also highly valued.
Hands-on experience working with routers, switches, firewalls, cabling, and wireless & other essential network hardware is preferred.
Working knowledge of Windows server operations and administration in a distributed network environment desired.
Physical Demands
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job.
This is largely a sedentary role; however, some use of equipment may require occasional lifting, carrying, pushing, and/or pulling, as necessary.
Travel
Ability to travel to locations in KY, WV, and IN, as required, if troubleshooting cannot be completed remotely. May require overnight stays. Valid US driver's license.",#N/A,201 to 500 Employees,Company - Private,"Energy, Mining & Utilities",Energy & Utilities,#N/A,$100 to $500 million (USD)
"Gradient AI
4.8",4.8,"Boston, MA",Senior Data Engineer,"Gradient AI:
Gradient AI delivers state-of-the art artificial intelligence and Machine Learning solutions to the trillion-dollar insurance industry. AI has emerged as a disruptive force revolutionizing the way insurance professionals achieve their objectives, and Gradient AI is leading the charge. Our team is made up of Data Science and Insurance Technology experts with a history of building wildly successful technology companies. If you are passionate about making a difference for customers, and collaborating with passionate colleagues, then Gradient AI might be the right place for you.
Role:
If you want to work on cutting-edge technology with friendly, intelligent people in a highly collaborative environment, then Gradient AI is the right place for you. We are looking for a Data Engineer to join our team to design and implement data pipelines, optimize existing models, and create standardization as we move into new lines of business. The ideal candidate is a life-long learner who knows object-oriented coding and has experience with business intelligence reporting tools.
Responsibilities:
Analyze complex data schemas and relationships to implement information models to capture the data and data relationships.
Create ETL processing logic to manage large volume client data sets associated with various types of insurance lines including Worker's Compensation, Health, and Auto.
Integrate 3rd party commercial and public domain data sets to enhance and extend Gradient AI's ML data assets.
Design and implement data pipelines for processing data as part of the overall ML Ops strategy within Gradient AI's Cloud deployment.
Collaborate with data scientists to ""featurize"" the information models and construct Machine Learning models.
Assist software engineering team to design and create integration APIs and applications that sit on top of Gradient AI's ML data assets.
Qualifications:
5+ years of relevant working experience
Fluency in SQL and experience with relational and non-relational/alternative databases; Postgres is a desirable
Demonstrated ability to write effective, scalable Python code (or other similar object-oriented languages)
Experience with ETL / BPM. Exposure to tools like Databricks Amazon Glue, Apache Spark, Apache Airflow is desirable
Exposure working in a cloud-computing environment (e.g., AWS, GCP, Azure)
Comfortable with Linux, including developing shell scripts
Experience working with insurance data is desirable
What We Offer:
We are an equal opportunity employer that offers a number of benefits and perks to accommodate all types. Bring your authentic self to work in our supportive workplace where we offer:
A fun and fast-paced startup culture
A culture of employee engagement, diversity and inclusion
Full benefits package including medical, dental, vision, 401k, disability, life insurance, and more
Unlimited vacation days and ample holidays- we all work hard and take time for ourselves when we need it
Competitive salary and generous stock options - we all get to own a piece of what we're building
Ample opportunities to learn and take on new responsibilities","$128,983 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2018,Unknown / Non-Applicable
"Cisco Systems
4.4",4.4,"Portland, OR",Data Engineer,"Who we are
Cisco’s Workforce Experience organization is one of Cisco’s fastest growing teams, and the Chief Data and Analytics Office (CDAO) group is transforming how Cisco delivers value to our customers & partners via our product portfolio. The CDAO organization is the team that is in the epicenter of developing modern, cloud native technologies in support of the evolving CX Product portfolio. To lay hold of this exciting vision, we are looking for top talent to deliver high performing, distributed engineering teams that will redefine success for Cisco and our customers.



What you'll do
Software is a team sport. The best software is built by the highest functioning teams peopled with supremely skilled & fiercely collaborative engineers. You will throw your lot in with fellow engineers to build high quality, high scale & customer delighting SaaS products. We are looking for engineers with a growth mindset, eager to do their best work and build the best products possible. Simply put, you will deliver excellent software on a results-driven, high-trust team.



Primary Responsibilities include:
Build data pipelines in a scalable cloud environment
Partner closely with product management to identify, scope, estimate work for team
Lead teams in the delivery of data pipelines
Mentor junior engineers to produce their best work
Take ownership for large portions of the platform, across different engineering teams, and help us design, deliver and maintain that code going forward

Who you'll work with
Residing in the WFX organization, you will partner with teams within the WFX and CX organization to design, develop and deliver best in class SaaS solutions. You will work with peer teams in Product Management, Operations and UX Design to build the world’s best customer experience for our customers, partners and delivery engineers. Every day and every interaction will be unique, but you will likely engage with a diverse representation of engineers, product owners, product managers, business units, IT and WFX/CX leadership.



Who you are
Required Skills/Experience
8+ years designing, developing and deploying data pipelines
5+ years of shipping cloud native software in AWS
Experience taking difficult problems and translating them into solutions
Experience in creating and maintaining large scale distributed cloud native technology stacks that achieve over 99.98% uptime
Strong ability to collaborate, finding win-win solutions within Engineering and partnering with Product
Strong communication skills and ability to engage, interface and influence partners
Experience with Spark, Presto, AirFlow,Kinesis , Kafka streams
Experience with AWS Services, Linux, Docker, CloudFormation, Terraform, DynamoDB, Aurora
Experience with Programming languages such as GoLang, Python
Familiarity with AI/ML solutions

#WeAreCisco, where each person is unique, but we bring our talents to work as a team and make a difference powering an inclusive future for all.
We embrace digital, and help our customers implement change in their digital businesses. Some may think we’re “old” (36 years strong) and only about hardware, but we’re also a software company. And a security company. We even invented an intuitive network that adapts, predicts, learns and protects. No other company can do what we do – you can’t put us in a box!
But “Digital Transformation” is an empty buzz phrase without a culture that allows for innovation, creativity, and yes, even failure (if you learn from it.)
Day to day, we focus on the give and take. We give our best, give our egos a break, and give of ourselves (because giving back is built into our DNA.) We take accountability, bold steps, and take difference to heart. Because without diversity of thought and a dedication to equality for all, there is no moving forward.
So, you have colorful hair? Don’t care. Tattoos? Show off your ink. Like polka dots? That’s cool. Pop culture geek? Many of us are. Passion for technology and world changing? Be you, with us!

Message to applicants applying to work in the U.S.:

When available, the salary range posted for this position reflects the projected hiring range for new hire, full-time salaries in U.S. locations, not including equity or benefits. For non-sales roles the hiring ranges reflect base salary only; employees are also eligible to receive annual bonuses. Hiring ranges for sales positions include base and incentive compensation target. Individual pay is determined by the candidate's hiring location and additional factors, including but not limited to skillset, experience, and relevant education, certifications, or training. Applicants may not be eligible for the full salary range based on their U.S. hiring location. The recruiter can share more details about compensation for the role in your location during the hiring process.
U.S. employees have access to quality medical, dental and vision insurance, a 401(k) plan with a Cisco matching contribution, short and long-term disability coverage, basic life insurance and numerous wellbeing offerings. Employees receive up to twelve paid holidays per calendar year, which includes one floating holiday, plus a day off for their birthday. Employees accrue up to 20 days of Paid Time Off (PTO) each year and have access to paid time away to deal with critical or emergency issues without tapping into their PTO. We offer additional paid time to volunteer and give back to the community. Employees are also able to purchase company stock through our Employee Stock Purchase Program.
Employees on sales plans earn performance-based incentive pay on top of their base salary, which is split between quota and non-quota components. For quota-based incentive pay, Cisco pays at the standard rate of 1% of incentive target for each 1% revenue attainment against the quota up to 100%. Once performance exceeds 100% quota attainment, incentive rates may increase up to five times the standard rate with no cap on incentive compensation. For non-quota-based sales performance elements such as strategic sales objectives, Cisco may pay up to 125% of target. Cisco sales plans do not have a minimum threshold of performance for sales incentive compensation to be paid.","$132,088 /yr (est.)",10000+ Employees,Company - Public,Information Technology,Enterprise Software & Network Solutions,1984,$10+ billion (USD)
"Location3
3.6",3.6,United States,Data Engineer,"This is a remote position, but we are currently only able to hire within the U.S.

About the Data Engineer position
We are looking for a Data Engineer who will drive the design, development, and future state of our enterprise data systems.
Location3 is committed to creating a diverse and inclusive company culture, and our team does not discriminate against candidates and employees because of their disability, sex, race, gender identity, sexual orientation, religion, national origin, age, veteran status, or any other protected status under law. Additionally, Location3 is committed to diverse and equitable hiring practices. If you are a candidate that identifies as diverse and would like to self-identify, you can do so in the application. Providing this information is completely voluntary
Expected Salary Range - $95,000 to $135,000 annually depending on experience.
The Stack:
Front End: Angular, Typescript, RxJs, Nx, Akita, Jasmine, Visual Studio Code
Back End: C# Web API, .Net Core, MongoDb, xUnit, Visual Studio
Azure: Service Bus, App Services, Data Factory, Functions, and WebJobs
Data Warehouse: Azure Synapse Analytics (Synapse Studio, Dedicated SQL Pool, Data Factory, Data Lake Storage)
Architecture: Microservices with RESTFul APIs, event-driven, message-based
Hosted: 100% in Azure using Azure DevOps Build/Release pipelines
Data Engineer Responsibilities:
Responsible for the design, development, and deployment of the enterprise’s overall data strategy to include, but not limited to, data model designs, data collection, storage, transformation, distribution, governance, security, and usage
Review existing data architectures to determine overall effectiveness and compliance with original objectives, develop comprehensive strategies for improving or replacing underperforming areas and present these plans to executives
Designing and integrating new partner data (consumption of new data which shows up in a data lake/storage)
Research new technologies, data modeling methods, and management systems, to determine which ones should be incorporated into the overall company strategy, and develop implementation timelines and cost to deploy and maintain
Maintaining and Monitoring All SQL Databases (Azure Synapse SQL, Azure SQL, and NoSQL)
Convert business needs into data and system requirements, align business processes with IT systems, and manage the complex flow of data and information within the organization
Create & maintain existing ELT Pipelines & Data Flows in Azure Synapse Studio
Create & maintain existing Stored Procedures & Views serving Reporting features
Perform Power BI Shared Dataset Modeling for consumption by our Reporting Team
Provide Power BI technical guidance & best practices to our Reporting Team
Active participant of an Agile/Scrum team
Evangelize data strategies and directives upstream (Executive Leadership) and downstream (Engineering Team)
Report to senior management
Data Engineer Requirements:
MS/BS degree in Computer Science/Engineering or similar
5-10 years experience working as a Data Engineer
Strong knowledge of database structures, data mining, visualization, and machine learning skills
Ability to implement common data management and reporting technologies, data visualization, and predictive analytics
Experience working with structured and unstructured data
Excellent organizational and analytical abilities
Outstanding problem solver
Preferred Skills:
Applicable Industry Certifications
Digital marketing / Social Media experience
Experience working with private and sensitive enterprise information
Power BI, Power Apps, Power Automate, and Power Virtual Agents
In-depth knowledge of a wide range of established and emerging data technologies
Confident in decision making and the ability to explain processes
Interpersonal and customer service skills
Build & maintain Azure DevOps Release Pipelines
About US
Creative Thinkers, Data Geeks & Digital Enthusiasts - Location3 Media is a digital marketing company built to improve the findability and performance of consumer and retail brands through enterprise-level and local digital marketing solutions. Founded in 1999 and located in the heart of Denver, Location3 has a staff of 60+ full-time employees who service global, national and local brands. More than half of Location3's client base has worked with the agency for at least three years, as Location3 improves the findability and performance of every client they partner with.
Why Us?
Location3 is looking for passionate people with innovative thinking who want to work with a performance-driven team. We emphasize working hard to bring our clients the results they seek and celebrating those wins together in a positive and fun work environment. We offer the benefit of being a remote work organization, but we also strongly believe that collaboration is key to driving outcomes. Our company culture, our ongoing education and training programs, and our technology infrastructure all contribute to that goal. We also believe that bonding is equally important - whether it's in-person or over Zoom. We have dedicated annual company events like golf day, ski day, and monthly events like town halls, team happy hours, team trivia and team cooking demonstrations that help to create synergy among colleagues and teams. On top of working and playing hard together, we also offer a very competitive benefits package, complete with medical, dental, vision, matching 401K, a wellbeing stipend, summer and fall Fridays, remote work equipment, and more.","$115,000 /yr (est.)",51 to 200 Employees,Company - Private,Media & Communication,Advertising & Public Relations,1999,$5 to $25 million (USD)
"Zodiac-Solutions
3.3",3.3,"Fort Mill, SC",Azure Solution Lead/Sr Data Engineer,"Azure Solution Lead/Sr Data Engineer:
Location: Fort Mill, SC and NJ (Hybrid 2 days onsite)
Experience – 15-20 years
Responsibilities:
Be part of strategic planning in defining the road maps and environments that supports our clients’ Azure data and advanced analytics initiatives.
Provide support in defining the scope and sizing of work.
Working closely with various enterprise architects, Information security teams, Data management team, to ensure the architected solution meets all the needs of a customer, from a functionality perspective and IT solution engineering perspective.
Translate business requirements into technology solutions.
Lead POCs and help our customers in choosing the right technologies to solve the business problems.
Lead designing of all aspects of our data solution including artifact creation such as diagrams, playbooks, and other technical documents.
Mentor and guide Jr. team members to deliver the solutions on-time.
Create various architecture blueprints and, be part of and work with the development team to deliver the vision.
Skills:
Over all 15-20 years of experience with Data Management, Big Data and Analytics.
At least 5 to 8 years of experience in architecting and implementing cloud native data solutions using Microsoft Azure.
Hands-on experience with data architecture, data management, designing and building highly scalable ELT processes.
At least 4 to 5 years of experience with big data using spark and java for data processing.
Apache Spark experience using Scala or PySpark or pre-packaged tools like Databrick is plus.
At least 3 years of experience in implementing the data solutions on MS Azure using Azure Data Factory, MS Synapse.
Good understanding of various file storage formats.
Experience in building and using ARM templates to provision and manage IaC.
Experience in implementing Big Data platform using event-based architecture.
One to two years of experience in Azure Synapse Analytics is plus.
At least one year experience with unified data governance solution using MS Purview.
Developing the CICD pipeline for Azure Infrastructure, version control strategy and Integrate source control (Azure repos)
Experience with java programming is a plus.
Extensive hands-on experience in designing and tuning ETL/ELT process development by using cloud native technologies.
In-depth understanding of various storage services offered by Azure.
Experience with implementation of data security, encryption, PII/PSI legislation, identity and access management across sources and environments.
Experience with data process Orchestration, end-to-end design and build process of Near-Real Time and Batch Data Pipelines.
Certification in Azure data engineering and solution architecture Azure is must.
Strong client-facing communication and facilitation skills.
Job Type: Full-time
Salary: $140,000.00 - $160,000.00 per year
Experience level:
11+ years
Schedule:
8 hour shift
Ability to commute/relocate:
Fort Mill, SC 29707: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Management: 9 years (Preferred)
cloud native: 5 years (Preferred)
Scala: 6 years (Preferred)
Work Location: One location","$150,000 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$1 to $5 million (USD)
"FinditParts
4.8",4.8,"Los Angeles, CA",Senior Data Warehouse Engineer,"FinditParts is the nation's largest eCommerce provider of heavy-duty truck and trailer parts. From hard-to-find parts to everyday preventative maintenance items, we offer more than 3 million heavy-duty OEM, branded, and aftermarket parts ready to ship. Each month thousands of repair shops, fleets, and owner-operators rely on FinditParts to streamline their part sourcing efforts and keep their trucks on the road.

As the industry leader in parts discovery through visual identification technology and aces/pies, we simplify the complexity of finding the right part to fit any commercial vehicle, reducing the time and frustration associated with parts sourcing.
Founded in 2010, with offices in Los Angeles and the Philippines, FinditParts is well-funded, having recently raised $30 million in Series A funding, and profitable.
SUMMARY
The Senior Data Warehouse Engineer, reporting to the Director of Analytics, will be responsible for leading the complete end-to-end architectural blueprint and development of our Data Warehouse system. A successful candidate will deliver a thoughtful data storage system that will store large data sets through various data intelligence environments. The Senior Data Warehouse Engineer will have a high level of impact, by providing architectural solutions to the design, development, testing, and deployment, and allowing FinditParts to store large volumes of data efficiently and effectively for the organization as a whole.
RESPONSIBILITIES
Lead the architectural design blueprint and development of our data warehouse
End-to-end ownership during the design, development, testing, and deployment
Evaluate architectural and software solutions that will deliver appropriate solutions
Develop and maintain data pipelines as well as API-based or file-based data flows between source systems and the data warehouse
Translate client user requirements into a technical architecture vision and implementation plan
Architect the data intelligence environments, including data lakes, data marts, and metadata repositories
Consult with leadership to define goals and requirements, develop technical requirements, deliver analysis, and thoughtful conclusions with data to provide actionable insights
Build, review, and audit existing ETL jobs and SQL queries
Identify gaps and develop a plan to integrate current systems with a desired future state
Perform root cause analysis of data failures and update existing processes to prevent re-occurring issues
Collaborate with various teams to gather information and create visually appealing, engaging, and informative reports in Tableau
Work closely with the BI team, Product team, and Analytics teams to ensure the Data Warehouse is structured in a way to meet data needs for all business teams
QUALIFICATIONS
5+ years of industry experience in Data Warehousing and/or Data Engineering
Bachelor's Degree in Computer Science, Engineering, Statistics, Mathematics, or another quantitative field
Proven experience managing and transforming data in a data warehouse
Proven experience in manipulating, processing, and extracting value from large data-sets
Familiarity with data schema designs that best support business needs and reporting
High level of experience extracting/cleansing data and generating insights from large transactional data sets using SQL, R, Python, and/or Spark on the cloud
Experience building and managing data pipelines and repositories in cloud environments such as Google Cloud, Microsoft Azure or AWS
Experience in Looker, Redshift, Apache Spark, Spark Structured Streaming, SQL, etc
Experience with cloud computing with Dataproc, Databricks, or similar technologies
Strong problem-solving/analytical abilities
BENEFITS
100% US-based remote
Competitive salary, bonus, and equity
Flexible PTO policy
Paid Medical, dental, and vision insurance options for Employees
Fun and energizing start-up environment
""The US base salary range for this full-time position is $125,000 - $165,000 + bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about specific salary range for your preferred location during the hiring process. Please note that the compensation details listed in the US role postings reflect the base salary only and do not include bonus, equity, or benefits.""
FinditParts is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran or any other characteristic protected by law. FinditParts conforms to the letter of all applicable laws and regulations.","$130,474 /yr (est.)",1 to 50 Employees,Company - Private,Retail & Wholesale,Automotive Parts & Accessories Stores,2010,Unknown / Non-Applicable
"TIDAL
4.2",4.2,"New York, NY","Senior Data Engineer - Infrastructure, TIDAL","Company Description TIDAL is a global music and entertainment streaming platform committed to creating a deeper connection between artists and fans through its library of more than 80 million songs, over 350,000 high quality videos, and available in over 60 countries. In addition, TIDAL offers its subscribers exclusive access to high-profile music and music videos, original content series, podcasts, documentaries, livestream concerts, tickets, merchandise and live experiences. Together, TIDAL and Block will be music-obsessed and artist-focused while we explore new artist tools, listener experiences, and access to financial systems that help artists be more successful.
Job Description

Data is an integral part of TIDAL’s success, and the Business Intelligence team provides the ability for our company to deliver several key initiatives.
We are hiring a Senior Data Engineer who will lead the efforts to deliver data products that improve the decision-making capabilities in shaping the future of TIDAL. You will be responsible for collecting data about TIDAL’s performance, interpreting that data to identify problem areas and areas of improvement, then sharing that information with others. You will collaborate with several stakeholders, including executives across key disciplines such as Marketing, Product, and Finance & Strategy. The ideal candidate will have both hands on and leadership experience in data engineering.
Responsibilities:
You will lead the technical implementation of data projects within our Data Lakehouse, Business Intelligence reporting solution across TIDAL.You will review and verify customer data, direct the dissemination of data to the data lakehouse, and establish policies and protocols for the collection and examination of data. You will also manage and lead medium to large cross functional technology data projects such as introducing a new self-service model for stakeholders to access and use data.
You Will:
Provide technical leadership for TIDAL data projects
Continuously streamline and improve processes related to the service delivery area.
Collaborate with the Business, Marketing, Finance, and external stakeholders to identify, establish scope and implement solutions that support the company’s growth, scalability, and overall success.
Perform stakeholder management to gather and document data warehouse and reporting requirements to ensure development work meets business objectives
Be an expert in the business’ need for data - understand how it informs their strategy and use that knowledge to deliver new insights.
Provides services and data support, from data lakehouse to business insights, including the data pipelines and analytics systems
Partner with our data partners such as the decentralized analytics teams to help them discover new data solutions, support the development of data products, and promote data best practices.
Serves the needs of data producers by providing a seamless integration for event data and change data capture.
Assists data owners to publish and monitor data streams
Owns the data lakehouse and manage the unified data storage for all teams
Ensures that all data is compliant with GDPR, CCPA, and PII

Qualifications
5+ years of computer science, data science, or engineering experience
Experience in building large scale distributed systems in a product environment
Able to communicate and influence across disparate groups and levels in the organization
Knowledge of database and data warehousing cloud technologies (Redshift, Snowflake, AWS RDS, and S3)
Experience in the Big Data technologies (Hadoop, Hive, Spark, Kafka, Flink etc.)
Proven experience and knowledge of data warehousing and business intelligence, with emphasis on data architecture, sourcing, reporting and analytics through the entire project lifecycle
Demonstrable understanding of development processes and agile methodologies
Strong leadership skills – experience managing a team to drive high-impact work; a proven track record of leading, mentoring, hiring, and scaling Business Intelligence teams

Additional Information

Block takes a market-based approach to pay, and pay may vary depending on your location. U.S. locations are categorized into one of four zones based on a cost of labor index for that geographic area. The successful candidate’s starting pay will be determined based on job-related skills, experience, qualifications, work location, and market conditions. These ranges may be modified in the future.

Zone A: USD $167,300 - USD $204,500
Zone B: USD $158,900 - USD $194,300
Zone C: USD $150,600 - USD $184,000
Zone D: USD $142,200 - USD $173,800
To find a location’s zone designation, please refer to this resource. If a location of interest is not listed, please speak with a recruiter for additional information.
Benefits include the following:
Healthcare coverage
Retirement Plans including company match
Employee Stock Purchase Program
Wellness programs, including access to mental health, 1:1 financial planners, and a monthly wellness allowance
Paid parental and caregiving leave
Paid time off
Learning and Development resources
Paid Life insurance, AD&D. and disability benefits
Perks such as WFH reimbursements and free access to caregiving, legal, and discounted resources
This role is also eligible to participate in Block's equity plan subject to the terms of the applicable plans and policies, and may be eligible for a sign-on bonus. Sales roles may be eligible to participate in a commission plan subject to the terms of the applicable plans and policies. Pay and benefits are subject to change at any time, consistent with the terms of any applicable compensation or benefit plans.
US and Canada EEOC Statement
We’re working to build a more inclusive economy where our customers have equal access to opportunity, and we strive to live by these same values in building our workplace. Block is a proud equal opportunity employer. We work hard to evaluate all employees and job applicants consistently, without regard to race, color, religion, gender, national origin, age, disability, pregnancy, gender expression or identity, sexual orientation, citizenship, or any other legally protected class.
We believe in being fair, and are committed to an inclusive interview experience, including providing reasonable accommodations to disabled applicants throughout the recruitment process. We encourage applicants to share any needed accommodations with their recruiter, who will treat these requests as confidentially as possible. Want to learn more about what we’re doing to build a workplace that is fair and square? Check out our I+D page.
Additionally, we consider qualified applicants with criminal histories for employment on our team, and always assess candidates on an individualized basis.

Block, Inc. (NYSE: SQ) is a global technology company with a focus on financial services. Made up of Square, Cash App, Spiral, TIDAL, and TBD, we build tools to help more people access the economy. Square helps sellers run and grow their businesses with its integrated ecosystem of commerce solutions, business software, and banking services. With Cash App, anyone can easily send, spend, or invest their money in stocks or Bitcoin. Spiral (formerly Square Crypto) builds and funds free, open-source Bitcoin projects. Artists use TIDAL to help them succeed as entrepreneurs and connect more deeply with fans. TBD is building an open developer platform to make it easier to access Bitcoin and other blockchain technologies without having to go through an institution.","$141,532 /yr (est.)",5001 to 10000 Employees,Company - Public,Information Technology,Internet & Web Services,2009,$1 to $5 billion (USD)
"Tek Ninjas
4.0",4.0,"Carrollton, TX",CLOUD BIG DATA ENGINEER,"CLOUD BIG DATA ENGINEER

Looking for Cloud Big Data Engineer in Carrollton, TX area,
Candidate should work on Amazon Web Services (AWS) using EC2 for computing and S3 as storage.
ETL process jobs using Ab-Initio in dynamic, high-volume environment.
Design and deploy well-tuned Ab-Initio graphs for ODS and DSS instances using both Linux and UNIX environments.
Test all applications and transport data to target Warehouse tables, schedule and run extraction and load process by using Informatica Workflow Manager.
Should be familiar with Hadoop, Hive, Pig, Sqoop, HBase, Cassandra, Spark, Spark Streaming, Spark SQL, Oozie, Zookeeper, Kafka, Flume, MapReduce framework, Yarn, Scala and related technicalities.
Req: MS/BS in Computer Science/any engineering/closely related field. Ex: MS+1yr/BS+5yrs in MicroStrategy developer/any software-related experience.

Should be WILLING TO TRAVEL/RELOCATE TO CLIENT SITES THROUGHOUT THE US.

Email: hr@tekninjas.com, Tek Ninjas Solutions, LLC, 4425 Plano Pkwy, Ste#1402, Carrollton, TX 75010.","$111,153 /yr (est.)",51 to 200 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Pacific Northwest National Laboratory
4.0",4.0,United States,Data Engineer 3,"Overview:
The Pacific Northwest National Laboratory (PNNL) and the Environmental Molecular Sciences Laboratory (EMSL) seek a junior-level Data Engineer to further EMSL’s data management and integrated research goals. The candidate should have experience in software development and managing scientific data using FAIR principles.
Rockstar Rewards:
Employees and their families are offered medical insurance, dental insurance, vision insurance, health savings account, flexible spending accounts, basic life insurance, disability insurance*, employee assistance program, business travel insurance, tuition assistance, supplemental parental bonding leave**, surrogacy and adoption assistance, and fertility support. Employees are automatically enrolled in our company funded pension plan* and may enroll in our 401k savings plan. Employees may accrue up to 120 vacation hours per year and may receive ten paid holidays per year.
Research Associates excluded.
**Once eligibility requirements are met.

Click Here For Rockstar Rewards
Responsibilities:
Developing and optimizing capabilities at the division level and identify mission challenges and formulate engineering solutions methodically.
Generates new ideas for proposals and business development opportunities of small to medium proposals. Stay current about data management and database technology developments
Collaborate with researchers on the data development and applying analysis techniques to a rich variety of scientific instrument data form experiment
Contribute to innovative application of advanced simulation and modeling to research problems relevant to DOE-BER missions where computational approaches are significant.
Design, build, deploy & maintain data pipelines, ETL workflows, data, and computing environments; or implement and configure third-party solutions, including participation in software engineering work in Python or other programming languages.
Work on data migrations and ingestion projects with users to design large-scale data processing systems, develop data pipelines optimized for scaling, and troubleshoot potential data platform challenges.
Ensure quality, availability, and integrity of data, solutions, and respective systems and integrate proactive strategies and best practices regarding data integrity, security, and scalability.
Partner cross-functionally to build and improve new/existing constructs and solve data engineering problems. Supports scoping, scheduling, and budgeting at a project or major task level.
Qualifications:
Minimum Qualifications:
BS/BA and 5+ years of relevant work experience -OR-
MS/MA and 3+ years of relevant work experience -OR-
PhD with 1+ year of relevant experience
Preferred Qualifications:
ETL, SQL, NoSQL, and Data Management frameworks
Backend languages and tools such as Python, PostgreSQL, and database management
Experience with Linux computing environments.
Hazardous Working Conditions/Environment:
Not applicable.
Additional Information:

Not applicable.

“Referral Eligible”
Testing Designated Position (TDP):
This is not a Testing Designated Position (TDP).
About PNNL:
Pacific Northwest National Laboratory (PNNL) is a world-class research institution powered by a highly educated, diverse workforce committed to the values of Integrity, Creativity, Collaboration, Impact, and Courage. Every year, scores of dynamic, driven people come to PNNL to work with renowned researchers on meaningful science, innovations and outcomes for the U.S. Department of Energy and other sponsors; here is your chance to be one of them!

At PNNL, you will find an exciting research environment and excellent benefits including health insurance, flexible work schedules and telework options. PNNL is located in eastern Washington State—the dry side of Washington known for its stellar outdoor recreation and affordable cost of living. The Lab’s campus is only a 45-minute flight (or ~3-hour drive) from Seattle or Portland, and is serviced by the convenient PSC airport, connected to 8 major hubs.
Commitment to Excellence, Diversity, Equity, Inclusion, and Equal Employment Opportunity:
Our laboratory is committed to a diverse and inclusive work environment dedicated to solving critical challenges in fundamental sciences, national security, and energy resiliency. We are proud to be an Equal Employment Opportunity and Affirmative Action employer. In support of this commitment, we encourage people of all racial/ethnic identities, women, veterans, and individuals with disabilities to apply for employment.

Pacific Northwest National Laboratory considers all applicants for employment without regard to race, religion, color, sex (including pregnancy, sexual orientation, and gender identity), national origin, age, disability, genetic information (including family medical history), protected veteran status, and any other status or characteristic protected by federal, state, and/or local laws.

We are committed to providing reasonable accommodations for individuals with disabilities and disabled veterans in our job application procedures and in employment. If you need assistance or an accommodation due to a disability, contact us at careers@pnnl.gov.
Drug Free Workplace:
PNNL is committed to a drug-free workplace supported by Workplace Substance Abuse Program (WSAP) and complies with federal laws prohibiting the possession and use of illegal drugs.

If you are offered employment at PNNL, you must pass a drug test prior to commencing employment. PNNL complies with federal law regarding illegal drug use. Under federal law, marijuana remains an illegal drug. If you test positive for any illegal controlled substance, including marijuana, your offer of employment will be withdrawn.",#N/A,5001 to 10000 Employees,Company - Private,"Energy, Mining & Utilities",Energy & Utilities,1965,$1 to $5 billion (USD)
"Goodship
2.6",2.6,Remote,Senior Infrastructure Engineer - Data,"About GoodShip
GoodShip, the first neutral platform in the $900B US truckload market, offers a comprehensive solution for procurement, performance measurement, and business optimization. Our platform empowers shippers and freight carriers to transition from traditional spreadsheet and email management to a more streamlined, AI-powered system that provides automated reports, insightful analytics, and carrier scorecards. By integrating seamlessly with any shipper TMS, GoodShip enhances collaboration among brokers and carriers.
Backed by leading venture firms such as FUSE, Ironspring Ventures, and Chicago Ventures along with founders of leading freight-tech companies, including Convoy, Stord, Project44, and FreightWaves, GoodShip is positioned for success with significant financial support and strong early traction.
Your Role
As a Senior Data Infrastructure Engineer, you'll join our mission to build a modern freight platform to empower shippers. You will be responsible for building out our data and general infrastructure to support the growing needs of the product and business.
Key Responsibilities:
Own the data ingestion architecture & system from our customers
Design new data pipelines to support the growing needs of the business
Own the strategy for scaling our data infrastructure
Improve our general engineering infrastructure to ensure operational excellence
Establish and promote best engineering practices to facilitate our ongoing growth.
What We're Looking For:
5+ years of industry experience in software development as part of a professional team.
Significant experience maintaining and scaling data infrastructure systems
Strong familiarity with the data technology ecosystem
BS degree or higher in Computer Science/Engineering, or equivalent experience.
A strong sense of ownership and responsibility.
A growth mindset, coupled with the ability to quickly learn new technologies and concepts to meet customer needs.
Proven ability to foster diverse, flexible, and empathetic engineering teams.
Nice To Have
Experience building enterprise SaaS applications
Familiarity with trucking or logistics industry
Experience with AWS, Javascript/Typescript, GraphQL, React, or PostgreSQL
Benefits
Generous equity compensation
100% Employer paid health benefits for employees
Paid time off
Opportunity to work on cutting-edge technology that directly makes an impact on customers and the freight industry",#N/A,1 to 50 Employees,Company - Private,Transportation & Logistics,Shipping & Trucking,#N/A,$5 to $25 million (USD)
"Tesla
3.6",3.6,"Palo Alto, CA","Data Engineer, Automation and Analytics","What to Expect
The electrical component team in Supply Chain is looking for a highly skilled and motivated Data Engineer to support our Supply Chain Organization. You will plan effective data storage, security, sharing and publishing within the organization, including our Global Supply Managers as well as Design Engineer on a regular basis. You will have the responsibility to maintain large supply chain datasets and formulate applicable data-driven solutions which can effectively utilized by the global supply management team. It is essential that you can think strategically, connecting the dots in the bigger picture, as well as being comfortable in the details of the deliverables. A candidate for this role needs to be a self-motivated for data analytics and process raw, unstructured data using batch and real time processing frameworks.
What You’ll Do
Design, code, test, correct and document programs and scripts using agreed standards and tools to achieve a well-engineered result
Drive problem solving and continuous improvement initiatives to improve supply chain operations processes
Ensure data quality and implement tools and frameworks for automating the identification of data quality issues
Collaborate with internal and external data providers on data validation providing feedback and making customized changes to data feeds and data mappings
Think strategically and considering Tesla’s global supply chain structure
Build and maintain purchasing database on a regular basis
Make data and reporting updates as needed to ensure accuracy
Develop data-based solutions to complex purchasing challenges and be able to present relevant information in non-specialist forum
Identify process gaps and areas for optimization and automation
Own system requirements, and participate in the software development process from design to user acceptance testing
What You’ll Bring
Master’s degree in science or engineering or equivalent experience or evidence of exceptional ability
Strong experience with relational databases like SQL Server, MySQL and Vertica.
Proficiency with SQL, a good understanding of database systems, highly skilled in programming (R, Python). Apache Airflow is a plus
Structured, data-driven and quantitative problem-solving approach
Experience creating dashboards using Tableau (or similar) visualization tools
Experience with design, development, and implementation of highly scalable, high-volume source of truth systems for different business areas, developing and maintaining web services in an agile environment
Experience creating Full-Stack UI is a plus
Work experience in material planning, supply chain, purchasing, or manufacturing is a plus
Strong organizational skills and the ability to prioritize and manage multiple projects simultaneously with complex and demanding deadlines
Exceptional communications, strategic thinking, and interpersonal skills
Proven ability to set expectations, manage to deadlines and hold individuals and teams accountable to critical milestones
Collaborative, flexible, open working style and an ability to establish trust and credibility",#N/A,10000+ Employees,Company - Public,Manufacturing,Transportation Equipment Manufacturing,2003,$1 to $5 billion (USD)
"Resolution Life
3.5",3.5,"Sydney, FL",Senior Data Engineer,"At Resolution Life, we’re proud to have evolved into a global business under the Resolution Life name. For customers, advisers, companies and the industry. We’re making an impact worldwide
Resolution Life Group is a global life insurance group focusing on the acquisition and management of in-force life insurance policies. With assets of $31 billion and 1.5 million customers, Resolution Life are providing existing customers with life insurance, super and investments.

Why us?
Our platform vision is to be the leading in-force specialist life-insurer in Australasia by 2024, by being customer-obsessed and data-driven.
We are one of the first life insurers globally to operate in an entirely Enterprise Agile environment. The strategic priorities of the platform are focused on ensuring the business is future-fit and sustainable, and to grow through bolt-on acquisition of in-force portfolios. We are guided by core behaviours that inform the way Resolution Life team members show up each day and interact with others.
The Opportunity
The individual will play a hands-on role and work closely with business and technology stakeholders to deliver an enterprise-wide data solution in our data platform to grow and retain data domain expertise to support our data driven vision and reduce our ongoing operating costs and technical debit.
Your Story
Align Architecture, Solution Design with business requirements.
Design, Build and Maintain Physical data model in data platform layers.
Map business glossary, data definition and data sets to physical data model
Build ETL that collects, manage, and convert raw data into consumable tables applying technical logic based on business rules.
Responsible to defining and building job orchestration.
Build of database objects by applying application best practices
Build of reusable components and frameworks based on application best practices.
Build a robust codebase to improve data reliability, efficiency, latency, and quality.
Build code and maintain version control by following Software development lifecycle.
Responsible for unit testing and system testing and system integration testing.
Build a test-driven development approach to identify data anomalies and data quality issues.
Support SIT, UAT and Performance Testing
Support Code deployment and Code stabilisation in UAT, Pre-Prod and Prod environments
Present details technical design at DnA design forum and obtain approval.
Be a key technical leader across data platforms technologies including Snowflake, SQL Server, Informatica Cloud, H2O, DBT, advanced SQL, Power BI and Azure / AWS Cloud
Critical Skills
At Resolution Life, we have identified the following critical skills which are key to success in our culture:
Customer Focused: Passionate drive to delight our customers and offer unique solutions that deliver on their expectations.
Critical Thinking: Thoughtful process of analysing data and problem-solving data to reach a well-reasoned solution.
Team Mentality: Partnering effectively to drive our culture and execute on our common goals.
Business Acumen: Appreciation and understanding of the financial services industry in order to make sound business decisions.
Learning Agility: Openness to new ways of thinking and acquiring new skills to retain a competitive advantage.
What Will We Do For You:
Our culture underpins our values and guides our decision making. It's also what makes Resolution Life a great place to work.
Resolution Life Australasia supports virtual working, and our enduring primary place of work continues to be “virtual” with the physical office and home office used interchangeably. We recognise that our workers can contribute and connect equally regardless of where they are located, and we have seen and experienced the wellbeing and benefits that come from working at home. This means some of us work at home most of the time, in the office most of the time or a balanced mix.
Every day is an opportunity to grow – and we hope to offer our people a career, not just a job.
The learning and development opportunities we offer include supporting the completion of executive-level short courses, access to leading online learning tools, on the job training, and mentoring by highly experienced business leaders.
Join us
Before commencing employment in this role you will need to provide two references, full working rights and complete police and credit checks through an online provider.
As an equal opportunity employer strongly committed to working in a diverse and inclusive workforce you will be provided with any support or accessibility requirements throughout your interview process. Please feel free to contact our Talent Team directly at talent@resolutionlife.com.au.
Privacy Policy
Please refer to our
Privacy Policy
to learn about how we use the information you give us, alternatively you can view the same information by navigating to the page
https://www.resolutionlife.com.au/privacy
.","$105,758 /yr (est.)",201 to 500 Employees,Company - Private,Insurance,Insurance Carriers,2003,Unknown / Non-Applicable
"Notion
4.8",4.8,"San Francisco, CA","Data Engineer, Finance","About Us:
We're on a mission to make it possible for every person, team, and company to be able to tailor their software to solve any problem and take on any challenge. Computers may be our most powerful tools, but most of us can't build or modify the software we use on them every day. At Notion, we want to change this with focus, design, and craft.
We've been working on this together since 2016, and have customers like Pixar, Mitsubishi, Figma, Plaid, Match Group, and thousands more on this journey with us. Today, we're growing fast and excited for new teammates to join us who are the best at what they do. We're passionate about building a company as diverse and creative as the millions of people Notion reaches worldwide.
Notion is an in person company, and currently requires its employees to come to the office for two Anchor Days (Mondays & Thursdays) and requests that employees spend the majority of their week in the office (including a third day).
About The Role:
As Notion continues to grow rapidly, we're seeking talented data engineers to join our team and help us build the foundational datasets and pipelines for robust financial reporting. You'll be at the forefront of integrating our product, financial, and business systems to create rock solid processes that will propel us forward. If you're passionate about analytics use cases, data models, and solving complex data problems, then we want you on our team.
What You'll Achieve:
You'll build core datasets to serve as the sources of truth for Notion's financial reporting, integrating data from financial systems, business systems data and Notion's product.
You'll partner closely with our Finance, Monetization Engineering, Business Intelligence and Data Science teams to support critical financial reporting and analysis needs.
You'll design, build and monitor pipelines that meet today's requirements but can gracefully scale with our growing data size.
You'll help democratize access to high quality financial data across Finance, Staff and go-to-market teams.
Skills You'll Need to Bring:
You've spent 4+ years as a data engineer building core datasets and supporting business verticals as needed, ideally in product and business areas with high data volumes. You are passionate about analytics use cases, data models and solving complex data problems.
You've built integrations with and reporting datasets for payments, finance and business systems like Stripe, Netsuite, Adaptive, Anaplan, Salesforce and/or others.
You are a self-starter and continuously gather and synthesize high-impact needs from business partners, design and implementing the appropriate technical solutions, and effectively communicating about deliverables, timelines and tradeoffs
You have hands-on experience shipping scalable data solutions in the cloud (e.g AWS, GCP, Azure), across multiple data stores (e.g Snowflake, Redshift, Hive, SQL/NoSQL, columnar storage formats) and methodologies (e.g dimensional modeling, data marts, star/snowflake schemas)
You are a SQL expert. You intimately understand aggregation functions, window functions, UDFs, self-joins, partitioning and clustering approaches to run correct and highly-performant queries
You are comfortable with object-oriented programming paradigms (e.g Python, Java, Scala)
Nice to Haves:
You have hands-on experience in designing and building highly scalable and reliable data pipelines using BigData stack (e.g Airflow, DBT, Spark, Hive, Parquet/ORC, Protobuf/Thrift, etc)
You have hands-on experience building payment processing and invoice systems or have worked closely with teams that do this.
We hire talented and passionate people from a variety of backgrounds because we want our global employee base to represent the wide diversity of our customers. If you're excited about a role but your past experience doesn't align perfectly with every bullet point listed in the job description, we still encourage you to apply. If you're a builder at heart, share our company values, and enthusiastic about making software toolmaking ubiquitous, we want to hear from you.
Notion is proud to be an equal opportunity employer. We do not discriminate in hiring or any employment decision based on race, color, religion, national origin, age, sex (including pregnancy, childbirth, or related medical conditions), marital status, ancestry, physical or mental disability, genetic information, veteran status, gender identity or expression, sexual orientation, or other applicable legally protected characteristic. Notion considers qualified applicants with criminal histories, consistent with applicable federal, state and local law. Notion is also committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or an accommodation made due to a disability, please let your recruiter know.
Notion is committed to providing highly competitive cash compensation, equity, and benefits. The compensation offered for this role will be based on multiple factors such as location, the role's scope and complexity, and the candidate's experience and expertise, and may vary from the range provided below. For roles based in San Francisco, the estimated base salary range for this role is $130,000 - $250,000 per year.
#LI-Onsite","$190,000 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2016,Unknown / Non-Applicable
"McDonald's Corporation
3.5",3.5,"Chicago, IL","Data Engineer, Global Data, CRM","Company Description

McDonald’s evolving Accelerating the Arches growth strategy puts our customers and people first, and leverages our competitive advantages to strengthen our brand. We are recognized on lists like Fortune’s Most Admired Companies and Fast Company’s Most Innovative Companies.
Doubling Down on the 4Ds (Delivery, Digital, Drive Thru, and Development)
Our growth pillars emphasize the important role technology plays as the leading, global omni-channel restaurant brand. Technology enables the organization through digital technology, and improving the customer, crew and employee experience each and every day.
Global Technology forging the way
Leading the digitization of our business is the Technology organization made up of intrapreneurs who build industry defining tech using the latest innovations and platforms, like AI and edge computing to deliver on the next set of cutting-edge opportunities for the business. At McDonald’s you get to solve technology innovation challenges at an incredible scale, and work across global teams who are always hungry for a challenge. This provides access to exciting career paths for technologists. It’s bonus points when you get to see your family and friends use the tech you build at their favorite McD restaurant.

Job Description

We are seeking a highly motivated individual for the Data Engineering Manager for the Marketer Customer Data Platform team on Global Data and Architecture. This role will manage, develop, and lead the execution and implementation cloud-based data delivery systems for the MCD Customer Data Platform. This role will specialize in leading and managing the engineering teams in both onshore and offshore locations, as well as working with the relevant product management teams on overall product direction and implementation of the product vision.
What you will do:
Lead and manage the day-to-day operations for the CRM Data Engineering team with both, onshore and offshore, resources to with a focus on CDP/ CRM customer data technologies
Independently drive efforts in marketing technology and automation, collaborating with key stakeholders and business units. Must be able to find marketing solutions alongside building a long-term strategic vision of a marketing ecosystem across a highly technical, complex data stack
Participate in activities of the data engineering team on the solution, design, architect, implementation, support, and monitoring of CRM & CDP related data products
Develop and implement monitoring and remediation structure and applications for critical data flows
Continuously develop and deploy solutions to streamline and enhance the processes, logic, and tactics around customer cohort identification, and targeting via corporate CRM, marketing, and advertising systems. Additionally, develop the overall best practice framework for use by
Gather business and functional requirements and translate into robust, scalable, and user-friendly solutions

Qualifications
Bachelor’s degree in Information Technology or related technical fields such as computer science, marketing tech, mathematics, engineering, etc.
Minimum 2-5 years of relevant work experience in data analytics, engineering, computer science, mathematics or similar logic based fields
Minimum 2 years hands on experience with complex marketing automation tech stacks, leveraging CDP tools such as mParticle, Segment, and Tealium
Strong analytics skills, root cause analysis experiences on internal and external data and processes to answer business questions and improve application performance
Comfortable partnering and working with a wide range of stakeholders at various levels of management and leadership
Ability to focus on details as well as big picture
Competence and comfort focusing on specific detail engineering questions, as well as “big picture” questions
Ability to leverage critical thinking, problem-solving, and analytical skills to improve user experience, adoption, and outcomes
Curious, task-oriented self-starter with a strong bias for action; Proven track record of “getting things done”
Self-motivated with ability to set priorities and mentor others in a performance driven environment.
Passion for technology and innovation & uncompromising attention to detail & passion for problem solving
Desired Skills

Extensive experience working with Agile methodologies and expertise in production deployment processes and continuous deployment best practices.
1-2 years of hands-on experience with Braze and/or similar market automation delivery platforms
1-2 years of hands on experience with marketing automation and technology experience, understanding key components of marketing technology stacks, including CDP tools such as mParticle, Tealium, or Segment
Experience with clickstream tracking architecture implementations, and knowledge of Adobe Analytics and/or Google Analytics
Demonstrated ability to problem solve in complex technology solutioning
Experience developing Confluence pages and working with Jira
Demonstrable knowledge of multiple programming languages leveraged in martech/adtech stacks – javascript, ruby (liquid logic), html/css, python etc.

Additional Information

McDonald’s is committed to providing qualified individuals with disabilities reasonable accommodations to perform the essential functions of their jobs. Additionally, if you (or another applicant of whom you are aware) require assistance accessing or reading this job posting or otherwise seek assistance in the application process, please contact recruiting.supportteam@us.mcd.com
McDonald’s provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to sex, sex stereotyping, pregnancy (including pregnancy, childbirth, and medical conditions related to pregnancy, childbirth, or breastfeeding), race, color, religion, ancestry or national origin, age, disability status, medical condition, marital status, sexual orientation, gender, gender identity, gender expression, transgender status, protected military or veteran status, citizenship status, genetic information, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.
Nothing in this job posting or description should be construed as an offer or guarantee of employment.","$104,720 /yr (est.)",10000+ Employees,Company - Public,Restaurants & Food Service,Restaurants & Cafes,1955,$10+ billion (USD)
"Decision Point Healthcare
5.0",5.0,"Boston, MA",Data Pipeline Engineer,"Boston, MA
Full-Time

We need your help.
We’re looking for a self-motivated and detail-oriented data engineer to join our DataOps team. This opportunity will enable you contribute to the operation, support, and enhancement of our mission critical data operations platform and the development of data pipelines. Our data operations platform supports high volume, high velocity data ingestion and curation to support our existing, and rapidly expanding, health plan client base.
The position:
Design and develop scalable data pipeline processes (including ingestion, cleansing, curation, unification, etc.)
Automate the processing of inbound client data feeds
Design and develop tools and processes to support automated data profiling and data quality methodologies
Work with our data science team to assist with the development of feature store data including data prep, enrichment, and feature engineering for AI/ML
Write and maintain documentation on data pipelines
Provide periodic support to our customer success team Skills & Experience
BS / MS in Computer Science, Engineering, or applicable experience
3+ Year using Python (Pandas/NumPy) in a production environment
3+ Year using PowerShell in a production environment
Expertise with ETL/ELT and the development of automated validation and data pipelines
Understand database design and data manipulation and transformation methodologies
Keen understanding of EDW, master data management and other database design principles
Experience designing solutions using a range of AWS Services
Experience with data engineering and workflow management frameworks such as Airflow and dbt
Comfortable working with high volume data in a variety of formats
Experience with CI/CD such as Jenkins
Experience with version control tools: Git preferred
Excellent verbal and written communication
Familiarity with healthcare data is a plus
Familiarity with ML pipelines, principles and libraries is a plus
Experience with REST API is a plus
Why Decision Point?:
We’re as passionate about our people as we are about making our mark on healthcare. Fostering a fun and challenging environment that’s centered around personal and professional growth has brought us to where we are today. We are constantly seeking out new ways to reinvest in our team members because let’s face it, we all do our best work when we feel valued.
Meaningful work
Remote friendly environment
We encourage outside the box ideas
Great healthcare coverage
Competitive compensation","$87,903 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,2013,Less than $1 million (USD)
"PRGX Global, Inc
3.7",3.7,"Atlanta, GA",Senior Software Engineer - Data Technologies,"SENIOR SOFTWARE ENGINEER, DATA INTELLIGENCE TECHNOLOGIES
This position is responsible for creating high priority differentiated technology solutions that solve important real-world problems. The position partners closely with the Head of Data Analytics, data scientists, business leaders, clients, and other stakeholders to create roadmaps, scope programs aligning with business priorities, define milestones and success metrics, and create scalable, secure, reliable, and efficient data products and platforms which meet our clients’ needs and contribute to the growth of PRGX.

Job Responsibilities:
Participate in the conceptualization of new products.
• Work with our data scientists to turn large-scale, diverse, and often unstructured data into a meaningful source of insights for our clients. • Design, develop, deliver, implement, and maintain high-quality, secure, reliable, and scalable data applications on time and on budget.
Design, code, configure, test, and document deliverables using agile methodologies.
Collaborate with key stakeholders on coding standards, processes, tools, and frameworks required for the delivery of features.
Guide technical and design decisions based on experience.
Develop proof of concept work and prototyping when necessary.
Collaborate with business leaders to understand business requirements relating to features to be delivered.
Identify common patterns and foster development of reusable components and standards.
Contribute to an innovation culture by evaluating new processes and technologies that can be used to enhance future features.

Ideal Candidate Characteristics:
Enjoys and excels in environments where they must tackle and solve new and increasingly complex client business challenges and issues, incorporating the newest ideas and technologies to deliver solutions quickly.
Possesses a high level of self-awareness. Maintains composure and professionalism when under pressure.
Possesses strong systems and critical thinking skills and the ability to draw on disparate information to identify insights, and design and deliver solutions.
Driven to meet or exceed specific goals and objectives as quickly as possible.
Entrepreneurial spirit. Forward-thinking and adaptable in dynamic situations. Knowledge & Qualifications:
Bachelor’s degree in Computer Science, Computer Engineering, or related field. Master’s degree preferred.
Minimum of 10 years’ software development experience.
Minimum of 5 years’ experience working with complex data sets and developing data-centric applications.
Minimum of 5 years’ experience with commercializing and scaling white label Qlik applications.
Experience with big data and big data tools such as SQL and Python.
MySQL, Spark, Scala, Hive, Minio, Trino, Airflow, Presto and other ETL solutions experience desired.
Experience in CPG manufacturing, wholesale, distribution, or retail is highly desirable, and preference will be given to candidates with this experience.
Strong strategic thinking and problem-solving skills.
• Excellent communication, collaboration, and leadership skills. • Ability to build and maintain relationships with key stakeholders.
Strong business acumen.","$114,592 /yr (est.)",1001 to 5000 Employees,Company - Public,Financial Services,Accounting & Tax,1996,$25 to $100 million (USD)
"Rippling
3.4",3.4,"San Francisco, CA",Senior Data Engineer,"About Rippling
Rippling is the first way for businesses to manage all of their HR & IT—payroll, benefits, computers, apps, and more—in one unified workforce platform.

By connecting every business system to one source of truth for employee data, businesses can automate all of the manual work they normally need to do to make employee changes. Take onboarding, for example. With Rippling, you can just click a button and set up a new employees’ payroll, health insurance, work computer, and third-party apps—like Slack, Zoom, and Office 365—all within 90 seconds.

Based in San Francisco, CA, Rippling has raised $700M from the world’s top investors—including Kleiner Perkins, Founders Fund, Sequoia, and Bedrock—and was named one of America’s best startup employers by Forbes (#12 out of 500).
About The Role
We are looking for a hands-on senior engineer to play a key role in Rippling’s data team. As a senior engineering resource on the data engineering team, you will be leading the design and development of systems that will enable analytics, experimentation, and user-facing features. You will be closely involved with multiple data adjacent teams and stakeholders, alongside helping junior talent in the team learn and grow.
The Data Engineering Team at Rippling is a combination of warehousing and data platform engineering, supporting a variety of orgs across the company (Data Science, Marketing, Bizops, Revops, Finance to name a few). Here’s an idea of some of the initiatives we’re working on:
A realtime, central data lake to operationalize warehouse data.
Creating a metrics layer to make reporting more efficient and accurate.
Building a catalog to make our data assets searchable and easy to discover.
Making our globally distributed data stack compliant and scalable
What You'll Do
Help architect, build, and scale our data pipelines from our OLTP database, other internal systems and third party tools to our warehouse (Snowflake)
Leverage data technologies like Kafka, Presto, Flink, Airflow, Mongo, Snowflake and Spark
Support reporting, data science, operations and machine learning functions
Create data platforms, data lakes, and data ingestion systems that work at scale
Define and enforce data quality checks and audits for code warehousing datasets
Define and support internal SLAs for core data sets
Qualifications
5+ Years experience in Data and Software Engineering
Expertise in writing complex data transforms in SQL and Python
Knowledge of data warehousing concepts around building custom ETL integrations, building data infrastructure (SCD,CDC,Snapshots,indexing,partitioning)
Knowledge on Data Security and Governance (nice to have)
Experience in analytics, dimensional modeling, and ETL optimization preferred
BS/BA in a technical field such as Computer Science or Mathematics preferred
Additional Information
This role will receive a competitive salary + benefits + equity. The salary for US-based employees will be aligned with one of the ranges below based on location; see which tier applies to your location here.

Tier 1: $174,000 - $237,000/year
Tier 2: $156,000 - $214,000/year
Tier 3: $147,000 - $202,000/year
A variety of factors are considered when determining someone’s compensation–including a candidate’s professional background, experience, and location. Final offer amounts may vary from the amounts listed above.","$205,500 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,2016,$25 to $100 million (USD)
"DocuSign
3.7",3.7,"San Francisco, CA",Senior Data Engineer,"Company Overview

DocuSign helps organizations connect and automate how they agree. Our flagship product, eSignature, is the world’s #1 way to sign electronically on practically any device, from virtually anywhere, at any time. Today, more than a million customers and a billion users in over 180 countries use DocuSign to accelerate the process of doing business and simplify people’s lives.


What you'll do

DocuSign is seeking a talented and results-oriented Senior Data Engineer to focus on delivering trusted data to the business. The Senior Data Engineer delivers data for analytics using our Enterprise Data Warehouse, enabling the global DocuSign analytics community via curated, governed and cleansed data. As a member of the Global Data and Analytics team, the Data Engineer leverages a variety of technologies to accomplish this goal, ranging tools like Airflow, Matillion, dbt, Snowflake and Fivetran to languages like SQL and Python. The successful candidate will develop solutions with innovative cloud technologies, work on a variety of fast-paced assignments, and partner with world-class technical and business teams to maximize the value of data.

This position is an individual contributor role reporting to the Manager, Data Analytics.

Responsibility
Build data pipelines using Fivetran, dbt/Matillion, Snowflake and Airflow
Develop and maintain data documentation including ERD, data dictionaries, data lineage and metadata
Ensure data quality and integrity by implementing appropriate data validation and cleansing techniques
Monitor and troubleshoot data pipelines to identify and resolve issues in a timely manner
Build POCs to validate new concepts and new technologies
Collaborate with business, engineering, and data science teams to understand their data needs and design efficient solutions to support their requirements


Job Designation

Hybrid:
Employee divides their time between in-office and remote work. Access to an office location is required. (Frequency: Minimum 2 days per week; may vary by team but will be weekly in-office expectation)


What you bring

Basic
Bachelor’s Degree in Computer Science, Data Analytics, Information Systems, relevant field or equivalent work experience
8+ years of relevant experience
Experience with database and data warehouse concepts such as facts and dimensions to design and develop data models that support enterprise reporting and analytics needs
5+ years of dimensional and relational data modeling experience
Experience with modern data integration and transformation tools such as Fivetran, Dbt, and Matillion
Experience with workflow orchestration tools such as Airflow
Experience with MPP databases like Snowflake, Redshift and BigQuery
Experience with cloud platforms like AWS, Azure and GCP
Experience with versioning tools like git
Experience working with tools like Jira and Confluence
Experience with SQL and Python
Experience with document and data debugging

Preferred
Ability to work independently with minimal supervision, as well as in a team environment
Excellent communication skills
Eye for detail, good data intuition, and a passion for data quality
Comfortable working in a rapidly changing environment with ambiguous requirements
Organizational and time management skills, with the ability to prioritize tasks and meet deadlines


Wage Transparency

Based on applicable legislation, the below details pay ranges in the following locations:

California: $130,500 - $208,050 base salary

Illinois and Colorado: $123,700 - $174,700 base salary

Washington and New York (including NYC metro area): $123,700 - $184,275 base salary

This role is also eligible for bonus, equity and
benefits.


Life at DocuSign

Working here
DocuSign is committed to building trust and making the world more agreeable for our employees, customers and the communities in which we live and work. You can count on us to listen, be honest, and try our best to do what’s right, every day. At DocuSign, everything is equal.

We each have a responsibility to ensure every team member has an equal opportunity to succeed, to be heard, to exchange ideas openly, to build lasting relationships, and to do the work of their life. Best of all, you will be able to feel deep pride in the work you do, because your contribution helps us make the world better than we found it. And for that, you’ll be loved by us, our customers, and the world in which we live.

Accommodation
DocuSign provides reasonable accommodations for qualified individuals with disabilities in job application procedures. If you need such an accommodation, including if you need accommodation to properly utilize our online system, you may contact us at accommodations@docusign.com.

If you experience any technical difficulties or issues during the application process, or with our interview tools, please reach out to us at taops@docusign.com for assistance.

Our global benefits
Paid time off
Take time to unwind with earned days off, plus paid company holidays based on your region.
Paid parental leave
Take up to six months off with your child after birth, adoption or foster care placement.
Full health benefits
Options for 100% employer-paid health plans from day one of employment.
Retirement plans
Select retirement and pension programs with potential for employer contributions.
Learning & development
Grow your career with coaching, online courses and education reimbursements.
Compassionate care leave
Paid time off following the loss of a loved one and other life-changing events.","$149,200 /yr (est.)",5001 to 10000 Employees,Company - Public,Information Technology,Information Technology Support Services,2003,$1 to $5 billion (USD)
Stratford Solutions Inc.,#N/A,"New York, NY",Senior Data Engineer – Specialist 3,"Job Title: Senior Data Engineer Specialist 3

Job Location: Remote

Job Type: 12 Month Contract

Work Schedule: Monday-Friday 9AM-5PM

Pay Rate: $160/hr

Tasks:
Collaborate with stakeholders to define and understand data needs.
Design and develop efficient data architectures that can support large-scale data processing and storage requirements.
Develop and maintain data pipelines, data models, and ETL processes that align with business requirements, data quality standards, and industry best practices.
Work closely with other data engineering teams to build and maintain reusable data pipelines and tools, enabling faster time-to-market for data-driven solutions.
Monitor, troubleshoot, and optimize data pipelines and processes for performance, reliability, and scalability.
Ensure the quality and integrity of various datasets across different platforms and data sources.
Continuously evaluate and recommend emerging technologies and methodologies to improve data engineering processes, workflows, and performance.
Mentor and guide junior data engineers on technical best practices, code reviews, and design patterns to ensure high-quality, scalable, and maintainable data engineering solutions.
Perform special technology-related projects, as assigned.
MANDATORY SKILLS/EXPERIENCE

Note: Candidates who do not have the mandatory skills will not be considered

12+ years of experience in data engineering with a focus on designing and building scalable data platforms using cloud technologies.
Strong experience designing and implementing data models and schemas in Snowflake.
Define Snowflake databases objects to support efficient data storage and retrieval.
Expert proficiency in SQL and one or more programming languages such as Python, Java, or Scala.
Knowledge of cloud-based data platforms such as AWS, Azure, or GCP, including experience with cloud-based storage, compute, and data processing services.
Experience designing and implementing ETL and data integration pipelines, and familiarity with data modeling concepts and database design principles.
Excellent communication skills and the ability to collaborate effectively with cross-functional teams.
Experience with Agile methodologies and working in an Agile team environment.
Experience developing production-grade, large-scale data solutions using cloud technologies.
Experience managing data orchestration at scale using tools such as Airflow and Dagster.
Familiarity with version control systems (e.g., Git) and CI/CD principles.
DESIRABLE SKILLS/EXPERIENCE:
Experience developing dashboards and reports in applications such as Oracle Analytics Server (OAS), Microsoft Power BI, and Google Looker.
Thorough experience with data integration tools such as Informatica Intelligent Cloud Services and MuleSoft.
Experience using Azure services for Security, Blob Storage, Data Lake, Databricks, Data Factory etc.
Experience with Azure Monitoring services
Microsoft Certified Azure Solutions Architect Expert or a Snowpro Certification or a similar one",$160.00 /hr (est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,$1 to $5 million (USD)
"DRW
4.3",4.3,"Edison, NJ",Data Center Engineer (East Coast),"DRW is a technology-driven, diversified principal trading firm. We trade our own capital at our own risk, across a broad range of asset classes, instruments and strategies, in financial markets around the world. As the markets have evolved over the past 25 years, so has DRW – maximizing opportunities to include real estate, cryptoassets and venture capital. With over 1200 employees at our Chicago headquarters and offices around the world, we work together to solve complex problems, challenge consensus and deliver meaningful results. It's a place of high expectations, deep curiosity and thoughtful collaboration.
We are currently looking for a Data Center Engineer to cover the NY/NJ metro area and assist with managing the architecture and operations of our data centers in that region. This person will also occasionally provide support for other regions as needed.
What you'll be working on:
Maintaining integrity of global data centers
Racking and cabling servers and other equipment
Performing network infrastructure cabling
Setting standards of excellence for all data center operations
Participating in expansion planning for all DRW data centers
Monitoring environmental conditions of all data centers
Maintaining documentation within DCIM tool
Maintaining optimal A/C and power conditions for data centers and facilities
Performing Windows & Linux server administration
What's required:
3+ years of hands-on data center engineering experience.
Previous experience doing network infrastructure cabling.
Familiarity with HP, IBM, and Dell Servers, Blades, SAN and Cisco equipment.
Previous experience working with backup software.
Previous hands-on Windows and Linux server administration experience preferred.
Ability to effectively communicate and collaborate with internal team members and external vendors.
Must be a team player and self-starter.
Strong documentation skills to capture architecture details, procedures, and other relevant data.
Strong verbal and written communication skills.
Regular local travel (approximately 75% of the time); occasional international travel.
Must have valid driver's license.
For more information about DRW's processing activities and our use of job applicants' data, please view our Privacy Notice at https://drw.com/privacy-notice.
California residents, please review the California Privacy Notice for information about certain legal rights at https://drw.com/california-privacy-notice.
#LI-LT1","$74,320 /yr (est.)",1001 to 5000 Employees,Company - Private,Financial Services,Investment & Asset Management,1992,Unknown / Non-Applicable
"Overhaul
3.5",3.5,"Austin, TX",Data Platform Engineer,"Who We Are
Overhaul is a supply chain integrity solutions company that allows shippers to connect disparate sources of data into the first fully transparent situational analysis engine designed for the logistics industry. Data that is transformed into critical insights can instantly trigger corrective actions, impacting everything from temperature control to handling requirements or package-level tracking, ensuring cargo arrives at its destination safely, undamaged, and on time. We are a dynamic, innovative, and fun team who is highly committed to our customers' experiences and our Mission and Vision.
The Role
At Overhaul, we're building the future of supply chain monitoring technology. As a Data Platform Engineer, you'll work with our platform and data analytics teams to understand and improve our data model, research and implement business intelligence tools to allow us to better serve our data internally and externally, and work with other engineers to design better systems for data ingestion and processing. Our company is growing fast, and we need you to help us deliver the best results to our users as we continue that trend.
Responsibilities:
Analyze existing tools, including business intelligence, databases, messaging services, etc. and provide solution recommendations
Deploy and manage data software solutions in a cloud environment (We use AWS and Azure)
Analyze custom data models and provide solutions for data ingestion/ETL
Create custom data software solutions for our internal and external customers using workflow tools such as Argo Workflows
Required Skills and Qualifications:
Experience doing data model analysis
Experience using cloud based technologies for data ingestion, analysis, and extraction (AWS Lambda, Workflow solutions, Snowflake, etc)
Excellent written and oral communication skills
Preferred Qualifications:
Software Engineering experience beyond writing simple scripts, preferably in Python
Experience using workflow solutions such as Argo Workflows, Prefect, Kubeflow, or Jenkins to implement data pipelines
Our Core Values and how they benefit you as an ""Overhauler""
Authenticity, Receptivity and Trust
Extremely competitive base salary package
401(k) with Overhaul match
Flexible working schedules
Remote, hybrid, and/or In-office*
Encouragement and Learning
Progressive advancement opportunity & career mobility
Paid development personal stipend
Monthly lunch and learns
2 Unique learning systems w/Instructor led content
Wellness and Integrity
Rotating Overhaul ""Perks @ work"" (Discounts and Freebies)
Overhaul fully provided healthcare plan
Employee assistance & wellbeing programs
New Parent/Family/Caregiver leave(s)
Daily BAMM time (body and mind movement)
Life by design vacation policy
Diversity and Inclusivity Statement:
Overhaul has always been, and always will be, committed to diversity and inclusion. Our Overhaul Culture Code's top listed commitment is to ""Diversity and Synergy."" All aspects of employment will be based on merit, competence, performance, and business needs. We do not discriminate on the basis of race, color, religion, marital status, age, national origin, ancestry, physical or mental disability, medical condition, pregnancy, genetic information, gender, sexual orientation, gender identity or expression, veteran status, or any other status protected under federal, state, or local law. We strongly encourage people from underrepresented groups to apply!
#BI-Remote","$102,068 /yr (est.)",51 to 200 Employees,Company - Private,Transportation & Logistics,Shipping & Trucking,2016,$25 to $100 million (USD)
"Harvard University
4.3",4.3,"Boston, MA","Big Data Engineer, Digital Transformation","Position Description
Be a pioneer in business, education, and global impact by joining the Harvard Business School Digital Transformation team - a “startup with assets,” where you will have the chance to deploy cutting-edge digital- and emerging-technology education solutions. Where else can you make a difference at the intersection of cutting-edge technology, world-class education, noble purpose, and timeless legacy?

As a Big Data Engineer, you will bring to life and implement architecture blueprints and help our team by mapping out solutions to some of their complex technical challenges. You'll provide technical expertise, mitigate risk and offer solutions tailored for their needs. From migrations of existing workloads to building advanced cloud solutions, you'll help shape and build to increase agility, improve security, reduce costs and meet utilization targets. You will work closely with the Data Engineering, Data Science, Infrastructure architecture & Platform teams and will focus on creating blueprints to nourish a culture of engineering excellence. You will report into the Sr. Managing Director Big Data Governance.

Duties and Responsibilities:
Support data engineering needs across HBS, including technical support and training in Big Data frameworks and ways of working, revision and integration of source code, to release and source code quality control
Designing and producing high performing stable end-to-end applications to perform complex processing of batch and streaming massive volumes of data in a multi-tenancy big data platform in the cloud, and output insights back to business systems according to their requirements.
Design and implement core platform capabilities, tools, processes, ways of working and conventions under agile development to support the integration of data sourcing and use cases implementation, towards reusability, to ease up delivery and ensure standardization across data deliverables in the platform.
Working with the Group architecture team to define the strategy for evolving the Big Data capability, including solution architectural decisions aligned with the platform architecture
Defining the technologies to be used on the Big Data Platform and investigating new technologies to identify where they can bring benefits
Collaborating with implementation efforts with teams of technical resources within HBS, Harvard, and vendors as needed
Performing other duties as assigned and/or as necessary
Basic Qualifications
Minimum of five years’ post-secondary education or relevant work experience
Additional Qualifications and Skills
Bachelor’s degree in Mathematics, Physics, Computer Science, Engineering, Statistics, or an equivalent technical discipline or 5 years of experience is required
3-5 years of experience in a hands-on technical role as an engineer and/or data engineering role working in large-scale big data environments is required
Experience with common build tools, unit, integration, functional and performance testing from automation perspective, and continuous delivery, under agile practices is necessary
Expert-level experience in designing, building and managing applications to process large amounts of data in a cloud ecosystem or other big data frameworks is a must
Additional Information
This role has the possibility of being a remote or hybrid position. You must reside in one of the following states: CA, CT, GA, IL, MA, MD, ME, NH, NJ, NY, RI, VA, VT or WA. There may be periodic visits to our Boston, MA based campus. In a hybrid role, you are required to be onsite at our Boston, MA based campus a determined number of days per month. Specific days and schedule will be determined between you and your manager.

We may conduct candidate interviews virtually (phone and/or via Zoom) and/or in-person for this role.

A cover letter is required to be considered for this opportunity.

Harvard Business School will not offer visa sponsorship for this opportunity.

Culture of Inclusion: The work and well-being of HBS is profoundly strengthened by the diversity of our network and our differences in background, culture, national origin, religion, sexual orientation, and life experiences. Explore more about HBS work culture here https://www.hbs.edu/employment .
Benefits
We invite you to visit Harvard’s Total Rewards website to learn more about our outstanding benefits package, which may include:
Paid Time Off: 3-4 weeks of accrued vacation time per year (3 weeks for support staff and 4 weeks for administrative/professional staff), 12 accrued sick days per year, 12.5 holidays plus a Winter Recess in December/January, 3 personal days per year (prorated based on date of hire), and up to 12 weeks of paid leave for new parents who are primary care givers.
Health and Welfare: Comprehensive medical, dental, and vision benefits, disability and life insurance programs, along with voluntary benefits. Most coverage begins as of your start date.
Work/Life and Wellness: Child and elder/adult care resources including on campus childcare centers, Employee Assistance Program, and wellness programs related to stress management, nutrition, meditation, and more.
Retirement: University-funded retirement plan with contributions from 5% to 15% of eligible compensation, based on age and earnings with full vesting after 3 years of service.
Tuition Assistance Program: Competitive program including $40 per class at the Harvard Extension School and reduced tuition through other participating Harvard graduate schools.
Tuition Reimbursement: Program that provides 75% to 90% reimbursement up to $5,250 per calendar year for eligible courses taken at other accredited institutions.
Professional Development: Programs and classes at little or no cost, including through the Harvard Center for Workplace Development and LinkedIn Learning.
Commuting and Transportation: Various commuter options handled through the Parking Office, including discounted parking, half-priced public transportation passes and pre-tax transit passes, biking benefits, and more.
Harvard Facilities Access, Discounts and Perks: Access to Harvard athletic and fitness facilities, libraries, campus events, credit union, and more, as well as discounts to various types of services (legal, financial, etc.) and cultural and leisure activities throughout metro-Boston.
Job Function
Information Technology
Department Office Location
USA - MA - Boston
Job Code
I1458P IT Rprting and Analyt Prof IV
Work Format
Hybrid (partially on-site, partially remote)
Department
Digital Transformation
Sub-Unit
-
Time Status
Full-time
Salary Grade
058
Union
00 - Non Union, Exempt or Temporary
Pre-Employment Screening
Criminal, Education, Identity
Commitment to Equity, Diversity, Inclusion, and Belonging
Harvard University views equity, diversity, inclusion, and belonging as the pathway to achieving inclusive excellence and fostering a campus culture where everyone can thrive. We strive to create a community that draws upon the widest possible pool of talent to unify excellence and diversity while fully embracing individuals from varied backgrounds, cultures, races, identities, life experiences, perspectives, beliefs, and values.
EEO Statement
We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, gender identity, sexual orientation, pregnancy and pregnancy-related conditions, or any other characteristic protected by law.",#N/A,10000+ Employees,College / University,Education,Colleges & Universities,1636,$10+ billion (USD)
#N/A,#N/A,"Phoenix, AZ",Data Warehouse Engineer - Remote,"At UnitedHealthcare, we’re simplifying the health care experience, creating healthier communities and removing barriers to quality care. The work you do here impacts the lives of millions of people for the better. Come build the health care system of tomorrow, making it more responsive, affordable and equitable. Ready to make a difference? Join us and start doing your life's best work.(sm)

You’ll enjoy the flexibility to work remotely* from anywhere within the U.S. as you take on some tough challenges.

Primary Responsibilities:
Participate in design and delivery of large-scale data warehouse infrastructure solutions to support enterprise-wide reporting, analytics, and data science needs
Work with healthcare business stakeholders to identify key business problems, then diagnose, analyze and synthesize advanced data engineering solutions in areas such as: local market health plan support; clinical and regulatory analytics; Claims Operations; Finance/Actuarial; and Quality
Develop a deep understanding of primary data domains (claims, clinical, membership, financial) and their impact on business processes
Assist team in and work directly on transition to solutions which leverage modernized technology such as Snowflake, Azure, ADF, Databricks, Kafka streaming
Demonstrate analytical and problem-solving skills, including the ability to bring clarity to complex or ambiguous scenarios, identify root causes and recommend solutions
Adhere to data engineering best practices and code quality standards
Identify and resolve technical issues and design challenges

You’ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.
Required Qualifications:
2+ years of experience in data warehouse development and design
2+ years of data analysis
2+ years of Snowflake or other SQL platforms such as Oracle
Experience with mid - high volume ETL/ELT data processes and pipelines
Experience with DevOps, CI/CD tools, Git, preferably in cloud environments
Experience with formal System Development Lifecycle and Data Quality processes
Experience and comfort to work well in a fast-paced environment

Preferred Qualifications:
Bachelor's Degree
Healthcare data experience (experience with claims, clinical or call systems)
Experience with Snowflake or Azure cloud data warehouse technologies
Experience in building and managing a technology portfolio
Experience with Agile development methodology
Proven team oriented, communicative, proactive

Careers with UnitedHealthcare. Work with a Fortune 5 organization that’s serving millions of people as we transform health care with bold ideas. Bring your energy for driving change for the better. Help us improve health access and outcomes for everyone, as we work to advance health equity, connecting people with the care they need to feel their best. As an industry leader, our commitment to improving lives is second to none.

California, Colorado, Connecticut, Nevada, New Jersey, New York, Rhode Island, or Washington Residents Only: The salary range for California, Colorado, Connecticut, Nevada, New Jersey, New York, Rhode Island or Washington residents is $67,800 to $133,100. Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, UnitedHealth Group offers benefits such as, a comprehensive benefits package, incentive and recognition programs, equity stock purchase and 401k contribution (all benefits are subject to eligibility requirements). No matter where or when you begin a career with UnitedHealth Group, you’ll find a far-reaching choice of benefits and incentives.

*All employees working remotely will be required to adhere to UnitedHealth Group’s Telecommuter Policy

At UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone-of every race, gender, sexuality, age, location and income-deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes - an enterprise priority reflected in our mission.




Diversity creates a healthier atmosphere: UnitedHealth Group is an Equal Employment Opportunity/Affirmative Action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.

UnitedHealth Group is a drug - free workplace. Candidates are required to pass a drug test before beginning employment.","$100,450 /yr (est.)",10000+ Employees,Company - Public,Healthcare,Health Care Services & Hospitals,1977,$10+ billion (USD)
"Fidelity Investments
4.3",4.3,"Durham, NC",Senior Data Engineer,"Job Description:
The Role
Do you enjoy the challenge of solving problems and working in a fast-paced environment where no two days are the same? Are you able to identify big picture ideas/concepts and translate those into well-defined and executable solutions? If so, the Senior Data Engineer role may be the right next step in your career!

The Team
Data Consulting is a group within Fidelity Workplace Investing built to serve and partner with Fidelity’s strategic clients on their data and technology needs. The Data Technology Team's dynamic and diverse skills are leveraged to complete sophisticated and exciting projects in partnership with both internal and external partners.

We design and build customized solutions to address unique scenarios on behalf of clients. We extract data from a variety of sources (Snowflake, SQLMI, client feeds/files, etc.), transform the data, then output the results based on requirements.

Our team engages across Workplace Consulting in data quality improvement efforts that span single and multiple product lines (i.e. Health and Welfare, Defined Benefit, Defined Contribution, Stock Plan Services).

The Expertise You Have and The Skills You Bring
5+ years in Technology with 3 years in developing and deploying data solutions
Strong understanding of data analysis concepts and associated tools such as SQL, Python, Stored Procedures, etc.
Extensive experience with SQL required; knowledge of MySQL, Snowflake (workspaces) preferred
Proficiency with ETL methods and tools – Informatica and Snap Logic preferred
Systems analysis background with an emphasis on legacy process decomposition and debugging/tuning/refactoring
Strong analytical and problem-solving skills, ability to research and find answers to technical challenges and learn new technologies as required
Prior exposure to data warehousing concepts and dimensional data models
Excellent written and verbal communication skills including experience writing documentation
Excellent collaboration skills to work with multiple teams in the organization
Ability to deal with ambiguity and work in a fast-paced environment
You like to take ownership, resolve issues and care about the quality of your work
Experience with WI Defined Benefit, Defined Contribution, Health & Welfare and/or Stock Plan Service business, data process flows and recordkeeping processes a plus
Knowledge of Fidelity backend product tables (WIDE, Snowflake, DBR1, etc.) a plus
Hands on experience with AWS or Azure cloud concepts is a plus
Project management experience is a plus
The Value You Deliver
Communicating with Business and Technical Consultants across WorkPlace Consulting practice areas on a wide variety of projects, you will analyze and maintain processes, identify data issues, perform root cause analysis, and help us to streamline existing processes.
Please see below for the salary range for work locations in Colorado only:
$64,000 - $100,000 per year
This position is eligible for incentive compensation or an annual bonus opportunity.
Please see below for the salary range for work locations in New York City, Westchester County, NY and Jersey City, NJ only:
N/A
Please see below for the salary range for work locations in California only:
N/A
Please see below for the salary range for work locations in Washington only:
N/A
Certifications:
Company Overview
Fidelity Investments is a privately held company with a mission to strengthen the financial well-being of our clients. We help people invest and plan for their future. We assist companies and non-profit organizations in delivering benefits to their employees. And we provide institutions and independent advisors with investment and technology solutions to help invest their own clients' money.

Join Us
At Fidelity, you'll find endless opportunities to build a meaningful career that positively impacts peoples' lives, including yours. You can take advantage of flexible benefits that support you through every stage of your career, empowering you to thrive at work and at home. Honored with a Glassdoor Employees' Choice Award, we have been recognized by our employees as a Best Place to Work in 2023. And you don't need a finance background to succeed at Fidelity—we offer a range of opportunities for learning so you can build the career you've always imagined.
At Fidelity, our goal is for most people to work flexibly in a way that balances both personal and business needs with time onsite and offsite through what we’re calling “Dynamic Working”. Most associates will have a hybrid schedule with a requirement to work onsite at a Fidelity work location for at least one week, 5 consecutive days, every four weeks. These requirements are subject to change.
We invite you to Find Your Fidelity at fidelitycareers.com.

Fidelity Investments is an equal opportunity employer. We believe that the most effective way to attract, develop and retain a diverse workforce is to build an enduring culture of inclusion and belonging.
Fidelity will reasonably accommodate applicants with disabilities who need adjustments to participate in the application or interview process. To initiate a request for an accommodation, contact the HR Accommodation Team by sending an email to accommodations @fmr.com, or by calling 800-835-5099, prompt 2, option 3.","$82,000 /yr (est.)",10000+ Employees,Company - Private,Financial Services,Investment & Asset Management,1946,$10+ billion (USD)
Pyramid Global Hospitality Corporate Offices - Boston,#N/A,"Boston, MA",Data Engineer Manager,"About Us:
At Pyramid Global Hospitality, people come first. As a company that values its employees, Pyramid Global Hospitality is dedicated to creating a supportive and inclusive work environment that fosters diversity, growth, development, and wellbeing. Our commitment to a People First culture is reflected in our approach to employee development, employee benefits and our dedication to building meaningful relationships.

Pyramid Global Hospitality offers a range of employment benefits, including comprehensive health insurance, retirement plans, and paid time off, as well as unique perks such as on-site wellness programs, local discounts, and employee rates on hotel stays. In addition, Pyramid Global Hospitality is committed to providing ongoing training and development opportunities to help our people build the skills and knowledge they need to advance their careers.

Whether you are just starting out in the hospitality industry or are a seasoned professional, Pyramid Global Hospitality offers a supportive and collaborative work environment that encourages growth and fosters success, in over 230 properties worldwide. Join their team and experience the benefits of working for a company that values its employees and is committed to creating exceptional guest experiences.

Check out this video for more information on our great company!
Location Description:

Pyramid Global Hospitality (“Pyramid”) is a leading hotel management company, operating in the US, Caribbean, and Western Europe. With portfolio revenues exceeding $3 billion, Pyramid manages 230 hotels, resorts, and conference centers, both branded and independent. The firm maintains offices in Boston (Headquarters), Cincinnati, Houston, and London. Additional information about Pyramid can be found at www.pyramidglobal.com

In 2021, Pyramid and Benchmark Resorts and Hotels merged to add an additional 59 Managed or Asset Managed Resorts and over 10,000 additional team members. The two companies share the same company culture, values and philosophies. We are growing and opportunities abound!

What really sets Pyramid apart from our competitors is our reputation as an employer. Professional growth is not just possible throughout the company but planned and encouraged. The Leadership Team at Pyramid consider team member development its first priority, understanding that success is only achieved in a workplace where every contributor is respected and recognized. This is why we deliver superior results.

There is opportunity to work directly with senior leaders, experience stretch assignments and learn hospitality management from industry giants. You will come to know a distinctive people centric culture that is at the core of all we do. The decisions we make and the paths we take are bound by a commitment to our Owners, Associates, Customers and the Communities where we work. We attract the most talented associates in the industry, and actively encourage candidates with a “hospitality spirit” who may be thinking about a career change to join our team.

Overview:
Our organization is seeking a highly motivated Manager of Data Engineering to oversee and advance our data management capabilities. You will be joining a dynamic team and playing a critical role in managing data pipelines that provide decision-making information used by our field leaders, corporate executives, and property owners. Timely, accurate information is our competitive advantage and this role is pivotal to our success.

The primary focus involves leveraging data management best practices to improve data reliability and quality. This includes ensuring day-to-day operational excellence, enabling new data source integration supporting the company’s aggressive growth plans, and evolving our data capabilities.

This leadership role involves strategic design and operational execution. You will join a team led by the Vice President of Data Sciences and play a crucial part in advancing our data strategy and supporting the company's ever-expanding appetite for information.

Essential Functions:

As the Data Engineering Manager, your responsibilities will include but not be limited to:
Support and improve the data pipeline operations – extract/transform/load data from various sources, manage storage, make data available for analytics
Resolve data integrity issues with measures to permanently address the root cause
Architect data management practices to ensure high-quality data
Drive thought-leadership for optimizing our data pipelines for advanced analytics
Implement best-practice methodologies to accelerate our data velocity
Automate data management processes for more efficient data capture, storage, and availability
Establish a data governance capability to solidify ownership across the data life cycle
Facilitate collaboration with stakeholders from across the company to create improvements in effective data use
Stay current with the latest technologies and methodologies
Other responsibilities as assigned
Qualifications:
A Bachelor's degree in Computer Science, Information Systems, or a related field
Demonstrate data management leadership experience
Microsoft Azure Synapse Analytics ecosystem
Azure SQL Data Warehousing experience
ETL expertise with large, complex, heterogeneous datasets
Accomplished work with PowerBI, star-schemas, and analytics data prep
Solid understanding of data analysis and machine learning
A track record of delivering successful data management improvement projects
Excellent communication and interpersonal skills
Strong problem-solving skills and the ability to work independently
A commitment learning about the latest technologies and methodologies

If you're a highly motivated individual with experience in data management and a desire to drive process improvement, we want to hear from you!
Compensation Range: The compensation for this position is $110,000.00/Yr. - $125,000.00/Yr. based on qualifications and experience.","$117,500 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"CVS Health
3.1",3.1,Texas,Data Engineer,"Assists in the development of large-scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs

Applies understanding of key business drivers to accomplish own work

Uses expertise, judgment and precedents to contribute to the resolution of moderately complex problems

Leads portions of initiatives of limited scope, with guidance and direction

Writes ETL (Extract / Transform / Load) processes, designs database systems and develops tools for real-time and offline analytic processing

Collaborates with client team to transform data and integrate algorithms and models into automated processes

Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines

Uses programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems

Builds data marts and data models to support clients and other internal customers

Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards

Pay Range
The typical pay range for this role is:
Minimum: 70,000
Maximum: 140,000

This pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls. The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors. This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above.

In addition to your compensation, enjoy the rewards of an organization that puts our heart into caring for our colleagues and our communities. The Company offers a full range of medical, dental, and vision benefits. Eligible employees may enroll in the Company's 401(k) retirement savings plan, and an Employee Stock Purchase Plan is also available for eligible employees. The Company provides a fully-paid term life insurance plan to eligible employees, and short-term and long term disability benefits. CVS Health also offers numerous well-being programs, education assistance, free development courses, a CVS store discount, and discount programs with participating partners. As for time off, Company employees enjoy Paid Time Off (PTO) or vacation pay, as well as paid holidays throughout the calendar year. Number of paid holidays, sick time and other time off are provided consistent with relevant state law and Company policies.

For more detailed information on available benefits, please visit
jobs.CVSHealth.com/benefits

Required Qualifications
3+ years of progressively complex related experience

Experience with bash shell scripts, UNIX utilities & UNIX Commands

Preferred Qualifications
Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources

Ability to understand complex systems and solve challenging analytical problems

Strong problem-solving skills and critical thinking ability

Strong collaboration and communication skills within and across teams

Knowledge in Java, Python, Hive, Cassandra, Pig, MySQL or NoSQL or similar

Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment

Experience building data transformation and processing solutions

Has strong knowledge of large-scale search applications and building high volume data pipelines

Education
Bachelor's degree required

Master’s degree or PhD preferred

Business Overview
Bring your heart to CVS Health Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver. Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable. We strive to promote and sustain a culture of diversity, inclusion and belonging every day. CVS Health is an affirmative action employer, and is an equal opportunity employer, as are the physician-owned businesses for which CVS Health provides management services. We do not discriminate in recruiting, hiring, promotion, or any other personnel action based on race, ethnicity, color, national origin, sex/gender, sexual orientation, gender identity or expression, religion, age, disability, protected veteran status, or any other characteristic protected by applicable federal, state, or local law. We proudly support and encourage people with military experience (active, veterans, reservists and National Guard) as well as military spouses to apply for CVS Health job opportunities.","$105,000 /yr (est.)",10000+ Employees,Company - Public,Healthcare,Health Care Services & Hospitals,1963,$10+ billion (USD)
Cox powered by Atrium,#N/A,United States,Data Engineer - Remote (2023-5940),"Minimum Qualifications:
A minimum of 3+ years related work experience
Working experience with data modeling, data access, schemas, and data storage techniques within Snowflake
Working experience with batch and real-time data processing frameworks
Working experience working with relational databases such as SQL
Working experience with business intelligence tools and platforms
Working experience with data quality tools
Experience with ETL processes and tools
Preferred Qualifications:
Working experience with Snowflake
Working experience with Google Analytics BigQuery
Working Experience with Tableau Server
Working experience with SQL Server Integration Services
Pay Range:
$40-$49/hr
Requisition Disclaimer:
This job posting is for a temporary role as an employee of Atrium on assignment at Cox. The individual selected for this role will be offered the role as an employee of Atrium; compensation, medical benefits, fringe benefits and other terms and conditions of employment shall be presented by Atrium upon offer. The pay rate range provided is a reasonable estimate of the anticipated compensation range for this job at the time of posting. The actual pay rate will be based on a number of factors, including skills, competencies, experience, location and/or being pursued and other job-related factors permitted by law. In addition, this role will be eligible for overtime pay, in accordance with federal and state requirements

By applying for this position you agree to the Atrium Terms and Conditions. Agreeing to these terms, includes permission to use the email address and mobile phone number you provide during the application process or throughout the duration of your prospective or actual employees to notify you of job openings, profiles, articles, news, and other employment-related information, as well as to notify you of special promotions or additional products and services offered by us or our affiliates and partners (collectively, “Atrium Alerts”). Atrium Alerts may be sent by email, phone or text message. Your personal information will be safely stored in our database. Atrium does not sell your personal information to third parties. Text message and data rates may apply. To OPT OUT of text messaging or to modify your communication preferences for Atrium Alerts at any time, please contact us at privacyadministrator@atriumstaff.com.

If you do not agree with the Atrium Terms and Conditions, you can still complete your application for this position by emailing your resume to our team at coxrecruit@atriumworks.com. Please include the job title in the subject of your email.

As a woman-owned firm, Atrium values diversity. We are an equal opportunity employer and will consider all applications without regard to race, sex, age, color, religion, national origin, veteran status, disability, genetic information or any other characteristic protected by law. We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.
Posting: #zip",$44.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
#N/A,#N/A,"Minnetonka, MN",Data Engineer - Remote,"At UnitedHealthcare, we’re simplifying the health care experience, creating healthier communities and removing barriers to quality care. The work you do here impacts the lives of millions of people for the better. Come build the health care system of tomorrow, making it more responsive, affordable and equitable. Ready to make a difference? Join us and start doing your life's best work.(sm)

Positions in this function are responsible for the management and manipulation of mostly structured data, with a focus on building business intelligence tools, conducting analysis, performing normalization operations, and assuring data quality. Depending on the specific role and business line, example responsibilities in this function could include creating specifications to bring data into a common structure, creating product specifications and models, developing data solutions to support analyses, performing analysis, interpreting results, developing actionable insights and presenting recommendations for use across the company. Roles in this function could partner with stakeholders to understand data requirements and develop tools and models such as segmentation, dashboards, data visualizations, decision aids and business case analysis to support the organization. Other roles involved could include producing and managing the delivery of activity and value analytics to external stakeholders and clients. Team members will typically use business intelligence, data visualization, query, analytic and statistical software to build solutions, perform analysis and interpret data.
General Job Profile
This position will work with business partners and executive level stakeholders within our Core Operations to develop advanced data and reporting solutions. This position provides visibility, analytical, ad hoc and development capabilities managing data and insights from different functional areas across the Core Operations landscape.

You’ll enjoy the flexibility to work remotely* from anywhere within the U.S. as you take on some tough challenges.

Primary Responsibilities:
Working in a matrix environment by partnering with other partners in the Core Operations team to deliver data and reporting needed to support claim audit and operations
Works in an Agile framework within a matrix environment working in sprints and utilizing agile tools (e.g., Swift Kanban)
Instills an Agile framework within the team and across the matrix environment to operate as applicable and fully utilizing KanBan tools
Build, maintain and/or adhere to a structured data management process to be used across all datasets with a focus on quality and accuracy
Identify and participate in the resolution of data integrity issues and organizational problems

Functional Competencies
Demonstrate and apply understanding of UnitedHealth Group's business (e.g., specific business capabilities, functions, processes, and business cycles) and knowledge of operations, goals, and policies and procedures of internal business partners (e.g., information contacts) to provide effective support to internal and/or external customers
Manage and protect data, adhering to applicable legal/regulatory requirements (e.g., HIPAA, PHI, PII, DOI, state and federal regulations)
Propose and/or define long-term strategies for implementing process and/or data and reporting improvements
Identify and/or provide opportunities for additional training and learning to support process and report improvements
Create and/or update presentation documents and materials to summarize results (e.g., written reports; PowerPoint deck; Tableau; graphs and/or charts)
Develop business context diagrams (e.g., business data flows, process flows) to analyze/confirm the definition of project requirements
Demonstrate understanding of the difference between business requirements and technical solutions and define approach for storing and updating business requirements
Collaborate with business and technical stakeholders (e.g., business owners, process owners, domain experts) to identify specific business requirements
Perform reviews with all stakeholders to obtain approval/signoff of project requirements documents (e.g., walkthroughs)
Update progress to project schedule to track/measure progress one’s progress fulfilling aligned tasks. In addition to supporting ongoing monitoring by keeping project documentation or applications updated (e.g., Swift KanBan)

You’ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.
Required Qualifications:
Bachelor’s degree or Work Equivalent
5+ years of SQL/TSQL Development experience
5+ years of experience relational databases, database structures and design, systems design, data management, data warehouse
3+ years of experience with Data Modeling, ETL construction with advanced job scheduling
3+ years of experience performing significant data analysis and report development
3+ years recent experience in a customer facing role leading and/or managing projects, including requirements and documentation
3+ years designing reporting databases/data warehouses
2+ years of developing reports in Tableau and/or PowerBI
Expert level of programming and troubleshooting knowledge
Advanced proficiency in Microsoft Excel, Word, Power Point and Visio

Careers with UnitedHealthcare. Work with a Fortune 5 organization that’s serving millions of people as we transform health care with bold ideas. Bring your energy for driving change for the better. Help us improve health access and outcomes for everyone, as we work to advance health equity, connecting people with the care they need to feel their best. As an industry leader, our commitment to improving lives is second to none.

California, Colorado, Connecticut, Nevada, New York, Rhode Island, or Washington Residents Only: T he salary range for California, Colorado, Connecticut, Nevada, New York, Rhode Island or Washington residents is $67,800 to $133,100. Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, UnitedHealth Group offers benefits such as, a comprehensive benefits package, incentive and recognition programs, equity stock purchase and 401k contribution (all benefits are subject to eligibility requirements). No matter where or when you begin a career with UnitedHealth Group, you’ll find a far-reaching choice of benefits and incentives.

*All employees working remotely will be required to adhere to UnitedHealth Group’s Telecommuter Policy

At UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone-of every race, gender, sexuality, age, location and income-deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes - an enterprise priority reflected in our mission.




Diversity creates a healthier atmosphere: UnitedHealth Group is an Equal Employment Opportunity/Affirmative Action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.

UnitedHealth Group is a drug - free workplace. Candidates are required to pass a drug test before beginning employment.","$100,450 /yr (est.)",10000+ Employees,Company - Public,Healthcare,Health Care Services & Hospitals,1977,$10+ billion (USD)
"Kent State University
4.2",4.2,Ohio,Lead Data Engineer,"Participate in architectural design, optimization, implementation, and administration of our modern data warehousing environment. Work with analysts across the university to determine needs and develop data models to support those needs . Optimize data flow and collection from and between multiple complex cloud, on-premises, and hybrid systems. Reports to designated supervisor.

Additional Basic Function – if applicable:

The Data Management & Analytics team at Kent State University is looking for a Lead Data Engineer to join our team to help us with some exciting data adventures! We are currently working on building a modern analytics solution. Come join us while we build a data lake and data marts that will be used to create a university wide and system agnostic view our data to enable our decision makers to get the data they need and to provide new opportunities in support of predictive analytics and machine learning. Our team is also working with others in the Division of Information Technology to build a data pipeline to enable a consistent, governed way to provide data for application and system integrations.

Examples of Duties:

Duties/essential functions may include, but not be limited to, the following:
Participate in and/or facilitate data discovery sessions with business subject matter experts to translate business needs into enterprise dimensional and relational data models for reporting and analytics.
Design, create and maintain optimal data lake, data mart, data warehouse, and data integration architectures.
Assemble university-wide complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal data process improvements such as automating manual processes, optimizing data delivery, ensuring data integrity, re-designing infrastructure for greater scalability, etc.
Build the infrastructure and integrations required for optimal ETL/ELT of data from a wide variety of complex cloud, hybrid, and on-premises data sources.
Participate in development and implementation of data lifecycle of the modern data warehousing environment.
Lead and educate and mentor team members on best practices. Perform code and model design reviews.
Provide oversight / guidance related to data issues needing technical expertise.
Perform related duties as assigned.
JOB COMPETENCIES:
Communicate effectively with technical and non-technical users.
Maintain cooperative working relationships.
Manage time and effectively balance multiple evolving priorities.
Work effectively with very limited oversight.
Establish estimates and timelines for specific applications/projects and take direct accountability for results.
Establish focused, measurable goals for self and others.

Additional Examples of Duties – if applicable:

Minimum Qualifications:

Bachelor’s degree in computer science, information technology or a related field of study. Minimum of seven years progressive experience in the following:
Analyzing and translating business needs into long-term solution data models.
Experience data modeling principles/methods to design dimensional data models from the ground up.
Working with relational databases, as well as working familiarity with a variety of databases.
Advanced logical data model design, data warehouse design, and data integration.
Performing root cause analysis to perform troubleshooting on data flows through complex systems.
Analyzing internal and external data and processes for a business to build models to answer specific business questions.
Building processes for data transformation.
Developing and maintaining system documentation, metadata, data standards, and data quality metrics for data management systems.
Experience writing advanced SQL.
License/Certification:

Knowledge Of:
Advanced logical data design, data warehouse design, and data integration as well as the management of web content or other unstructured data
Common software application packages and tools for performance monitoring and issues tracking
Testing practices, application debugging, and troubleshooting procedures
Software development life cycle, structured programming, object-oriented design and development techniques, and change management
Managing the development and maintenance of system documentation, metadata, data standards, and data quality metrics for the data management system
Advanced working knowledge of SQL
Skill In:
Time management with the ability to set priorities to coordinate multiple assignments with fluctuating and time-sensitive deadlines
Written and interpersonal communication, with the ability to present complex technical information in a clear and concise manner to a variety of audiences
Ability To:
Foster positive and professional working relationships; effectively handle interpersonal interactions at all levels; and respond appropriately to conflicts and problems
Work with technical and non-technical staff to identify user needs and translate them into technology-based solutions
Keep abreast of industry trends

Preferred Qualifications – if applicable:
Experience working in a higher education institution.
Experience facilitating data discovery sessions to gather requirements needed to build data models.
Experience building solutions to support analytics, machine learning, and data science.
Experience with datalake and data warehouse technologies.
Experience with data analytics tools.
Experience developing solutions that are incorporated into all aspects of an organization’s data services to achieve data governance and data quality best practices.

Assessments:

Asterisk (*) indicates knowledge, skills, abilities which require assessments

Working Conditions / Physical Requirements:

Working Schedule:

Flexible hours and remote work options available

Additional Information:

Must pass a security check.

We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.","$96,808 /yr (est.)",5001 to 10000 Employees,College / University,Education,Colleges & Universities,1910,$500 million to $1 billion (USD)
"Long Finch Technologies
4.6",4.6,"Bellevue, WA",Data Engineer- AZURE with Strong SPARK,"Job Title: Data Engineer
Location: Bellevue, WA Onsite
Duration: Full Time
Job Description
Data Engineering experience primarily on Spark. Someone who has worked on Azure cloud with knowledge on Azure DEVOPS (CI/CD & Infrastructure as a code), ADF, ADW & Power BI. Apache Nifi will be good to have.Power BI: Understand business requirements to set functional specifications for reporting applications Build automated reports and dashboards with the help of Power BI reporting tool Be experienced in tools and systems on MS SQL Server BI Stack, including SSRS and TSQL, Power Query, MDX, PowerBI, and DAX Be able to quickly shape data into reporting and analytics solutions Have knowledge of database fundamentals such as multidimensional database design, relational database design, and more Create functional reporting Study, analyze and understand business requirements in context to business intelligence. Design and map data models to shift raw data into meaningful insights. Utilize Power BI to build interactive and visually appealing dashboards and reports. Spot key performance indicators with apt objectives Run DAX queries and functions in Power BI Should have an edge over making DAX queries in Power BI desktop. Developing visual reports, KPI scorecards, and dashboards using Power BI desktop. Connecting data sources, importing data, and transforming data for Business intelligence.
Job Type: Full-time
Salary: $100,000.00 - $105,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Vision insurance
Schedule:
8 hour shift
Ability to commute/relocate:
Bellevue, WA 98004: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 5 years (Required)
Azure: 3 years (Required)
Spark: 3 years (Required)
Work Location: One location","$102,500 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2012,$25 to $100 million (USD)
"Tetra Tech
3.9",3.9,"Arlington, VA",Data Engineer/Data Analyst,"Position Summary:
Segue Technologies, A Tetra Tech Company seeks a Data Engineer/Data Analyst to support our Federal Government clients. This position will leverage data to improve Business Intelligence (BI) in order to provide more value to end users.
This is a full-time remote position. Occasional travel expected to the Indian Affairs Office in Albuquerque, NM or nearest Indian Affairs office (less than 5%)

Job Duties and Responsibilities include but are not limited to:
Assess data quality, identifying suspect data via value distributions, cross field correlations, etc.
Formulate Extraction, Transformation, and Data Loading strategies to deal with anomalous data in a manner where the impact on the business domain is understood
Model, map and design data structures and data architecture to implement data mart or warehouse
Identify patterns and relationships within data that have explanatory power within a business domain, using appropriate methodologies to provide different types of information, such as Descriptive, Inferential, and Predictive
Design and develop data products that fulfill the business needs, such as reports, visualizations, dashboards, analyses, etc.
Perform and interpret data studies and product experiments concerning new data sources or new uses for existing data sources
Develop prototypes, proof of concepts, predictive models, and custom analysis
Design and build new data set processes for modeling and data mining
Search data for relevant patterns and significant relationships
Recognize non-uniformity of data and resolve inconsistencies
Devise tools to help business users gain new insights and improve overall business performance
Additional tasks as required

Required Skills:
Bachelor’s degree in Data Sciences, Statistics, Computer Science, IT Systems, or another technical field, or related major. Relevant technical experience may be substituted for the bachelor’s degree
Must have expert level Tableau server and cloud skillset and experience
Must have expert level proficiency in using Microsoft Excel with strong emphasis on data analysis
Experience developing data pipelines and ETL for dashboards, reports and for data warehousing
Ability to organize, interpret and present data
Experience identifying patterns and relationships within data
Relational database design, modeling (theory) experience
Data warehousing design, modeling (theory) experience
Data quality assessment and impact management experience
Experience communicating and working with stakeholders across the enterprise
Experience architecting and developing data pipeline and related processes
Experience working with data modeling/design tools such as SQL Developer Designer, etc.
Experience with SQL, SQL Server RDBMS, NoSQL databases, TOAD, and Data modeling tools
Excellent problem solving, debugging, and troubleshooting skills
Ability to prepare and interpret complex reports and dashboards
Ability to use independent judgment and to know when to escalate issues
Strong communication skills in multiple formats, such as writing, explaining, listening, asking, teaching, mentoring, etc.
Must hold or be able to pass a Federal Background investigation to obtain a T1 (also known as Public Trust or NACI)

Desired Skills:
Experience using AWS Cloud resources and data services
Experience working with IBM Maximo and Facilities related data
Segue Technologies is a wholly owned subsidiary of Tetra Tech, Inc. Segue is based out of Arlington, VA, with a presence in 14 states and DC. We support Federal and DoD organizations to develop and enhance mission-critical business systems. We provide custom software applications, solve data management problems, and support the evolution of the mobile workforce.

At Segue Technologies health and safety play a vital role in our success. Segue’s employees work together to comply with all applicable health & safety practices and protocols, including health orders and regulations related to COVID-19 that are mandated by local, state and federal authorities.

Our compensation package includes: Competitive Annual Salaries, Rewards and Recognition Program, Employee Stock Purchase Plan, Paid Time Off that Increases with Seniority, Paid Holidays, Life and Disability Insurance, 401K Retirement Plan with Employer Contribution, Dental, Vision, and Health Insurance, Flexible Spending Account, Tuition and Training Reimbursement.
Segue Technologies, A Tetra Tech Company is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by law. Please visit our website, www.seguetech.com/careers, to submit an application.","$85,833 /yr (est.)",10000+ Employees,Company - Public,"Construction, Repair & Maintenance Services",Architectural & Engineering Services,1966,$1 to $5 billion (USD)
"Rokt
3.8",3.8,"New York, NY","Senior Software Engineer, Data Engineering - NYC","Company Description

Jobs for Humanity is dedicated to building an inclusive and just employment ecosystem. Therefore, we have dedicated this job posting to individuals coming from the following communities: Refugee, Neurodivergent, Single Parent, Blind or Low Vision, Deaf or hard of hearding, Black, Hispanic, Asian, Military Veterans, the Elderly, the LGBTQ, and Justice Impacted individuals. If you identify with any of the following communities do not hesitate to register.

Company Name: Rokt

Job Description

About Rokt
Rokt is the global leader in ecommerce technology, helping companies seize the full potential of every transaction moment to grow revenue and acquire new customers at scale. Live Nation, Groupon, Staples, Lands' End, Fanatics, UrbanStems, GoDaddy, Vistaprint and HelloFresh are among the more than 2,500 leading global businesses and advertisers that are using Rokt's solutions to drive more value through every transaction by offering highly relevant messages to their customers at the moment they are most likely to convert.
With our December 2021 Series E raise of USD$325M, Rokt is expanding rapidly and globally – operating in 19 countries across North America, Europe and the Asia-Pacific region with the largest office in NYC and a major R&D hub in Sydney. With annual revenues of more than US$200M and vibrant company culture, Rokt has been listed in ‘Great Places to Work’ in the US and Australia. Our award-winning culture is guided by our five core values: Smart with Humility, Own the Outcomes, Force for Good, Conquer New Frontiers, and Enjoy the Ride. These values help us attract, engage, and develop the right talent around the globe and ensure we have the right conditions to do our best work. Keen to join a fast-growing company and a vibrant culture? Learn more at rokt.com.
The Rokt engineering team builds best-in-class ecommerce technology that provides personalized and relevant experiences for customers globally and empowers marketers with sophisticated, AI-driven tooling to better understand consumers. Our bespoke platform handles millions of transactions per day and considers billions of data points which give engineers the opportunity to build technology at scale, collaborate across teams and gain exposure to a wide range of technology. We are expanding rapidly in our major R&D centers in NYC and Sydney. We are passionate about using intelligent systems to improve the transaction moment for retailers everywhere. Come join us and build the future!
Requirements
About the role
We’re working on building a platform for reporting and analytics that is able to handle huge amounts of data in a real-time fashion that would allow us to uncover new insights and help us make decisions.
Our goal is to unlock data and make it available to various users starting from other engineers to end business users and clients. We value pragmatic solutions and simplicity that help us build reliable and fast systems.
Outcomes & responsibilities
Build distributed, high-volume data pipelines and storage that power our reporting and analytics
Work on real-time distributed OLAP custom solutions
Do it with Spark, Kafka, Airflow, and other open-source technologies
Work all over the stack, moving fluidly between programming languages: Scala, Python, and more
You'll help define the processes and infrastructure to transform and make data readily available across the company
Join a tightly knit team solving hard problems the right way
Own meaningful parts of our service, have an impact, grow with the company
Take responsibility for system health, monitoring and alerting, and CI/CD pipelines
Support and mentor other engineers on best practices, architecture, and quality
Capabilities & requirements
You have built and operated data pipelines for real customers in production systems
You are fluent in several programming languages (JVM & otherwise)
You’ve worked with data stores and/or data warehouses, such as AWS Redshift, Snowflake, Clickhouse, or others
You have hands-on experience with BigData frameworks (Hadoop, Hive, Spark, etc.)
You’re able to explain advanced technical concepts in a simple manner and cater to your audience
You enjoy wrangling huge datasets and helping others unlock new insights
You’re concerned about resiliency, high-availability, data quality, and other aspects of a critical system
Benefits
Force for Good. We actively invest in the growth of our people and the strengthening of our communities. Our NYC office is 100% vaccinated to keep our employees and community safe and healthy. We require all Rokt’stars as well as anyone else who will be onsite at the Rokt NYC office – clients, contractors, vendors, and suppliers – to show proof of vaccination and their booster shot.
Work with the greatest talent in town. Our recruiting process is tough. We hold a high bar because we have a high-performing, high-velocity culture - we only want the brightest and the best.
Join a community. We believe the best things happen when we come together to solve complex problems and make meaningful connections with each other through interest groups, sports clubs, and social events.
Accelerate your career. Develop through our global training events, ‘Level Up’ investment, online training courses, and our fantastic people leaders. Take your career to Rokt’speed - Grow your career in our rapidly growing company.
Take a break. When you work hard, we know you also need to rest. We offer generous time off and parental leave policies, as well as mental health and wellness days for all employees. We also offer a paid Rokt’star Sabbatical for employees who have been with us for 3 years or more.
Stay happy and healthy. Enjoy catered lunch 3 times a week and healthy snacks in the office. Plus join the gym on us! In the US, access generous retirement plans like a 4% dollar-for-dollar 401K matching plan and get fully funded premium health insurance for your whole family. And our NYC office is dog-friendly!
Become a shareholder. All Rokt’stars have stock options. If we succeed, everyone enjoys the upside.
See the world! Along with our global all-staff events in amazing locations (Phuket, Thailand in January 2020, Hawaii in May 2022), we also offer generous relocation packages for those interested in moving to another Rokt office. We have cool offices in great cities - New York, Sydney, London, Singapore, Tokyo.
Get the best of both worlds with a hybrid workplace. We currently work 3 days a week in office, allowing you to enjoy the best of both worlds (please note: this is subject to change based on the needs of the business and some support roles still require a full time presence). One week per quarter, you also have the flexibility to work from anywhere.
We believe in equality. Rokt is an Equal Opportunity Employer and recognizes that a diverse workforce is crucial to our success as a business. We would love you to apply for one of our open roles - irrespective of socio-economic status or background, age, gender identity, race, religion, sexual orientation, color, pregnancy, carer/family responsibilities, national and social origin, political opinion, marital, veteran, or disability status.


Salary range: $120,000 - $200,000 / year
#LI-Hybrid","$160,000 /yr (est.)",201 to 500 Employees,Company - Private,Media & Communication,Advertising & Public Relations,2012,Unknown / Non-Applicable
"Inovalon
3.1",3.1,United States,Senior Software Data Engineer,"Inovalon was founded in 1998 on the belief that technology, and data specifically, would empower the transformation of the entire healthcare ecosystem for the better, improving both outcomes and economics. At Inovalon, we believe that when our customers are successful in their missions, healthcare improves. Therefore, we focus on empowering them with data-driven solutions. And the momentum is building.
Together, as ONE Inovalon, we are a united force delivering solutions that address healthcare's greatest needs. Through our mission-based culture of inclusion and innovation, our organization brings value not just to our customers, but to the millions of patients and members they serve.
Overview: The Senior Data Engineer is responsible for contributing to data warehouse/lake, data operations organization. They will support existing data operations such as data pipeline, ETL, data lake / warehouse, data structure development, and troubleshooting of data issues. They will be part of the team which is building next generation data & reporting platform. This platform will cater internal business stakeholders and external customers to provide insights & forecasting to understand current state of business, improve decision-making for their tactical and strategic goals & KPIs. This position may require independent work, sharing information and assisting others with work request.
Duties and Responsibilities:
Work with the agile team to participate in agile ceremonies like grooming, planning, standup, retrospective, demos
Actively contribute to grooming, and standup, create & update tasks, estimate and status
Write complex queries, stored procedures, functions, SSIS Packages for various job execution
Work with data architects and business analysts to create a logical data model and create DDL scripts for physical model creation
Design and develop modern ETL framework utilizing tools like ADF (Azure Data Factory), MS-SSIS etc
Design and develop ETL pipelines, using SQL, Stored procedures/functions to extract data from various sources and load into warehouse
Design STAR or SNOWFLAKE database schema utilizing industry best practices to build Data warehouse, data marts, views, cubes and data sets/products
Design and code various data architecture component like data validation, cleansing, de-duping, Symantec layer etc
Design and develop data export frameworks to extract data from the warehouse, transform, pre-aggregate, perform calculations and load into various data marts for Analytics use
Design & develop configurable data export framework to extract data from Data warehouse and data marts to generate reports for internal and external customers in .csv, flat files and
Work on large data to ensure configurable ingestion of data, dynamic rule & validation of data, cleansing, transforming and loading into the data warehouse
Design and implement data validation and quality checks to ensure the accuracy and completeness of the data in the data warehouse
Build data marts which will be utilized by Analytics tools like PowerBI and work with PowerBI developer to optimize database schema and queries
Perform performance of queries and data processing, identify and resolve any issues looking at query plans, create appropriate indexes, resolve dead locks and create table hints
Participate in design discussions, data analysis, data model creation etc
Product quality design diagrams (using MS-Visio, Draw.io etc) and documentations (MS-Work, Excel etc)
Maintain compliance with Inovalon's policies, procedures and mission statement;
Adhere to all confidentiality and HIPAA requirements as outlined within Inovalon's Operating Policies and Procedures in all ways and at all times with respect to any aspect of the data handled or services rendered in the undertaking of the position; and
Fulfill those responsibilities and/or duties that may be reasonably provided by Inovalon for the purpose of achieving operational and financial success of the Company.
Job Requirements:
Minimum of 7 years industry experience working in data & reporting area, knowledge of healthcare data will be a plus
5+ years working on Datawarehouse, ETL process/pipeline, Data Workflow, Query Plans & Optimization
5+ experience in MS SQL, T-SQL, ETL Jobs
5+ experience in Microsoft tools like SSMS, SSIS, SQL Server
5+ years' experience in writing query plans, indexes etc
2+ years working with Analytics tools (like Tableau, PowerBI – Preferred)
2+ years' experience working on Azure Cloud is preferred utilizing ADF (Azure Data Factory), Azure Delta Lake, Azure SQL Server, Data Sync, Log Insights and Analytics
Experience working with Role based security at database, ETL jobs, data exports level
Strong understanding of database concepts and schema (like star, snowflake schema)
Experience with HIPPA and PHI will be a plus
Ability to effectively communicate with internal and external customers
Excellent verbal and written communication skills
Excellent computer proficiency (MS Office – Word, Excel and Outlook)
Must be able to work under pressure and meet deadlines; and
Ability to work independently and to carry out tasks to completion following standard accepted practices
Education:
BS degree in Computer Science or Computer Engineering, Business, or equivalent experience.
Physical Demands and Work Environment:
Sedentary work (i.e., sitting for long periods of time);
Exerting up to 10 pounds of force occasionally and/or negligible amount of force;
Frequently or constantly to lift, carry push, pull or otherwise move objects and repetitive motions;
Subject to inside environmental conditions; and
Some travel (less than 10%) may be required for this position, primarily for training and collaboration purposes.
Studies have shown that women and people of color are less likely to apply for jobs unless they believe they meet every one of the qualifications listed in a job description. If you don't meet every qualification listed but are excited about our mission and the work described, we encourage you to apply regardless. Inovalon is most interested in finding the best candidate for the job and you may be just the right person for this or other roles.
By embracing diversity, equity and inclusion we enhance our work environment and drive business success. Inovalon strives to reflect the diversity of the communities where we operate and of our clients and everyone whom we serve. We endeavor to create a culture of inclusion in which our associates feel empowered to bring their full, authentic selves to work and pursue their professional goals in an equitable setting. We understand that by fostering this type of culture, and welcoming different perspectives, we generate innovation and growth.
Inovalon is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirement.
The Company maintains a drug free work environment for all of its associates, which includes employees, contractors and vendors. It is unlawful for associates to manufacture, sell, distribute, dispense, possess or use any controlled substance or marijuana in the workplace and doing so will result in disciplinary action, up to and including termination of employment or the contracted relationship.",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Software Development,1998,$500 million to $1 billion (USD)
"Ghirardelli Chocolate Company
3.6",3.6,"Rogers, AR",Sr. Category Analyst - Data Engineer,"This Sr. Category Analyst - Data Engineer will join our Category Advisor Team and be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for the advisor team. The ideal candidate is an experienced data pipeline builder who enjoys optimizing data systems and building them from the ground up, guiding continuous business improvement and providing actionable insights to maximize Walmart's category volume and share.


The Category Advisor Team Leads the planning and development with Walmart in the creation of best-in-class chocolate category insights and planograms. Works directly with the Walmart buyer and Category Advisor Team to analyze, optimize and execute confection business with a focus on premium chocolate.
Requirements:
Category Analytics Management
Create data tools for analytics that assist in building and optimizing evaluation of Walmart's category strategies and 4P tactical solutions
Represent Premium data analytics in strategic category settings, operating in a firewall environment
Develops a deep understanding of Walmart's strategies, initiatives and performance
Analyzes data to help influence broader Advisor team and Buyer
Examples:
Planogram optimization & performance tracking
Assortment evaluation
Pricing & Promotional strategy
Display execution & performance tracking
Other Advisory Team Responsibilities
Attend store visits to assess planograms and store level conditions
Assist with custom research analytics and reporting
Assist with shelf resets in Walmart layout center and in Ghirardelli office – may include transporting product for resets","$77,235 /yr (est.)",1001 to 5000 Employees,Company - Private,Manufacturing,Food & Beverage Manufacturing,1852,$500 million to $1 billion (USD)
"Dealer Tire
3.6",3.6,"Cleveland, OH",Data Engineer,"Who We Are
We’re Dealer Tire, a family-owned, international distributor of tires and parts established in 1918 in Cleveland, OH. We’re laser focused on helping the world’s largest and most trusted auto manufacturers grow their tire business—in fact, we’ve sold more than 60 million tires to date. We’re a thriving company, and we’re looking for driven individuals to join our team. That’s where you come in!
As a Data Engineer at Dealer Tire, you will be part of a highly skilled team of innovative data professionals who are responsible for designing and implementing our enterprise data lake, data model, and the ETL/ELT pipelines that feed our business and analytical systems. You and your teammates will collaborate with internal and external customers and our affiliate companies, empowering them to solve business problems and gain powerful insights using high-performance datasets, enterprise analytics systems, and self-service tools such as Alteryx, R, Python, and Power BI.
As a Data Engineer, your essential job functions will include the following
Collaborate with business and IT teams to implement a comprehensive and easily expandable Enterprise Data Model based on business need.
Be a thought leader on future data usage (Data Futurist) and a catalyst for converting data into a knowledge base.
Contribute, build, and participate in the design and enhancement of our enterprise data model, enterprise analytics systems, data marts, and data warehouse/data lake.
Build robust, scalable, and high-performing ETL/ELT solutions involving structured & unstructured data.
Leverage appropriate design patterns for the problem being solved, such as near-real-time/change data capture, batch processing, streaming data, etc.
Collaborate with internal and external customers, our affiliates, and other IT teams to define and implement a roadmap for the enterprise data model and the systems that support it, including the maintenance of consistent data entity and element definitions across multiple environments.
Participate in design reviews to foster team accountability and maintain a high standard of quality.
Evangelize and democratize our data assets and empower users with self-service tools and training to enable them to leverage those assets effectively.
Engage with multiple concurrent projects both within your team and with other teams.
Articulate ideas and architectural concepts clearly and concisely through verbal and written communication.
Monitor and measure system and process performance and make corrections and enhancements where needed as part of a focus on continual improvement.
Provide on-call/after-hours support for processes and systems owned by the team on a rotating basis.
Other Duties as Assigned
Position Requirements
3-5 years of experience in a Data Engineer or comparable role is required.
Strong skills in querying databases (t-SQL, PL-SQL, etc.), data modeling (including data warehouse design concepts such as star schema design, etc.), data engineering, and data reverse engineering skills are required.
Strong verbal and written communication skills are required.
Ability to work cross-functionally in a fast-paced, high growth environment and manage multiple concurrent workstreams and priorities is a must.
A bachelor’s degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field is a plus.
Experience with any of the following technologies is a plus: SAP Data Integrator, Syniti Data Replication, Microsoft SQL Server, SQL Server Integration Services (SSIS), Oracle Business Intelligence/Oracle Analytics Server, JD Edwards, DB2, Python, Power Shell, R, Scala, Go, Amazon AWS (EC2 instances, Redshift, lambda functions, S3, etc.), Google Analytics, Google Big Query, Snowflake, Alteryx, Power BI, and Windows and Linux servers.
Experience performing root cause analysis on datasets and processes to answer specific business questions, identify opportunities for improvement, or resolve system or process issues is a plus.
General expertise in enterprise IT architecture (databases, ERP, middleware, UI, networking, infrastructure) is a plus.
Experience implementing high availability (HA) and disaster recovery (DR) solutions is a plus.
Competencies Required
Results Orientation
Agility
Initiative
Influence
Customer Focus
Business Acumen
Strategic Thinking
Organizational Agility
Relationship Building
Physical Job Requirements
Continuous viewing from and inputting data to a computer screen
Sitting for long periods of time
Travel as necessary
Drug Policy:
Dealer Tire is a drug-free environment. All applicants being considered for employment must pass a pre-employment drug screen before beginning work.
Why Dealer Tire: An amazing opportunity to join a growing organization, built on the efforts of hard working, innovative, and team-oriented people. We offer a competitive salary + bonus, and a comprehensive benefit package including: paid time off, medical, dental, vision, and 401k matching (50% on the dollar up to 7% of employee contribution).
EOE Statement: Dealer Tire is an Equal Employment Opportunity (EEO) employer and does not discriminate on the basis of race, color, national origin, religion, gender, age, veteran status, political affiliation, sexual orientation, marital status or disability (in compliance with the Americans with Disabilities Act*), or any other legally protected status, with respect to employment opportunities.
ADA Disclosure: Any candidate who feels that they may need an accommodation to complete this application, or any portions of same, based on the impact of a disability should contact Dealer Tire’s Human Resources Department to discuss your specific needs. Please feel free to contact us at ADAAAccommodation@dealertire.com or via phone at 833-483-8232.","$92,386 /yr (est.)",501 to 1000 Employees,Company - Private,Retail & Wholesale,Wholesale,2001,$1 to $5 billion (USD)
"Pacific Northwest National Laboratory
4.0",4.0,United States,Data Engineer 3,"Overview:
The Pacific Northwest National Laboratory (PNNL) and the Environmental Molecular Sciences Laboratory (EMSL) seek a junior-level Data Engineer to further EMSL’s data management and integrated research goals. The candidate should have experience in software development and managing scientific data using FAIR principles.
Rockstar Rewards:
Employees and their families are offered medical insurance, dental insurance, vision insurance, health savings account, flexible spending accounts, basic life insurance, disability insurance*, employee assistance program, business travel insurance, tuition assistance, supplemental parental bonding leave**, surrogacy and adoption assistance, and fertility support. Employees are automatically enrolled in our company funded pension plan* and may enroll in our 401k savings plan. Employees may accrue up to 120 vacation hours per year and may receive ten paid holidays per year.
Research Associates excluded.
**Once eligibility requirements are met.

Click Here For Rockstar Rewards
Responsibilities:
Developing and optimizing capabilities at the division level and identify mission challenges and formulate engineering solutions methodically.
Generates new ideas for proposals and business development opportunities of small to medium proposals. Stay current about data management and database technology developments
Collaborate with researchers on the data development and applying analysis techniques to a rich variety of scientific instrument data form experiment
Contribute to innovative application of advanced simulation and modeling to research problems relevant to DOE-BER missions where computational approaches are significant.
Design, build, deploy & maintain data pipelines, ETL workflows, data, and computing environments; or implement and configure third-party solutions, including participation in software engineering work in Python or other programming languages.
Work on data migrations and ingestion projects with users to design large-scale data processing systems, develop data pipelines optimized for scaling, and troubleshoot potential data platform challenges.
Ensure quality, availability, and integrity of data, solutions, and respective systems and integrate proactive strategies and best practices regarding data integrity, security, and scalability.
Partner cross-functionally to build and improve new/existing constructs and solve data engineering problems. Supports scoping, scheduling, and budgeting at a project or major task level.
Qualifications:
Minimum Qualifications:
BS/BA and 5+ years of relevant work experience -OR-
MS/MA and 3+ years of relevant work experience -OR-
PhD with 1+ year of relevant experience
Preferred Qualifications:
ETL, SQL, NoSQL, and Data Management frameworks
Backend languages and tools such as Python, PostgreSQL, and database management
Experience with Linux computing environments.
Hazardous Working Conditions/Environment:
Not applicable.
Additional Information:

Not applicable.

“Referral Eligible”
Testing Designated Position (TDP):
This is not a Testing Designated Position (TDP).
About PNNL:
Pacific Northwest National Laboratory (PNNL) is a world-class research institution powered by a highly educated, diverse workforce committed to the values of Integrity, Creativity, Collaboration, Impact, and Courage. Every year, scores of dynamic, driven people come to PNNL to work with renowned researchers on meaningful science, innovations and outcomes for the U.S. Department of Energy and other sponsors; here is your chance to be one of them!

At PNNL, you will find an exciting research environment and excellent benefits including health insurance, flexible work schedules and telework options. PNNL is located in eastern Washington State—the dry side of Washington known for its stellar outdoor recreation and affordable cost of living. The Lab’s campus is only a 45-minute flight (or ~3-hour drive) from Seattle or Portland, and is serviced by the convenient PSC airport, connected to 8 major hubs.
Commitment to Excellence, Diversity, Equity, Inclusion, and Equal Employment Opportunity:
Our laboratory is committed to a diverse and inclusive work environment dedicated to solving critical challenges in fundamental sciences, national security, and energy resiliency. We are proud to be an Equal Employment Opportunity and Affirmative Action employer. In support of this commitment, we encourage people of all racial/ethnic identities, women, veterans, and individuals with disabilities to apply for employment.

Pacific Northwest National Laboratory considers all applicants for employment without regard to race, religion, color, sex (including pregnancy, sexual orientation, and gender identity), national origin, age, disability, genetic information (including family medical history), protected veteran status, and any other status or characteristic protected by federal, state, and/or local laws.

We are committed to providing reasonable accommodations for individuals with disabilities and disabled veterans in our job application procedures and in employment. If you need assistance or an accommodation due to a disability, contact us at careers@pnnl.gov.
Drug Free Workplace:
PNNL is committed to a drug-free workplace supported by Workplace Substance Abuse Program (WSAP) and complies with federal laws prohibiting the possession and use of illegal drugs.

If you are offered employment at PNNL, you must pass a drug test prior to commencing employment. PNNL complies with federal law regarding illegal drug use. Under federal law, marijuana remains an illegal drug. If you test positive for any illegal controlled substance, including marijuana, your offer of employment will be withdrawn.",#N/A,5001 to 10000 Employees,Company - Private,"Energy, Mining & Utilities",Energy & Utilities,1965,$1 to $5 billion (USD)
"Tek Ninjas
4.0",4.0,"Carrollton, TX",CLOUD BIG DATA ENGINEER,"CLOUD BIG DATA ENGINEER

Looking for Cloud Big Data Engineer in Carrollton, TX area,
Candidate should work on Amazon Web Services (AWS) using EC2 for computing and S3 as storage.
ETL process jobs using Ab-Initio in dynamic, high-volume environment.
Design and deploy well-tuned Ab-Initio graphs for ODS and DSS instances using both Linux and UNIX environments.
Test all applications and transport data to target Warehouse tables, schedule and run extraction and load process by using Informatica Workflow Manager.
Should be familiar with Hadoop, Hive, Pig, Sqoop, HBase, Cassandra, Spark, Spark Streaming, Spark SQL, Oozie, Zookeeper, Kafka, Flume, MapReduce framework, Yarn, Scala and related technicalities.
Req: MS/BS in Computer Science/any engineering/closely related field. Ex: MS+1yr/BS+5yrs in MicroStrategy developer/any software-related experience.

Should be WILLING TO TRAVEL/RELOCATE TO CLIENT SITES THROUGHOUT THE US.

Email: hr@tekninjas.com, Tek Ninjas Solutions, LLC, 4425 Plano Pkwy, Ste#1402, Carrollton, TX 75010.","$111,153 /yr (est.)",51 to 200 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"CVS Health
3.1",3.1,"Wellesley, MA",Data Engineer,"Caremark LLC, a CVS Health company is hiring for the following role in Wellesley, MA: Data Engineer to design, build and manage large scale data structures, pipelines and efficient Extract/Load/Transform (ETL) workflows to support business applications. Duties include: develop large scale data structures and pipelines to organize, collect and standardize data to generate insights and addresses reporting needs; write ETL (Extract/Transform/Load) processes, design database systems, and develop tools for real-time and offline analytic processing; collaborate with Data Science team to transform data and integrate algorithms and models into automated processes; leverage knowledge of Hadoop architecture, HDFS commands, and designing and optimizing queries to build data pipelines; utilize programming skills in Python, Java, or similar languages to build robust data pipelines and dynamic systems; build data marts and data models to support Data Science and other internal customers; integrate data from a variety of sources and ensure adherence to data quality and accessibility standards; analyze current information technology environments to identify and assess critical capabilities and recommend solutions; and experiment with available tools and advise on new tools to provide optimal solutions that meet the requirements dictated by the model/use case. Multiple positions.

Required Qualifications
Master’s degree (or foreign equivalent) in Computer Science, Data Science, Statistics, Mathematics, Analytics, or a related field and one (1) year of experience in the job offered or related occupation. Requires one (1) year of experience in each of the following: Analyzing large data sets from multiple data sources; Data analysis for retail and/or healthcare industries; Programming in Java, Python, or R; SAS or SQL programming languages; and Databases: Oracle, Teradata, or DB2.

Preferred Qualifications
See Required Qualifications.

Education
See Required Qualifications.

Business Overview
Bring your heart to CVS Health Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver. Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable. We strive to promote and sustain a culture of diversity, inclusion and belonging every day. CVS Health is an affirmative action employer, and is an equal opportunity employer, as are the physician-owned businesses for which CVS Health provides management services. We do not discriminate in recruiting, hiring, promotion, or any other personnel action based on race, ethnicity, color, national origin, sex/gender, sexual orientation, gender identity or expression, religion, age, disability, protected veteran status, or any other characteristic protected by applicable federal, state, or local law. We proudly support and encourage people with military experience (active, veterans, reservists and National Guard) as well as military spouses to apply for CVS Health job opportunities.",#N/A,10000+ Employees,Company - Public,Healthcare,Health Care Services & Hospitals,1963,$10+ billion (USD)
"Rippling
3.4",3.4,"San Francisco, CA",Senior Data Engineer,"About Rippling
Rippling is the first way for businesses to manage all of their HR & IT—payroll, benefits, computers, apps, and more—in one unified workforce platform.

By connecting every business system to one source of truth for employee data, businesses can automate all of the manual work they normally need to do to make employee changes. Take onboarding, for example. With Rippling, you can just click a button and set up a new employees’ payroll, health insurance, work computer, and third-party apps—like Slack, Zoom, and Office 365—all within 90 seconds.

Based in San Francisco, CA, Rippling has raised $700M from the world’s top investors—including Kleiner Perkins, Founders Fund, Sequoia, and Bedrock—and was named one of America’s best startup employers by Forbes (#12 out of 500).
About The Role
We are looking for a hands-on senior engineer to play a key role in Rippling’s data team. As a senior engineering resource on the data engineering team, you will be leading the design and development of systems that will enable analytics, experimentation, and user-facing features. You will be closely involved with multiple data adjacent teams and stakeholders, alongside helping junior talent in the team learn and grow.
The Data Engineering Team at Rippling is a combination of warehousing and data platform engineering, supporting a variety of orgs across the company (Data Science, Marketing, Bizops, Revops, Finance to name a few). Here’s an idea of some of the initiatives we’re working on:
A realtime, central data lake to operationalize warehouse data.
Creating a metrics layer to make reporting more efficient and accurate.
Building a catalog to make our data assets searchable and easy to discover.
Making our globally distributed data stack compliant and scalable
What You'll Do
Help architect, build, and scale our data pipelines from our OLTP database, other internal systems and third party tools to our warehouse (Snowflake)
Leverage data technologies like Kafka, Presto, Flink, Airflow, Mongo, Snowflake and Spark
Support reporting, data science, operations and machine learning functions
Create data platforms, data lakes, and data ingestion systems that work at scale
Define and enforce data quality checks and audits for code warehousing datasets
Define and support internal SLAs for core data sets
Qualifications
5+ Years experience in Data and Software Engineering
Expertise in writing complex data transforms in SQL and Python
Knowledge of data warehousing concepts around building custom ETL integrations, building data infrastructure (SCD,CDC,Snapshots,indexing,partitioning)
Knowledge on Data Security and Governance (nice to have)
Experience in analytics, dimensional modeling, and ETL optimization preferred
BS/BA in a technical field such as Computer Science or Mathematics preferred
Additional Information
This role will receive a competitive salary + benefits + equity. The salary for US-based employees will be aligned with one of the ranges below based on location; see which tier applies to your location here.

Tier 1: $174,000 - $237,000/year
Tier 2: $156,000 - $214,000/year
Tier 3: $147,000 - $202,000/year
A variety of factors are considered when determining someone’s compensation–including a candidate’s professional background, experience, and location. Final offer amounts may vary from the amounts listed above.","$205,500 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,2016,$25 to $100 million (USD)
#N/A,#N/A,"Phoenix, AZ",Data Warehouse Engineer - Remote,"At UnitedHealthcare, we’re simplifying the health care experience, creating healthier communities and removing barriers to quality care. The work you do here impacts the lives of millions of people for the better. Come build the health care system of tomorrow, making it more responsive, affordable and equitable. Ready to make a difference? Join us and start doing your life's best work.(sm)

You’ll enjoy the flexibility to work remotely* from anywhere within the U.S. as you take on some tough challenges.

Primary Responsibilities:
Participate in design and delivery of large-scale data warehouse infrastructure solutions to support enterprise-wide reporting, analytics, and data science needs
Work with healthcare business stakeholders to identify key business problems, then diagnose, analyze and synthesize advanced data engineering solutions in areas such as: local market health plan support; clinical and regulatory analytics; Claims Operations; Finance/Actuarial; and Quality
Develop a deep understanding of primary data domains (claims, clinical, membership, financial) and their impact on business processes
Assist team in and work directly on transition to solutions which leverage modernized technology such as Snowflake, Azure, ADF, Databricks, Kafka streaming
Demonstrate analytical and problem-solving skills, including the ability to bring clarity to complex or ambiguous scenarios, identify root causes and recommend solutions
Adhere to data engineering best practices and code quality standards
Identify and resolve technical issues and design challenges

You’ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.
Required Qualifications:
2+ years of experience in data warehouse development and design
2+ years of data analysis
2+ years of Snowflake or other SQL platforms such as Oracle
Experience with mid - high volume ETL/ELT data processes and pipelines
Experience with DevOps, CI/CD tools, Git, preferably in cloud environments
Experience with formal System Development Lifecycle and Data Quality processes
Experience and comfort to work well in a fast-paced environment

Preferred Qualifications:
Bachelor's Degree
Healthcare data experience (experience with claims, clinical or call systems)
Experience with Snowflake or Azure cloud data warehouse technologies
Experience in building and managing a technology portfolio
Experience with Agile development methodology
Proven team oriented, communicative, proactive

Careers with UnitedHealthcare. Work with a Fortune 5 organization that’s serving millions of people as we transform health care with bold ideas. Bring your energy for driving change for the better. Help us improve health access and outcomes for everyone, as we work to advance health equity, connecting people with the care they need to feel their best. As an industry leader, our commitment to improving lives is second to none.

California, Colorado, Connecticut, Nevada, New Jersey, New York, Rhode Island, or Washington Residents Only: The salary range for California, Colorado, Connecticut, Nevada, New Jersey, New York, Rhode Island or Washington residents is $67,800 to $133,100. Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, UnitedHealth Group offers benefits such as, a comprehensive benefits package, incentive and recognition programs, equity stock purchase and 401k contribution (all benefits are subject to eligibility requirements). No matter where or when you begin a career with UnitedHealth Group, you’ll find a far-reaching choice of benefits and incentives.

*All employees working remotely will be required to adhere to UnitedHealth Group’s Telecommuter Policy

At UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone-of every race, gender, sexuality, age, location and income-deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes - an enterprise priority reflected in our mission.




Diversity creates a healthier atmosphere: UnitedHealth Group is an Equal Employment Opportunity/Affirmative Action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.

UnitedHealth Group is a drug - free workplace. Candidates are required to pass a drug test before beginning employment.","$100,450 /yr (est.)",10000+ Employees,Company - Public,Healthcare,Health Care Services & Hospitals,1977,$10+ billion (USD)
"DocuSign
3.7",3.7,"San Francisco, CA",Senior Data Engineer,"Company Overview

DocuSign helps organizations connect and automate how they agree. Our flagship product, eSignature, is the world’s #1 way to sign electronically on practically any device, from virtually anywhere, at any time. Today, more than a million customers and a billion users in over 180 countries use DocuSign to accelerate the process of doing business and simplify people’s lives.


What you'll do

DocuSign is seeking a talented and results-oriented Senior Data Engineer to focus on delivering trusted data to the business. The Senior Data Engineer delivers data for analytics using our Enterprise Data Warehouse, enabling the global DocuSign analytics community via curated, governed and cleansed data. As a member of the Global Data and Analytics team, the Data Engineer leverages a variety of technologies to accomplish this goal, ranging tools like Airflow, Matillion, dbt, Snowflake and Fivetran to languages like SQL and Python. The successful candidate will develop solutions with innovative cloud technologies, work on a variety of fast-paced assignments, and partner with world-class technical and business teams to maximize the value of data.

This position is an individual contributor role reporting to the Manager, Data Analytics.

Responsibility
Build data pipelines using Fivetran, dbt/Matillion, Snowflake and Airflow
Develop and maintain data documentation including ERD, data dictionaries, data lineage and metadata
Ensure data quality and integrity by implementing appropriate data validation and cleansing techniques
Monitor and troubleshoot data pipelines to identify and resolve issues in a timely manner
Build POCs to validate new concepts and new technologies
Collaborate with business, engineering, and data science teams to understand their data needs and design efficient solutions to support their requirements


Job Designation

Hybrid:
Employee divides their time between in-office and remote work. Access to an office location is required. (Frequency: Minimum 2 days per week; may vary by team but will be weekly in-office expectation)


What you bring

Basic
Bachelor’s Degree in Computer Science, Data Analytics, Information Systems, relevant field or equivalent work experience
8+ years of relevant experience
Experience with database and data warehouse concepts such as facts and dimensions to design and develop data models that support enterprise reporting and analytics needs
5+ years of dimensional and relational data modeling experience
Experience with modern data integration and transformation tools such as Fivetran, Dbt, and Matillion
Experience with workflow orchestration tools such as Airflow
Experience with MPP databases like Snowflake, Redshift and BigQuery
Experience with cloud platforms like AWS, Azure and GCP
Experience with versioning tools like git
Experience working with tools like Jira and Confluence
Experience with SQL and Python
Experience with document and data debugging

Preferred
Ability to work independently with minimal supervision, as well as in a team environment
Excellent communication skills
Eye for detail, good data intuition, and a passion for data quality
Comfortable working in a rapidly changing environment with ambiguous requirements
Organizational and time management skills, with the ability to prioritize tasks and meet deadlines


Wage Transparency

Based on applicable legislation, the below details pay ranges in the following locations:

California: $130,500 - $208,050 base salary

Illinois and Colorado: $123,700 - $174,700 base salary

Washington and New York (including NYC metro area): $123,700 - $184,275 base salary

This role is also eligible for bonus, equity and
benefits.


Life at DocuSign

Working here
DocuSign is committed to building trust and making the world more agreeable for our employees, customers and the communities in which we live and work. You can count on us to listen, be honest, and try our best to do what’s right, every day. At DocuSign, everything is equal.

We each have a responsibility to ensure every team member has an equal opportunity to succeed, to be heard, to exchange ideas openly, to build lasting relationships, and to do the work of their life. Best of all, you will be able to feel deep pride in the work you do, because your contribution helps us make the world better than we found it. And for that, you’ll be loved by us, our customers, and the world in which we live.

Accommodation
DocuSign provides reasonable accommodations for qualified individuals with disabilities in job application procedures. If you need such an accommodation, including if you need accommodation to properly utilize our online system, you may contact us at accommodations@docusign.com.

If you experience any technical difficulties or issues during the application process, or with our interview tools, please reach out to us at taops@docusign.com for assistance.

Our global benefits
Paid time off
Take time to unwind with earned days off, plus paid company holidays based on your region.
Paid parental leave
Take up to six months off with your child after birth, adoption or foster care placement.
Full health benefits
Options for 100% employer-paid health plans from day one of employment.
Retirement plans
Select retirement and pension programs with potential for employer contributions.
Learning & development
Grow your career with coaching, online courses and education reimbursements.
Compassionate care leave
Paid time off following the loss of a loved one and other life-changing events.","$149,200 /yr (est.)",5001 to 10000 Employees,Company - Public,Information Technology,Information Technology Support Services,2003,$1 to $5 billion (USD)
"MUFG
3.5",3.5,"Jersey City, NJ","CyberSecurity Data Loss Prevention Engineer, Analyst - Hybrid - Irving, TX/ Jersey City, NJ/ Tempe, AZ","Do you want your voice heard and your actions to count?
Discover your opportunity with Mitsubishi UFJ Financial Group (MUFG), the 6th largest financial group in the world. Across the globe, we're 160,000 colleagues, striving to make a difference for every client, organization, and community we serve. We stand for our values, building long-term relationships, serving society, and fostering shared and sustainable growth for a better world.
With a vision to be the world's most trusted financial group, it's part of our culture to put people first, listen to new and diverse ideas and collaborate toward greater innovation, speed and agility. This means investing in talent, technologies, and tools that empower you to own your career.
Join MUFG, where being inspired is expected and making a meaningful impact is rewarded.
This is a hybrid position. The recruiter will provide additional details.
Job Summary
Security Engineer Analyst will ultimately be responsible for the day to day operations and architecture for the firm's user data loss prevention and data classification systems. This position will analyze data and information to maintain key metrics and collaborate with the rest of the engineers for the firm's DLP program strategy . The expectation is that this person will have an awareness of the DLP, data classification, and process automation, and able to recommend tuning and enhancements to the data security strategy and use cases. This role is for an entry level analyst with great technical acumen and analytical skills to aid in the vision of where to take our program.
Major Responsibilities
Develop and improve automated process, integration between applications, and operational efficiency
Develop and maintain data loss prevention and data classification rules and dashboards to meet enterprise security needs
Analyze, design, develop, and operate programs, shell scripts, tests, and infrastructure automation capabilities necessary for daily operations and custom processes.
Management of the technology and processes including monitoring, investigation, reporting, and rule maintenance.
Coordinate with internal Data Loss Prevention (DLP) and Insider Threat teams to develop and integrate events with the UEBA and SOAR technology.
Support security operations and other security partners in remediation or mitigation of security vulnerabilities and assist associated technical teams.
Advise Insider Threat responders as they develop and coordinate response, containment, and remediation capabilities as appropriate.
Qualifications
B.S. Computer Science or related field or equivalent experience
1+ years' overall technical experience in data loss prevention, data classification, incident response, security operations, or related information security field
1+ years' experience in application design/engineering, including but not limited to programming/scripting, Windows/Linux system administration, etc.
Experience in the banking or finance industries a plus
Understand data loss prevention, data classification, SIEM a plus
Scripting and program automation experience
Security and IT metrics experience a plus; report creation abilities strongly desired
Exceptional ability to execute and drive change while never losing site of the basics
Strategic, creative, and innovative mind
Zero tolerance for operational, design, and strategy oriented gaps
Absolute self-starter who will take the lead and initiative to find and solution problems
The typical base pay range for this role is between $90K - $115K depending on job-related knowledge, skills, experience and location. This role may also be eligible for certain discretionary performance-based bonus and/or incentive compensation. Additionally, our Total Rewards program provides colleagues with a competitive benefits package (in accordance with the eligibility requirements and respective terms of each) that includes comprehensive health and wellness benefits, retirement plans, educational assistance and training programs, income replacement for qualified employees with disabilities, paid maternity and parental bonding leave, and paid vacation, sick days, and holidays. For more information on our Total Rewards package, please click the link below.
MUFG Benefits Summary
The above statements are intended to describe the general nature and level of work being performed. They are not intended to be construed as an exhaustive list of all responsibilities duties and skills required of personnel so classified.
We are proud to be an Equal Opportunity/Affirmative Action Employer and committed to leveraging the diverse backgrounds, perspectives and experience of our workforce to create opportunities for our colleagues and our business. We do not discriminate on the basis of race, color, national origin, religion, gender expression, gender identity, sex, age, ancestry, marital status, protected veteran and military status, disability, medical condition, sexual orientation, genetic information, or any other status of an individual or that individual's associates or relatives that is protected under applicable federal, state, or local law.
#LI-Hybrid","$102,500 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Banking & Lending,1864,$10+ billion (USD)
"Kent State University
4.2",4.2,Ohio,Lead Data Engineer,"Participate in architectural design, optimization, implementation, and administration of our modern data warehousing environment. Work with analysts across the university to determine needs and develop data models to support those needs . Optimize data flow and collection from and between multiple complex cloud, on-premises, and hybrid systems. Reports to designated supervisor.

Additional Basic Function – if applicable:

The Data Management & Analytics team at Kent State University is looking for a Lead Data Engineer to join our team to help us with some exciting data adventures! We are currently working on building a modern analytics solution. Come join us while we build a data lake and data marts that will be used to create a university wide and system agnostic view our data to enable our decision makers to get the data they need and to provide new opportunities in support of predictive analytics and machine learning. Our team is also working with others in the Division of Information Technology to build a data pipeline to enable a consistent, governed way to provide data for application and system integrations.

Examples of Duties:

Duties/essential functions may include, but not be limited to, the following:
Participate in and/or facilitate data discovery sessions with business subject matter experts to translate business needs into enterprise dimensional and relational data models for reporting and analytics.
Design, create and maintain optimal data lake, data mart, data warehouse, and data integration architectures.
Assemble university-wide complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal data process improvements such as automating manual processes, optimizing data delivery, ensuring data integrity, re-designing infrastructure for greater scalability, etc.
Build the infrastructure and integrations required for optimal ETL/ELT of data from a wide variety of complex cloud, hybrid, and on-premises data sources.
Participate in development and implementation of data lifecycle of the modern data warehousing environment.
Lead and educate and mentor team members on best practices. Perform code and model design reviews.
Provide oversight / guidance related to data issues needing technical expertise.
Perform related duties as assigned.
JOB COMPETENCIES:
Communicate effectively with technical and non-technical users.
Maintain cooperative working relationships.
Manage time and effectively balance multiple evolving priorities.
Work effectively with very limited oversight.
Establish estimates and timelines for specific applications/projects and take direct accountability for results.
Establish focused, measurable goals for self and others.

Additional Examples of Duties – if applicable:

Minimum Qualifications:

Bachelor’s degree in computer science, information technology or a related field of study. Minimum of seven years progressive experience in the following:
Analyzing and translating business needs into long-term solution data models.
Experience data modeling principles/methods to design dimensional data models from the ground up.
Working with relational databases, as well as working familiarity with a variety of databases.
Advanced logical data model design, data warehouse design, and data integration.
Performing root cause analysis to perform troubleshooting on data flows through complex systems.
Analyzing internal and external data and processes for a business to build models to answer specific business questions.
Building processes for data transformation.
Developing and maintaining system documentation, metadata, data standards, and data quality metrics for data management systems.
Experience writing advanced SQL.
License/Certification:

Knowledge Of:
Advanced logical data design, data warehouse design, and data integration as well as the management of web content or other unstructured data
Common software application packages and tools for performance monitoring and issues tracking
Testing practices, application debugging, and troubleshooting procedures
Software development life cycle, structured programming, object-oriented design and development techniques, and change management
Managing the development and maintenance of system documentation, metadata, data standards, and data quality metrics for the data management system
Advanced working knowledge of SQL
Skill In:
Time management with the ability to set priorities to coordinate multiple assignments with fluctuating and time-sensitive deadlines
Written and interpersonal communication, with the ability to present complex technical information in a clear and concise manner to a variety of audiences
Ability To:
Foster positive and professional working relationships; effectively handle interpersonal interactions at all levels; and respond appropriately to conflicts and problems
Work with technical and non-technical staff to identify user needs and translate them into technology-based solutions
Keep abreast of industry trends

Preferred Qualifications – if applicable:
Experience working in a higher education institution.
Experience facilitating data discovery sessions to gather requirements needed to build data models.
Experience building solutions to support analytics, machine learning, and data science.
Experience with datalake and data warehouse technologies.
Experience with data analytics tools.
Experience developing solutions that are incorporated into all aspects of an organization’s data services to achieve data governance and data quality best practices.

Assessments:

Asterisk (*) indicates knowledge, skills, abilities which require assessments

Working Conditions / Physical Requirements:

Working Schedule:

Flexible hours and remote work options available

Additional Information:

Must pass a security check.

We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.","$96,808 /yr (est.)",5001 to 10000 Employees,College / University,Education,Colleges & Universities,1910,$500 million to $1 billion (USD)
"Long Finch Technologies
4.6",4.6,"Bellevue, WA",Data Engineer- AZURE with Strong SPARK,"Job Title: Data Engineer
Location: Bellevue, WA Onsite
Duration: Full Time
Job Description
Data Engineering experience primarily on Spark. Someone who has worked on Azure cloud with knowledge on Azure DEVOPS (CI/CD & Infrastructure as a code), ADF, ADW & Power BI. Apache Nifi will be good to have.Power BI: Understand business requirements to set functional specifications for reporting applications Build automated reports and dashboards with the help of Power BI reporting tool Be experienced in tools and systems on MS SQL Server BI Stack, including SSRS and TSQL, Power Query, MDX, PowerBI, and DAX Be able to quickly shape data into reporting and analytics solutions Have knowledge of database fundamentals such as multidimensional database design, relational database design, and more Create functional reporting Study, analyze and understand business requirements in context to business intelligence. Design and map data models to shift raw data into meaningful insights. Utilize Power BI to build interactive and visually appealing dashboards and reports. Spot key performance indicators with apt objectives Run DAX queries and functions in Power BI Should have an edge over making DAX queries in Power BI desktop. Developing visual reports, KPI scorecards, and dashboards using Power BI desktop. Connecting data sources, importing data, and transforming data for Business intelligence.
Job Type: Full-time
Salary: $100,000.00 - $105,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Vision insurance
Schedule:
8 hour shift
Ability to commute/relocate:
Bellevue, WA 98004: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 5 years (Required)
Azure: 3 years (Required)
Spark: 3 years (Required)
Work Location: One location","$102,500 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2012,$25 to $100 million (USD)
"Bloomerang
3.6",3.6,United States,Lead Data Engineer,"Bloomerang combines the best tools, resources, and people to provide a world-class experience for tens of thousands of nonprofits, allowing them to raise more money and do more good in the world. Our powerful software and stellar customer service have made us one of the highest rated fundraising/donor CRM on the market.
In addition to creating thriving nonprofits, we're also in the business of creating thriving employees. At Bloomerang, you'll be a part of a mission-driven culture built on the core values of Empathy, Unity, and Transparency. We know the key to our success is our people, and we're proud to be home to some of the most innovative and skilled employees in the workforce today.
The Role
We are seeking a skilled and motivated Data Engineer to join our fast-growing SaaS company. If you thrive on challenges and have a passion for scaling businesses, this is a rare opportunity to be part of an exciting sports startup. As a Data Engineer, you will collaborate with cross-functional teams to design and implement data solutions using Google BigQuery or Snowflake. Your responsibilities will include building data infrastructure, integrating databases and platforms, performing statistical analysis, and leveraging the right data technology to support our top-level business strategy. If you are driven, possess excellent analytical skills, and have experience in high-growth technology startups, we invite you to join our team and contribute to our data-driven success.
What You Will Do
Collaborate with other departments to design and implement data solutions that address business problems and support top-level business strategy.
Implement and support data tools such as Google BigQuery or Snowflake to enable efficient data processing and analysis.
Build data infrastructure to facilitate integrations with various databases (MySQL, PostgresQL), platforms, and APIs.
Utilize statistical analysis tools like SAS, R, or Python to perform data analysis and derive meaningful insights.
Pursue continuous improvement by identifying and implementing enhancements to data systems and processes.
Write documentation for data architecture and processes to ensure clarity and maintainability.
Apply analytical ability to collect, organize, model, analyze, and disseminate significant amounts of data.
What You Need to Succeed
4-5 years of experience working with Google BigQuery or Snowflake, demonstrating a solid understanding of their capabilities and best practices.
Familiarity with Agile Methodologies to facilitate iterative and efficient development cycles.
Experience with Google Cloud Machine Learning and Google Cloud AI Platform, enabling the utilization of advanced analytics and AI capabilities.
Knowledge and ability to create and administer data access policies and procedures, ensuring data security and governance.
Proficiency in creating dashboards and visualizations using tools like PowerBI, Tableau, or similar, to effectively communicate insights to stakeholders.
Experience with Azure SQL Managed Instances and SQL Elastic Pools, expanding your expertise in cloud-based data solutions.
Being high-energy with a hustle, motivation, and drive that's contagious, fostering a proactive and results-oriented work culture.
Outstanding communication and interpersonal skills, enabling effective collaboration and stakeholder management.
Strong problem-solving and prioritization skills, allowing you to address complex data challenges and meet project objectives.
Experience working in a high-growth early-stage technology startup company, demonstrating adaptability and a willingness to work in a dynamic environment.
Benefits
Health + Wellness
You'll have access to generous health, vision, and dental insurance options, as well as a free subscription to Bright, a wellness platform that offers live and on-demand fitness, meditation, mindfulness, and nutrition classes.

Time Off
You'll get a competitive PTO package that includes 20 PTO days, 3 flex days, 4 optional volunteer Days, 12 paid holidays, as well as paid parental leave.

401k
You'll receive a 401k match to help invest in your future.

Equipment
Everything you need to be successful, shipped right to your door.

Compensation
The salary range for this position is: $127,000 - $172,000. You may also be eligible for a discretionary bonus. Actual compensation within the range will be dependent on your skills, experience, qualifications, and location, as well as applicable employment laws.

Location
This is a permanent, full-time, fully remote position. Employees living in Indianapolis, IN are welcome to work from our company headquarters. We do not offer Visa sponsorship or relocation assistance at this time.

Accommodations
Applicants who require accommodations may contact careers@bloomerang.com to request an accommodation in completing an application.
Bloomerang is an Equal Opportunity Employer. Individuals seeking employment at Bloomerang are considered without regard to race, color, religion, national origin, age, sex, marital status, ancestry, physical or mental disability, veteran status, gender identity, or sexual orientation.","$149,500 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Computer Hardware Development,2012,$25 to $100 million (USD)
Cox powered by Atrium,#N/A,United States,Data Engineer - Remote (2023-5940),"Minimum Qualifications:
A minimum of 3+ years related work experience
Working experience with data modeling, data access, schemas, and data storage techniques within Snowflake
Working experience with batch and real-time data processing frameworks
Working experience working with relational databases such as SQL
Working experience with business intelligence tools and platforms
Working experience with data quality tools
Experience with ETL processes and tools
Preferred Qualifications:
Working experience with Snowflake
Working experience with Google Analytics BigQuery
Working Experience with Tableau Server
Working experience with SQL Server Integration Services
Pay Range:
$40-$49/hr
Requisition Disclaimer:
This job posting is for a temporary role as an employee of Atrium on assignment at Cox. The individual selected for this role will be offered the role as an employee of Atrium; compensation, medical benefits, fringe benefits and other terms and conditions of employment shall be presented by Atrium upon offer. The pay rate range provided is a reasonable estimate of the anticipated compensation range for this job at the time of posting. The actual pay rate will be based on a number of factors, including skills, competencies, experience, location and/or being pursued and other job-related factors permitted by law. In addition, this role will be eligible for overtime pay, in accordance with federal and state requirements

By applying for this position you agree to the Atrium Terms and Conditions. Agreeing to these terms, includes permission to use the email address and mobile phone number you provide during the application process or throughout the duration of your prospective or actual employees to notify you of job openings, profiles, articles, news, and other employment-related information, as well as to notify you of special promotions or additional products and services offered by us or our affiliates and partners (collectively, “Atrium Alerts”). Atrium Alerts may be sent by email, phone or text message. Your personal information will be safely stored in our database. Atrium does not sell your personal information to third parties. Text message and data rates may apply. To OPT OUT of text messaging or to modify your communication preferences for Atrium Alerts at any time, please contact us at privacyadministrator@atriumstaff.com.

If you do not agree with the Atrium Terms and Conditions, you can still complete your application for this position by emailing your resume to our team at coxrecruit@atriumworks.com. Please include the job title in the subject of your email.

As a woman-owned firm, Atrium values diversity. We are an equal opportunity employer and will consider all applications without regard to race, sex, age, color, religion, national origin, veteran status, disability, genetic information or any other characteristic protected by law. We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.
Posting: #zip",$44.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Pyramid Global Hospitality Corporate Offices - Boston,#N/A,"Boston, MA",Data Engineer Manager,"About Us:
At Pyramid Global Hospitality, people come first. As a company that values its employees, Pyramid Global Hospitality is dedicated to creating a supportive and inclusive work environment that fosters diversity, growth, development, and wellbeing. Our commitment to a People First culture is reflected in our approach to employee development, employee benefits and our dedication to building meaningful relationships.

Pyramid Global Hospitality offers a range of employment benefits, including comprehensive health insurance, retirement plans, and paid time off, as well as unique perks such as on-site wellness programs, local discounts, and employee rates on hotel stays. In addition, Pyramid Global Hospitality is committed to providing ongoing training and development opportunities to help our people build the skills and knowledge they need to advance their careers.

Whether you are just starting out in the hospitality industry or are a seasoned professional, Pyramid Global Hospitality offers a supportive and collaborative work environment that encourages growth and fosters success, in over 230 properties worldwide. Join their team and experience the benefits of working for a company that values its employees and is committed to creating exceptional guest experiences.

Check out this video for more information on our great company!
Location Description:

Pyramid Global Hospitality (“Pyramid”) is a leading hotel management company, operating in the US, Caribbean, and Western Europe. With portfolio revenues exceeding $3 billion, Pyramid manages 230 hotels, resorts, and conference centers, both branded and independent. The firm maintains offices in Boston (Headquarters), Cincinnati, Houston, and London. Additional information about Pyramid can be found at www.pyramidglobal.com

In 2021, Pyramid and Benchmark Resorts and Hotels merged to add an additional 59 Managed or Asset Managed Resorts and over 10,000 additional team members. The two companies share the same company culture, values and philosophies. We are growing and opportunities abound!

What really sets Pyramid apart from our competitors is our reputation as an employer. Professional growth is not just possible throughout the company but planned and encouraged. The Leadership Team at Pyramid consider team member development its first priority, understanding that success is only achieved in a workplace where every contributor is respected and recognized. This is why we deliver superior results.

There is opportunity to work directly with senior leaders, experience stretch assignments and learn hospitality management from industry giants. You will come to know a distinctive people centric culture that is at the core of all we do. The decisions we make and the paths we take are bound by a commitment to our Owners, Associates, Customers and the Communities where we work. We attract the most talented associates in the industry, and actively encourage candidates with a “hospitality spirit” who may be thinking about a career change to join our team.

Overview:
Our organization is seeking a highly motivated Manager of Data Engineering to oversee and advance our data management capabilities. You will be joining a dynamic team and playing a critical role in managing data pipelines that provide decision-making information used by our field leaders, corporate executives, and property owners. Timely, accurate information is our competitive advantage and this role is pivotal to our success.

The primary focus involves leveraging data management best practices to improve data reliability and quality. This includes ensuring day-to-day operational excellence, enabling new data source integration supporting the company’s aggressive growth plans, and evolving our data capabilities.

This leadership role involves strategic design and operational execution. You will join a team led by the Vice President of Data Sciences and play a crucial part in advancing our data strategy and supporting the company's ever-expanding appetite for information.

Essential Functions:

As the Data Engineering Manager, your responsibilities will include but not be limited to:
Support and improve the data pipeline operations – extract/transform/load data from various sources, manage storage, make data available for analytics
Resolve data integrity issues with measures to permanently address the root cause
Architect data management practices to ensure high-quality data
Drive thought-leadership for optimizing our data pipelines for advanced analytics
Implement best-practice methodologies to accelerate our data velocity
Automate data management processes for more efficient data capture, storage, and availability
Establish a data governance capability to solidify ownership across the data life cycle
Facilitate collaboration with stakeholders from across the company to create improvements in effective data use
Stay current with the latest technologies and methodologies
Other responsibilities as assigned
Qualifications:
A Bachelor's degree in Computer Science, Information Systems, or a related field
Demonstrate data management leadership experience
Microsoft Azure Synapse Analytics ecosystem
Azure SQL Data Warehousing experience
ETL expertise with large, complex, heterogeneous datasets
Accomplished work with PowerBI, star-schemas, and analytics data prep
Solid understanding of data analysis and machine learning
A track record of delivering successful data management improvement projects
Excellent communication and interpersonal skills
Strong problem-solving skills and the ability to work independently
A commitment learning about the latest technologies and methodologies

If you're a highly motivated individual with experience in data management and a desire to drive process improvement, we want to hear from you!
Compensation Range: The compensation for this position is $110,000.00/Yr. - $125,000.00/Yr. based on qualifications and experience.","$117,500 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"HARAMAIN SYSTEMS INC.
5.0",5.0,"New Richmond, WI",ETL-Data Engineer: Direct hire,"Role : Data Engineer
Location : Roseville MN. (Can be mostly remote, but ideally someone could go in occasionally as needed)

Essential Functions and Accountabilities of the Data Engineer:
Design and build data models and data transformations efficiently and reliably to fit the analytical needs of the business
Develop and maintain ETL processes utilizing Azure Cloud tools such as Azure Data Factory, FiveTran, DataBricks as well as efficient cloud data strategies to minimize costs
Engage with teams to push the boundaries of analytical insights, creating new product features using data, and powering data models
Anticipate, Identify and solve issues concerning data management to improve data quality
Understand and use continuous integration, test driven development and production deployment frameworks using DevOps and GIT
Develop and champion modern Data Engineering concepts to technical audience and business stakeholders
Manage and maintain relational databases, including but not limited to SQL Server
Maintain data integrity and transparency to increase data, reporting and dashboard confidence and consistency across all departments
Advocate for new sources of data to create actionable insights and recommendations
Define and communicate business rules and terms governing use of the data including security and data lifecycle management
Communicate technical and non-technical information clearly, concisely, and effectively both orally and in writing. Present findings, recommendations, and specifications in formal reports and/or oral presentations

Knowledge & Education Requirements:
BA/BS degree in Computer Science, Database Administration or similar work experience
3-5+ years of experience in Data Engineering
2 or more years of experience in data modeling, data architecture and ETL
Experience with tools and concepts related to data and analytics, such as dimensional modeling, reporting tools, data governance, data warehousing, structured and unstructured data
Proficient in at least one major programming language (e.g.. Java Script, Python) and comfortable working with SQL
Advanced understanding of analytics tools and methods, best practices and awareness of new techniques of analytics
Experience working with relational databases like SQL, Postgres and Data Bricks
Experience working with ETL tools like Azure Data Factory, Fivetran and SSIS

This is a remote position.","$57,125 /yr (est.)",51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,#N/A,Unknown / Non-Applicable
"Harvard University
4.3",4.3,"Boston, MA","Big Data Engineer, Digital Transformation","Position Description
Be a pioneer in business, education, and global impact by joining the Harvard Business School Digital Transformation team - a “startup with assets,” where you will have the chance to deploy cutting-edge digital- and emerging-technology education solutions. Where else can you make a difference at the intersection of cutting-edge technology, world-class education, noble purpose, and timeless legacy?

As a Big Data Engineer, you will bring to life and implement architecture blueprints and help our team by mapping out solutions to some of their complex technical challenges. You'll provide technical expertise, mitigate risk and offer solutions tailored for their needs. From migrations of existing workloads to building advanced cloud solutions, you'll help shape and build to increase agility, improve security, reduce costs and meet utilization targets. You will work closely with the Data Engineering, Data Science, Infrastructure architecture & Platform teams and will focus on creating blueprints to nourish a culture of engineering excellence. You will report into the Sr. Managing Director Big Data Governance.

Duties and Responsibilities:
Support data engineering needs across HBS, including technical support and training in Big Data frameworks and ways of working, revision and integration of source code, to release and source code quality control
Designing and producing high performing stable end-to-end applications to perform complex processing of batch and streaming massive volumes of data in a multi-tenancy big data platform in the cloud, and output insights back to business systems according to their requirements.
Design and implement core platform capabilities, tools, processes, ways of working and conventions under agile development to support the integration of data sourcing and use cases implementation, towards reusability, to ease up delivery and ensure standardization across data deliverables in the platform.
Working with the Group architecture team to define the strategy for evolving the Big Data capability, including solution architectural decisions aligned with the platform architecture
Defining the technologies to be used on the Big Data Platform and investigating new technologies to identify where they can bring benefits
Collaborating with implementation efforts with teams of technical resources within HBS, Harvard, and vendors as needed
Performing other duties as assigned and/or as necessary
Basic Qualifications
Minimum of five years’ post-secondary education or relevant work experience
Additional Qualifications and Skills
Bachelor’s degree in Mathematics, Physics, Computer Science, Engineering, Statistics, or an equivalent technical discipline or 5 years of experience is required
3-5 years of experience in a hands-on technical role as an engineer and/or data engineering role working in large-scale big data environments is required
Experience with common build tools, unit, integration, functional and performance testing from automation perspective, and continuous delivery, under agile practices is necessary
Expert-level experience in designing, building and managing applications to process large amounts of data in a cloud ecosystem or other big data frameworks is a must
Additional Information
This role has the possibility of being a remote or hybrid position. You must reside in one of the following states: CA, CT, GA, IL, MA, MD, ME, NH, NJ, NY, RI, VA, VT or WA. There may be periodic visits to our Boston, MA based campus. In a hybrid role, you are required to be onsite at our Boston, MA based campus a determined number of days per month. Specific days and schedule will be determined between you and your manager.

We may conduct candidate interviews virtually (phone and/or via Zoom) and/or in-person for this role.

A cover letter is required to be considered for this opportunity.

Harvard Business School will not offer visa sponsorship for this opportunity.

Culture of Inclusion: The work and well-being of HBS is profoundly strengthened by the diversity of our network and our differences in background, culture, national origin, religion, sexual orientation, and life experiences. Explore more about HBS work culture here https://www.hbs.edu/employment .
Benefits
We invite you to visit Harvard’s Total Rewards website to learn more about our outstanding benefits package, which may include:
Paid Time Off: 3-4 weeks of accrued vacation time per year (3 weeks for support staff and 4 weeks for administrative/professional staff), 12 accrued sick days per year, 12.5 holidays plus a Winter Recess in December/January, 3 personal days per year (prorated based on date of hire), and up to 12 weeks of paid leave for new parents who are primary care givers.
Health and Welfare: Comprehensive medical, dental, and vision benefits, disability and life insurance programs, along with voluntary benefits. Most coverage begins as of your start date.
Work/Life and Wellness: Child and elder/adult care resources including on campus childcare centers, Employee Assistance Program, and wellness programs related to stress management, nutrition, meditation, and more.
Retirement: University-funded retirement plan with contributions from 5% to 15% of eligible compensation, based on age and earnings with full vesting after 3 years of service.
Tuition Assistance Program: Competitive program including $40 per class at the Harvard Extension School and reduced tuition through other participating Harvard graduate schools.
Tuition Reimbursement: Program that provides 75% to 90% reimbursement up to $5,250 per calendar year for eligible courses taken at other accredited institutions.
Professional Development: Programs and classes at little or no cost, including through the Harvard Center for Workplace Development and LinkedIn Learning.
Commuting and Transportation: Various commuter options handled through the Parking Office, including discounted parking, half-priced public transportation passes and pre-tax transit passes, biking benefits, and more.
Harvard Facilities Access, Discounts and Perks: Access to Harvard athletic and fitness facilities, libraries, campus events, credit union, and more, as well as discounts to various types of services (legal, financial, etc.) and cultural and leisure activities throughout metro-Boston.
Job Function
Information Technology
Department Office Location
USA - MA - Boston
Job Code
I1458P IT Rprting and Analyt Prof IV
Work Format
Hybrid (partially on-site, partially remote)
Department
Digital Transformation
Sub-Unit
-
Time Status
Full-time
Salary Grade
058
Union
00 - Non Union, Exempt or Temporary
Pre-Employment Screening
Criminal, Education, Identity
Commitment to Equity, Diversity, Inclusion, and Belonging
Harvard University views equity, diversity, inclusion, and belonging as the pathway to achieving inclusive excellence and fostering a campus culture where everyone can thrive. We strive to create a community that draws upon the widest possible pool of talent to unify excellence and diversity while fully embracing individuals from varied backgrounds, cultures, races, identities, life experiences, perspectives, beliefs, and values.
EEO Statement
We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, gender identity, sexual orientation, pregnancy and pregnancy-related conditions, or any other characteristic protected by law.",#N/A,10000+ Employees,College / University,Education,Colleges & Universities,1636,$10+ billion (USD)
"Baylor Scott & White Health
3.9",3.9,"Dallas, TX",Snowflake DBT Data Engineer,"JOB SUMMARY
The EBI Developer II is accountable for delivery of modern business intelligence, analytics, and data warehousing solutions including design, development, testing, implementation, and maintenance of the enterprise business intelligence platforms. May specialize in BI/reporting, ETL, and/or database technologies. Works with other technical team members to deploy solutions using best practices for architecture, design, coding, and testing to help data movement/transformation, data testing, relational/multidimensional (OLAP) database objects, and data visualization. The Developer 2 assumes tasks of a moderate to high level of complexity and difficulty.
ESSENTIAL FUNCTIONS OF THE ROLE
Understands detailed end user specifications. Design and develop solutions to accommodate business requirements.
Tunes and optimizes process to improve the performance and works with DBAs on optimizing query response times.
Implements procedures to maintain, monitor, backup and recovery operations for the environment.
Helps production deployments and automation. Conduct troubleshooting production and performance issues.
Establishes security standards for projects, roles, users, privileges in different environments (DEV, QA, PROD etc.).
Is accountable for Installs, configures, and upgrades BI software. Own activities associated with maintaining repository and metadata information.
Performs code reviews and provide and Design and develop test plans for unit testing.
Helps EBI Developer 3 to maintain inventory of all the jobs and processes and their SLAs.
KEY SUCCESS FACTORS
Must have experience in development using ETL or BI tools.
Must have Snowflake experience.
Must have DBT experience.
Must have experience in Data Warehousing, with thorough knowledge of dimensional modeling.
Must have experience in database development, using SQL and PL/SQL, with in-depth knowledge of systematic functions.
Experience with SDLC a must, exposure to Agile/SCRUM practices is ideal.
Must possess excellent documentation and communication skills.
Experience with IBM Netezza, Microsoft SQL Server, ETL tools (Netezza, Informatica) as well as enterprise BI reporting tools (IBM Cognos, SAP Business Objects, Microsoft SSRS) preferred.
Experience in tools administration is preferred.
BENEFITS
Our competitive benefits package includes the following
Immediate eligibility for health and welfare benefits
401(k) savings plan with dollar-for-dollar match up to 5%
Tuition Reimbursement
PTO accrual beginning Day 1
Note: Benefits may vary based upon position type and/or level
QUALIFICATIONS
EDUCATION - Bachelor's or 4 years of work experience above the minimum qualification
EXPERIENCE - 2 Years of Experience","$76,462 /yr (est.)",10000+ Employees,Company - Private,Healthcare,Health Care Services & Hospitals,1903,$5 to $10 billion (USD)
"Humana
3.9",3.9,"Jersey City, NJ",Senior Data Engineer,"Humana is seeking a Senior Data Engineer to join our Fortune #40, Best Places to Work company and help us make a difference as we help our members achieve their best health!

Team cohesion and support amongst our team members for one another is of the utmost importance to us.

The Senior Data Engineer works in all data environments, which includes data design, database architecture, metadata, repository creation and most importantly Data Governance. The work assignments involve moderately complex to complex issues where the analysis of situations or data requires an in-depth evaluation of variable factors. This role will be helping build and architect systems mainly using Microsoft SQL Server and related technology.
Responsibilities
Responsibilities
The Sr. Data Engineer is responsible for developing blueprints for all data repositories, evaluating hardware and software platforms, and integrating systems. You will be joining a team with strong technical skills to assist in these duties.
The Sr. Data Engineer:
Translates business needs into long-term solutions.
Defines, designs and builds dimensional database schemas.
Evaluates reusability of current data for separate analyses.
Conducts data sheering to rid the system of old, unused or duplicate data.
Reviews object and data models and the metadata repository to structure the data for better management and quicker access.
Begins to influence department’s strategy.
Makes decisions on moderately complex to complex issues regarding technical approach for project components, and work is performed without direction.
Exercises considerable latitude in determining objectives and approaches to assignments.
Required Qualifications
Bachelor's Degree or equivalent relevant work experience
Advanced knowledge of SQL with the ability to write complex SQL queries and stored procedures
Ability to create Architectural documents E.g.. Data Flows, Entity Relationship, Dependencies, etc.
Deep understanding of SQL-based systems and experience with modern ETL tools
Experience with SSIS
Knowledge of relational and multi-dimensional database architectures
Preferred Qualifications
Healthcare industry experience
Experience in Cloud based data warehousing and code versioning tools
Hands-on experience in cloud technologies as it relates to migration, implementation and deployment.
Knowledge of the Agile methodologies
Experience with Power BI
Additional Information
Location/Work Style: Remote US
Why Humana?
At Humana, we know your well-being is important to you, and it’s important to us too. That’s why we’re committed to making resources available to you that will enable you to become happier, healthier, and more productive in all areas of your life. Just to name a few:
Work-Life Balance
Generous PTO package
Health benefits effective day 1
Annual Incentive Plan
401K - Excellent company match
Well-being program
Paid Volunteer Time Off
Student Loan Refinancing
If you share our passion for helping people, we likely have the right place for you at Humana.
Work at Home Guidance
To ensure Home or Hybrid Home/Office associates’ ability to work effectively, the self-provided internet service of Home or Hybrid Home/Office associates must meet the following criteria:
At minimum, a download speed of 25 Mbps and an upload speed of 10 Mbps is recommended; wireless, wired cable or DSL connection is suggested
Satellite, cellular and microwave connection can be used only if approved by leadership
Associates who live and work from Home in the state of California, Illinois, Montana, or South Dakota will be provided a bi-weekly payment for their internet expense.
Humana will provide Home or Hybrid Home/Office associates with telephone equipment appropriate to meet the business requirements for their position/job.
Work from a dedicated space lacking ongoing interruptions to protect member PHI / HIPAA information
#LI-Remote
This is a remote position
Scheduled Weekly Hours
40

Not Specified
0",#N/A,10000+ Employees,Company - Public,Healthcare,Health Care Services & Hospitals,1961,$10+ billion (USD)
"McDonald's Corporation
3.5",3.5,"Chicago, IL","Data Engineer, Global Data, CRM","Company Description

McDonald’s evolving Accelerating the Arches growth strategy puts our customers and people first, and leverages our competitive advantages to strengthen our brand. We are recognized on lists like Fortune’s Most Admired Companies and Fast Company’s Most Innovative Companies.
Doubling Down on the 4Ds (Delivery, Digital, Drive Thru, and Development)
Our growth pillars emphasize the important role technology plays as the leading, global omni-channel restaurant brand. Technology enables the organization through digital technology, and improving the customer, crew and employee experience each and every day.
Global Technology forging the way
Leading the digitization of our business is the Technology organization made up of intrapreneurs who build industry defining tech using the latest innovations and platforms, like AI and edge computing to deliver on the next set of cutting-edge opportunities for the business. At McDonald’s you get to solve technology innovation challenges at an incredible scale, and work across global teams who are always hungry for a challenge. This provides access to exciting career paths for technologists. It’s bonus points when you get to see your family and friends use the tech you build at their favorite McD restaurant.

Job Description

We are seeking a highly motivated individual for the Data Engineering Manager for the Marketer Customer Data Platform team on Global Data and Architecture. This role will manage, develop, and lead the execution and implementation cloud-based data delivery systems for the MCD Customer Data Platform. This role will specialize in leading and managing the engineering teams in both onshore and offshore locations, as well as working with the relevant product management teams on overall product direction and implementation of the product vision.
What you will do:
Lead and manage the day-to-day operations for the CRM Data Engineering team with both, onshore and offshore, resources to with a focus on CDP/ CRM customer data technologies
Independently drive efforts in marketing technology and automation, collaborating with key stakeholders and business units. Must be able to find marketing solutions alongside building a long-term strategic vision of a marketing ecosystem across a highly technical, complex data stack
Participate in activities of the data engineering team on the solution, design, architect, implementation, support, and monitoring of CRM & CDP related data products
Develop and implement monitoring and remediation structure and applications for critical data flows
Continuously develop and deploy solutions to streamline and enhance the processes, logic, and tactics around customer cohort identification, and targeting via corporate CRM, marketing, and advertising systems. Additionally, develop the overall best practice framework for use by
Gather business and functional requirements and translate into robust, scalable, and user-friendly solutions

Qualifications
Bachelor’s degree in Information Technology or related technical fields such as computer science, marketing tech, mathematics, engineering, etc.
Minimum 2-5 years of relevant work experience in data analytics, engineering, computer science, mathematics or similar logic based fields
Minimum 2 years hands on experience with complex marketing automation tech stacks, leveraging CDP tools such as mParticle, Segment, and Tealium
Strong analytics skills, root cause analysis experiences on internal and external data and processes to answer business questions and improve application performance
Comfortable partnering and working with a wide range of stakeholders at various levels of management and leadership
Ability to focus on details as well as big picture
Competence and comfort focusing on specific detail engineering questions, as well as “big picture” questions
Ability to leverage critical thinking, problem-solving, and analytical skills to improve user experience, adoption, and outcomes
Curious, task-oriented self-starter with a strong bias for action; Proven track record of “getting things done”
Self-motivated with ability to set priorities and mentor others in a performance driven environment.
Passion for technology and innovation & uncompromising attention to detail & passion for problem solving
Desired Skills

Extensive experience working with Agile methodologies and expertise in production deployment processes and continuous deployment best practices.
1-2 years of hands-on experience with Braze and/or similar market automation delivery platforms
1-2 years of hands on experience with marketing automation and technology experience, understanding key components of marketing technology stacks, including CDP tools such as mParticle, Tealium, or Segment
Experience with clickstream tracking architecture implementations, and knowledge of Adobe Analytics and/or Google Analytics
Demonstrated ability to problem solve in complex technology solutioning
Experience developing Confluence pages and working with Jira
Demonstrable knowledge of multiple programming languages leveraged in martech/adtech stacks – javascript, ruby (liquid logic), html/css, python etc.

Additional Information

McDonald’s is committed to providing qualified individuals with disabilities reasonable accommodations to perform the essential functions of their jobs. Additionally, if you (or another applicant of whom you are aware) require assistance accessing or reading this job posting or otherwise seek assistance in the application process, please contact recruiting.supportteam@us.mcd.com
McDonald’s provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to sex, sex stereotyping, pregnancy (including pregnancy, childbirth, and medical conditions related to pregnancy, childbirth, or breastfeeding), race, color, religion, ancestry or national origin, age, disability status, medical condition, marital status, sexual orientation, gender, gender identity, gender expression, transgender status, protected military or veteran status, citizenship status, genetic information, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.
Nothing in this job posting or description should be construed as an offer or guarantee of employment.","$104,720 /yr (est.)",10000+ Employees,Company - Public,Restaurants & Food Service,Restaurants & Cafes,1955,$10+ billion (USD)
"Apple
4.2",4.2,"Cupertino, CA",Big Data Platform Engineer,"Summary
Posted: Nov 20, 2022
Weekly Hours: 40
Role Number:200444140
The SWE (Software) Data Analytics team at Apple collects, processes, and analyzes diagnostics and usage data from Apple devices across the world. We leverage streaming and batch analytics solutions to generate data that advises and drives product strategies across all of Apple software and hardware development. We discuss, analyze, and implement ground breaking solutions to problems of scale and distributed computing and are looking to expand our team with an engineer passionate about the big data workspace! Kafka, Flume, Hadoop, Spark, and other innovative technologies are core to our large scale infrastructure. You will be collaborating with data analysts, device engineers, and diverse engineering teams and drive the development of data pipelines and services with a high degree of ownership.
Key Qualifications
Experience developing large scale distributed computing systems.
In-depth knowledge and experience in one or more large scale distributed technologies including but not limited to: Hadoop ecosystem, Kafka, Samza, Flink, Storm, Flume, HBase, Cassandra, Redshift, Vertica, Spark.
Passion for and understanding of key algorithms and tools for developing high efficiency data processing systems.
Proficient in working with Linux or other POSIX operating systems, shell scripting, and networking technologies.
Problem-solving and debugging skills with experience in one or more of the following languages: Java, Python, Scala, Go, or Ruby.
There is a lot of communication involved! Excellent interpersonal skills are highly valued.
Description
As part of a team of highly skilled data engineers you will own significant responsibility in crafting, developing and maintaining our large-scale ETL pipelines, storage, and processing services. You will build self-service analytics tools to help engineering teams derive concrete metrics out of large volumes of raw data. You will partner with data science and engineering teams and develop algorithms to answer sophisticated questions on usage of Apple products. You will work closely with the DevOps team and develop monitoring and alerting scripts on various data pipelines and jobs. You will have the opportunity to learn and work on the latest Big Data technologies, lead PoCs to exercise new insights and, influence the strategic direction of our technology stack.
Education & Experience
Bachelors in Computer Science or equivalent experience
Additional Requirements
Experience using data storage technologies such as Apache Parquet or Avro Experience in machine learning algorithms is a plus.
Testing tools and methodologies to test large scale distributed computing systems.
Experience in data modeling and developing SQL database solutions is a plus.
Validated software engineering experience and field in design, test, source code management, and CI/CD practices.
Pay & Benefits
At Apple, base pay is one part of our total compensation package and is determined within a range. This provides the opportunity to progress as you grow and develop within a role. The base pay range for this role is between $130,000 and $242,000, and your base pay will depend on your skills, qualifications, experience, and location.

Apple employees also have the opportunity to become an Apple shareholder through participation in Apple’s discretionary employee stock programs. Apple employees are eligible for discretionary restricted stock unit awards, and can purchase Apple stock at a discount if voluntarily participating in Apple’s Employee Stock Purchase Plan. You’ll also receive benefits including: Comprehensive medical and dental coverage, retirement benefits, a range of discounted products and free services, and for formal education related to advancing your career at Apple, reimbursement for certain educational expenses — including tuition. Additionally, this role might be eligible for discretionary bonuses or commission payments as well as relocation. Learn more about Apple Benefits.

Note: Apple benefit, compensation and employee stock programs are subject to eligibility requirements and other terms of the applicable plan or program.",#N/A,10000+ Employees,Company - Public,Information Technology,Computer Hardware Development,1976,$10+ billion (USD)
"Fidelity Investments
4.3",4.3,"Durham, NC",Senior Data Engineer,"Job Description:
The Role
Do you enjoy the challenge of solving problems and working in a fast-paced environment where no two days are the same? Are you able to identify big picture ideas/concepts and translate those into well-defined and executable solutions? If so, the Senior Data Engineer role may be the right next step in your career!

The Team
Data Consulting is a group within Fidelity Workplace Investing built to serve and partner with Fidelity’s strategic clients on their data and technology needs. The Data Technology Team's dynamic and diverse skills are leveraged to complete sophisticated and exciting projects in partnership with both internal and external partners.

We design and build customized solutions to address unique scenarios on behalf of clients. We extract data from a variety of sources (Snowflake, SQLMI, client feeds/files, etc.), transform the data, then output the results based on requirements.

Our team engages across Workplace Consulting in data quality improvement efforts that span single and multiple product lines (i.e. Health and Welfare, Defined Benefit, Defined Contribution, Stock Plan Services).

The Expertise You Have and The Skills You Bring
5+ years in Technology with 3 years in developing and deploying data solutions
Strong understanding of data analysis concepts and associated tools such as SQL, Python, Stored Procedures, etc.
Extensive experience with SQL required; knowledge of MySQL, Snowflake (workspaces) preferred
Proficiency with ETL methods and tools – Informatica and Snap Logic preferred
Systems analysis background with an emphasis on legacy process decomposition and debugging/tuning/refactoring
Strong analytical and problem-solving skills, ability to research and find answers to technical challenges and learn new technologies as required
Prior exposure to data warehousing concepts and dimensional data models
Excellent written and verbal communication skills including experience writing documentation
Excellent collaboration skills to work with multiple teams in the organization
Ability to deal with ambiguity and work in a fast-paced environment
You like to take ownership, resolve issues and care about the quality of your work
Experience with WI Defined Benefit, Defined Contribution, Health & Welfare and/or Stock Plan Service business, data process flows and recordkeeping processes a plus
Knowledge of Fidelity backend product tables (WIDE, Snowflake, DBR1, etc.) a plus
Hands on experience with AWS or Azure cloud concepts is a plus
Project management experience is a plus
The Value You Deliver
Communicating with Business and Technical Consultants across WorkPlace Consulting practice areas on a wide variety of projects, you will analyze and maintain processes, identify data issues, perform root cause analysis, and help us to streamline existing processes.
Please see below for the salary range for work locations in Colorado only:
$64,000 - $100,000 per year
This position is eligible for incentive compensation or an annual bonus opportunity.
Please see below for the salary range for work locations in New York City, Westchester County, NY and Jersey City, NJ only:
N/A
Please see below for the salary range for work locations in California only:
N/A
Please see below for the salary range for work locations in Washington only:
N/A
Certifications:
Company Overview
Fidelity Investments is a privately held company with a mission to strengthen the financial well-being of our clients. We help people invest and plan for their future. We assist companies and non-profit organizations in delivering benefits to their employees. And we provide institutions and independent advisors with investment and technology solutions to help invest their own clients' money.

Join Us
At Fidelity, you'll find endless opportunities to build a meaningful career that positively impacts peoples' lives, including yours. You can take advantage of flexible benefits that support you through every stage of your career, empowering you to thrive at work and at home. Honored with a Glassdoor Employees' Choice Award, we have been recognized by our employees as a Best Place to Work in 2023. And you don't need a finance background to succeed at Fidelity—we offer a range of opportunities for learning so you can build the career you've always imagined.
At Fidelity, our goal is for most people to work flexibly in a way that balances both personal and business needs with time onsite and offsite through what we’re calling “Dynamic Working”. Most associates will have a hybrid schedule with a requirement to work onsite at a Fidelity work location for at least one week, 5 consecutive days, every four weeks. These requirements are subject to change.
We invite you to Find Your Fidelity at fidelitycareers.com.

Fidelity Investments is an equal opportunity employer. We believe that the most effective way to attract, develop and retain a diverse workforce is to build an enduring culture of inclusion and belonging.
Fidelity will reasonably accommodate applicants with disabilities who need adjustments to participate in the application or interview process. To initiate a request for an accommodation, contact the HR Accommodation Team by sending an email to accommodations @fmr.com, or by calling 800-835-5099, prompt 2, option 3.","$82,000 /yr (est.)",10000+ Employees,Company - Private,Financial Services,Investment & Asset Management,1946,$10+ billion (USD)
"Match Group
3.7",3.7,"Dallas, TX",Sr. Data Engineer,"Know where you belong.

Match Group is a leading provider of dating products across the globe. Our portfolio includes Tinder, Match, Hinge, PlentyOfFish, The League, and others, each designed to spark meaningful connections for singles worldwide. Creating a sense of belonging doesn’t stop at our products - it’s the foundation of every team we hire.

As a Sr. Data Engineer, you will be implementing critical ETL pipelines and advancing best practices for the data engineering team, and the rest of the organization. You will work on delivering an actual big data architecture while concentrating on real-world problems such as privacy concerns.

This role is key to the success of Match Group. Not only will you help power the love lives of millions of people, but you will play a critical part in the functioning of every brand at Match Group (Match, Tinder, Hinge, Okcupid, PlentyofFish, BLK, and others), with stakeholders ranging from customer experience to marketing to leadership.

When it comes to dating, the connection starts online, but the real magic happens once you meet in real life (IRL). We think the same is true for creating the best platforms, so we work together IRL in our Dallas office 2 days/week.
How you'll make an impact:
Work with our Engineering teams to ensure data is flowing accurately through data creation to our presentation layers
Become an advocate for the Data Engineering team by developing and championing Data Engineering practices with the team and with the company at large
Improve our Data Engineering stack through containerization, data modeling, developing our ETL pipelines, and building scalable/reliable solutions
Work with stakeholders and translate their needs and expectations into action items and deliverables
Design and implement efficient data models to support business intelligence and data analytics
Develop and maintain data quality checks and data monitoring systems
Support existing on-prem infrastructure and help expand our processes into the cloud (AWS)
We could be a Match if you bring:
Expertise in SQL, Data Modeling, and Python
5+ years of professional/industry experience
Prior Airflow and/or Python experience is required
Used Redshift, Airflow, Spectrum and relational database like SQL Server
Capability to drive initiatives and articulate their value to Engineering and other stakeholders
Working knowledge of SSIS (SQL Server Integration Services), SSAS (SQL Server Analysis Services) and Amazon QuickSight is plus
What's the team like?
Our BI team is a service organization that delivers reporting solutions to the entire Match Group enterprise
The BI team is responsible for architecting and engineering new data systems and reporting to help facilitate business decision-making

#LI-CENTRAL
#MOGUL
#LI-CH1

Why Match Group?

Our mission is simple – to help people find love and happiness! We love our employees too and understand the importance of all life's milestones. Here are some of the benefits we are proud to offer:

Mind & Body – Medical, mental health, and wellness benefits to support your overall health and well-being
Financial Wellness – Competitive compensation, 100% employer match on 401k contributions up to 10% (cap at $10,000), as well as an employee stock purchase program to help you feel supported in your financial security
Unplug – Generous PTO and 15 paid holidays so you can unplug
Career – Annual training allowance for professional development and ERG membership opportunities and events so you feel connected and empowered in your work
Family – Families come in all shapes and sizes so we offer 20 weeks of 100% paid parental leave, fertility, adoption, and child care resources, as well as pet insurance and discounts
WFH Stipend – Hybrid/remote work allowance for full time employees to help you feel comfortable and efficient in your home office environment
Company Gatherings – We host fun happy hours and company events where our employees get to know each other and build a sense of connection and belonging!

We are proud to be an equal opportunity employer and we value the rich dynamics that diversity brings to our company. We do not discriminate on the basis of race, religion, color, creed, national origin, ancestry, disability, marital status, age, sexual orientation, sex (including pregnancy and sexual harassment), gender identity or expression, uniformed service or veteran status, genetic information, or any other legally protected characteristic. Period.","$104,956 /yr (est.)",1001 to 5000 Employees,Company - Public,Information Technology,Internet & Web Services,2015,$1 to $5 billion (USD)
"Blue Cross Blue Shield of Massachusetts
4.0",4.0,"Boston, MA",Data Engineer,"Ready to help us transform healthcare? Bring your true colors to blue.

Job Description Summary:
At Blue Cross Blue Shield of Massachusetts, we have an exciting opportunity for a Data Engineer to join our Core Data Engineering team of analytics delivery and insights. Primary focus of this role is to work on multiple applications alongside with other engineers to design, build & deliver data solutions. Other element of the role is to work in conjunction with Senior Data Engineer to contribute towards the delivery of Data Platform. This is hands on position with 80% focus towards data engineering and 20% towards best practice, unit testing and reviewing code.

Responsibilities:
The Data Engineer will be responsible for the development, implementation, testing, documentation, and maintenance of analytics deliver and insights data solution applications.

Performs data analysis required to troubleshoot data related issues and assist in the resolution of data issues.

Define relational tables, primary and foreign keys, and stored procedures to create a data model structure.

Partner directly with data owners and business SMEs to identify needed data sources, ensure the data is provided to project team in a reliable fashion, and prepare data in an optimal format for analysis

Develop ETL pipelines for optimal performance in collaboration with other data engineers.

Develop data integrations and data quality framework.

Analyze complex data elements, systems, data flows, dependencies, and relationships to contribute to conceptual, physical, and logical data models

Perform initial data quality checks on extracted data.

Automate manual processes and optimize data delivery.

Test the code using the appropriate testing approach.

Required Technical Skills

Experience or exposure to AWS cloud services (AWS Glue, Lambda, EMR, Athena, Sage maker etc.)

Experience or exposure to OLAP architecture (Snowflake, Redshift etc.)

Experience or exposure to big data tools: Hadoop, Spark, Kafka, Hive, Presto, Impala etc.

Experience or exposure to relational SQL and NoSQL databases, including Postgres, DynamoDB.

Experience or exposure to data pipeline and workflow management tools: Airflow, Oozie etc.

Experience or exposure to DevSecOps pipeline is preferred (Jenkins, Veracode, Sonar cube etc.)

Experience building ETL solutions and familiarity with tools IICS/Talend is preferred

Experience or exposure to stream-processing systems: Storm, Spark-Streaming, etc.

Expert in one of the object-oriented/object function scripting languages: Python/Spark is preferred, Java and Scala.

Work with various project teams to deliver on commitments within time and scope

Experience or a good understanding of different types of data ingestion formats like JSON, XML, parquet file

Experience in developing scalable data engineering platform using cloud infrastructure.

Experience or a good understanding and willingness to include Security at design, development, and testing

Experience in performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.

Experience in building processes for supporting data transformation, data structures, metadata, dependency, and workload management.

Understanding of servers, storage, and networking roles
Qualification

Bachelor’s degree in Computer Science, Math or Information Systems or related fields.

Strong verbal and written communication skills.

Ability to express complex technical concepts effectively, both verbally and in writing.

Ability to multi-task in a fast-paced, changing environment.

Demonstrate strong organization skills and detail oriented.

Experience or exposure to large-scale, complex data environments.

Ability to self-motivate and meet deadlines.

Intense desire to learn.

Exposure to an Enterprise Data Lake & Lakehouse concept.

Minimum Education Requirements:
High school degree or equivalent required unless otherwise noted above

Location Boston Time Type Full time

The job posting range is the lowest to highest salary we in good faith believe we would pay for this role at the time of this posting. We may ultimately pay more or less than the posted range, and the range may be modified in the future. An employee’s pay position within the salary range will be based on several factors including, but limited to, relevant education, qualifications, certifications, experience, skills, performance, shift, travel requirements, sales or revenue-based metrics, and business or organizational needs and affordability.

This job is also eligible for variable pay.

We offer comprehensive package of benefits including paid time off, medical/dental/vision insurance, 401(k), and a suite of well-being benefits to eligible employees.

Note: No amount of pay is considered to be wages or compensation until such amount is earned, vested, and determinable. The amount and availability of any bonus, commission, or any other form of compensation that are allocable to a particular employee remains in the Company's sole discretion unless and until paid and may be modified at the Company’s sole discretion, consistent with the law.

WHY Blue Cross Blue Shield of MA?
We understand that the confidence gap and imposter syndrome can prevent amazing candidates coming our way, so please don’t hesitate to apply. We’d love to hear from you. You might be just what we need for this role or possibly another one at Blue Cross Blue Shield of MA. The more voices we have represented and amplified in our business, the more we will all thrive, contribute, and be brilliant. We encourage you to bring us your true colors, , your perspectives, and your experiences. It’s in our differences that we will remain relentless in our pursuit to transform healthcare for ALL.

As an employer, we are committed to investing in your development and providing the necessary resources to enable your success. Learn how we are dedicated to creating an inclusive and rewarding workplace that promotes excellence and provides opportunities for employees to forge their unique career path by visiting our Company Culture page. If this sounds like something you’d like to be a part of, we’d love to hear from you. You can also join our Talent Community to stay “in the know” on all things Blue.

At Blue Cross Blue Shield of Massachusetts, we believe in wellness and that work/life balance is a key part of associate wellbeing. We provide a flexible hybrid work model in which roles are designated as resident (on site 4-5 days/week), mobile (on site 1-3 days/week), or eworker (on site 0-3 days/month).","$108,737 /yr (est.)",1001 to 5000 Employees,Nonprofit Organization,Insurance,Insurance Carriers,1937,$100 to $500 million (USD)
"Blue Cross Blue Shield of Arizona
4.2",4.2,United States,Data Engineer - REMOTE-AZ,"Awarded a Healthiest Employer, Blue Cross Blue Shield of Arizona aims to fulfill its mission to inspire health and make it easy. BCBSAZ offers a variety of health insurance products and services to meet the diverse needs of individuals, families, and small and large businesses as well as providing information and tools to help individuals make better health decisions.
Designs and implements business intelligence and extract, transform, and load (ETL) solutions using programming, performance tuning, data modeling. Creates databases optimized for performance, implementing schema changes, and maintaining data architecture standards across all of the business’s databases. Serves as a liaison between the Database Administration department and development teams.
REQUIRED QUALIFICATIONS
Required Work Experience
4 years of experience in computer programming, query design, and databases
6 years of experience in computer programming, query design, and databases (Senior level)
Required Education
High-School Diploma or GED in general field of study
PREFERRED QUALIFICATIONS
Preferred Work Experience
4+ years of experience building and managing complex Data Integration solutions.
4+ years of experience in computer programming, query design, and databases
4+ years of experience Database administration with SQL Server
6+ years of experience building and managing complex Data Integration solutions (Senior)
6+ years of experience in computer programming, query design, and databases (Senior)
6+ years of experience Database administration with SQL Server (Senior)
Preferred Education
Bachelor’s Degree in Information Technology or related field preferred
Master’s Degree in Information Technology or related field (Senior)

ESSENTIAL JOB FUNCTIONS AND RESPONSIBILITIES

Data Engineer – Performs job functions with general supervision and peer review
Learn area’s direct flow; and how it affects surrounding systems and operational areas.
Architect, design, construct, test, tune, deploy, and support Data Integration solutions for various data management systems.
Contribute to the team’s knowledge base with useful information such as adopted standards, procedure documentation, problem resolution advice, etc.
Participate in the promotion of SQL Server best practices
Collaborate with other technology teams and architects to define and develop solutions.
Research and experiment with emerging Data Integration technologies and tools.
Work with the team to establish and reinforce disciplined software development, processes, standards, and error recovery procedures are deployed; ensuring a high degree of data quality.
Support Enterprise database clustering, mirroring, replication among other SQL Server technologies.
Develop, write and implement processing requirements and post implementation review
Facilitate and/or create new procedures and processes that support advancing technologies or capabilities
Design & Implement Extract, Transform, and Load (ETL) solutions utilizing SSIS
Apply data mining rules
Create logic, system, and program flows for complex systems, including interfaces and metadata
Write and execute unit test plans. Track and resolve any processing issues.
Implement and maintain operational and disaster-recovery procedures.
Participate in the review of code and/or systems for proper design standards, content and functionality.
Participate in all aspects of the Software Development Life Cycle
Analyze files and map data from one system to another
Adhere to established source control versioning policies and procedures
Meet timeliness and accuracy goals.
Communicate status of work assignments to stakeholders and management.
Responsible for technical and production support documentation in accordance with department standards and industry best practices.
Maintain current knowledge on new developments in technology-related industries
Participate in corporate quality and data governance programs
The position requires a full-time work schedule. Full-time is defined as working at least 40 hours per week, plus any additional hours as requested or as needed to meet business requirements.
Senior - Performs job functions with minimal supervision in a Lead capacity
Research, analyze, track and resolve complex production problems.
Build, support, and maintain complex processes, programs, and data.
Evaluate high-level project information and assess project components to forecast work effort required.
Provide peer-level review and mentoring to peers.
Participate and/or lead complex technical projects.
Act as a subject matter expert in two or more of areas as assigned:
BI Production Reporting
DAtabase Architecture
Database Architecture and Support
Training
Act as primary operational contact for internal and external customers when needed or in the absence of manager.
Ensure Service Level Agreements between department and operational or technical areas are met.
Lead, develop and mentor staff by providing opportunities for growth through delegation, training, and assignment to various project teams.
Inform manager of any issues impacting the efficient and effective performance of the department including system, resource, and informational barriers; Provide timely feedback to team member on performance.
Assist the manager in the day-to-day operations of the department.
ALL LEVELS
Each progressive level includes the ability to perform the essential functions of any lower levels and mentor employees in those levels.
Maintain current on new developments in technology-related industries.
Participate in corporate quality and data governance programs.
Participate in on-call rotation.
Perform all other duties as assigned.
The position requires a full-time work schedule. Full-time is defined as working at least 40 hours per week, plus any additional hours as requested or as needed to meet business requirements. Participate in on-call rotation.
competencies
REQUIRED COMPETENCIES
Required Job Skills (Applies to All Levels)
Intermediate skill in use of office equipment, including copiers, fax machines, scanner and telephones
Intermediate PC proficiency in spreadsheet, database and word processing software
Advanced knowledge of business intelligence, programming, and data analysis sotfware
Intermediate knowledge of Microsoft SQL databases and database administration
Intermediate proficiency in T-SQL, NZ-SQL, PostgreSQL, NoSQL, Hadoop, data tuning, enterprise data modeling and schema change management.
Working technical knowledge of current software protocols and Internet standards to the extent that they apply to database administration.
Excellent database troubleshooting skills
Working technical knowledge of PowerShell.
Strong object-oriented design and analysis skills
Experience consuming, organizing and analyzing JSON and XML messages as data.
Required Professional Competencies (Applies to All Levels)
Knowledge of agile development practices
Strong analytical skills to support independent and effective decisions
Ability to prioritize tasks and work with multiple priorities, sometimes under limited time constraints.
Perserverance in the face of resistance or setbacks.
Effective interpersonal skills and ability to maintain positive working relationship with others.
Verbal and written communication skills and the ability to interact professionally with a diverse group, executives, managers, and subject matter experts.
Systems research and analysis. Ability to write and present business intelligence documentation
Demonstrate the ability to stay current on global threats and vulnerabilities.
Maintain confidentiality and privacy
Required Leadership Experience and Competencies
Build synergy with a diverse team in an ever changing environment
Facilitate and resolve customer requests and inquiries for all levels of management within the Corporation (Senior only)
Participation in one or more groups that aids in setting/enforcing standards and/or providing educational opportunities across the IT organization. (Senior only)
PREFERRED COMPETENCIES
Preferred Job Skills (Applies to All Levels)
Knowledge of HIPAA regulations
Advanced proficiency in spreadsheet, SQL queries, database, flow charting, and word processing software
Advanced knowledge of data mapping techniques
Advanced knowledge of computer operating systems
Advanced knowledge of decision support systems
Advanced knowledge of programming, database systems, and data management.
Advanced knowledge of decision support systems
Advanced knowledge of Business Objects
Preferred Professional Competencies (Applies to All Levels)
Advanced systems research and analysis expertise
Impeccable project management skills
Solid technical ability and problem solving skills
Knowledge of internal departments and operations
Strong technical documentation skills and a strong ability to translate technical concepts so that they are easily understood by laypersons
Preferred Leadership Experience and Competencies (Applies to Senior level)
Ability to provide mentoring and peer review to junior team members
Ability to build lesson plans and deliver lessons to junior team members
REQUIRED QUALIFICATIONS
Required Work Experience
4 years of experience in computer programming, query design, and databases
6 years of experience in computer programming, query design, and databases (Senior level)
Required Education
High-School Diploma or GED in general field of study
Required Licenses
N/A
Required Certifications
N/A
PREFERRED QUALIFICATIONS
Preferred Work Experience
4+ years of experience building and managing complex Data Integration solutions.
4+ years of experience in computer programming, query design, and databases
4+ years of experience Database administration with SQL Server
6+ years of experience building and managing complex Data Integration solutions (Senior)
6+ years of experience in computer programming, query design, and databases (Senior)
6+ years of experience Database administration with SQL Server (Senior)
Preferred Education
Bachelor’s Degree in Information Technology or related field preferred
Master’s Degree in Information Technology or related field (Senior)
Preferred Licenses
N/A
Preferred Certifications
MS SQL Certification or other certification in current programming languages
ESSENTIAL job functions AND RESPONSIBILITIES

DAta Engineer – Performs job functions with general supervision and peer review
Learn area’s direct flow; and how it affects surrounding systems and operational areas.
Architect, design, construct, test, tune, deploy, and support Data Integration solutions for various data management systems.
Contribute to the team’s knowledge base with useful information such as adopted standards, procedure documentation, problem resolution advice, etc.
Participate in the promotion of SQL Server best practices
Collaborate with other technology teams and architects to define and develop solutions.
Research and experiment with emerging Data Integration technologies and tools.
Work with the team to establish and reinforce disciplined software development, processes, standards, and error recovery procedures are deployed; ensuring a high degree of data quality.
Support Enterprise database clustering, mirroring, replication among other SQL Server technologies.
Develop, write and implement processing requirements and post implementation review
Facilitate and/or create new procedures and processes that support advancing technologies or capabilities
Design & Implement Extract, Transform, and Load (ETL) solutions utilizing SSIS
Apply data mining rules
Create logic, system, and program flows for complex systems, including interfaces and metadata
Write and execute unit test plans. Track and resolve any processing issues.
Implement and maintain operational and disaster-recovery procedures.
Participate in the review of code and/or systems for proper design standards, content and functionality.
Participate in all aspects of the Software Development Life Cycle
Analyze files and map data from one system to another
Adhere to established source control versioning policies and procedures
Meet timeliness and accuracy goals.
Communicate status of work assignments to stakeholders and management.
Responsible for technical and production support documentation in accordance with department standards and industry best practices.
Maintain current knowledge on new developments in technology-related industries
Participate in corporate quality and data governance programs
The position requires a full-time work schedule. Full-time is defined as working at least 40 hours per week, plus any additional hours as requested or as needed to meet business requirements.
Senior - Performs job functions with minimal supervision in a Lead capacity
Research, analyze, track and resolve complex production problems.
Build, support, and maintain complex processes, programs, and data.
Evaluate high-level project information and assess project components to forecast work effort required.
Provide peer-level review and mentoring to peers.
Participate and/or lead complex technical projects.
Act as a subject matter expert in two or more of areas as assigned:
BI Production Reporting
DAtabase Architecture
Database Architecture and Support
Training
Act as primary operational contact for internal and external customers when needed or in the absence of manager.
Ensure Service Level Agreements between department and operational or technical areas are met.
Lead, develop and mentor staff by providing opportunities for growth through delegation, training, and assignment to various project teams.
Inform manager of any issues impacting the efficient and effective performance of the department including system, resource, and informational barriers; Provide timely feedback to team member on performance.
Assist the manager in the day-to-day operations of the department.
ALL LEVELS
Each progressive level includes the ability to perform the essential functions of any lower levels and mentor employees in those levels.
Maintain current on new developments in technology-related industries.
Participate in corporate quality and data governance programs.
Participate in on-call rotation.
Perform all other duties as assigned.
The position requires a full-time work schedule. Full-time is defined as working at least 40 hours per week, plus any additional hours as requested or as needed to meet business requirements. Participate in on-call rotation.
competencies
REQUIRED COMPETENCIES
Required Job Skills (Applies to All Levels)
Intermediate skill in use of office equipment, including copiers, fax machines, scanner and telephones
Intermediate PC proficiency in spreadsheet, database and word processing software
Advanced knowledge of business intelligence, programming, and data analysis sotfware
Intermediate knowledge of Microsoft SQL databases and database administration
Intermediate proficiency in T-SQL, NZ-SQL, PostgreSQL, NoSQL, Hadoop, data tuning, enterprise data modeling and schema change management.
Working technical knowledge of current software protocols and Internet standards to the extent that they apply to database administration.
Excellent database troubleshooting skills
Working technical knowledge of PowerShell.
Strong object-oriented design and analysis skills
Experience consuming, organizing and analyzing JSON and XML messages as data.
Required Professional Competencies (Applies to All Levels)
Knowledge of agile development practices
Strong analytical skills to support independent and effective decisions
Ability to prioritize tasks and work with multiple priorities, sometimes under limited time constraints.
Perserverance in the face of resistance or setbacks.
Effective interpersonal skills and ability to maintain positive working relationship with others.
Verbal and written communication skills and the ability to interact professionally with a diverse group, executives, managers, and subject matter experts.
Systems research and analysis. Ability to write and present business intelligence documentation
Demonstrate the ability to stay current on global threats and vulnerabilities.
Maintain confidentiality and privacy
Required Leadership Experience and Competencies
Build synergy with a diverse team in an ever changing environment
Facilitate and resolve customer requests and inquiries for all levels of management within the Corporation (Senior only)
Participation in one or more groups that aids in setting/enforcing standards and/or providing educational opportunities across the IT organization. (Senior only)
PREFERRED COMPETENCIES
Preferred Job Skills (Applies to All Levels)
Knowledge of HIPAA regulations
Advanced proficiency in spreadsheet, SQL queries, database, flow charting, and word processing software
Advanced knowledge of data mapping techniques
Advanced knowledge of computer operating systems
Advanced knowledge of decision support systems
Advanced knowledge of programming, database systems, and data management.
Advanced knowledge of decision support systems
Advanced knowledge of Business Objects
Preferred Professional Competencies (Applies to All Levels)
Advanced systems research and analysis expertise
Impeccable project management skills
Solid technical ability and problem solving skills
Knowledge of internal departments and operations
Strong technical documentation skills and a strong ability to translate technical concepts so that they are easily understood by laypersons
Preferred Leadership Experience and Competencies (Applies to Senior level)
Ability to provide mentoring and peer review to junior team members
Ability to build lesson plans and deliver lessons to junior team members
Our Commitment
BCBSAZ does not discriminate in hiring or employment on the basis of race, ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, protected veteran status or any other protected group.
Thank you for your interest in Blue Cross Blue Shield of Arizona. For more information on our company, see azblue.com. If interested in this position, please apply.",#N/A,1001 to 5000 Employees,Nonprofit Organization,Insurance,Insurance Carriers,1939,Unknown / Non-Applicable
"Tietoevry
4.2",4.2,"Hudson, MA",Data Center and Network Engineer,"A leader digital services and software company
Tietoevry creates digital advantage for businesses and society. We are a leading digital services and software company with global presence. Our values - openness, trust, and diversity - and Nordic heritage steer our success.
Headquartered in Finland, Tietoevry employs around 24 000 experts globally. The company serves thousands of enterprises and public sector customers in more than 90 countries. Tietoevry’s annual turnover is approximately EUR 3 billion and its shares are listed on the NASDAQ in Helsinki and Stockholm as well as on the Oslo Børs.
Job description
Tieto U.S. Inc is growing its services to US customers, and we are looking for a skilled Junior-Midlevel Linux admin and Network Engineer for our Data Center R&D IT laboratory automation.
We are not a Cloud Based operation. AWS, Azure, Google cloud experience though a use-full Skill is not a required skill. This position values and requires hands on knowledge and skills with physical hardware and the different Linux OS flavors. Expert skills are not required. What is required is a ""solid foundation"" in Linux OS and a firm understanding of Networking, Hardware, Hardware and OS.
The ideal candidate should have skills and a firm understanding in the foundations of Linux and DevOps and able to handle the lab automation projects consisting of deployments of OS's, access administration, and day-to-day changes that our engineering group and customer needs on Bare Metal Servers. In leu of Devops/Automation we will strongly consider candidates skilled and competent within reason in any of the common and popular programming language such as Java, C++, C#, Python. It is our belief that individuals with these set of skills can easily navigate thru the day to day challenges while growing and incorporating new Automation and CI platforms and challenges into the mix.
Desired skills Example:
- (* a must have) *Linux User administration on Bare Metal. Setting up and tearing down access privileges, Linux deployment, and debugging Bare Metal servers and VM's. Automation skills with the use of Ansible, Jenkins, Puppet Vagrant, will be included in the daily routines.
-In leu of Automation/CI skills, reasonable competence in any of the common and popular programming language such as Java, C++, C#, Python.
- Strong understanding of the nuances of Linux across major Distributions
- Good Linux/Unix Networking skills i.e. config of the NICs via CLI environment or Editable Text Config files,
- Knowledge of firewalls set up, Boot process, LVM, EFI, and Legacy Boot and Hard Drive partitions, creating mount points.
- A Break-Fix (without breaking it) approach to troubleshooting.
- Debugging skills. Maintain an organized and methodical approach to debugging
- Be able to work around inconsistent/intermittent results/outcomes.
- Solid understanding of Hypervisors, Virtualization (VM's) their purpose and use.
- Understanding of Automation, Cloud Technologies, VM Deployment/Orchestration methods, and technologies i.e. PXE, Cloud, Containers.
- Strong scripting experience (enabling quick automation engineering)
- Experience in the Datacenter HW environment and familiarity with Deployment automation tools like MAAS or RedHat Beaker would be an additional asset
Additional Skills Requirements
Organizational Skills
Ability to switch gears while prioritizing tasks
Great with Communication
Ability to complete a task with little or no supervision once directed
Commitment to the Position with our Client
Time Management
Ability to work under timelines
Multi - Task
Methodical
Attention to detail
Willingness to Learn
Ability to think ahead, Planning
Join our growing team, with very generous benefits with health, insurances, 401k, vacation.
Location: Hudson, MA **THIS IS NOT A REMOTE POSITION
The position will be opened on 1/15/2023.
Job Type: Full-time
Pay: $55,000.00 - $70,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Life insurance
Paid time off
Vision insurance
Ability to commute/relocate:
Hudson, MA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Linux Admin: 1 year (Required)
Work Location: In person","$62,500 /yr (est.)",10000+ Employees,Company - Public,Information Technology,Information Technology Support Services,1968,$1 to $5 billion (USD)
"Inovalon
3.1",3.1,United States,Senior Software Data Engineer,"Inovalon was founded in 1998 on the belief that technology, and data specifically, would empower the transformation of the entire healthcare ecosystem for the better, improving both outcomes and economics. At Inovalon, we believe that when our customers are successful in their missions, healthcare improves. Therefore, we focus on empowering them with data-driven solutions. And the momentum is building.
Together, as ONE Inovalon, we are a united force delivering solutions that address healthcare's greatest needs. Through our mission-based culture of inclusion and innovation, our organization brings value not just to our customers, but to the millions of patients and members they serve.
Overview: The Senior Data Engineer is responsible for contributing to data warehouse/lake, data operations organization. They will support existing data operations such as data pipeline, ETL, data lake / warehouse, data structure development, and troubleshooting of data issues. They will be part of the team which is building next generation data & reporting platform. This platform will cater internal business stakeholders and external customers to provide insights & forecasting to understand current state of business, improve decision-making for their tactical and strategic goals & KPIs. This position may require independent work, sharing information and assisting others with work request.
Duties and Responsibilities:
Work with the agile team to participate in agile ceremonies like grooming, planning, standup, retrospective, demos
Actively contribute to grooming, and standup, create & update tasks, estimate and status
Write complex queries, stored procedures, functions, SSIS Packages for various job execution
Work with data architects and business analysts to create a logical data model and create DDL scripts for physical model creation
Design and develop modern ETL framework utilizing tools like ADF (Azure Data Factory), MS-SSIS etc
Design and develop ETL pipelines, using SQL, Stored procedures/functions to extract data from various sources and load into warehouse
Design STAR or SNOWFLAKE database schema utilizing industry best practices to build Data warehouse, data marts, views, cubes and data sets/products
Design and code various data architecture component like data validation, cleansing, de-duping, Symantec layer etc
Design and develop data export frameworks to extract data from the warehouse, transform, pre-aggregate, perform calculations and load into various data marts for Analytics use
Design & develop configurable data export framework to extract data from Data warehouse and data marts to generate reports for internal and external customers in .csv, flat files and
Work on large data to ensure configurable ingestion of data, dynamic rule & validation of data, cleansing, transforming and loading into the data warehouse
Design and implement data validation and quality checks to ensure the accuracy and completeness of the data in the data warehouse
Build data marts which will be utilized by Analytics tools like PowerBI and work with PowerBI developer to optimize database schema and queries
Perform performance of queries and data processing, identify and resolve any issues looking at query plans, create appropriate indexes, resolve dead locks and create table hints
Participate in design discussions, data analysis, data model creation etc
Product quality design diagrams (using MS-Visio, Draw.io etc) and documentations (MS-Work, Excel etc)
Maintain compliance with Inovalon's policies, procedures and mission statement;
Adhere to all confidentiality and HIPAA requirements as outlined within Inovalon's Operating Policies and Procedures in all ways and at all times with respect to any aspect of the data handled or services rendered in the undertaking of the position; and
Fulfill those responsibilities and/or duties that may be reasonably provided by Inovalon for the purpose of achieving operational and financial success of the Company.
Job Requirements:
Minimum of 7 years industry experience working in data & reporting area, knowledge of healthcare data will be a plus
5+ years working on Datawarehouse, ETL process/pipeline, Data Workflow, Query Plans & Optimization
5+ experience in MS SQL, T-SQL, ETL Jobs
5+ experience in Microsoft tools like SSMS, SSIS, SQL Server
5+ years' experience in writing query plans, indexes etc
2+ years working with Analytics tools (like Tableau, PowerBI – Preferred)
2+ years' experience working on Azure Cloud is preferred utilizing ADF (Azure Data Factory), Azure Delta Lake, Azure SQL Server, Data Sync, Log Insights and Analytics
Experience working with Role based security at database, ETL jobs, data exports level
Strong understanding of database concepts and schema (like star, snowflake schema)
Experience with HIPPA and PHI will be a plus
Ability to effectively communicate with internal and external customers
Excellent verbal and written communication skills
Excellent computer proficiency (MS Office – Word, Excel and Outlook)
Must be able to work under pressure and meet deadlines; and
Ability to work independently and to carry out tasks to completion following standard accepted practices
Education:
BS degree in Computer Science or Computer Engineering, Business, or equivalent experience.
Physical Demands and Work Environment:
Sedentary work (i.e., sitting for long periods of time);
Exerting up to 10 pounds of force occasionally and/or negligible amount of force;
Frequently or constantly to lift, carry push, pull or otherwise move objects and repetitive motions;
Subject to inside environmental conditions; and
Some travel (less than 10%) may be required for this position, primarily for training and collaboration purposes.
Studies have shown that women and people of color are less likely to apply for jobs unless they believe they meet every one of the qualifications listed in a job description. If you don't meet every qualification listed but are excited about our mission and the work described, we encourage you to apply regardless. Inovalon is most interested in finding the best candidate for the job and you may be just the right person for this or other roles.
By embracing diversity, equity and inclusion we enhance our work environment and drive business success. Inovalon strives to reflect the diversity of the communities where we operate and of our clients and everyone whom we serve. We endeavor to create a culture of inclusion in which our associates feel empowered to bring their full, authentic selves to work and pursue their professional goals in an equitable setting. We understand that by fostering this type of culture, and welcoming different perspectives, we generate innovation and growth.
Inovalon is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirement.
The Company maintains a drug free work environment for all of its associates, which includes employees, contractors and vendors. It is unlawful for associates to manufacture, sell, distribute, dispense, possess or use any controlled substance or marijuana in the workplace and doing so will result in disciplinary action, up to and including termination of employment or the contracted relationship.",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Software Development,1998,$500 million to $1 billion (USD)
"GEICO
3.0",3.0,"Chevy Chase, MD",Senior Data Engineer (Remote),"At GEICO, we are seeking a highly motivated Senior Data Engineer to join our vendor data product team. As a Senior Data Engineer, you will be responsible for creating products using data from both internal and external vendor data sources to help realize goals of attracting customers through more accurate pricing, greater customer retention, and an increase in underwriting profitability. The right candidate will develop well-designed, testable, and efficient data ingestion, data enrichment, and data transformation solutions using best software development practices. You should be an analytical thinker, a self-learner, and comfortable supporting the needs of multiple projects. This role is a part of the GEICO Data Movement team of Data, Security & Infrastructure (DSI) in the GEICO Technology Solutions organization.
In this role, you will:
Identify, design, and implement internal process improvements, automating manual processes, optimizing data delivery for greater scalability
Build the processes required for optimal extraction, transformation, and loading of data using a variety of languages and technologies such as Scala, Python, Kafka, Azure Data Factory, Fivetran/HVR, dbt, and Databricks
Collaborate with stakeholders including the Product, Data Engineering, and Agile Delivery teams in an agile environment to assist and resolve data-related issues and support data delivery needs
Work with other data platform or data domain teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies
Perform unit tests and conduct reviews with other team members to make sure code is rigorously designed, elegantly coded, and effectively tuned for performance
Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community
Experience & Skills:
At least 2 years of experience with designing, developing, implementing, and maintaining solutions for Big Data or data warehouse system
At least 2 Years of experience working in a cloud environment such as Azure, AWS or other private or public cloud
Experience performing root cause analysis on internal and external data and processes to answer business questions and identify opportunities for improvement
Strong analytical skills related to working with unstructured datasets
Good experience with bringing data into a centralized data repository or manipulating the available data to build additional data sets for Analytics and Reporting purposes.
Experienced with maintaining data quality throughout the lifecycle of the data.
Experience with Data Modeling, source to target mapping, automated testing frameworks, CI/CD pipelines and task automation using scripting
Experienced with working in Agile environment and end to end automation
Developing new and enhancing existing data processing (Data Ingest, Data Transformation, Data Store, Data Management, Data Quality) components
Strong working knowledge of SQL and the ability to write, debug and optimize SQL queries and ETL jobs to reduce the execution window or reduce resource utilization
Data Engineering experience focused on batch and real-time data pipelines development, Data processing/data transformation using ETL/ELT tools, SnowPipe, dbt, or Databricks
Experience with Cloud Data Warehouse solutions experience (Snowflake, Azure DW, Redshift or similar technology in other private or public clouds).
Complete software development lifecycle experience including design, documentation, implementation, testing, and deployment
Basic Qualifications:
Bachelor’s Degree in a computer-related field or equivalent professional experience required
At least 2 years of experience in data engineering using open-source technology stack along with cloud computing (AWS, Microsoft Azure, Google Cloud)
At least 2 years of experience with designing, developing, implementing, and maintaining solutions for data ingestion and transformation projects with dbt, SnowPipe, or DataBricks
At least 2 years of advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with cloud databases
Preferred Qualifications:
3+ years of experience with dbt, SnowPipe, or DataBricks
3+ years of experience working on real-time data and streaming applications (Spark Streaming or Kafka)
3+ years of experience working with Cloud Data Warehouse solutions (i.e., Snowflake, Synapse, Redshift)
3+ years of experience with Agile engineering practices
Benefits:
As a full time, associate, you’ll enjoy our
Total Rewards Program
to help secure your financial future and preserve your health and well-being, including:
Premier Medical, Dental and Vision Insurance with no waiting period**
Paid Vacation, Sick and Parental Leave
401(k) Plan with Profit Sharing
Tuition Assistance including Direct Billing and Reimbursement payment plan options
Paid Training, Licensures, and Certificates
Benefits may be different by location. Benefit eligibility requirements vary and may include length of service.
**Coverage begins with the pay period after hire date. Must enroll in New Hire Benefits within 30 days of the date of hire for coverage to take effect.
GEICO is proud to be an equal opportunity employer. We are committed to cultivating an environment where equal employment opportunities are available to all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability, or genetic information, in compliance with applicable federal, state and local law. GEICO celebrates diversity and believes it is critical to our success. As such, we are committed to recruit, develop, and retain the most talented individuals to join our team
#LI-AP1
Annual Salary
$72,000.00 - $185,000.00
The above annual salary range is a general guideline. Multiple factors are taken into consideration to arrive at the final hourly rate/ annual salary to be offered to the selected candidate. Factors include, but are not limited to, the scope and responsibilities of the role, the selected candidate’s work experience, education and training, the work location as well as market and business considerations.","$128,500 /yr (est.)",10000+ Employees,Subsidiary or Business Segment,Insurance,Insurance Carriers,1936,$10+ billion (USD)
"Save A Lot
3.0",3.0,"Saint Ann, MO",Data Analytics Engineer,"Purpose
The Data Insights Team is responsible for delivering enterprise-wide data, business analytics and data science functions and connecting business needs with supporting technology. These teams ensure that all data across the enterprise are well defined, of high quality, available to all necessary business and technology users, and easily used and understood. Additionally, these teams provide expertise in reporting, business intelligence, data science, data engineering and integration, and data quality improvement.
This role’s mission is to unlock our data’s power by finding innovative ways to analyze, organize, integrate, and visualize information. The IT Data Analytics Engineer will identify research needs and priorities based on in-depth understanding of the strategic questions that move the business forward.
Responsibilities
Partner closely with key stakeholders throughout the organization to analyze data and visualize it in new, accessible, and powerful ways
Act as a technical expert on the different reporting and business intelligence solutions
Take ownership of existing enterprise dashboards and data tools with an eye for improving those resources to improve self-service analytics
Work with large sets of data to build data model prototypes and analytics to support executive and company-wide reporting
Act as a liaison between business stakeholders and the data engineering group to set expectations, define requirements, perform data quality assurance testing, and deliver enterprise datasets for self-service reporting
Cultivate strong relationships with all stakeholders throughout the business and across the Data Insights Team
Run ad hoc analyses to answer specific day to day questions or support larger initiatives
Integrate data across multiple data sources to tell cohesive data stories
Foster continuous process improvement within the Data Insights organization and functions
Partner with security and risk teams to ensure secure, compliant analytics and reporting environments
About You
Bachelor's degree in computer science, statistics, mathematics, or related field of study
1+ year of experience delivering projects in a complex enterprise environment
2+ years of experience manipulating and analyzing complex data with SQL
1+ year of experience with data visualization tools, preferably Power BI and/or Tableau
1+ year relevant experience with a proven track record of leveraging analytics to drive business impact
Experience with SSRS and Dax is a plus
Strong planning and organizational skills
Experience working on large portfolios of inter-related projects to deliver an enterprise data ecosystem, achieving target outcomes and business value within budget and timeline
Able to quickly adapt techniques to the target environment and stakeholders
Experience in a retail environment preferred
Demonstrated experience building and maintaining strong relationships with business and IT partners
Excellent communication skills, both written and verbal, with ability to communicate effectively at all levels of the organization
Physical Requirements
Ability to travel up to ~10% of the time, which may include weekends and evenings, as needed
Most work is performed in a temperature-controlled environment
Incumbent may sit for long periods of time at a desk or computer terminal
Incumbent may use calculators, keyboards, telephone and other office equipment in the course of a normal workday
Stooping, bending, twisting and reaching may be required in completion of job duties
Our Values
Ability to demonstrate, understand and apply our workplace values
Simplicity (operate) – the drive to identify root cause and innovate to remove complexity in order to deliver the best outcome
Heart (emotion) – the passion that drives you to get up every day and work hard to strive for excellence
Performance Excellence (mindset) – clearly defining high expectations, driving ownership of key roles and responsibilities, executing with integrity and emphasis while creating a culture of accountability
Respect (philosophy) – an inclusive sense of pride to achieve high performing results in a simple way, every day in every way","$84,070 /yr (est.)",5001 to 10000 Employees,Company - Private,Retail & Wholesale,Grocery Stores,1977,$1 to $5 billion (USD)
"Mizuho Americas
3.4",3.4,"New York, NY",IT Data BI Engineer,"Join the Mizuho team as a Business Intelligence Engineer!
This position is for a Business Intelligence / Reporting engineer with a background in SQL and data warehousing for enterprise level systems. Strong Power BI experience is a must especially in developing dashboards and canned reports.
In this role you will be responsible for the design and development of reports, dashboards and ad-hoc queries. The position calls for someone that is comfortable working with business users along with business analyst expertise. Experience with the following technologies will be required:
Power BI Dashboards and Paginated Reports
Snowflake
PBRS
Power On
SQL Server 2016
Data warehousing
Qualifications:
Minimum 3+ years of combined experience in data warehousing/business intelligence/analytics and reporting systems.
Minimum 3+ years of relational and multi-dimensional (OLAP) data modeling Proficiency in the use of SQL, including relational and dimensional database structures, query optimization, specifically with SQL Server
Strong knowledge of Fixed Income, Equity and Derivative businesses
Strong ability to analyze user requirements, make recommendations and implement solutions
Self-driven and should be able to troubleshoot and provide quick resolutions to issues.
Full project management and development life cycle experience
Strong oral and written communication skills
Strong presentation and interpersonal skills
Ability to prioritize and execute in a high-pressured environment
University bachelor degree (Computer Science, Information Systems or Computer Engineering)
The expected base salary ranges from $104k-$160k. Salary offers are based on a wide range of factors including relevant skills, training, experience, education, and, where applicable, certifications and licenses obtained. Market and organizational factors are also considered. In addition to salary and a generous employee benefits package, successful candidates are eligible to receive a discretionary bonus.
Other requirements
Mizuho has in place a remote working program, with varying opportunities for remote work depending on the nature of the role, needs of your department, as well as local laws and regulatory obligations.
Company Overview
Mizuho Americas is a leading financial institution comprising several legal entities, which together offer clients corporate and investment banking, financing, securities, treasury services, asset management, research and more. Mizuho’s operations in the Americas connect a broad client base of major corporations, financial institutions and public sector groups to local markets and a vast global network. Mizuho Americas is an integral part of the Japan-based Mizuho Financial Group, Inc. (NYSE: MFG), which is comprised of offices in nearly 40 countries, approximately 60,000 employees, and assets of more than USD 1.8 trillion. Learn more at mizuhoamericas.com.
Mizuho Bank Ltd. offers a competitive total rewards package.
We are an EEO/AA Employer - M/F/Disability/Veteran.
We participate in the E-Verify program.
We maintain a drug-free workplace and perform pre-employment substance abuse testing.
#LI-MIZUHO","$132,000 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Investment & Asset Management,2002,Unknown / Non-Applicable
"Zazmic
4.2",4.2,United States,Data Engineer (Looker),"Europe, Latin America
Senior
We are seeking a highly skilled and experienced Senior Data Engineer to join Zazmic team.
As a Senior Data Engineer, you will be responsible for designing, developing, and maintaining our data infrastructure, as well as leveraging Looker to analyze data and identify patterns across various tables.
If you have a strong background in scripting, Python, ETL pipeline development, and expertise in utilizing Looker and BigQuery, we encourage you to apply.
Key Responsibilities:
Utilize Looker to explore and analyze data from multiple tables, identifying patterns, trends, and insights
Collaborate with cross-functional teams to understand business requirements and translate them into technical solutions
Optimize data workflows and processes to improve efficiency and reliability
Develop and implement data quality checks and ensure data accuracy and integrity
Perform data modeling and schema design to support reporting and analytics needs
Stay up-to-date with the latest industry trends and technologies related to data engineering and analytics
Minimum Requirements :
4+ years of professional experience as a Data Engineer or similar role.
Good proficiency in scripting languages, particularly Python.
Extensive experience in building and maintaining ETL pipelines
Proficiency in utilizing Looker for data exploration, visualization, and analysis
Solid understanding of relational databases and data warehousing concepts
Hands-on experience with BigQuery or other cloud-based data warehouses
Strong SQL skills and ability to write complex queries for data extraction and manipulation
Why join us:
Ability to work remotely from anywhere in the world
Close cooperation with the development team and client
Opportunity to influence product development
We cover English classes (with a native speaker)
Boost professional brand: you can participate in local conferences as a listener or as a speaker
Regular team buildings: have fun with teammates
Gifts for significant life events (marriage, childbirth)
Tech and non-tech Zazmic Communities: support and share experience with each other",#N/A,201 to 500 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2012,Unknown / Non-Applicable
"Highmark Health
3.6",3.6,Pennsylvania,Analytic Data Engineer,"Company :
Highmark Health
Job Description :
JOB SUMMARY
This job architects and engineers solutions associated with analytic data for the organization and, working closely with the IT teams, assists with the design, build, and upkeep for these solutions. This includes creating pathways for analysts to access operational, derived, and external data sets. The incumbent is responsible for the operation of Data Platforms as they are associated with analytic data discovery.
ESSENTIAL RESPONSIBILITIES
Receiving some direction, work closely with IT to architect and engineer solutions to provide views for the Analytic Data Warehouse. This would include working with the proper the teams, assisting with the design, building out the design, and providing upkeep for the solution.
Assemble, test, process, and maintain the Analytic Discovery Platform for the analytics organizations. This will include working to maintain pipelines with key analytic platforms throughout the organization.
Work with alternative analytic data systems to incorporate them into the operational data flow for the Analytics Teams. This may include products purchased by the organization that must be ingested or modeled/derived data maintained by analytic teams.
Complete tasks associated with a project. Meet with customers as part of a team lead by Lead or Senior.
Other duties as assigned.
EDUCATION

Required
Bachelor's Degree in Computer Systems Analysis, Data Processing, Healthcare Informatics, Management Information Systems, or related field or relevant experience and/or education as determined by the company in lieu of bachelor's degree.

Preferred
None
EXPERIENCE
Required
3 - 5 years of Data Analytics Experience
Preferred
1 - 3 years of Data Warehousing Experience
1 -3 years of Database Administration Experience
1 - 3 years of Healthcare Industry Experience
Experience writing SAS and SQL ETL with the ability to dissect, analyze, and update large production datasets.
Experience in healthcare quality measurement and analytics related to HEDIS, CMS Stars, CHIP, QARR, or other state regulated quality programs.
LICENSES OR CERTIFICATIONS
Required
None
Preferred
None
SKILLS
Microsoft Office
SAS
Language Requirement (other than English)
None
Travel Required
0% - 25%
PHYSICAL, MENTAL DEMANDS and WORKING CONDITIONS
Position Type
Office-Based
Teaches / trains others regularly
Occasionally
Travel regularly from the office to various work sites or from site-to-site
Rarely
Works primarily out-of-the office selling products/services (sales employees)
Never
Physical work site required
Yes
Lifting: up to 10 pounds
Occasionally
Lifting: 10 to 25 pounds
Rarely
Lifting: 25 to 50 pounds
Never
Disclaimer: The job description has been designed to indicate the general nature and essential duties and responsibilities of work performed by employees within this job title. It may not contain a comprehensive inventory of all duties, responsibilities, and qualifications required of employees to do this job.
Compliance Requirement: This position adheres to the ethical and legal standards and behavioral expectations as set forth in the code of business conduct and company policies.
As a component of job responsibilities, employees may have access to covered information, cardholder data, or other confidential customer information that must be protected at all times. In connection with this, all employees must comply with both the Health Insurance Portability Accountability Act of 1996 (HIPAA) as described in the Notice of Privacy Practices and Privacy Policies and Procedures as well as all data security guidelines established within the Company’s Handbook of Privacy Policies and Practices and Information Security Policy. Furthermore, it is every employee’s responsibility to comply with the company’s Code of Business Conduct. This includes but is not limited to adherence to applicable federal and state laws, rules, and regulations as well as company policies and training requirements.
Pay Range Minimum:
$57,700.00
Pay Range Maximum:
$106,700.00
Base pay is determined by a variety of factors including a candidate’s qualifications, experience, and expected contributions, as well as internal peer equity, market, and business considerations. The displayed salary range does not reflect any geographic differential Highmark may apply for certain locations based upon comparative markets.
Highmark Health and its affiliates prohibit discrimination against qualified individuals based on their status as protected veterans or individuals with disabilities, and prohibit discrimination against all individuals based on their race, color, age, religion, sex, national origin, sexual orientation/gender identity or any other category protected by applicable federal, state or local law. Highmark Health and its affiliates take affirmative action to employ and advance in employment individuals without regard to race, color, age, religion, sex, national origin, sexual orientation/gender identity, protected veteran status or disability.
EEO is The Law
Equal Opportunity Employer Minorities/Women/Protected Veterans/Disabled/Sexual Orientation/Gender Identity ( https://www.eeoc.gov/sites/default/files/migrated_files/employers/poster_screen_reader_optimized.pdf )
We endeavor to make this site accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact number below.
For accommodation requests, please contact HR Services Online at HRServices@highmarkhealth.org
California Consumer Privacy Act Employees, Contractors, and Applicants Notice","$106,700 /yr (est.)",10000+ Employees,Nonprofit Organization,Healthcare,Health Care Services & Hospitals,#N/A,$10+ billion (USD)
"PRGX Global, Inc
3.7",3.7,"Atlanta, GA",Senior Software Engineer - Data Technologies,"SENIOR SOFTWARE ENGINEER, DATA INTELLIGENCE TECHNOLOGIES
This position is responsible for creating high priority differentiated technology solutions that solve important real-world problems. The position partners closely with the Head of Data Analytics, data scientists, business leaders, clients, and other stakeholders to create roadmaps, scope programs aligning with business priorities, define milestones and success metrics, and create scalable, secure, reliable, and efficient data products and platforms which meet our clients’ needs and contribute to the growth of PRGX.

Job Responsibilities:
Participate in the conceptualization of new products.
• Work with our data scientists to turn large-scale, diverse, and often unstructured data into a meaningful source of insights for our clients. • Design, develop, deliver, implement, and maintain high-quality, secure, reliable, and scalable data applications on time and on budget.
Design, code, configure, test, and document deliverables using agile methodologies.
Collaborate with key stakeholders on coding standards, processes, tools, and frameworks required for the delivery of features.
Guide technical and design decisions based on experience.
Develop proof of concept work and prototyping when necessary.
Collaborate with business leaders to understand business requirements relating to features to be delivered.
Identify common patterns and foster development of reusable components and standards.
Contribute to an innovation culture by evaluating new processes and technologies that can be used to enhance future features.

Ideal Candidate Characteristics:
Enjoys and excels in environments where they must tackle and solve new and increasingly complex client business challenges and issues, incorporating the newest ideas and technologies to deliver solutions quickly.
Possesses a high level of self-awareness. Maintains composure and professionalism when under pressure.
Possesses strong systems and critical thinking skills and the ability to draw on disparate information to identify insights, and design and deliver solutions.
Driven to meet or exceed specific goals and objectives as quickly as possible.
Entrepreneurial spirit. Forward-thinking and adaptable in dynamic situations. Knowledge & Qualifications:
Bachelor’s degree in Computer Science, Computer Engineering, or related field. Master’s degree preferred.
Minimum of 10 years’ software development experience.
Minimum of 5 years’ experience working with complex data sets and developing data-centric applications.
Minimum of 5 years’ experience with commercializing and scaling white label Qlik applications.
Experience with big data and big data tools such as SQL and Python.
MySQL, Spark, Scala, Hive, Minio, Trino, Airflow, Presto and other ETL solutions experience desired.
Experience in CPG manufacturing, wholesale, distribution, or retail is highly desirable, and preference will be given to candidates with this experience.
Strong strategic thinking and problem-solving skills.
• Excellent communication, collaboration, and leadership skills. • Ability to build and maintain relationships with key stakeholders.
Strong business acumen.","$114,592 /yr (est.)",1001 to 5000 Employees,Company - Public,Financial Services,Accounting & Tax,1996,$25 to $100 million (USD)
"Wallero
4.4",4.4,"Seattle, WA",Data Engineer,"Hi,
Pretty much interested in your resume, we have an urgent opening for Data Engineer role with one of our clients based in Seattle WA.
Position: Data Engineer
Location: Seattle WA - Remote (100%)
Description:
Good Experience in reporting space including PowerBI, ADF, and possibly standing up a data warehouse/data mart.
The main scope centering is around reporting.
Experience with SQL, and agile framework including requirement gathering and story/feature writing skills.
Need someone strong who is comfortable leading and guiding teams in that capacity.
Migrating data from third party to Azure Cloud
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Seattle, WA 98101: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",$75.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2015,$1 to $5 million (USD)
"MetroStar
3.7",3.7,United States,Data Engineer (Mid),"As a Data Engineer (Mid), you'll bring creative architect solutions to end customers with the goal to make an impact across the federal government.
We know that you can't have great technology services without amazing people. At MetroStar, we are obsessed with our people and have led a two-decade legacy of building the best and brightest teams. Because we know our future relies on our deep understanding and relentless focus on our people, we live by our mission: A passion for our people. Value for our customers.
If you think you can see yourself delivering our mission and pursuing our goals with us, then check out the job description below!
What you'll do:
Work with AI team members to operationalize data pipelines and ML tasks.
Provide day-to-day support of deploying Python-native ML pipelines and perform data engineering tasks to enable AI/ML capabilities.
Present results to a diverse audience in presentation or report form.
Support architectural leadership, technical support, and advisement services to ensure identity management system technologies are integrated and meeting the appropriate security requirements.
Support leadership who engage with senior level executives at a public facing Federal agency and provide subject matter expertise in security architecture and other key domain areas.
What you'll need to succeed:
5+ years of experience in Data/ML engineering (if school experience is used, at most that would contribute to 2 years of actual experience).
Experience with ETL, Data Labeling and Data Prep.
Experience designing, implementing, and maintaining data architecture and services to be used for AI/ML. Additionally, operationalizing and maintaining AI/ML models in production.
The ability to perform data analytics on program related or system related activities. This will include assessing performance and manual processes implementing methods/algorithms to automate/optimize
A bachelor's degree in Computer Science, Information Technology Management or Engineering, or other comparable degree or experience and the ability to obtain and maintain DHS Suitability.

Like we said, we are obsessed with our people. That's why we offer a generous benefits package, professional growth, and valuable time to recharge. Learn more about our company culture code and benefits. Plus, check out our accolades.
Don't meet every single requirement?
Studies have shown that women, people of color and the LGBTQ+ community are less likely to apply to jobs unless they meet every single qualification. At MetroStar we are dedicated to building a diverse, inclusive, and authentic culture, so, if you're excited about this role, but your previous experience doesn't align perfectly with every qualification in the job description, we encourage you to go ahead and apply. We pride ourselves on making great matches, and you may be the perfect match for this role or another one we have. Best of luck! – The MetroStar People & Culture Team
What we want you to know:
In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification form upon hire.
MetroStar Systems is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. The statements herein are intended to describe the general nature and level of work being performed by employees and are not to be construed as an exhaustive list of responsibilities, duties, and skills required of personnel so classified. Furthermore, they do not establish a contract for employment and are subject to change at the discretion of MetroStar Systems.
Not ready to apply now?
Sign up to join our newsletter here.",#N/A,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1999,$25 to $100 million (USD)
BIGCLFY,#N/A,"Dallas, TX","W2/ 1099, No C2C - SSIS ETL Data Engineer (Minneapolis, MN/ Dallas-Irving)","No C2C. It's W2/1099 Requirement
Only taking USC, GC or H4 EAD, H1 Transfer, L2 Visa or TN Visa - they will not take OPT EAD or CPT
Need to have confirmation in writing that you will be onsite by Day 1
ALL ROLES ARE HYBRID IN THE OFFICE (2-3 DAYS/WEEK) UNLESS REMOTE IS NOTED
Please submit qualified candidates and include FULL LEGAL NAME as it appears on the passport
If your candidate is relocating, need to have confirmation in writing that they will be onsite by Day 1
Senior SSIS/ETL Data Engineer
12 months – 3 days in office
Minneapolis, MN or Dallas-Irving, TX
5+ years of experience with SQL Server 2016 (or newer) and SSIS
7+ years of data engineering
Must be capable of leading teams
2+ years of experience working with large scale SQL implementations
Experience designing, developing, and maintaining complex stored procedures, functions, and views using T-SQL
Experience designing, developing, and maintaining ETL processes using SSIS
Should understand how to unit test their product
Hands on experience with Visual Studio to document
Experience in financial or regulatory environments highly preferred
Ideal candidates will have experience creating conceptual, logical, and physical database designs
Preference given to candidates with extensive knowledge and experience with performance monitoring, database and SQL tuning
Bachelor’s or Master's in computer science or related field or equivalent work experience
Job Type: Contract
Pay: From $73.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Chandler, AZ 85225: Reliably commute or planning to relocate before starting work (Preferred)
Application Question(s):
Expected Payrate on W2/1099 Tax Term? Please mention. Thanks.
Experience:
Risk Analytics: 5 years (Preferred)
Work Location: In person",$73.00 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Codinix Technologies
5.0",5.0,"Boston, MA",Cloud Data Engineer - AWS,"Only US Citizen
Strong in warehouse technologies (Hadoop, Spark, AWS, etc.)
Bachelor's degree in Computer Science, Information Systems, or another related field
5+ years of data engineering, and cloud engineering experience
Experience in Agile project methodologies and DevOps
Experience with AWS (certification preferred; including Athena, Redshift, Glue), Cloudera
Experience building a CI/CD pipeline to deploy applications on AWS (GitLab, Terraform)
Advanced experience programming and scripting on Unix/Linux (i.e. Python, Bash)
Experience with CTRL-M, Cron (other) scheduling of batch jobs
Experience migrating legacy systems to cloud-ready architectures
Job Type: Contract
Pay: Up to $75.00 per hour
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Boston, MA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",$75.00 /hr (est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Provista Software Corporation,#N/A,Remote,Data Engineer,"As a successful candidate, you’ll play a pivotal role in the data conversion and business intelligence solution efforts during project implementation for our state and local clients. We are looking for you to join our software implementation team as a Senior Data Engineer.
Qualification:
5+ years of experience working independently as ETL developer on data migration projects that covered diverse database platforms and complex data conversions.
Must have experience across leading Database Platforms especially Oracle, Azure SQL, Snowflake.
Expert level experience with Extraction Transform Load (ETL) plan, protocols, and tools.
Expert level experience with programming languages SQL, Phyton, Java, C#.
Good at communication with clients and internally.
Expert at problem solving and troubleshooting skills.
Bachelor's degree, preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience
Must be a U.S. Citizen or Green Card Holder.
Functional Responsibility:
Data Conversion -
In collaboration with Data Stewards, discover inventory of legacy data sources and all data the client expects to be migrated to destination Modified Off the Shelf (MOTS) solution.
Complete gap assessment between legacy data and MOTS data requirements, and options to address the gaps.
Develop data migration map to minimally include data dictionary, detailed data map of all elements of the current database, data transformation rules, relationship rules, validation rules, process of migrating images, and process for recovering missing or erroneous data.
Identify issues, risks, and/or barriers that may interfere with data migration and recommendations to resolve or mitigate.
Conduct testing and quality assurance to ensure the data is migrated accurately.
Migrate data from legacy to destination MOTS solution progressively using a sample set of data and the entirety of the data prior to deployment into relevant go-live environments (e.g., staging, production, etc.)
Prepare data migration reports that includes – record counts of source vs migrated vs modified; field level validation of data at row and column; discovered exceptions.
Business Intelligence -
Develop ETL pipelines in and out of Snowflake data warehouse using combination of Python, SnowSQL, and SQL.
In collaboration with Data Stewards, create Snowflake data mart as curated subset of data to support the analytics and business intelligence needs of subgroup of users.
Translate requirements for business intelligence and reporting to database design and reporting design.
Conduct discovery sessions with users to capture business intelligence reports.
Develop Tableau self-service ad hoc reports to support operational users.
Develop Tableau Dashboards to support executive, management, and operational users.
About us:
Provista Software Corporation is fast growing Software Solution provider for US State and Local Government Clients.
Job Type: Full-time
Pay: $67,107.98 - $134,285.66 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Life insurance
Paid time off
Parental leave
Professional development assistance
Vision insurance
Compensation package:
Yearly pay
Experience level:
5 years
Schedule:
8 hour shift
Application Question(s):
What is your most preferred work time zone ?
Experience:
ETL: 5 years (Required)
Data warehouse: 1 year (Preferred)
Tableau: 1 year (Preferred)
Talend: 1 year (Preferred)
Microsoft SQL Server: 5 years (Required)
Work Location: Remote","$100,697 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Spartan Technologies
3.4",3.4,"Kansas City, MO",Data Engineer I,"Spartan Technologies, Inc. - Kansas City, MO
Applicants Need to Know
100% remote in East Coast or Central US only
No Sponsorship
Contract to Hire
Hourly Rate: $45 on W2
Data Engineer I
The Data Engineer is responsible for processing structured and unstructured data, validating data quality, and developing and supporting data products for analytics. Brings together data from different sources into a common dataset with common KPIs and dimensions for stakeholders. Understands the analytics challenges that the marketing organization faces in their day-to-day work and partners with them to design viable data solutions.
Your Job
Gathers and processes raw, structured, semi-structured, and unstructured data using batch and real-time data processing frameworks.
Understand the existing source system data models and perform end to end data validation to maintain accuracy.
Implements and optimizes data solutions in enterprise data warehouses and big data repositories.
Ensures data quality and implements tools and frameworks for automating the identification of data quality issues.
Develop re-useable processes that can be leveraged and standardized for multiple instances
Works with internal and external data providers on data validation providing feedback and making customized changes to data feeds and data mappings.
Recommends improvements to processes, technology, and interfaces that improve the effectiveness of the team and reduce technical debt.
Ensure performance by monitoring existing workflows to ensure they are working correctly and that they reconcile with the source.
Design and develop ETL workflows and datasets to be used in extracts by our BI reporting tool Tableau.
Will write complex SQL queries with multiple joins to automate/manipulate these reporting extracts
Works in partnership with marketing stakeholders concerning the accuracy of data and efficiency of processes
Qualifications
Working experience with data modelling, data access, schemas, and data storage techniques within Snowflake
Working experience with batch and real-time data processing frameworks
Working experience working with relational databases such as SQL
Working experience with business intelligence tools and platforms
Working experience with data quality tools
Experience with ETL processes and tools
Desirable:
Working experience with Snowflake
Working experience with Google Analytics BigQuery
Working Experience with Tableau Server
Working experience with SQL Server Integration Services",$45.00 /hr (est.),1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$5 to $25 million (USD)
"PRIMUS Global Services, Inc
4.1",4.1,"Minneapolis, MN","Data Engineer – Hadoop, Python – REMOTE WORK 43198","We have an immediate long-term opportunity with one of our key clients for a position of Data Engineer, to work on a remote basis.

The ideal applicant must have extensive experience with Python, Hadoop, Hive and SQL.

**ALL successful candidates for this position are required to work directly for PRIMUS. No agencies please**

For immediate consideration, please contact:

Tejaswini
PRIMUS Global Services
Direct: 972-798-2662
Desk: 972-753-6500 Ext: 204
Email: jobs@primusglobal.com","$91,633 /yr (est.)",501 to 1000 Employees,Private Practice / Firm,Information Technology,Information Technology Support Services,2002,$5 to $25 million (USD)
IT Customer Service - POST,#N/A,"Newark, NJ",Data Center Plant Engineer,"Job Title: ISP-OSP Data Center Plant Engineer
Location: Newark NJ, (M-F)
Travel Required: As Needed
Overview
UC-ISP/OSP engineer is responsible for cable management including structured cabling, carrier data and voice circuit cables/equipment, rack-and-stack, and oversight of power, backup power, network equipment, and environmental needs to cabinets. Leveraging Clients Data Center Infrastructure Management System, (DCIM) tool to maintain, report and monitor.
Conducting capacity assessments of existing infrastructure to ensure that it can support future growth to ensure compliance with federal regulations of the Data Center infrastructure Standards. Projects include design and build outs of the physical plant within the typical WAN/LAN space within data centers, telco rooms, MDFs and IDFs as needed.
Functions:
Perform initial engineering surveys based on project needs.
Document engineering surveys to determine feasibility and the estimated cost.
Obtain authorization to proceed and confirm funding.
Create detailed engineering drawings of build, floor plans, equipment elevation drawings, structured cabling diagrams, power, and environmental needs.
Designs must conform to state, local, national, and Client standards and codes, and the NJ historical and or environmental commissions approvals if needed.
Based on the engineering documents create a SOW (Scope of Work) and BOM (Bill of Materials)
Submit to Client manager to approve and proceed when funding is validated.
Create a construction package and submit to contractors or in house for RFQ.
Review quotes and choose both low bid and qualified contractor.
Work with all stakeholders to date and time work schedules.
Confirm workers are safety trained and conform to Client, FRA, and OSHA requirements.
Supervise and manage installations and conduct inspections.
Review work to ensure contractor met obligations under terms of contract.
Inform Client managers of status to approve invoices.
Coordinate with Client network engineering to connect, configure, and test equipment.
Review and confirm a successful installation were completed and working as designed prior to cutting over to production.
Assist with Break fix of IT infrastructure troubles or repairs.
Daily travel to location depending on scheduled or unscheduled work
Job Types: Full-time, Contract
Experience level:
5 years
Schedule:
8 hour shift
Work setting:
In-person
Experience:
Data Center: 5 years (Required)
WAN/LAN: 3 years (Required)
cabling: 3 years (Required)
Work Location: One location",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Zenotis Technologies inc,#N/A,"Addison, TX",Big Data Engineer,"Role: Big Data Engineer
Location – Addison, TX/ Charlotte, NC / Chandler, AZ (Hybrid – 3 days onsite 2 days remote)
Position Summary:
Looking for a big data engineer who loves solving complex problems across a full spectrum of technologies. The ideal candidate is excited by experimentation and looking for a new challenge that stretches their talents. The big data engineer will help ensure that our technological infrastructure operates seamlessly in support of business objectives.
The engineer must be able to:
· Translate complex technical and functional requirements into detailed designs
· Guide the development and testing teams in developing and implement pipelines that extract, transform, and load data into an information product that helps the organization reach its strategic goals
· Focus on ingesting, storing, processing, and analyzing large datasets
· Deep understanding on how to create scalable, high-performance web services for tracking data
· Investigate alternatives for data storing and processing to ensure the implementation of the most streamlined solutions
Primary Skill
Hadoop
Secondary Skill
Kafka
Tertiary Skill
Oracle Exadata
Required Skills
· Translate complex technical and functional requirements into detailed designs
· Design data pipelines using design tools and provide solution intent in provided templates
· Work closely with data science team to implement data analytics pipelines
· Analyze vast number of data stores to uncover insights
· Define data governance policies and support data-versioning processes
· Maintain security and data privacy, working closely with data protection guidelines.
· Desired Skills
· Experience with Python, Spark, and Hive
· Understanding of data-warehousing and data-modeling techniques
· Knowledge of industry-wide visualization and analytics tools (ex: Tableau, R)
· Strong data engineering skills with Hadoop platform
· Experience with streaming frameworks such as Kafka
· Knowledge of Core Java, Linux, SQL, and any scripting language
· Good interpersonal skills and positive attitude
Job Type: Full-time
Salary: Up to $70.00 per hour
Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
8 hour shift
Work Location: On the road",$70.00 /hr (est.),201 to 500 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Maven Workforce
4.1",4.1,"McLean, VA",Data Engineer,"MUST HAVES: Hands-on experience with Informatica PowerCenter and/or Informatica Intelligent Cloud Services (IICS), Cloud Data Integration (CDI) , Cloud AWS,UNIX scripting, and any other ETL tools.
Job responsibilities: –
The candidates will be having the IICS module of Informatica more than any other platform modules.
Strong hands-on experience building ETL pipelines and Data Replication
3+ yrs of experience with IICS is required.
Must be hand Ons with AWS services like ECS,S3,EKS etc.
Cloud Data Integration is also required.
Data Engineering Background experience with Spark, Hive is also required.
Preferred:
SQL is preferred, but basic knowledge is required
Job Type: Contract
Salary: $60.00 per hour
Ability to commute/relocate:
Mclean, VA 22101: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica PowerCenter: 7 years (Required)
Informatica Intelligent Cloud Services (IICS): 7 years (Required)
Cloud Data Integration (CDI): 7 years (Required)
UNIX scripting: 7 years (Required)
ETL tools: 7 years (Required)
Work Location: One location
Speak with the employer
+91 7328456015",$60.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2008,Unknown / Non-Applicable
"Ripple Effect Consulting
5.0",5.0,"Denver, CO",Data Engineer - 004/005,"Data Engineer
Hybrid-remote based in Denver, CO | $80-$160k
Job Overview:
We are looking for a savvy Data Engineer to join our growing team of analytics experts. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
Responsibilities for Data Engineer
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Qualifications for Data Engineer
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:
1) Experience with big data tools
2) Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
3) Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
4) Experience with cloud services
5) Experience with stream-processing systems: Storm, Spark-Streaming, etc.
6) Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Job Type: Full-time
Pay: $80,000.00 - $160,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Denver, CO: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person","$120,000 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Synovize
5.0",5.0,Remote,Big Data Engineer,"Synovize is a cutting-edge technology company that specializes in providing advanced data analytics and intelligence solutions. Our mission is to empower businesses with the tools and insights they need to make data-driven decisions and drive innovation. We are committed to delivering exceptional results by combining our expertise in big data technologies with our passion for creating impactful solutions.
As a Big Data Engineer at Synovize, you will have the opportunity to work on exciting projects that involve designing and implementing scalable data solutions. You will collaborate with a talented team of professionals, including software developers, data scientists, and domain experts, to tackle complex data engineering challenges and deliver high-quality solutions to our clients.
Job Title:
Big Data Engineer
Company: Synovize
Number of Positions: 2
Location: Remote
Length: 12 Months
Tax Terms: 1099/C2C or W-2
Rate/Range: $70-$90/hr
Work Authorization: US Citizen
Security Clearance: None
Skills Needed:
Proficiency in Java and Kotlin programming languages.
Strong knowledge of AWS (Amazon Web Services) and its big data offerings.
Experience with BigData technologies, frameworks, and tools.
Familiarity with Kafka for streaming data processing.
Proficiency in Python programming language.
Responsibilities:
Design and develop scalable data pipelines to process, integrate, and analyze large volumes of structured and unstructured data.
Implement data ingestion, transformation, and storage mechanisms using big data technologies and frameworks.
Collaborate with data scientists and analysts to understand data requirements and optimize data models for analytics and machine learning.
Utilize AWS services and tools to build and maintain data infrastructure, ensuring data quality, integrity, and security.
Troubleshoot and resolve data-related issues, optimizing performance and efficiency.
Stay up-to-date with emerging trends and advancements in big data technologies, recommending and implementing improvements to existing systems and processes.
Requirements:
Bachelor's or master's degree in computer science, engineering, or a related field.
Proven experience as a Big Data Engineer or similar role, with a strong understanding of big data technologies and architectures.
Proficiency in Java and Kotlin programming languages.
Strong knowledge of AWS and its big data offerings.
Experience with BigData technologies, frameworks, and tools.
Familiarity with Kafka for streaming data processing.
Proficiency in Python programming language.
Strong problem-solving skills and ability to work in a fast-paced, collaborative environment.
Excellent communication and interpersonal skills.
Join Synovize and be part of a dynamic team that is at the forefront of data innovation. Together, we will drive meaningful change and unlock the full potential of data-driven decision-making.
Job Types: Full-time, Contract
Pay: $70.00 - $90.00 per hour
Benefits:
401(k)
Dental insurance
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Vision insurance
Schedule:
10 hour shift
4 hour shift
8 hour shift
Choose your own hours
Application Question(s):
Are you a US Citizen?
Years of experience in Java?
Years of experience in Kotlin?
Years of experience in big data?
Years of experience in AWS?
Security Clearance
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$80.00 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Southern Glazer’s Wine and Spirits
3.6",3.6,"Dallas, TX",Data Engineer,"What You Need To Know
Open the door to a groundbreaking tech career with an industry leader. Southern Glazer’s Wine & Spirits is North America’s preeminent wine and spirits distributor, as well as a family-owned, privately held company with a 50+ year legacy of success. To create a new era in alcohol beverage sales and service, we’re heavily invested in the most transformative new technologies – and the most brilliant tech professionals. Southern Glazer’s was named by Newsweek as a Most Loved Workplace and is included on the Forbes lists for Largest Private Companies and Best Employers for Diversity.
As a full-time employee, you can choose from a full menu of our Top Shelf Benefits, including comprehensive medical and prescription drug coverage, dental and vision plans, tax-saving Flexible Spending Accounts, disability coverage, life insurance plans, and a 401(k) plan. We also offer tuition reimbursement, a wellness program, parental leave, vacation accrual, paid sick leave, and more.
We offer continuous learning and career growth in a fast-paced environment where you are respected, your voice is heard, and technology is part of our strategy for success. If you’re looking to fill your glass with opportunity, come join our FAMILY.
Overview
The Data Engineer's role is to design, develop, maintain and enhance interfaces and connectivity to the Data Warehouse ecosystem by coding with a technical language to meet business requirements and business objectives. This can include taking technical specifications and developing an application or integration of data between applications, testing, as well as, completing the appropriate technical documentation. The Data Engineer will use best practices in software development and adhere to SGWS development standards, as well as, focus on quality and innovation. The Data Engineer may also be responsible for delivering support to end users in the organization for specific code, including troubleshooting code.
Specialized Skills and Technologies
Strong PL/SQL skills
Experience in ETL Tools (Preferrable Informatica)
Data Warehouse techniques will be a plus
Experience in cloud platforms like Azure or AWS will be a plus
Knowledge of UNIX/Linux, shell scripting, Python will be a plus
Experience developing Application Programming Interfaces (API's) will be a plus
Experience in Hadoop will be a plus
Primary Responsibilities
Design, develop, implement, and support software applications
Drive technical validity of solution.
Develop user documentation as well as in-code documentation to explain designs and participate/support user training
Structure requirements to facilitate automation of acceptance tests
In conjunction with Data Management Group, develop routines and procedures that provide data quality checks and balances on data delivery/ingestion
Collaborate across the BI / Analytics, Data Management Group, Enterprise Insights and Analytics teams to establish standards, reusable data models and best practices for delivery/ingestion of data from/to Data Warehouse - This includes Publish/Subscription and API options
Obtain any certifications needed to effectively support applications in scope
Support the development of business and technical process documentation and training materials
Structure requirements to facilitate automation of acceptance tests
Provide support for software applications under area of responsibility
Drive Behavior-Driven-Design (BDD) process
Perform other job-related duties as assigned
Minimum Qualifications
Bachelor’s Degree or a combination of work experience and education
Knowledge in application and software development
Knowledge of software design and programming principles
Proficient oral and written communication skills, ability to influence outcomes, and strong attention to detail
Strong analytical, mathematic, and problem-solving skills
Strong team player with ability to demonstrate Agile delivery values working both within a team and working independently
Strategic thinker – can develop a plan to meet a long-term objective
Agile Delivery Values
Openness – Team and stakeholders agree to be open about all work and challenges
Commitment – Personally commit to achieving the goals of the team
Respect – Respect your team members to be capable and independent
Courage – You have courage to do the right thing and work on tough problems
Focus – Everyone focus on the work in the sprint and the goal of the scrum team. Rise and fall as a team
Physical Demands
Physical demands include a considerable amount of time sitting and typing/keyboarding, using a computer (e.g., keyboard, mouse, and monitor), or mobile device
Physical demands with activity or condition may occasionally include walking, bending, reaching, standing, squatting, and stooping
May require occasional lifting/lowering, pushing, carrying, or pulling up to 20lbs
EEO Statement
Southern Glazer's Wine and Spirits, an Affirmative Action/EEO employer, prohibits discrimination and harassment of any type and provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. Southern Glazer's Wine and Spirits provides competitive compensation based on estimated performance level consistent with the past relevant experience, knowledge, skills, abilities and education of employees. Unless otherwise expressly stated, any pay ranges posted here are estimates from outside of Southern Glazer's Wine and Spirits and do not reflect Southern Glazer's pay bands or ranges.","$97,837 /yr (est.)",10000+ Employees,Company - Private,Retail & Wholesale,Wholesale,1968,$10+ billion (USD)
"Kaizen Analytix
3.9",3.9,"Dallas, TX",Cloud Data Engineer: AWS,"Cloud Data Engineer: AWS
Kaizen Analytix LLC, an analytics services company, is seeking a qualified Cloud Data Engineer Contractor to support data science or data analytics projects. Responsibilities, capabilities for success, and job requirements are below. This role requires an estimated 40 hours/week commitment for approximately 24 weeks.
Responsibilities:
Candidate will be responsible for supporting a data science or data analytics project.
Work with client IT to understand access and use of source-of-truth data systems and downstream data sources.
Work with data scientist or visualization experts to determine data requirements such as structure, latency, availability.
Work with IT Security to assess risk of using sensitive data and how these data can be used throughout the project life cycle.
Develop an as-is and to-be state for data architecture.
Capabilities for Success:
In addition to key responsibilities, Candidate should demonstrate an ability to…
Work with the project manager and the business stakeholders to define upstream requirements for data.
Contribute to estimating input and time required for data engineering development tasks.
Contribute to client demonstrations of solution or presentations on architecture.
Proactively raise risks and issues to internal project manager
Work with data scientist or analysts to proactively assist in data concerns
Job Requirements:
Education: Bachelor's degree required
Extensive Experience in Data Engineering – consuming, wrangling, validating, developing pipelines for data.
Extensive Experience with AWS
Must have Solutions Architect Certification
Strong knowledge of databases, ETL concepts, SQL, and NoSQL sources.
Experience in: Python (data packages such as Pandas, PySpark, SQL Alchemy, pydantic, etc.) and SQL.
Position Type: 1099 Contractor",$57.50 /hr (est.),1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,#N/A,Unknown / Non-Applicable
"Evergreen Residential Holdings, LLC
5.0",5.0,"Dallas, TX",Engineer Data & Analytics,"We are Evergreen Residential, a high growth early-stage institutional investment platform in the single-family residential sector. Our team is collaborative, open-minded and curious. Transparency is a core value, we speak our minds, are responsible for our actions and celebrate our wins. We are serious about the business without taking ourselves too seriously. We look for people who thrive in an entrepreneurial and fast paced environment. If you are self-motivated and mission driven with a 'can do' mindset and see solutions where others may see problems, come and grow with us!
We offer a flexible, empowering culture, competitive compensation and benefits, and potential for career growth through working closely with, and learning from, our experienced leadership team.
As a technical/engineering expert, you pride yourself on being able to quickly build strong business relationships both internally and externally e.g., with the leadership team, current and potential investors. With a passion for keeping current with advancements of the field, you deploy technology and data resources to provide innovative solutions to business needs.
The Role: Priorities can often change in a fast-paced environment like ours. Initial focus is to work with external data purchased, and to harvest internal data and work within Snowflake warehouse for use in our 3rd party property mgt system and BI reporting tool. Overall ensure there is one source of truth.

The role includes, but is not limited to, the following responsibilities:
Designing and implementing data pipelines to extract, transform, and load data from various sources into a centralized data repository
Developing and maintaining data processing and storage infrastructure
Establish productive relationships and effective communications with Company leadership to understand business drivers and align on required outcomes
Collaborating with data analysts to ensure that data is readily available for analysis and modeling
Optimizing database performance and troubleshooting issues as they arise
Implementing data security and access controls to protect sensitive data
Highlight key trends derived from data analysis and be a resource for improving data proficiency throughout the organization
Staying up-to-date with emerging trends and technologies in data engineering
Leverage historical data and predictive models to identify key historical factors that impact critical KPIs, and recommend actions to drive future performance
Ensure scientific method and research are key drivers of the product roadmap
What You Will Bring to the Table:
Knowledge of data modeling, database design, and ETL best practices
At least 3-5 years of experience in data engineering or a related field
Proficiency in one or more programming languages such as Python, Java, or Scala
Experience with data processing and storage technologies such as Hadoop, Spark, Kafka, Snowflake, and NoSQL databases
Experience in real estate investment and/or rental sector highly desirable
Prior experience managing a team of direct reports within the Data Science, Data Engineering, Analytics space in the SFR or Multifamily industry
Significant Experience building, motivating, and retaining a high- performing, flexible and collaborative data and analytics function
Proven hands-on technical background in data science, business intelligence or data engineering with demonstrated strategic impact at an executive level
A strong problem solver with experience building technical strategy and understanding technical tradeoffs and risk
Collaborative team player, you are truly a ""do-er"", happy to be a hands-on problem-solver to move the data program forward
Excellent communication skills – verbal and written
About Evergreen Residential
Founded in 2021, Evergreen Residential is a full-service SFR platform leveraging proven operational practices and the latest technological advances to optimize investor returns and achieve positive outcomes for our residents and the communities in which we operate. We offer a full suite of services, including Investment Management, Asset Origination, and Advisory Services. The firm is headquartered in Dallas with offices in New York City.
The leadership team has extensive experience dating back to the early institutionalization of SFR and unrivaled depth of experience in the complete asset life cycle. We are built to withstand changing market conditions, and our business produces resilient, predictable cash flows and margins. We are committed to charting new paths and using data to achieve best-in-class results. Our business is evergreen.
Beyond financial returns, the Company is committed to measurable impact objectives. We believe that inclusive and equitable management, environmentally sustainable long-term strategies, and resident-focused policies are good business - for our residents, our investors, and our team. We are committed to using environmentally sustainable practices and empowering our residents to improve their financial health.
Our cornerstone values - Accountability, Transparency and Partnership - are built on a foundation of Integrity and provide the roadmap for our daily actions, interactions and decisions.

Equal Opportunities and Other Employment Statements
We are deeply committed to building a workplace and community where inclusion is not only valued, but prioritized. We take pride in being an equal opportunity employer and seek to create a welcoming environment based on mutual respect, and to recruit, develop and retain the most talented people from a diverse candidate pool. All employment decisions shall be made without regard to race, color, religion, gender, gender identity or expression, family status, marital status, sexual orientation, national origin, genetics, neuro-diversity, disability, age, or veteran status, or any other basis as protected by federal, state, or local law.","$98,425 /yr (est.)",51 to 200 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Breadboard,#N/A,"New York, NY",Data Engineer,"About Us
We’re on a mission to revolutionize the global electronics supply chain.
During the pandemic we saw how the inefficiencies in electronics manufacturing can have massive global impact. From over $200B lost in car manufacturing to delays on laptops, gaming systems and much more.
At Breadboard we are rethinking the electronics supply chain down to the finest detail. We’re designing the new operating system in which this entire system will run. We’ve already secured seed funding and are now looking to expand our engineering team to help us set the foundation for the future.
Why work with us?
As a part of our team, you will have the unique opportunity to design and implement the data platform for Breadboard, a critical component in our business.
As one of the first data engineers to join the Breadboard team you will be partnering with the CTO, our founding engineers, and will set the foundation for the design of the operating system for the electronics supply chain.
You will be involved in all aspects of the software development process, architectural decisions, recruiting, and building the organization in general.
We’re a tight-knit, intense team and you will have a ton of end to end ownership.
Work with seasoned founders who have had successful exits.
This is not only a unique opportunity for someone who wants to get their hands dirty, but also take that step back and collaborate with the executive team on setting a strategic future.
You’ll be responsible for
Data Management: Develop and automate robust data acquisition, processing, and normalization pipelines from various sources to ensure data quality and consistency.
Database and ETL Architecture: Design, implement, and optimize a database system for efficient data storage, retrieval, and analysis.
Collaboration on Machine Learning: The prospects of working with data scientists to leverage extracted data features, contributing to the development and refinement of a machine learning model for component matching.
API and UI Integration: Collaborate with the software development team to facilitate seamless integration between the database, machine learning models, and user interface.
Continuous Improvement and Compliance: Monitor system performance and user feedback for continuous improvements, while ensuring adherence to data privacy and security standards.

Requirements
Experience: A minimum of 3-5 years of experience in a data engineering role, preferably in the electronics, manufacturing industry or supply chain industry.
Technical Skills: Proficiency in SQL and experience with database design, data modeling, and performance optimization. Strong coding skills in a major programming language, such as Python or TypeScript. Experience with data processing tools and libraries (e.g., pandas, Spark). Experience with pipeline and workflow orchestration tools, such Airflow. Experience with Snowflake or similar cloud-based data warehousing solutions.
Data Acquisition and Processing: Experience with data acquisition methods, such as APIs and web scraping. Familiarity with data cleaning, normalization, and feature extraction techniques.
Machine Learning: Familiarity with machine learning concepts and algorithms, and experience collaborating with data scientists or machine learning engineers.
API Development: Experience with API development and integration, preferably in a microservices architecture.
Data Governance: Knowledge of data privacy and security regulations, and experience implementing data governance measures.
Bonus Points
Entrepreneurial mindset (we encourage all employees to be future founders and this can be a great stepping stone towards that goal).
Customer-centric and passionate about helping small businesses grow.
Previous experience at a high-growth, fast-paced startup.
Experience with our stack (and some others that may be included shortly):
React, Material UI, Typescript, CSS
NestJS, GraphQL, TypeORM, Knex.js, Objection.js, PostgreSQL, gRPC, Kafka
AWS, Docker, Kubernetes, Helm, Github Actions, CI/CD pipelines.
Compensation + Benefits
Competitive cash compensation
Stock ownership at a fast growing company
Health/dental/vision insurance—100% coverage option
The ability to push for other benefits you think are important
Our Engineering Culture
Openness to diverse opinions and backgrounds - Testing our assumptions
Supported career growth with a high emphasis on learning and mentorship
Lean development
End to end ownership and accountability. We’re all going to make mistakes. It’s about how you own up to them and learn from them.

Equal Opportunity Employer
Breadboard Software does not discriminate in employment on the basis of race, color, religion, sex (including pregnancy and gender identity), national origin, political affiliation, sexual orientation, marital status, disability, genetic information, age, membership in an employee organization, retaliation, parental status, military service, or other non-merit factor.",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"ProIT Inc.
5.0",5.0,"Bellevue, WA",Azure Data Engineer,"Data Engineering experience primarily on Spark. Someone who has worked on Azure cloud with knowledge on Azure DEVOPS (CI/CD & Infrastructure as a code), ADF, ADW & Power BI. Apache Nifi will be good to have.Power BI: Understand business requirements to set functional specifications for reporting applications Build automated reports and dashboards with the help of Power BI reporting tool Be experienced in tools and systems on MS SQL Server BI Stack, including SSRS and TSQL, Power Query, MDX, PowerBI, and DAX Be able to quickly shape data into reporting and analytics solutions Have knowledge of database fundamentals such as multidimensional database design, relational database design, and more Create functional reporting Study, analyze and understand business requirements in context to business intelligence. Design and map data models to shift raw data into meaningful insights. Utilize Power BI to build interactive and visually appealing dashboards and reports. Spot key performance indicators with apt objectives Run DAX queries and functions in Power BI Should have an edge over making DAX queries in Power BI desktop. Developing visual reports, KPI scorecards, and dashboards using Power BI desktop. Connecting data sources, importing data, and transforming data for Business intelligence.
Spark ,Azure DEVOPS (CI/CD & Infrastructure as a code), ADF, ADW & Power
Job Type: Full-time
Pay: $100,154.64 - $104,132.47 per year
Ability to commute/relocate:
Bellevue, WA 98004: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Hybrid remote in Bellevue, WA 98004","$102,144 /yr (est.)",51 to 200 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"WorkCog
4.3",4.3,"Atlanta, GA",Sr. AWS Data Engineer,"Position: AWS Data Engineer
Location: Atlanta ,GA
Duration: 12+ Months
Interview: Video
JD
Responsibilities
Bachelor’s degree in Computer Science, Software Engineering, or other related field, or equivalent work experience.
Extensive experience in data modeling, writing & performance tuning complex SQL queries & user-defined functions
Hands-on experience in design, implementation and orchestration of data pipelines and ETL batch Jobs.
Hands-on experience on all aspects of data handling: cleansing and standardization, normalization, merging, change data capture, data integration, reconciliation, and transformations
Strong experience in working with Big Data technologies such as Hive/Spark, Data lake and Data warehouse using GCP and/or AWS Services
Ability to write robust code in scripting languages like Python, Scala, PySpark or similar.
Good to Have
Associate Architect or equivalent cloud certification
Experience with processing large datasets using AWS and GCP UDFs (User defined functions)
Experience in ML probability & statistics algorithms, frameworks like Keras or PyTorch and tools like Jupyter notebooks.
Job Type: Contract
Pay: $45.83 - $52.42 per hour
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Atlanta, GA: Reliably commute or planning to relocate before starting work (Preferred)
Experience:
SQL (Preferred)
Informatica (Preferred)
Data warehouse (Preferred)
Work Location: In person",$49.13 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable
Kanini,#N/A,Remote,Data Engineer,"About Kanini
Kanini provides Agile Software Development, Cloud Computing, Data Science, and Location Intelligence services to public and private organizations. We have successfully served our clients in government, finance, transportation, utility, and software industries since 2003.
Why you should join
Working at Kanini is flexible and personal. We are a highly motivated, collaborative team experimenting with the latest technologies. We are committed to everyone having a healthy work/life balance, and we provide extensive mentorship and training resources to help you succeed.
Kanini is looking for a Big Data Engineer who has a deep experience in Data Engineering, AWS, Python, Data Lakes.
Required Skills
At least 10 years software development experience
At least 5 years leading at least one Scrum team of data engineers building data-intensive products with a modern tech stack.
Significant experience with big data ETL pipeline development with Spark, Hive, and related technologies
Significant experience with a general-purpose programming language such as Python, Scala, or Java
Experience with Spark framework and related tools (PySpark, Scala, SparkR, Spark SQL, Spark UI)
Experience with Hadoop ecosystem using HDFS, ADLS Gen2, or AWS S3
Experience with data visualization development using Python, Tableau, or PowerBI
Experience with Azure, AWS or GCP
Solid understanding of performance tuning concepts for relational and distributed database systems
Familiarity with distributed programming, big data concepts, and cloud computing
Education Qualifications
Bachelor’s degree in computer science/Engineering or Technology related field or possess equivalent work experience.
Preferred Qualifications
Cloud certifications from Azure, AWS or GCP
Big data, data engineering or data science certifications from recognized vendors such as Databricks & Cloudera
Kanini Software Solutions, Inc. does not discriminate in employment matters based on race, gender, religion, age, national origin, citizenship, veteran status, family status, disability status, or any other protected class. We support workplace diversity. If you have a disability, please let us know if there is anything we can do to improve the interview process for you; we’re happy to accommodate.
Kanini Software Solutions, Inc., 25 Century Blvd., Ste. 602, Nashville, TN 37214
Job Types: Full-time, Contract
Pay: $60.00 - $70.00 per hour
Work Location: Remote",$65.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Inovalon
3.1",3.1,United States,Software Data Engineer,"Inovalon was founded in 1998 on the belief that technology, and data specifically, would empower the transformation of the entire healthcare ecosystem for the better, improving both outcomes and economics. At Inovalon, we believe that when our customers are successful in their missions, healthcare improves. Therefore, we focus on empowering them with data-driven solutions. And the momentum is building.
Together, as ONE Inovalon, we are a united force delivering solutions that address healthcare's greatest needs. Through our mission-based culture of inclusion and innovation, our organization brings value not just to our customers, but to the millions of patients and members they serve.
Overview: The Software Data Engineer is responsible for contributing to data pipelines, ETL, data warehouse, jobs, data operations. They will be part of the team which is building next generation data & reporting platform. This platform will cater internal business stakeholders and external customers to provide insights & forecasting to understand current state of business, improve decision-making for their tactical and strategic goals & KPIs. This position may require independent work, sharing information and assisting others with work request.
Duties and Responsibilities:
Work with the agile team to participate in agile ceremonies like grooming, planning, standup, retrospective, demos
Actively contribute to grooming, and standup, create & update tasks, estimate and status
Work with data architects and business analysts to create a logical data model and create DDL scripts for physical database creation
Work on large data to ensure ingestion of data, dynamic rule & validation of data, cleansing, transforming, and loading into the data warehouse
Write complex queries, stored procedures, functions, SSIS Packages for various job execution
Develop modern ETL framework utilizing tools like ADF (Azure Data Factory), MS-SSIS etc
Develop STAR or SNOWFLAKE database schema utilizing industry best practices to build Data warehouse, data marts, views, and data sets/products
Develop ETL pipelines, using SQL, Stored procedures/functions to extract data from various sources and load into warehouse
Develop Symantec layer and data export frameworks to extract the data from the warehouse, transform, pre-aggregate, perform calculations and load into various data marts for Analytics use
Develop configurable export framework to extract data from Data warehouse and data marts to generate reports for internal and external customers in .csv, flat files and
Design and implement data validation and quality checks to ensure the accuracy and completeness of the data in the data warehouse
Perform performance of queries and data processing, identify and resolve any issues
Work and communicate in a cross-functional geographically dispersed team environment comprised of software engineers and product managers; and
Ensure compliance to company procedures when making changes and implementing code.
Maintain compliance with Inovalon's policies, procedures and mission statement;
Adhere to all confidentiality and HIPAA requirements as outlined within Inovalon's Operating Policies and Procedures in all ways and at all times with respect to any aspect of the data handled or services rendered in the undertaking of the position; and
Fulfill those responsibilities and/or duties that may be reasonably provided by Inovalon for the purpose of achieving operational and financial success of the Employer.
Job Requirements:
Minimum two (2) years related experience required; healthcare industry experience preferred.
Strong understanding to develop SQL queries for data analysis.
Experience working on Azure Cloud is preferred
3+ experience in MS SQL, T-SQL, ETL Jobs
3+ experience in Microsoft tools like SSMS, SSIS, SQL Server
Strong understanding of database concepts and schema (like star, snowflake schema)
Ability to learn quickly and independently
Ability to effectively communicate with internal and external customers
Experience with test driven development methodologies.
Education:
Bachelor's degree in Computer Science, Software Engineering, or Information Technology.
Physical Demands and Work Environment:
Sedentary work (i.e., sitting for long periods of time);
Exerting up to 10 pounds of force occasionally and/or negligible amount of force
Frequently or constantly to lift, carry push, pull or otherwise move objects and repetitive motions
Subject to inside environmental conditions; and
Studies have shown that women and people of color are less likely to apply for jobs unless they believe they meet every one of the qualifications listed in a job description. If you don't meet every qualification listed but are excited about our mission and the work described, we encourage you to apply regardless. Inovalon is most interested in finding the best candidate for the job and you may be just the right person for this or other roles.
By embracing diversity, equity and inclusion we enhance our work environment and drive business success. Inovalon strives to reflect the diversity of the communities where we operate and of our clients and everyone whom we serve. We endeavor to create a culture of inclusion in which our associates feel empowered to bring their full, authentic selves to work and pursue their professional goals in an equitable setting. We understand that by fostering this type of culture, and welcoming different perspectives, we generate innovation and growth.
Inovalon is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirement.
The Company maintains a drug free work environment for all of its associates, which includes employees, contractors and vendors. It is unlawful for associates to manufacture, sell, distribute, dispense, possess or use any controlled substance or marijuana in the workplace and doing so will result in disciplinary action, up to and including termination of employment or the contracted relationship.",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Software Development,1998,$500 million to $1 billion (USD)
"AbleTo
3.7",3.7,"New York, NY",Data Engineer,"About AbleTo
Join our mission-driven organization, where your work matters and a diversity of ideas and backgrounds are welcomed. AbleTo is the leading provider of high quality, technology-enabled behavioral health care. We believe that everyone deserves access to high-quality care and offer a suite of technology-enabled services to empower people to lead better lives through better mental health. A proprietary platform connects individuals with AbleTo highly trained licensed providers who deliver weekly sessions by phone or video supported by an integrated digital experience. Members also have access to mental health coaches, and digital support programs. Our outcomes-focused approach is proven to improve both behavioral and physical health and lower medical costs.

Overview
Data is integral to nearly everything that we do at AbleTo, and as a result the data team plays a key role at our company. As a Data Engineer at AbleTo, you will grow and develop your skills as a data engineer to build a world class data platform to support our mental health care mission.

What You Will Do
Build and maintain batch and real-time ETL pipelines in a Google Cloud Platform architecture (BigQuery, Dataproc, Firestore, Informatica, Python, etc.)
Identify code quality issues and implement tests to improve future processes.
Document orchestration work-flows and create run books
Implement data integrity tests to ensure we are ingesting accurate data.
Implement high-quality test-driven code.
Participate in daily team stand ups and other Agile ceremonies

Key Initiatives over the next 12 months
Data Mart client migration
Data Mart enhancements
Automation of manual day to day operational tasks
Decommission legacy pipelines

Who You Are:
1+ years of experience coding in Python.
Experience working with SQL (structured query language).
Familiarity with structuring and writing ETLs.
Experience working with Orchestration tools (Airflow) and Cloud Databases (BigQuery) is a plus.
Experience working in an Agile Development environment is a plus.
BS in Information Systems, Computer Science or related field is preferred.
You are excited to work with data!
Curiosity to dive deeply into issues and feel empowered to make a meaningful impact at a mission-driven company.
Committed to agile development and value delivery and solid engineering principles, as well as continuously improving our craft.
You are a collaborator. You build and maintain strong, productive working relationships with your customers and stakeholders.
You are empathetic and seek to understand each individual's diversity of background and experience contributes to making up a great team.
You drive to improve yourself and others.
You are excited about learning new languages, tools, techniques and technologies.

Why You Should Join Our Team:
We're proud to be a Great Place to Work-Certified™ company. We want you to show up and feel your best at work, and that means respecting your time outside of work. Our inclusive, flexible workspace prioritizes a work/life balance. We offer competitive salaries, comprehensive health benefits (for full-time employees), and professional perks such as 401K matching, fully funded HRA, and generous time off, including mental health days because your well-being is important to us.
At AbleTo, we're empowering people to get better and stay better. Want to join us? Take the next step in your career by applying for this role today.
The salary range is $40,000 to $90,000. Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, AbleTo offers benefits such as a comprehensive benefits package, incentive and recognition programs, and 401k contribution (all benefits are subject to eligibility requirements).

#LI-Remote
Follow AbleTo on LinkedIn, Twitter, and Instagram!
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.
AbleTo is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or veteran status. AbleTo is an E-Verify company.","$65,000 /yr (est.)",501 to 1000 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2008,$25 to $100 million (USD)
"PrizePicks
4.8",4.8,"Atlanta, GA",Data Engineer,"At PrizePicks, we are the fastest growing sports company in North America, as recognized by Inc. 5000. As the leading platform for Daily Fantasy Sports, we cover a diverse range of sports leagues, including the NFL, NBA, and Esports titles like League of Legends and CS : GO. Our team of over 200 employees thrives in an inclusive culture that values individuals from diverse backgrounds, regardless of their level of sports fandom. Ready to reimagine the DFS industry together?
Our Analytics Team is responsible for building and maintaining analytics tools and workflows to support the PrizePicks business. By developing, maintaining, and testing data generation infrastructures, you will enable the PrizePicks business to make smarter, better, and faster data-driven decisions.
What you'll do:
Create, maintain and orchestrate optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Improve and streamline data systems to drive innovation within Prizepicks
What you have:
3+ years of experience building frameworks for data ingestion pipelines but real time and batch using data modeling, ETL/ELT processes
Stellar SQL skills, experience building DBT pipelines in production and know your way around structured, semi-structured and unstructured data.
Have built and optimized 'big data' data pipelines, architectures and data sets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Experience with Python and other object oriented scripting languages
Cloud experience: AWS, EC2, EMR, RDS, Redshift, GCP, etc..
Not required but would be awesome if you have experience with any of the following data pipeline and workflow tools: Azkaban, Luigi, Airflow, Prefect, etc.
Where you'll live:
Anywhere in the US is fine (we are based in Atlanta, GA)
Benefits you'll receive:
In addition to your great compensation package, company subsidized medical/dental/vision coverage plans and matching 401(k), we'll shower you with perks including:
Break room with ping pong, endless snacks and in-office lunch once a week
Unlimited PTO to encourage a healthy work/life balance (2 week min required!)
Modern work schedule focused on getting the job done, not hours clocked
Workplace flexibility
Company and team outings, we encourage a tight-knit workplace
Generous Maternity AND Paternity leave (16 weeks!)
Annual bonus & stock options
Wellness program
Company equipment provided (Windows & Mac options)
Annual performance reviews with opportunity for growth and career development
#LI-REMOTE

You must be authorized to work for any employer in the U.S. We are unable to sponsor or take over sponsorship or an employment Visa at this time.
PrizePicks is an Equal Opportunity Employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.","$97,814 /yr (est.)",1 to 50 Employees,Company - Private,"Arts, Entertainment & Recreation",Sports & Recreation,2015,Unknown / Non-Applicable
"CVS Health
3.1",3.1,"Wellesley, MA",Data Engineer,"Caremark LLC, a CVS Health company is hiring for the following role in Wellesley, MA: Data Engineer to design, build and manage large scale data structures, pipelines and efficient Extract/Load/Transform (ETL) workflows to support business applications. Duties include: develop large scale data structures and pipelines to organize, collect and standardize data to generate insights and addresses reporting needs; write ETL (Extract/Transform/Load) processes, design database systems, and develop tools for real-time and offline analytic processing; collaborate with Data Science team to transform data and integrate algorithms and models into automated processes; leverage knowledge of Hadoop architecture, HDFS commands, and designing and optimizing queries to build data pipelines; utilize programming skills in Python, Java, or similar languages to build robust data pipelines and dynamic systems; build data marts and data models to support Data Science and other internal customers; integrate data from a variety of sources and ensure adherence to data quality and accessibility standards; analyze current information technology environments to identify and assess critical capabilities and recommend solutions; and experiment with available tools and advise on new tools to provide optimal solutions that meet the requirements dictated by the model/use case. Multiple positions.

Required Qualifications
Master’s degree (or foreign equivalent) in Computer Science, Data Science, Statistics, Mathematics, Analytics, or a related field and one (1) year of experience in the job offered or related occupation. Requires one (1) year of experience in each of the following: Analyzing large data sets from multiple data sources; Data analysis for retail and/or healthcare industries; Programming in Java, Python, or R; SAS or SQL programming languages; and Databases: Oracle, Teradata, or DB2.

Preferred Qualifications
See Required Qualifications.

Education
See Required Qualifications.

Business Overview
Bring your heart to CVS Health Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver. Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable. We strive to promote and sustain a culture of diversity, inclusion and belonging every day. CVS Health is an affirmative action employer, and is an equal opportunity employer, as are the physician-owned businesses for which CVS Health provides management services. We do not discriminate in recruiting, hiring, promotion, or any other personnel action based on race, ethnicity, color, national origin, sex/gender, sexual orientation, gender identity or expression, religion, age, disability, protected veteran status, or any other characteristic protected by applicable federal, state, or local law. We proudly support and encourage people with military experience (active, veterans, reservists and National Guard) as well as military spouses to apply for CVS Health job opportunities.",#N/A,10000+ Employees,Company - Public,Healthcare,Health Care Services & Hospitals,1963,$10+ billion (USD)
"Lcp Tracker Inc
4.4",4.4,"New Braunfels, TX",Data Engineer,"Company Summary
LCPtracker, Inc. is a leading software service provider specializing in construction site compliance related software, headquartered in Orange, CA. Our main solution, LCPtracker Pro, is a powerful web-based SaaS solution for collecting, verifying, and managing certified payrolls and other labor compliance related documents. Over 200 government agencies and 100,000 contractors have used LCPtracker for their certified payroll reporting.
In 2023, our growth continues at a rapid pace, making LCPtracker one of the fastest growing small companies in Orange County, California, recognized by the Orange County Business Journal. In 2017, 2018, 2019, 2020, 2021 and 2022, LCPtracker was recognized as an Orange County ""Best Places to Work"" by the Orange County Register.

Position Summary
As a Data Engineer at LCPtracker, you will leverage your experience with SQL, ETL, and reporting to drive the design and development of data-driven solutions. The position is responsible for performing advanced technical and analytical work in the development and support of standardized and customized reports, as well as the testing and maintaining of data integrity.
Other Duties: Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.

Core Competencies
Confidentiality: This role may be privy to confidential and/or sensitive information. Must demonstrate integrity in maintaining confidential and sensitive information and demonstrate strict adherence to organizational policies and procedures.
Communication Proficiency: uses friendly and proficient communication to interact with a wide range of people, frequently exchanging information about office operations.
Time Management: Must manage their own time. They use an electronic calendar in an email program to set meetings, to request others to attend and to coordinate their responses. They respond to requests for attendance at various meetings.
Initiative and Proactivity: Correctly anticipates a need, volunteers readily, and acts without being told to do so. Brings new ideas to the company. Undertakes self-development activities; seeks increased responsibilities; takes calculated risks; looks for and takes advantage of opportunities; asks for and offers help when needed.
Drive for Results: Is goal-oriented; maintains focus on the objective.
Problem Solving, Personal Judgment: Identifies and resolves problems in a timely manner; gathers and analyzes information skillfully; develops alternative solutions; works well in group problem-solving situations; uses reason even when dealing with emotional topics. Solicits and applies feedback.
Quality Management: Looks for ways to improve and promote quality; demonstrates accuracy and thoroughness. Does not cut corners; monitors work to ensure quality; applies feedback to improve performance.
Primary Duties and Responsibilities
Develop, modify, maintain, and support custom reports (MS SQL, SSRS) for both ad-hoc and ongoing business needs.
Develop MS SQL objects (tables, stored procedures, functions, views, etc.) as applicable.
Create and customize weekly, monthly, quarterly, and annual reports using Microsoft Excel or other reporting tools as applicable.
Ensure high data quality through regular quality checks.
Extract, filter, and aggregate data through logical queries and programming.
Maintain a high level of confidentiality and use discretion when needed.
Perform other work including specific tasks or special projects as required.
Promote and maintain positive morale through teamwork.
Other Duties: Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.

Salary Range
Data Engineer rate $100,000 to $140,000 annual salary
Pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, and experience. The total compensation package for this position may also include other elements dependent on the position offered. Details of participation in these benefit plans will be provided if an employee receives an offer of employment.

Benefits
Paid Time Off
9 Paid Holidays
Phantom Stock
401k Plan with up to 4% company match
Medical Benefits (Health, Vision and Dental)
Life Insurance
LTD & STD

Work Environment
This position performs its duties from our New Braunfels, TX office. This position operates in a professional office environment and role routinely uses standard office equipment such as computers, phones, mobile devices, photocopiers, filing cabinets and fax machines.

Physical Requirements
While performing the functions of this job, the employee is regularly required to sit; frequently required to talk and hear, use hands and fingers to type, scroll and use computer equipment. The employee is required to have close visual acuity to perform an activity such as: preparing and analyzing data and figures; transcribing; viewing a computer terminal; extensive reading; visual inspection of text/data in both print and electronic forms.
Ability to lift and move up to 25 pounds.
Position Type and Expected Hours of Work
This is a full-time exempt position reporting to our New Braunfels, TX office M-F 8am – 5pm. Days/hours worked are dependent on the workload at the time. General availability and presence in the office is expected during regular business hours Monday-Friday. However, some flexibility is allowed. Occasional evening and weekend work may be required as job duties demand.

Travel
There is no major travel requirement for this position. However, infrequent travel may be necessary to visit remote office(s), attend conferences/industry events, etc. Attendance at our corporate Staff Retreat is required. This event is a 2-3-day retreat. Attendance at our annual User Conference as assigned.

LCPtracker, Inc. is an equal opportunity employer of all qualified individuals. All applicants will be afforded equal opportunity without discrimination because of race, color, religion, sex, sexual orientation, marital status, order of protection status, national origin or ancestry, citizenship status, age, physical or mental disability unrelated to ability, military status or an unfavorable discharge from military service. LCPtracker, Inc. will consider for employment qualified applicants with criminal histories in a manner consistent with all federal, state, and local ordinances.
LCPtracker is committed to the full inclusion of all qualified individuals. In keeping with our commitment, LCPtracker will take steps to assure that people with disabilities are provided reasonable accommodations. Accordingly, if reasonable accommodation is required to fully participate in the job application or interview process, to perform the essential functions of the position, and/or to receive all other benefits and privileges of employment, please contact the LCPtracker Human Resources Department at HR@lcptracker.com.
Education and Experience
MUST HAVE:
Bachelor’s degree in software engineering disciplines, computer science or other related field and/or the equivalent combination of education and experience.
7+ years of experience with SSRS reporting tools and MS SQL server.
7+ years of experience using ETL tools such as SSIS and/or ADF.
7+ years of experience in Data Warehousing with SSAS or AAS.
Experience in applying security to SSAS or AAS models using authentication frameworks such as AAD or Active Directory.
Adept at queries, report writing and presenting findings.
Ability to comprehend, analyze, and systematically compile technical, statistical, and information into comprehensive reports or other formats.
Effective business writing and composition skills with good command of the English language.
Ability to independently plan, organize, and complete a variety of projects within established standards, objectives and time frames.
Ability to work in fast-paced, multi-tasking environment with shifting priorities and demanding deadlines.
Ability to work independently in finding solutions
Ability to work in an agile work environment
Ability to work in a team environment
Must be detailed-oriented and able to effectively prioritize and organize workload, with efficient time management.
Minimum 1-year experience working on Scrum teams.
Basic understanding of the Agile methodology.
Scrum certifications are a plus.
Strong interpersonal communication skills.","$120,000 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,1992,$1 to $5 million (USD)
"Acrisure Technology Group
3.9",3.9,"Austin, TX",Data Engineer,"Data Engineer
Hybrid Position (3 days per week average in Downtown Austin, TX or Grand Rapids, MI office)
Note: This is a full-time, in-house position. We do not offer C2C or C2H employment and are not able to sponsor visas for this position.
Acrisure Technology Group (ATG) is a fast-paced, AI-driven team building innovative software to disrupt the $6T+ insurance industry. Our mission is to help the world share its risk more intelligently to power a more vibrant economy. To do this, we are transforming insurance distribution and underwriting into a science.
At the core of our operating model is our technology: we're building the premier AI Factory in the world for risk and applying it at the center of Acrisure, a privately held company recognized as one of the world's top 10 insurance brokerages and the fastest growing insurance brokerage globally. By using the latest technology and advances in AI to push the boundaries of understanding risk, we are systematically converting data into predictions, insights, and choices, and we believe we can remove the constraints associated with scale, scope, and learning that have existed in the insurance industry for centuries.
We are a small team of extremely high-caliber engineers, technologists, and successful startup founders, with diverse backgrounds across industries and technologies. Our engineers have worked at large companies such as Google and Amazon, hedge funds such as Two Sigma and Jump Trading, and a variety of smaller startups that quickly grew such as Indeed, Bazaarvoice, RetailMeNot, and Vrbo.
The Role
The Business Intelligence team's mission is to unify data across the enterprise to optimize business decisions made at the strategic, tactical, and operational levels of the organization. We accomplish this by providing an enterprise data warehouse, data lake, reporting platform, and business processes that provide quality data, in a timely fashion, from any channel of the company and present them in such a manner as to maximize the value of that data for both internal and external customers.
The Data Engineer is responsible for designing and developing moderate to complex ETL processes required to populate a data lake and structured data warehouse which supply data for the machine learning, AI & BI teams. Responsibility includes working with a team of contracted developers as well as coaching and mentoring junior and mid-level developers. Ensuring high quality and best practices are maintained through the development cycle is key to this position.
You will interact with some of the top technologists on the planet. Our technology runs on Google Cloud and is configured with Kubernetes, leveraging various services in that environment. Our data storage layer includes BigQuery, BigTable, and Postgres. We code primarily in Kotlin, Python, Java, and JavaScript and make use of many frameworks, including Dataflow, Cloud AI Platform, KubeFlow, Spring, and React.
Here are some of the ways in which you'll achieve impact
Leverage established guidelines and custom designs to create complex ETL processes to meet the needs of the business
Develop from strategic and non-strategic data sources including data preparation/ETL and modeling for data visualizations in a self-service platform
Contribute to the definition and development of the overall reporting roadmap
Translate reporting requirements into reporting models, visualizations and reports by having a strong understanding of the enterprise architecture
Standardize reporting that helps generate efficiencies, optimization, and end user standards
Integrate dashboards and reports from a variety of sources, ensuring that they adhere to data quality, usability, and business rule standards
Independently determine methods and procedures for new or existing requirements and functionality
Work closely with analysts and data engineers to identify opportunities and assess improvements of our products and services
Contribute to workshops with the business user community to further their knowledge and use of the data ecosystem
Produce and maintain accurate project documentation
Collaborate with various data providers to resolve dashboard, reporting and data related issues
Perform Data Services reporting benchmarking, enhancements, optimizations, and platform analytics
Participate in the research, development, and adoption of trends in reporting and analytics
Mentor BI Developers and BI Analysts
Other projects as assigned in order to support necessary business goals across teams
You may be fit for this role if you have
Minimum 5 years required, particularly in an Azure environment with Azure Data Bricks, Azure Data Factory, Azure Data Lake
Minimum 5 years designing data warehouses, data modeling, and end-to-end ETL processes in a MS-SQL environment
Minimum 2 years developing machine learning models with Azure ML, ML Flow, BQML
Expert working knowledge of SQL, Python and Spark (and ideally PySpark) with a demonstrated ability to create ad-hoc SQL queries to analyze data, create prototypes, etc required.
Successfully delivered 2+ end to end projects – from Inception to Execution - in Data Engineering / Data Science / Data Integration as a Tech Senior/Principal
Ability to Analyze, summarize, and characterize large or small data sets with varying degrees of fidelity or quality, and identify and explain any insights or patterns within them.
Experience with multi-source data warehouses
Strong skills in in data analytics and reporting, particularly with Power BI
Experience with other cloud environments (GCS, AWS) a definite plus
Strong experience creating reports, dashboards, and/or summarizing large amounts of data into actionable intelligence to drive business decisions required
Strong understanding of core principles of data science and machine learning; experience developing solutions using related tools and libraries
Hands on experience building logical data models and physical data models and using tools like ER/Studio/Idera
Write SQL fluently, recognize and correct inefficient or error-prone SQL, and perform test-driven validation of SQL queries and their results
Proficient in writing Spark sql using complex syntax and logic like analytic functions etc.
Well versed in Data Lake & Delta Lake Concepts
Well versed in Databricks usage in dealing with Delta tables (external \ managed)
Well versed with Key Vault \ create & maintenance and usage of secrets in both Databricks & ADF
Should be knowledgeable in Stored procedures \ functions and be able to use them by ADF & Databricks as this is a widely used Practice internally
Familiar with DevOps process for Azure artifacts and database artifacts
Well versed with ADF concepts like chaining pipelines, passing parameters, using APIs for ADF & Databricks to perform various activities.
Experience creating and sharing standards, best practices, documentation, and reference examples for data warehouse, integration/ETL systems, and end user reporting
Apply disciplined approach to testing software and data, identifying data anomalies, and correcting both data errors and their root causes
Academics: Undergraduate degree preferred or equivalent experience along with a demonstrated desire for continuing education and improvement
Location: Austin, TX or Grand Rapids, MI
We are interested in every qualified candidate who is eligible to work in the United States. We are not able to sponsor visas for this position.","$96,659 /yr (est.)",5001 to 10000 Employees,Company - Private,Insurance,Insurance Agencies & Brokerages,2005,$1 to $5 billion (USD)
"Unilever
4.1",4.1,Remote,Health & Wellbeing Data Engineer,"Health & Well Data Engineer
Remote – USA; Los Angeles
Health & Wellness is a strategic Unilever global business unit established to capture the growth opportunity in the €140bn global consumer health segment defined by Vitamins, Minerals, and Supplements (VMS) product category. Our ambition is to build a €5bn business globally; and be a global top 3 player within this space.
As of today, our brands include Equilibra in Italy; OLLY, SmartyPants, Liquid IV, Onnit, Welly and Nutrafol in the US with ~€1.4bn in turnover. The H&W data and analytics team operates across all these brands in one common data function with the aim of helping these brands to grow through smarter, faster, and better data driven decision making. The data team within H&W was set up in May 2022 and is looking to expand. This is a fantastic opportunity to shape the future of data in a global business unit with the latest technologies in the cloud.
What you will do:
collaborate with other engineering and business teams within H&W and Unilever to solve complex challenges using data.
drive forward best practice by building out data frameworks and design patterns to be utilized across the H&W data infrastructure.
build production ready distributed ETL data pipelines from a wide range of different sources (APIs, flat files, databases, ERP systems etc.)
build performant and reliable data models to democratize the use of data across H&W
build integrations to facilitate the use of other technologies in H&W ecosystem (e.g Kinnaxis, Anaplan)
review and deploy code from other team members as part of a DevOps process, providing coaching and mentoring to junior developers.
manage access within the environments to ensure data security protocols are being met.
contribute to architectural and governance decision marking within the H&W data ecosystem.
Who you are:
Passionate about all things data
Entrepreneurial Self-starter with the ability to thrive in a fast paced start up environment.
Fast learner with the ability to pick up new technologies quickly.
Creative problem solver who thinks outside the box
What you will bring:
3-5 years’ Data engineering experience with strong pyspark and SQL skills
1+ year Distributed computing experience [databricks/Splunk]
1+ year experience working with data in cloud environments [GCP/Azure/AWS]
Bachelor’s degree required.
Experience with streaming workloads is a plus
Experience working within a delta lakehouse in databricks is a plus
Tech Stack:
Cloud agnostic: Databricks (pyspark, scala, SQL)
Azure: Azure Data Factory, Azure Logic Apps, Azure Data Lake Storage (ADLS), Azure Blob Storage, Azure Machine Learning, PBI (data modelling, DAX)
GCP: Google Big Query, Google Cloud Storage
AWS: S3
Pay: The pay range for this position is $83,200 - $124,700. Unilever takes into consideration a wide range of factors that are utilized in making compensation decisions including, but not limited to, skill sets, experience and training, licensure and certifications, qualifications and education, and other business and organizational needs. Bonus: This position is bonus eligible. Long-Term Incentive (LTI): This position is LTI eligible. Benefits: Unilever employees are eligible to participate in our benefits plan. Should the employee choose to participate, they can choose from a range of benefits to include, but is not limited to, health insurance (including prescription drug, dental, and vision coverage), retirement savings benefits, life insurance and disability benefits, parental leave, sick leave, paid vacation, and holidays, as well as access to numerous voluntary benefits. Any coverages for health insurance and retirement benefits will be in accordance with the terms and conditions of the applicable plans and associated governing plan documents.
-
Unilever is an organization committed to diversity and inclusion to drive our business results and create a better future every day for our diverse employees, global consumers, partners, and communities. We believe a diverse workforce allows us to match our growth ambitions and drive inclusion across the business. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, age, national origin, or protected veteran status and will not be discriminated against on the basis of disability.

Employment is subject to verification of pre-screening tests, which may include drug screening, background check, credit check and DMV check.

If you are an individual with a disability in need of assistance at any time during our recruitment process, please contact us at
NA.Accommodations@unilever.com
. Please note: This email is reserved for individuals with disabilities in need of assistance and is not a means of inquiry about positions or application statuses.
#LI-Remote","$103,950 /yr (est.)",10000+ Employees,Company - Public,Manufacturing,Consumer Product Manufacturing,1872,$10+ billion (USD)
"iSpace, Inc.
3.8",3.8,"Houston, TX",Data Engineer- Databricks,"Company Description

iSpace is a global services company focused on outsourcing, consulting and staffing. Over the last decade, we have helped numerous corporations and institutions reach their business objectives and IT goals.
iSpace services are centered in three areas - IT Outsourcing, IT Staff Augmentation, and Business Process Outsourcing. Our team of seasoned professionals based in the United States and India focus on providing results, driving innovation and affecting outcomes. Our clients have come to rely on our expertise, our commitment to quality, customer service and our innovative approach to problem solving to help create sustainable value for their customers and shareholders.
Specializing in Healthcare, Entertainment, Automobile and Financial Services, we work with Fortune 1000 companies throughout the United States. Our commitment to customer satisfaction is reflected in the fact that over 90% of our client base have remained with us for over 5 years.

Job Description

Title: Data Engineer- Databricks
Location: Houston TX or Chicago IL
Duration: Full Time but open to contract to hire

REMOTE Work - Candidate must be based out of CST or EST time zones

Job Responsibilities
As a member of the Concert Business Systems development team the BI Data engineer performs a wide range of data modeling, engineering, architecture and data management activities.
Follow best practices in areas of data modeling, data interoperability, metadata management
Work to streamline existing ETL processes, migrate to new platforms and improve data processing while building functional data lake for the business
Apply best practices in data engineering processes, data architecture, data security, documentation. Deep understanding of agile methodologies is required
Gather, analyze and communicate requirements for the platform functionality based on needs off implementing ETL processes.
Actively engage and lead efforts of modernizing legacy systems, streamlining traditional ETL processes and documenting all systems and processes. Work with operations team to ensure that maintenance responsibilities are not part of data engineering day to day, including delivering easy to maintain, self-healing, scalable data pipelines
Actively participate in building a data driven culture, contributing to the data community and supporting data enablement team in efforts related to making data easily accessible, ready in time, discoverable, usable and easy to understand
Join continuous innovation efforts, bring and implement ideas to advance data engineering team into the center of excellence.

TECHNICAL SKILLS/COMPETENCIES
Demonstrated data modelling and engineering skills for scalability, data streaming, self-healing and scalable process design, data partitioning, distributed data processing, metadata management.
Advanced programming skills in Python, PySpark API, advanced Spark SQL.
Hands on experience with Databricks Delta Lake architectures.
Strong understanding of data warehousing techniques.
Deep understanding of query plans, ETL best practices for timeseries reporting, advanced dimensional modeling
Experience writing SQL queries for customer applications and troubleshooting.
Hands on experience using BI tools such as Tableau, Business objects, Cognos, Power BI etc.
Expertise in understanding complex business needs, analyzing, designing and developing solutions.
Experience with technical stack which includes
Data Lake technologies like S3, AWS Data Lake, Databricks Delta lake;
Data warehousing tech like Snowflake, Teradata, Oracle EDW;
Deep understanding of operationalizing ETL processes using Spark and modern streaming
ETL technologies like Spark Streaming, StreamSets, Kafka Connect

Candidates applying for this position should have a Bachelor's degree in computer science or related field plus six years demonstrated work experience. Ideal candidates are highly motivated, resourceful, and capable of coming up to speed quickly

Additional Information

All your information will be kept confidential according to EEO guidelines.","$91,811 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2000,$5 to $25 million (USD)
Go Intellects Inc,#N/A,"Washington, DC",Data Engineer,"Work Location: REMOTE (1 – 2 Days On-site/Week may require)
Required Skills:
· Collect, manage, and convert raw data accurately and reliably
· Organize data systems for subgroup access and analyses
· Configure and sustain data cloud structures
· Must have expertise in Data Visualization Tools (Tableau)
· Data Modeling/Science as Python/SAS
· Should have AWS cloud native services, security, data pipeline
· Able to work with structured and unstructured data.
· Validate outputs of data pipelines
· Degree in Data Engineering preferred.
Job Type: Full-time
Pay: Up to $150,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Application Question(s):
Do you have, ""Active Secret (or) Top Secret Security Clearance""?
Work Location: In person","$150,000 /yr (est.)",Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Nike
4.2",4.2,"Beaverton, OR",Senior Data Engineer (Remote Option*),"Become a Part of the NIKE, Inc. Team
NIKE, Inc. does more than outfit the world’s best athletes. It is a place to explore potential, obliterate boundaries and push out the edges of what can be. The company looks for people who can grow, think, dream and create. Its culture thrives by embracing diversity and rewarding imagination. The brand seeks achievers, leaders and visionaries. At NIKE, Inc. it’s about each person bringing skills and passion to a challenging and constantly evolving game.
NIKE is a technology company. From our flagship website and five-star mobile apps to developing products, managing big data and providing leading edge engineering and systems support, our teams at NIKE Global Technology exist to revolutionize the future at the confluence of tech and sport. We invest and develop advances in technology and employ the most creative people in the world, and then give them the support to constantly innovate, iterate and serve consumers more directly and personally. Our teams are innovative, diverse, multidisciplinary and collaborative, taking technology into the future and bringing the world with it.

WHO WE ARE LOOKING FOR
We are looking for a senior data engineer who will be responsible for designing, building, and maintaining the infrastructure and systems necessary for processing and analyzing large volumes of data. They play a crucial role in ensuring the availability, reliability, and efficiency of data pipelines and workflows within an organization.

WHAT YOU WILL WORK ON
As a Senior Data Engineer, we are responsible for designing, implementing, and optimizing data architectures that enable efficient data extraction, transformation, and loading (ETL) processes. Your expertise will contribute to the seamless flow of data from various sources into our data storage and analytical systems. This role will report to the Manager of North America Global Operations and Logistics Analytics Engineering.

WHO YOU WILL WORK WITH
We work closely with other data engineers, software engineers, analysts, and stakeholders to develop and maintain robust data infrastructure and pipelines.
Collaborate with cross-functional teams to understand data requirements and translate them into scalable and efficient data solutions.
Design, build, and maintain data pipelines and ETL processes to ensure reliable and timely data ingestion, transformation, and delivery.
Develop and optimize data models, schemas, and databases to support data analysis and reporting needs.
Implement data quality and validation processes to ensure the accuracy, completeness, and consistency of data.
Monitor and troubleshoot data pipelines, identifying and resolving issues related to data processing, performance, and reliability.
Implement data security and privacy measures to safeguard sensitive information.
Continuously evaluate and recommend improvements to data infrastructure, tools, and technologies to improve efficiency and scalability.
Collaborate with users and analysts to provide them with the necessary data sets and ensure smooth data access for their analysis and analytical needs.
Document data engineering processes, best practices, and technical specifications.

WHAT YOU BRING
Bachelor's or Master's degree in Computer Science, Information Systems, or a related field.
5+ years of experience as a data engineer or in a similar role, with a strong understanding of data integration, ETL processes, and data modeling.
Proficiency in programming languages such as Python, SQL for data processing and automation
Experience with distributed computing frameworks like Apache Spark, or similar technologies.
Experience with relational databases, analytical databases, NoSQL, and query optimization techniques.
Experience with orchestration tools, such as Airflow.
Experience with data warehousing concepts and technologies, such as Snowflake, or Amazon Redshift.
Experience with cloud platforms like AWS, Azure, or GCP, and related services for data storage and processing.
Familiar with CI/CD, Infrastructure as Code, Terraform, or Cloud Formation
Understanding of data governance, data security, and privacy principles.
Remote Work Option – Open to remote work, except cannot work in South Dakota, Vermont, and West Virginia. These candidates will be required to relocate.

The annual base salary for this position ranges from $98,000 to $192,500. Actual salary will vary based on a candidate’s location, qualifications, skills, and experience.

Information about benefits
#LI-DS2
NIKE, Inc. is a growth company that looks for team members to grow with it. Nike offers a generous total rewards package, casual work environment, a diverse and inclusive culture, and an electric atmosphere for professional development. No matter the location, or the role, every Nike employee shares one galvanizing mission: To bring inspiration and innovation to every athlete* in the world.
NIKE, Inc. is committed to employing a diverse workforce. Qualified applicants will receive consideration without regard to race, color, religion, sex, national origin, age, sexual orientation, gender identity, gender expression, veteran status, or disability.","$145,250 /yr (est.)",10000+ Employees,Company - Public,Manufacturing,Consumer Product Manufacturing,1972,$10+ billion (USD)
"MUFG
3.5",3.5,"Jersey City, NJ","CyberSecurity Data Loss Prevention Engineer, Analyst - Hybrid - Irving, TX/ Jersey City, NJ/ Tempe, AZ","Do you want your voice heard and your actions to count?
Discover your opportunity with Mitsubishi UFJ Financial Group (MUFG), the 6th largest financial group in the world. Across the globe, we're 160,000 colleagues, striving to make a difference for every client, organization, and community we serve. We stand for our values, building long-term relationships, serving society, and fostering shared and sustainable growth for a better world.
With a vision to be the world's most trusted financial group, it's part of our culture to put people first, listen to new and diverse ideas and collaborate toward greater innovation, speed and agility. This means investing in talent, technologies, and tools that empower you to own your career.
Join MUFG, where being inspired is expected and making a meaningful impact is rewarded.
This is a hybrid position. The recruiter will provide additional details.
Job Summary
Security Engineer Analyst will ultimately be responsible for the day to day operations and architecture for the firm's user data loss prevention and data classification systems. This position will analyze data and information to maintain key metrics and collaborate with the rest of the engineers for the firm's DLP program strategy . The expectation is that this person will have an awareness of the DLP, data classification, and process automation, and able to recommend tuning and enhancements to the data security strategy and use cases. This role is for an entry level analyst with great technical acumen and analytical skills to aid in the vision of where to take our program.
Major Responsibilities
Develop and improve automated process, integration between applications, and operational efficiency
Develop and maintain data loss prevention and data classification rules and dashboards to meet enterprise security needs
Analyze, design, develop, and operate programs, shell scripts, tests, and infrastructure automation capabilities necessary for daily operations and custom processes.
Management of the technology and processes including monitoring, investigation, reporting, and rule maintenance.
Coordinate with internal Data Loss Prevention (DLP) and Insider Threat teams to develop and integrate events with the UEBA and SOAR technology.
Support security operations and other security partners in remediation or mitigation of security vulnerabilities and assist associated technical teams.
Advise Insider Threat responders as they develop and coordinate response, containment, and remediation capabilities as appropriate.
Qualifications
B.S. Computer Science or related field or equivalent experience
1+ years' overall technical experience in data loss prevention, data classification, incident response, security operations, or related information security field
1+ years' experience in application design/engineering, including but not limited to programming/scripting, Windows/Linux system administration, etc.
Experience in the banking or finance industries a plus
Understand data loss prevention, data classification, SIEM a plus
Scripting and program automation experience
Security and IT metrics experience a plus; report creation abilities strongly desired
Exceptional ability to execute and drive change while never losing site of the basics
Strategic, creative, and innovative mind
Zero tolerance for operational, design, and strategy oriented gaps
Absolute self-starter who will take the lead and initiative to find and solution problems
The typical base pay range for this role is between $90K - $115K depending on job-related knowledge, skills, experience and location. This role may also be eligible for certain discretionary performance-based bonus and/or incentive compensation. Additionally, our Total Rewards program provides colleagues with a competitive benefits package (in accordance with the eligibility requirements and respective terms of each) that includes comprehensive health and wellness benefits, retirement plans, educational assistance and training programs, income replacement for qualified employees with disabilities, paid maternity and parental bonding leave, and paid vacation, sick days, and holidays. For more information on our Total Rewards package, please click the link below.
MUFG Benefits Summary
The above statements are intended to describe the general nature and level of work being performed. They are not intended to be construed as an exhaustive list of all responsibilities duties and skills required of personnel so classified.
We are proud to be an Equal Opportunity/Affirmative Action Employer and committed to leveraging the diverse backgrounds, perspectives and experience of our workforce to create opportunities for our colleagues and our business. We do not discriminate on the basis of race, color, national origin, religion, gender expression, gender identity, sex, age, ancestry, marital status, protected veteran and military status, disability, medical condition, sexual orientation, genetic information, or any other status of an individual or that individual's associates or relatives that is protected under applicable federal, state, or local law.
#LI-Hybrid","$102,500 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Banking & Lending,1864,$10+ billion (USD)
Provista Software Corporation,#N/A,Remote,Data Engineer,"As a successful candidate, you’ll play a pivotal role in the data conversion and business intelligence solution efforts during project implementation for our state and local clients. We are looking for you to join our software implementation team as a Senior Data Engineer.
Qualification:
5+ years of experience working independently as ETL developer on data migration projects that covered diverse database platforms and complex data conversions.
Must have experience across leading Database Platforms especially Oracle, Azure SQL, Snowflake.
Expert level experience with Extraction Transform Load (ETL) plan, protocols, and tools.
Expert level experience with programming languages SQL, Phyton, Java, C#.
Good at communication with clients and internally.
Expert at problem solving and troubleshooting skills.
Bachelor's degree, preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience
Must be a U.S. Citizen or Green Card Holder.
Functional Responsibility:
Data Conversion -
In collaboration with Data Stewards, discover inventory of legacy data sources and all data the client expects to be migrated to destination Modified Off the Shelf (MOTS) solution.
Complete gap assessment between legacy data and MOTS data requirements, and options to address the gaps.
Develop data migration map to minimally include data dictionary, detailed data map of all elements of the current database, data transformation rules, relationship rules, validation rules, process of migrating images, and process for recovering missing or erroneous data.
Identify issues, risks, and/or barriers that may interfere with data migration and recommendations to resolve or mitigate.
Conduct testing and quality assurance to ensure the data is migrated accurately.
Migrate data from legacy to destination MOTS solution progressively using a sample set of data and the entirety of the data prior to deployment into relevant go-live environments (e.g., staging, production, etc.)
Prepare data migration reports that includes – record counts of source vs migrated vs modified; field level validation of data at row and column; discovered exceptions.
Business Intelligence -
Develop ETL pipelines in and out of Snowflake data warehouse using combination of Python, SnowSQL, and SQL.
In collaboration with Data Stewards, create Snowflake data mart as curated subset of data to support the analytics and business intelligence needs of subgroup of users.
Translate requirements for business intelligence and reporting to database design and reporting design.
Conduct discovery sessions with users to capture business intelligence reports.
Develop Tableau self-service ad hoc reports to support operational users.
Develop Tableau Dashboards to support executive, management, and operational users.
About us:
Provista Software Corporation is fast growing Software Solution provider for US State and Local Government Clients.
Job Type: Full-time
Pay: $67,107.98 - $134,285.66 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Life insurance
Paid time off
Parental leave
Professional development assistance
Vision insurance
Compensation package:
Yearly pay
Experience level:
5 years
Schedule:
8 hour shift
Application Question(s):
What is your most preferred work time zone ?
Experience:
ETL: 5 years (Required)
Data warehouse: 1 year (Preferred)
Tableau: 1 year (Preferred)
Talend: 1 year (Preferred)
Microsoft SQL Server: 5 years (Required)
Work Location: Remote","$100,697 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Codinix Technologies
5.0",5.0,"Boston, MA",Cloud Data Engineer - AWS,"Only US Citizen
Strong in warehouse technologies (Hadoop, Spark, AWS, etc.)
Bachelor's degree in Computer Science, Information Systems, or another related field
5+ years of data engineering, and cloud engineering experience
Experience in Agile project methodologies and DevOps
Experience with AWS (certification preferred; including Athena, Redshift, Glue), Cloudera
Experience building a CI/CD pipeline to deploy applications on AWS (GitLab, Terraform)
Advanced experience programming and scripting on Unix/Linux (i.e. Python, Bash)
Experience with CTRL-M, Cron (other) scheduling of batch jobs
Experience migrating legacy systems to cloud-ready architectures
Job Type: Contract
Pay: Up to $75.00 per hour
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Boston, MA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",$75.00 /hr (est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Wallero
4.4",4.4,"Seattle, WA",Data Engineer,"Hi,
Pretty much interested in your resume, we have an urgent opening for Data Engineer role with one of our clients based in Seattle WA.
Position: Data Engineer
Location: Seattle WA - Remote (100%)
Description:
Good Experience in reporting space including PowerBI, ADF, and possibly standing up a data warehouse/data mart.
The main scope centering is around reporting.
Experience with SQL, and agile framework including requirement gathering and story/feature writing skills.
Need someone strong who is comfortable leading and guiding teams in that capacity.
Migrating data from third party to Azure Cloud
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Seattle, WA 98101: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",$75.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2015,$1 to $5 million (USD)
"MetroStar
3.7",3.7,United States,Data Engineer (Mid),"As a Data Engineer (Mid), you'll bring creative architect solutions to end customers with the goal to make an impact across the federal government.
We know that you can't have great technology services without amazing people. At MetroStar, we are obsessed with our people and have led a two-decade legacy of building the best and brightest teams. Because we know our future relies on our deep understanding and relentless focus on our people, we live by our mission: A passion for our people. Value for our customers.
If you think you can see yourself delivering our mission and pursuing our goals with us, then check out the job description below!
What you'll do:
Work with AI team members to operationalize data pipelines and ML tasks.
Provide day-to-day support of deploying Python-native ML pipelines and perform data engineering tasks to enable AI/ML capabilities.
Present results to a diverse audience in presentation or report form.
Support architectural leadership, technical support, and advisement services to ensure identity management system technologies are integrated and meeting the appropriate security requirements.
Support leadership who engage with senior level executives at a public facing Federal agency and provide subject matter expertise in security architecture and other key domain areas.
What you'll need to succeed:
5+ years of experience in Data/ML engineering (if school experience is used, at most that would contribute to 2 years of actual experience).
Experience with ETL, Data Labeling and Data Prep.
Experience designing, implementing, and maintaining data architecture and services to be used for AI/ML. Additionally, operationalizing and maintaining AI/ML models in production.
The ability to perform data analytics on program related or system related activities. This will include assessing performance and manual processes implementing methods/algorithms to automate/optimize
A bachelor's degree in Computer Science, Information Technology Management or Engineering, or other comparable degree or experience and the ability to obtain and maintain DHS Suitability.

Like we said, we are obsessed with our people. That's why we offer a generous benefits package, professional growth, and valuable time to recharge. Learn more about our company culture code and benefits. Plus, check out our accolades.
Don't meet every single requirement?
Studies have shown that women, people of color and the LGBTQ+ community are less likely to apply to jobs unless they meet every single qualification. At MetroStar we are dedicated to building a diverse, inclusive, and authentic culture, so, if you're excited about this role, but your previous experience doesn't align perfectly with every qualification in the job description, we encourage you to go ahead and apply. We pride ourselves on making great matches, and you may be the perfect match for this role or another one we have. Best of luck! – The MetroStar People & Culture Team
What we want you to know:
In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification form upon hire.
MetroStar Systems is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. The statements herein are intended to describe the general nature and level of work being performed by employees and are not to be construed as an exhaustive list of responsibilities, duties, and skills required of personnel so classified. Furthermore, they do not establish a contract for employment and are subject to change at the discretion of MetroStar Systems.
Not ready to apply now?
Sign up to join our newsletter here.",#N/A,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1999,$25 to $100 million (USD)
"Maven Workforce
4.1",4.1,"McLean, VA",Data Engineer,"MUST HAVES: Hands-on experience with Informatica PowerCenter and/or Informatica Intelligent Cloud Services (IICS), Cloud Data Integration (CDI) , Cloud AWS,UNIX scripting, and any other ETL tools.
Job responsibilities: –
The candidates will be having the IICS module of Informatica more than any other platform modules.
Strong hands-on experience building ETL pipelines and Data Replication
3+ yrs of experience with IICS is required.
Must be hand Ons with AWS services like ECS,S3,EKS etc.
Cloud Data Integration is also required.
Data Engineering Background experience with Spark, Hive is also required.
Preferred:
SQL is preferred, but basic knowledge is required
Job Type: Contract
Salary: $60.00 per hour
Ability to commute/relocate:
Mclean, VA 22101: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica PowerCenter: 7 years (Required)
Informatica Intelligent Cloud Services (IICS): 7 years (Required)
Cloud Data Integration (CDI): 7 years (Required)
UNIX scripting: 7 years (Required)
ETL tools: 7 years (Required)
Work Location: One location
Speak with the employer
+91 7328456015",$60.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2008,Unknown / Non-Applicable
Zenotis Technologies inc,#N/A,"Addison, TX",Big Data Engineer,"Role: Big Data Engineer
Location – Addison, TX/ Charlotte, NC / Chandler, AZ (Hybrid – 3 days onsite 2 days remote)
Position Summary:
Looking for a big data engineer who loves solving complex problems across a full spectrum of technologies. The ideal candidate is excited by experimentation and looking for a new challenge that stretches their talents. The big data engineer will help ensure that our technological infrastructure operates seamlessly in support of business objectives.
The engineer must be able to:
· Translate complex technical and functional requirements into detailed designs
· Guide the development and testing teams in developing and implement pipelines that extract, transform, and load data into an information product that helps the organization reach its strategic goals
· Focus on ingesting, storing, processing, and analyzing large datasets
· Deep understanding on how to create scalable, high-performance web services for tracking data
· Investigate alternatives for data storing and processing to ensure the implementation of the most streamlined solutions
Primary Skill
Hadoop
Secondary Skill
Kafka
Tertiary Skill
Oracle Exadata
Required Skills
· Translate complex technical and functional requirements into detailed designs
· Design data pipelines using design tools and provide solution intent in provided templates
· Work closely with data science team to implement data analytics pipelines
· Analyze vast number of data stores to uncover insights
· Define data governance policies and support data-versioning processes
· Maintain security and data privacy, working closely with data protection guidelines.
· Desired Skills
· Experience with Python, Spark, and Hive
· Understanding of data-warehousing and data-modeling techniques
· Knowledge of industry-wide visualization and analytics tools (ex: Tableau, R)
· Strong data engineering skills with Hadoop platform
· Experience with streaming frameworks such as Kafka
· Knowledge of Core Java, Linux, SQL, and any scripting language
· Good interpersonal skills and positive attitude
Job Type: Full-time
Salary: Up to $70.00 per hour
Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
8 hour shift
Work Location: On the road",$70.00 /hr (est.),201 to 500 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
BIGCLFY,#N/A,"Dallas, TX","W2/ 1099, No C2C - SSIS ETL Data Engineer (Minneapolis, MN/ Dallas-Irving)","No C2C. It's W2/1099 Requirement
Only taking USC, GC or H4 EAD, H1 Transfer, L2 Visa or TN Visa - they will not take OPT EAD or CPT
Need to have confirmation in writing that you will be onsite by Day 1
ALL ROLES ARE HYBRID IN THE OFFICE (2-3 DAYS/WEEK) UNLESS REMOTE IS NOTED
Please submit qualified candidates and include FULL LEGAL NAME as it appears on the passport
If your candidate is relocating, need to have confirmation in writing that they will be onsite by Day 1
Senior SSIS/ETL Data Engineer
12 months – 3 days in office
Minneapolis, MN or Dallas-Irving, TX
5+ years of experience with SQL Server 2016 (or newer) and SSIS
7+ years of data engineering
Must be capable of leading teams
2+ years of experience working with large scale SQL implementations
Experience designing, developing, and maintaining complex stored procedures, functions, and views using T-SQL
Experience designing, developing, and maintaining ETL processes using SSIS
Should understand how to unit test their product
Hands on experience with Visual Studio to document
Experience in financial or regulatory environments highly preferred
Ideal candidates will have experience creating conceptual, logical, and physical database designs
Preference given to candidates with extensive knowledge and experience with performance monitoring, database and SQL tuning
Bachelor’s or Master's in computer science or related field or equivalent work experience
Job Type: Contract
Pay: From $73.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Chandler, AZ 85225: Reliably commute or planning to relocate before starting work (Preferred)
Application Question(s):
Expected Payrate on W2/1099 Tax Term? Please mention. Thanks.
Experience:
Risk Analytics: 5 years (Preferred)
Work Location: In person",$73.00 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Synovize
5.0",5.0,Remote,Big Data Engineer,"Synovize is a cutting-edge technology company that specializes in providing advanced data analytics and intelligence solutions. Our mission is to empower businesses with the tools and insights they need to make data-driven decisions and drive innovation. We are committed to delivering exceptional results by combining our expertise in big data technologies with our passion for creating impactful solutions.
As a Big Data Engineer at Synovize, you will have the opportunity to work on exciting projects that involve designing and implementing scalable data solutions. You will collaborate with a talented team of professionals, including software developers, data scientists, and domain experts, to tackle complex data engineering challenges and deliver high-quality solutions to our clients.
Job Title:
Big Data Engineer
Company: Synovize
Number of Positions: 2
Location: Remote
Length: 12 Months
Tax Terms: 1099/C2C or W-2
Rate/Range: $70-$90/hr
Work Authorization: US Citizen
Security Clearance: None
Skills Needed:
Proficiency in Java and Kotlin programming languages.
Strong knowledge of AWS (Amazon Web Services) and its big data offerings.
Experience with BigData technologies, frameworks, and tools.
Familiarity with Kafka for streaming data processing.
Proficiency in Python programming language.
Responsibilities:
Design and develop scalable data pipelines to process, integrate, and analyze large volumes of structured and unstructured data.
Implement data ingestion, transformation, and storage mechanisms using big data technologies and frameworks.
Collaborate with data scientists and analysts to understand data requirements and optimize data models for analytics and machine learning.
Utilize AWS services and tools to build and maintain data infrastructure, ensuring data quality, integrity, and security.
Troubleshoot and resolve data-related issues, optimizing performance and efficiency.
Stay up-to-date with emerging trends and advancements in big data technologies, recommending and implementing improvements to existing systems and processes.
Requirements:
Bachelor's or master's degree in computer science, engineering, or a related field.
Proven experience as a Big Data Engineer or similar role, with a strong understanding of big data technologies and architectures.
Proficiency in Java and Kotlin programming languages.
Strong knowledge of AWS and its big data offerings.
Experience with BigData technologies, frameworks, and tools.
Familiarity with Kafka for streaming data processing.
Proficiency in Python programming language.
Strong problem-solving skills and ability to work in a fast-paced, collaborative environment.
Excellent communication and interpersonal skills.
Join Synovize and be part of a dynamic team that is at the forefront of data innovation. Together, we will drive meaningful change and unlock the full potential of data-driven decision-making.
Job Types: Full-time, Contract
Pay: $70.00 - $90.00 per hour
Benefits:
401(k)
Dental insurance
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Vision insurance
Schedule:
10 hour shift
4 hour shift
8 hour shift
Choose your own hours
Application Question(s):
Are you a US Citizen?
Years of experience in Java?
Years of experience in Kotlin?
Years of experience in big data?
Years of experience in AWS?
Security Clearance
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$80.00 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"WorkCog
4.3",4.3,"Atlanta, GA",Sr. AWS Data Engineer,"Position: AWS Data Engineer
Location: Atlanta ,GA
Duration: 12+ Months
Interview: Video
JD
Responsibilities
Bachelor’s degree in Computer Science, Software Engineering, or other related field, or equivalent work experience.
Extensive experience in data modeling, writing & performance tuning complex SQL queries & user-defined functions
Hands-on experience in design, implementation and orchestration of data pipelines and ETL batch Jobs.
Hands-on experience on all aspects of data handling: cleansing and standardization, normalization, merging, change data capture, data integration, reconciliation, and transformations
Strong experience in working with Big Data technologies such as Hive/Spark, Data lake and Data warehouse using GCP and/or AWS Services
Ability to write robust code in scripting languages like Python, Scala, PySpark or similar.
Good to Have
Associate Architect or equivalent cloud certification
Experience with processing large datasets using AWS and GCP UDFs (User defined functions)
Experience in ML probability & statistics algorithms, frameworks like Keras or PyTorch and tools like Jupyter notebooks.
Job Type: Contract
Pay: $45.83 - $52.42 per hour
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Atlanta, GA: Reliably commute or planning to relocate before starting work (Preferred)
Experience:
SQL (Preferred)
Informatica (Preferred)
Data warehouse (Preferred)
Work Location: In person",$49.13 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable
"AbleTo
3.7",3.7,"New York, NY",Data Engineer,"About AbleTo
Join our mission-driven organization, where your work matters and a diversity of ideas and backgrounds are welcomed. AbleTo is the leading provider of high quality, technology-enabled behavioral health care. We believe that everyone deserves access to high-quality care and offer a suite of technology-enabled services to empower people to lead better lives through better mental health. A proprietary platform connects individuals with AbleTo highly trained licensed providers who deliver weekly sessions by phone or video supported by an integrated digital experience. Members also have access to mental health coaches, and digital support programs. Our outcomes-focused approach is proven to improve both behavioral and physical health and lower medical costs.

Overview
Data is integral to nearly everything that we do at AbleTo, and as a result the data team plays a key role at our company. As a Data Engineer at AbleTo, you will grow and develop your skills as a data engineer to build a world class data platform to support our mental health care mission.

What You Will Do
Build and maintain batch and real-time ETL pipelines in a Google Cloud Platform architecture (BigQuery, Dataproc, Firestore, Informatica, Python, etc.)
Identify code quality issues and implement tests to improve future processes.
Document orchestration work-flows and create run books
Implement data integrity tests to ensure we are ingesting accurate data.
Implement high-quality test-driven code.
Participate in daily team stand ups and other Agile ceremonies

Key Initiatives over the next 12 months
Data Mart client migration
Data Mart enhancements
Automation of manual day to day operational tasks
Decommission legacy pipelines

Who You Are:
1+ years of experience coding in Python.
Experience working with SQL (structured query language).
Familiarity with structuring and writing ETLs.
Experience working with Orchestration tools (Airflow) and Cloud Databases (BigQuery) is a plus.
Experience working in an Agile Development environment is a plus.
BS in Information Systems, Computer Science or related field is preferred.
You are excited to work with data!
Curiosity to dive deeply into issues and feel empowered to make a meaningful impact at a mission-driven company.
Committed to agile development and value delivery and solid engineering principles, as well as continuously improving our craft.
You are a collaborator. You build and maintain strong, productive working relationships with your customers and stakeholders.
You are empathetic and seek to understand each individual's diversity of background and experience contributes to making up a great team.
You drive to improve yourself and others.
You are excited about learning new languages, tools, techniques and technologies.

Why You Should Join Our Team:
We're proud to be a Great Place to Work-Certified™ company. We want you to show up and feel your best at work, and that means respecting your time outside of work. Our inclusive, flexible workspace prioritizes a work/life balance. We offer competitive salaries, comprehensive health benefits (for full-time employees), and professional perks such as 401K matching, fully funded HRA, and generous time off, including mental health days because your well-being is important to us.
At AbleTo, we're empowering people to get better and stay better. Want to join us? Take the next step in your career by applying for this role today.
The salary range is $40,000 to $90,000. Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, AbleTo offers benefits such as a comprehensive benefits package, incentive and recognition programs, and 401k contribution (all benefits are subject to eligibility requirements).

#LI-Remote
Follow AbleTo on LinkedIn, Twitter, and Instagram!
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.
AbleTo is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or veteran status. AbleTo is an E-Verify company.","$65,000 /yr (est.)",501 to 1000 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2008,$25 to $100 million (USD)
"Unilever
4.1",4.1,Remote,Health & Wellbeing Data Engineer,"Health & Well Data Engineer
Remote – USA; Los Angeles
Health & Wellness is a strategic Unilever global business unit established to capture the growth opportunity in the €140bn global consumer health segment defined by Vitamins, Minerals, and Supplements (VMS) product category. Our ambition is to build a €5bn business globally; and be a global top 3 player within this space.
As of today, our brands include Equilibra in Italy; OLLY, SmartyPants, Liquid IV, Onnit, Welly and Nutrafol in the US with ~€1.4bn in turnover. The H&W data and analytics team operates across all these brands in one common data function with the aim of helping these brands to grow through smarter, faster, and better data driven decision making. The data team within H&W was set up in May 2022 and is looking to expand. This is a fantastic opportunity to shape the future of data in a global business unit with the latest technologies in the cloud.
What you will do:
collaborate with other engineering and business teams within H&W and Unilever to solve complex challenges using data.
drive forward best practice by building out data frameworks and design patterns to be utilized across the H&W data infrastructure.
build production ready distributed ETL data pipelines from a wide range of different sources (APIs, flat files, databases, ERP systems etc.)
build performant and reliable data models to democratize the use of data across H&W
build integrations to facilitate the use of other technologies in H&W ecosystem (e.g Kinnaxis, Anaplan)
review and deploy code from other team members as part of a DevOps process, providing coaching and mentoring to junior developers.
manage access within the environments to ensure data security protocols are being met.
contribute to architectural and governance decision marking within the H&W data ecosystem.
Who you are:
Passionate about all things data
Entrepreneurial Self-starter with the ability to thrive in a fast paced start up environment.
Fast learner with the ability to pick up new technologies quickly.
Creative problem solver who thinks outside the box
What you will bring:
3-5 years’ Data engineering experience with strong pyspark and SQL skills
1+ year Distributed computing experience [databricks/Splunk]
1+ year experience working with data in cloud environments [GCP/Azure/AWS]
Bachelor’s degree required.
Experience with streaming workloads is a plus
Experience working within a delta lakehouse in databricks is a plus
Tech Stack:
Cloud agnostic: Databricks (pyspark, scala, SQL)
Azure: Azure Data Factory, Azure Logic Apps, Azure Data Lake Storage (ADLS), Azure Blob Storage, Azure Machine Learning, PBI (data modelling, DAX)
GCP: Google Big Query, Google Cloud Storage
AWS: S3
Pay: The pay range for this position is $83,200 - $124,700. Unilever takes into consideration a wide range of factors that are utilized in making compensation decisions including, but not limited to, skill sets, experience and training, licensure and certifications, qualifications and education, and other business and organizational needs. Bonus: This position is bonus eligible. Long-Term Incentive (LTI): This position is LTI eligible. Benefits: Unilever employees are eligible to participate in our benefits plan. Should the employee choose to participate, they can choose from a range of benefits to include, but is not limited to, health insurance (including prescription drug, dental, and vision coverage), retirement savings benefits, life insurance and disability benefits, parental leave, sick leave, paid vacation, and holidays, as well as access to numerous voluntary benefits. Any coverages for health insurance and retirement benefits will be in accordance with the terms and conditions of the applicable plans and associated governing plan documents.
-
Unilever is an organization committed to diversity and inclusion to drive our business results and create a better future every day for our diverse employees, global consumers, partners, and communities. We believe a diverse workforce allows us to match our growth ambitions and drive inclusion across the business. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, age, national origin, or protected veteran status and will not be discriminated against on the basis of disability.

Employment is subject to verification of pre-screening tests, which may include drug screening, background check, credit check and DMV check.

If you are an individual with a disability in need of assistance at any time during our recruitment process, please contact us at
NA.Accommodations@unilever.com
. Please note: This email is reserved for individuals with disabilities in need of assistance and is not a means of inquiry about positions or application statuses.
#LI-Remote","$103,950 /yr (est.)",10000+ Employees,Company - Public,Manufacturing,Consumer Product Manufacturing,1872,$10+ billion (USD)
IT Customer Service - POST,#N/A,"Newark, NJ",Data Center Plant Engineer,"Job Title: ISP-OSP Data Center Plant Engineer
Location: Newark NJ, (M-F)
Travel Required: As Needed
Overview
UC-ISP/OSP engineer is responsible for cable management including structured cabling, carrier data and voice circuit cables/equipment, rack-and-stack, and oversight of power, backup power, network equipment, and environmental needs to cabinets. Leveraging Clients Data Center Infrastructure Management System, (DCIM) tool to maintain, report and monitor.
Conducting capacity assessments of existing infrastructure to ensure that it can support future growth to ensure compliance with federal regulations of the Data Center infrastructure Standards. Projects include design and build outs of the physical plant within the typical WAN/LAN space within data centers, telco rooms, MDFs and IDFs as needed.
Functions:
Perform initial engineering surveys based on project needs.
Document engineering surveys to determine feasibility and the estimated cost.
Obtain authorization to proceed and confirm funding.
Create detailed engineering drawings of build, floor plans, equipment elevation drawings, structured cabling diagrams, power, and environmental needs.
Designs must conform to state, local, national, and Client standards and codes, and the NJ historical and or environmental commissions approvals if needed.
Based on the engineering documents create a SOW (Scope of Work) and BOM (Bill of Materials)
Submit to Client manager to approve and proceed when funding is validated.
Create a construction package and submit to contractors or in house for RFQ.
Review quotes and choose both low bid and qualified contractor.
Work with all stakeholders to date and time work schedules.
Confirm workers are safety trained and conform to Client, FRA, and OSHA requirements.
Supervise and manage installations and conduct inspections.
Review work to ensure contractor met obligations under terms of contract.
Inform Client managers of status to approve invoices.
Coordinate with Client network engineering to connect, configure, and test equipment.
Review and confirm a successful installation were completed and working as designed prior to cutting over to production.
Assist with Break fix of IT infrastructure troubles or repairs.
Daily travel to location depending on scheduled or unscheduled work
Job Types: Full-time, Contract
Experience level:
5 years
Schedule:
8 hour shift
Work setting:
In-person
Experience:
Data Center: 5 years (Required)
WAN/LAN: 3 years (Required)
cabling: 3 years (Required)
Work Location: One location",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Breadboard,#N/A,"New York, NY",Data Engineer,"About Us
We’re on a mission to revolutionize the global electronics supply chain.
During the pandemic we saw how the inefficiencies in electronics manufacturing can have massive global impact. From over $200B lost in car manufacturing to delays on laptops, gaming systems and much more.
At Breadboard we are rethinking the electronics supply chain down to the finest detail. We’re designing the new operating system in which this entire system will run. We’ve already secured seed funding and are now looking to expand our engineering team to help us set the foundation for the future.
Why work with us?
As a part of our team, you will have the unique opportunity to design and implement the data platform for Breadboard, a critical component in our business.
As one of the first data engineers to join the Breadboard team you will be partnering with the CTO, our founding engineers, and will set the foundation for the design of the operating system for the electronics supply chain.
You will be involved in all aspects of the software development process, architectural decisions, recruiting, and building the organization in general.
We’re a tight-knit, intense team and you will have a ton of end to end ownership.
Work with seasoned founders who have had successful exits.
This is not only a unique opportunity for someone who wants to get their hands dirty, but also take that step back and collaborate with the executive team on setting a strategic future.
You’ll be responsible for
Data Management: Develop and automate robust data acquisition, processing, and normalization pipelines from various sources to ensure data quality and consistency.
Database and ETL Architecture: Design, implement, and optimize a database system for efficient data storage, retrieval, and analysis.
Collaboration on Machine Learning: The prospects of working with data scientists to leverage extracted data features, contributing to the development and refinement of a machine learning model for component matching.
API and UI Integration: Collaborate with the software development team to facilitate seamless integration between the database, machine learning models, and user interface.
Continuous Improvement and Compliance: Monitor system performance and user feedback for continuous improvements, while ensuring adherence to data privacy and security standards.

Requirements
Experience: A minimum of 3-5 years of experience in a data engineering role, preferably in the electronics, manufacturing industry or supply chain industry.
Technical Skills: Proficiency in SQL and experience with database design, data modeling, and performance optimization. Strong coding skills in a major programming language, such as Python or TypeScript. Experience with data processing tools and libraries (e.g., pandas, Spark). Experience with pipeline and workflow orchestration tools, such Airflow. Experience with Snowflake or similar cloud-based data warehousing solutions.
Data Acquisition and Processing: Experience with data acquisition methods, such as APIs and web scraping. Familiarity with data cleaning, normalization, and feature extraction techniques.
Machine Learning: Familiarity with machine learning concepts and algorithms, and experience collaborating with data scientists or machine learning engineers.
API Development: Experience with API development and integration, preferably in a microservices architecture.
Data Governance: Knowledge of data privacy and security regulations, and experience implementing data governance measures.
Bonus Points
Entrepreneurial mindset (we encourage all employees to be future founders and this can be a great stepping stone towards that goal).
Customer-centric and passionate about helping small businesses grow.
Previous experience at a high-growth, fast-paced startup.
Experience with our stack (and some others that may be included shortly):
React, Material UI, Typescript, CSS
NestJS, GraphQL, TypeORM, Knex.js, Objection.js, PostgreSQL, gRPC, Kafka
AWS, Docker, Kubernetes, Helm, Github Actions, CI/CD pipelines.
Compensation + Benefits
Competitive cash compensation
Stock ownership at a fast growing company
Health/dental/vision insurance—100% coverage option
The ability to push for other benefits you think are important
Our Engineering Culture
Openness to diverse opinions and backgrounds - Testing our assumptions
Supported career growth with a high emphasis on learning and mentorship
Lean development
End to end ownership and accountability. We’re all going to make mistakes. It’s about how you own up to them and learn from them.

Equal Opportunity Employer
Breadboard Software does not discriminate in employment on the basis of race, color, religion, sex (including pregnancy and gender identity), national origin, political affiliation, sexual orientation, marital status, disability, genetic information, age, membership in an employee organization, retaliation, parental status, military service, or other non-merit factor.",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Ripple Effect Consulting
5.0",5.0,"Denver, CO",Data Engineer - 004/005,"Data Engineer
Hybrid-remote based in Denver, CO | $80-$160k
Job Overview:
We are looking for a savvy Data Engineer to join our growing team of analytics experts. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
Responsibilities for Data Engineer
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Qualifications for Data Engineer
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:
1) Experience with big data tools
2) Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
3) Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
4) Experience with cloud services
5) Experience with stream-processing systems: Storm, Spark-Streaming, etc.
6) Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Job Type: Full-time
Pay: $80,000.00 - $160,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Denver, CO: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person","$120,000 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Spartan Technologies
3.4",3.4,"Kansas City, MO",Data Engineer I,"Spartan Technologies, Inc. - Kansas City, MO
Applicants Need to Know
100% remote in East Coast or Central US only
No Sponsorship
Contract to Hire
Hourly Rate: $45 on W2
Data Engineer I
The Data Engineer is responsible for processing structured and unstructured data, validating data quality, and developing and supporting data products for analytics. Brings together data from different sources into a common dataset with common KPIs and dimensions for stakeholders. Understands the analytics challenges that the marketing organization faces in their day-to-day work and partners with them to design viable data solutions.
Your Job
Gathers and processes raw, structured, semi-structured, and unstructured data using batch and real-time data processing frameworks.
Understand the existing source system data models and perform end to end data validation to maintain accuracy.
Implements and optimizes data solutions in enterprise data warehouses and big data repositories.
Ensures data quality and implements tools and frameworks for automating the identification of data quality issues.
Develop re-useable processes that can be leveraged and standardized for multiple instances
Works with internal and external data providers on data validation providing feedback and making customized changes to data feeds and data mappings.
Recommends improvements to processes, technology, and interfaces that improve the effectiveness of the team and reduce technical debt.
Ensure performance by monitoring existing workflows to ensure they are working correctly and that they reconcile with the source.
Design and develop ETL workflows and datasets to be used in extracts by our BI reporting tool Tableau.
Will write complex SQL queries with multiple joins to automate/manipulate these reporting extracts
Works in partnership with marketing stakeholders concerning the accuracy of data and efficiency of processes
Qualifications
Working experience with data modelling, data access, schemas, and data storage techniques within Snowflake
Working experience with batch and real-time data processing frameworks
Working experience working with relational databases such as SQL
Working experience with business intelligence tools and platforms
Working experience with data quality tools
Experience with ETL processes and tools
Desirable:
Working experience with Snowflake
Working experience with Google Analytics BigQuery
Working Experience with Tableau Server
Working experience with SQL Server Integration Services",$45.00 /hr (est.),1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$5 to $25 million (USD)
"Southern Glazer’s Wine and Spirits
3.6",3.6,"Dallas, TX",Data Engineer,"What You Need To Know
Open the door to a groundbreaking tech career with an industry leader. Southern Glazer’s Wine & Spirits is North America’s preeminent wine and spirits distributor, as well as a family-owned, privately held company with a 50+ year legacy of success. To create a new era in alcohol beverage sales and service, we’re heavily invested in the most transformative new technologies – and the most brilliant tech professionals. Southern Glazer’s was named by Newsweek as a Most Loved Workplace and is included on the Forbes lists for Largest Private Companies and Best Employers for Diversity.
As a full-time employee, you can choose from a full menu of our Top Shelf Benefits, including comprehensive medical and prescription drug coverage, dental and vision plans, tax-saving Flexible Spending Accounts, disability coverage, life insurance plans, and a 401(k) plan. We also offer tuition reimbursement, a wellness program, parental leave, vacation accrual, paid sick leave, and more.
We offer continuous learning and career growth in a fast-paced environment where you are respected, your voice is heard, and technology is part of our strategy for success. If you’re looking to fill your glass with opportunity, come join our FAMILY.
Overview
The Data Engineer's role is to design, develop, maintain and enhance interfaces and connectivity to the Data Warehouse ecosystem by coding with a technical language to meet business requirements and business objectives. This can include taking technical specifications and developing an application or integration of data between applications, testing, as well as, completing the appropriate technical documentation. The Data Engineer will use best practices in software development and adhere to SGWS development standards, as well as, focus on quality and innovation. The Data Engineer may also be responsible for delivering support to end users in the organization for specific code, including troubleshooting code.
Specialized Skills and Technologies
Strong PL/SQL skills
Experience in ETL Tools (Preferrable Informatica)
Data Warehouse techniques will be a plus
Experience in cloud platforms like Azure or AWS will be a plus
Knowledge of UNIX/Linux, shell scripting, Python will be a plus
Experience developing Application Programming Interfaces (API's) will be a plus
Experience in Hadoop will be a plus
Primary Responsibilities
Design, develop, implement, and support software applications
Drive technical validity of solution.
Develop user documentation as well as in-code documentation to explain designs and participate/support user training
Structure requirements to facilitate automation of acceptance tests
In conjunction with Data Management Group, develop routines and procedures that provide data quality checks and balances on data delivery/ingestion
Collaborate across the BI / Analytics, Data Management Group, Enterprise Insights and Analytics teams to establish standards, reusable data models and best practices for delivery/ingestion of data from/to Data Warehouse - This includes Publish/Subscription and API options
Obtain any certifications needed to effectively support applications in scope
Support the development of business and technical process documentation and training materials
Structure requirements to facilitate automation of acceptance tests
Provide support for software applications under area of responsibility
Drive Behavior-Driven-Design (BDD) process
Perform other job-related duties as assigned
Minimum Qualifications
Bachelor’s Degree or a combination of work experience and education
Knowledge in application and software development
Knowledge of software design and programming principles
Proficient oral and written communication skills, ability to influence outcomes, and strong attention to detail
Strong analytical, mathematic, and problem-solving skills
Strong team player with ability to demonstrate Agile delivery values working both within a team and working independently
Strategic thinker – can develop a plan to meet a long-term objective
Agile Delivery Values
Openness – Team and stakeholders agree to be open about all work and challenges
Commitment – Personally commit to achieving the goals of the team
Respect – Respect your team members to be capable and independent
Courage – You have courage to do the right thing and work on tough problems
Focus – Everyone focus on the work in the sprint and the goal of the scrum team. Rise and fall as a team
Physical Demands
Physical demands include a considerable amount of time sitting and typing/keyboarding, using a computer (e.g., keyboard, mouse, and monitor), or mobile device
Physical demands with activity or condition may occasionally include walking, bending, reaching, standing, squatting, and stooping
May require occasional lifting/lowering, pushing, carrying, or pulling up to 20lbs
EEO Statement
Southern Glazer's Wine and Spirits, an Affirmative Action/EEO employer, prohibits discrimination and harassment of any type and provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. Southern Glazer's Wine and Spirits provides competitive compensation based on estimated performance level consistent with the past relevant experience, knowledge, skills, abilities and education of employees. Unless otherwise expressly stated, any pay ranges posted here are estimates from outside of Southern Glazer's Wine and Spirits and do not reflect Southern Glazer's pay bands or ranges.","$97,837 /yr (est.)",10000+ Employees,Company - Private,Retail & Wholesale,Wholesale,1968,$10+ billion (USD)
"Kaizen Analytix
3.9",3.9,"Dallas, TX",Cloud Data Engineer: AWS,"Cloud Data Engineer: AWS
Kaizen Analytix LLC, an analytics services company, is seeking a qualified Cloud Data Engineer Contractor to support data science or data analytics projects. Responsibilities, capabilities for success, and job requirements are below. This role requires an estimated 40 hours/week commitment for approximately 24 weeks.
Responsibilities:
Candidate will be responsible for supporting a data science or data analytics project.
Work with client IT to understand access and use of source-of-truth data systems and downstream data sources.
Work with data scientist or visualization experts to determine data requirements such as structure, latency, availability.
Work with IT Security to assess risk of using sensitive data and how these data can be used throughout the project life cycle.
Develop an as-is and to-be state for data architecture.
Capabilities for Success:
In addition to key responsibilities, Candidate should demonstrate an ability to…
Work with the project manager and the business stakeholders to define upstream requirements for data.
Contribute to estimating input and time required for data engineering development tasks.
Contribute to client demonstrations of solution or presentations on architecture.
Proactively raise risks and issues to internal project manager
Work with data scientist or analysts to proactively assist in data concerns
Job Requirements:
Education: Bachelor's degree required
Extensive Experience in Data Engineering – consuming, wrangling, validating, developing pipelines for data.
Extensive Experience with AWS
Must have Solutions Architect Certification
Strong knowledge of databases, ETL concepts, SQL, and NoSQL sources.
Experience in: Python (data packages such as Pandas, PySpark, SQL Alchemy, pydantic, etc.) and SQL.
Position Type: 1099 Contractor",$57.50 /hr (est.),1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,#N/A,Unknown / Non-Applicable
"PrizePicks
4.8",4.8,"Atlanta, GA",Data Engineer,"At PrizePicks, we are the fastest growing sports company in North America, as recognized by Inc. 5000. As the leading platform for Daily Fantasy Sports, we cover a diverse range of sports leagues, including the NFL, NBA, and Esports titles like League of Legends and CS : GO. Our team of over 200 employees thrives in an inclusive culture that values individuals from diverse backgrounds, regardless of their level of sports fandom. Ready to reimagine the DFS industry together?
Our Analytics Team is responsible for building and maintaining analytics tools and workflows to support the PrizePicks business. By developing, maintaining, and testing data generation infrastructures, you will enable the PrizePicks business to make smarter, better, and faster data-driven decisions.
What you'll do:
Create, maintain and orchestrate optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Improve and streamline data systems to drive innovation within Prizepicks
What you have:
3+ years of experience building frameworks for data ingestion pipelines but real time and batch using data modeling, ETL/ELT processes
Stellar SQL skills, experience building DBT pipelines in production and know your way around structured, semi-structured and unstructured data.
Have built and optimized 'big data' data pipelines, architectures and data sets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Experience with Python and other object oriented scripting languages
Cloud experience: AWS, EC2, EMR, RDS, Redshift, GCP, etc..
Not required but would be awesome if you have experience with any of the following data pipeline and workflow tools: Azkaban, Luigi, Airflow, Prefect, etc.
Where you'll live:
Anywhere in the US is fine (we are based in Atlanta, GA)
Benefits you'll receive:
In addition to your great compensation package, company subsidized medical/dental/vision coverage plans and matching 401(k), we'll shower you with perks including:
Break room with ping pong, endless snacks and in-office lunch once a week
Unlimited PTO to encourage a healthy work/life balance (2 week min required!)
Modern work schedule focused on getting the job done, not hours clocked
Workplace flexibility
Company and team outings, we encourage a tight-knit workplace
Generous Maternity AND Paternity leave (16 weeks!)
Annual bonus & stock options
Wellness program
Company equipment provided (Windows & Mac options)
Annual performance reviews with opportunity for growth and career development
#LI-REMOTE

You must be authorized to work for any employer in the U.S. We are unable to sponsor or take over sponsorship or an employment Visa at this time.
PrizePicks is an Equal Opportunity Employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.","$97,814 /yr (est.)",1 to 50 Employees,Company - Private,"Arts, Entertainment & Recreation",Sports & Recreation,2015,Unknown / Non-Applicable
"PRIMUS Global Services, Inc
4.1",4.1,"Minneapolis, MN","Data Engineer – Hadoop, Python – REMOTE WORK 43198","We have an immediate long-term opportunity with one of our key clients for a position of Data Engineer, to work on a remote basis.

The ideal applicant must have extensive experience with Python, Hadoop, Hive and SQL.

**ALL successful candidates for this position are required to work directly for PRIMUS. No agencies please**

For immediate consideration, please contact:

Tejaswini
PRIMUS Global Services
Direct: 972-798-2662
Desk: 972-753-6500 Ext: 204
Email: jobs@primusglobal.com","$91,633 /yr (est.)",501 to 1000 Employees,Private Practice / Firm,Information Technology,Information Technology Support Services,2002,$5 to $25 million (USD)
"Caterpillar
4.0",4.0,"Peoria, IL","Data Engineer, Cat Digital","Career Area:
Digital
Job Description:
Cat Digital is the digital and technology arm of Caterpillar Inc., responsible for bringing world class digital capabilities to our products and services. With almost one million connected assets worldwide, we're focused on using IoT and other data, technology, advanced analytics and AI capabilities to help our customers build a better world.
This is position is in Connected Data Quality team in Cat Digital. The team is responsible for building tools, dashboards and processes to enable (E2E) telemetry data quality monitoring, finding source of quality issues and work with process partners to resolve the problems at source.

Job Duties: As a Data Engineer you will be responsible for building scalable, high performance infrastructure and data driven and predictive analytics applications that provide actionable insights across all Caterpillar businesses. The position will be part of Caterpillar’s fast-moving and engineering-driven digital organization with highly motivated engineers who tackle challenges and problems that are critical to realizing significant business outcomes. Data engineers work with data scientists, business analysts, and others as part of a team that assembles large, complex data sets that provide competitive advantage.
Build infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Design, develop, and maintain performant and scalable applications
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability
Perform debugging, troubleshooting, modifications and unit testing of integration solutions
Operationalize the developed jobs and processes and processes.
Create databases and infrastructure to processing data at scale
Create solutions and methods to monitor systems and solutions
Automate code testing and pipelines
Engage directly with business partners to participate in design and development of data integration/transformation solutions per functional requirements.
Work in a scaled Agile environment accountable to deliver results in sprints.
Engage and actively seek industry perspectives through external engagements such as hackathons, peer groups, etc.
Generate, prepare, and catalog APIs
Work with UI Designer to build user interfaces per design specifications
Employee is also responsible for performing other job duties as assigned by Caterpillar management from time to time.
Required Skills:
BS or MS degree in computer science or computer engineering
5+ years of software development experience or at least 3 year of experience with master’s degree in object-oriented/object function scripting languages: Python, Java, Javascript, C++, Scala, etc.
3+ years of Python coding experience
Understanding of data structures, algorithms, profiling & optimization.
Understanding of SQL, ETL design, and data modeling techniques
Top candidates will also have:
2+ years of experience developing, deploying, and maintaining software in AWS cloud and working with AWS services: S3, DynamoDB, RDS, SageMaker, ECS, EMR, Lambda, Athena, AWS Glue, CloudFormation
2+ years of experience in developing scripts, procedures in snowflake.
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
Experience with using CI/CD tools such as Jenkins, GoCD, Azure Devops etc.
Experience with automated build automation tools (Maven, etc.).
Advanced level of experience with object oriented programming, data structures and algorithms.
Knowledge of enterprise data sources and uses
Working within an Agile framework (ideally Scrum)
#LI
#B
Relocation is available for this position.Visa sponsorship available for eligible applicants.
EEO/AA Employer. All qualified individuals - Including minorities, females, veterans and individuals with disabilities - are encouraged to apply.
Not ready to apply? Submit your information to our Talent Network here .","$86,643 /yr (est.)",10000+ Employees,Company - Public,Manufacturing,Machinery Manufacturing,1925,$10+ billion (USD)
Kanini,#N/A,Remote,Data Engineer,"About Kanini
Kanini provides Agile Software Development, Cloud Computing, Data Science, and Location Intelligence services to public and private organizations. We have successfully served our clients in government, finance, transportation, utility, and software industries since 2003.
Why you should join
Working at Kanini is flexible and personal. We are a highly motivated, collaborative team experimenting with the latest technologies. We are committed to everyone having a healthy work/life balance, and we provide extensive mentorship and training resources to help you succeed.
Kanini is looking for a Big Data Engineer who has a deep experience in Data Engineering, AWS, Python, Data Lakes.
Required Skills
At least 10 years software development experience
At least 5 years leading at least one Scrum team of data engineers building data-intensive products with a modern tech stack.
Significant experience with big data ETL pipeline development with Spark, Hive, and related technologies
Significant experience with a general-purpose programming language such as Python, Scala, or Java
Experience with Spark framework and related tools (PySpark, Scala, SparkR, Spark SQL, Spark UI)
Experience with Hadoop ecosystem using HDFS, ADLS Gen2, or AWS S3
Experience with data visualization development using Python, Tableau, or PowerBI
Experience with Azure, AWS or GCP
Solid understanding of performance tuning concepts for relational and distributed database systems
Familiarity with distributed programming, big data concepts, and cloud computing
Education Qualifications
Bachelor’s degree in computer science/Engineering or Technology related field or possess equivalent work experience.
Preferred Qualifications
Cloud certifications from Azure, AWS or GCP
Big data, data engineering or data science certifications from recognized vendors such as Databricks & Cloudera
Kanini Software Solutions, Inc. does not discriminate in employment matters based on race, gender, religion, age, national origin, citizenship, veteran status, family status, disability status, or any other protected class. We support workplace diversity. If you have a disability, please let us know if there is anything we can do to improve the interview process for you; we’re happy to accommodate.
Kanini Software Solutions, Inc., 25 Century Blvd., Ste. 602, Nashville, TN 37214
Job Types: Full-time, Contract
Pay: $60.00 - $70.00 per hour
Work Location: Remote",$65.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"FACEBOOK APP
5.0",5.0,Remote,Azure Data Engineer,"Responsibilities:
Create ER diagrams and write relational database queries
Create database objects and maintain referential integrity
Configure, deploy and maintain database
Participate in development and maintenance of Data warehouses
Design, develop and deploy SSIS packages
Creating and deploying reports
Provide technical design, coding assistance to the team to accomplish the project deliverables as planned/scoped.
Ability to talk to client and get the Business Requirements
Skills:
Azure Data Factory
Azure Devops
Azure Storage/ Data Lake
Extraction, Transformation and Loading
Analytics development
Report Development
Relational database and SQL language
Other Requirements:
· Should be well versed with Data Structures & algorithms
· Understanding of software development lifecycle
· Excellent analytical and problem-solving skills.
· Ability to work independently as a self-starter, and within a team environment.
· Good Communication skills- Written and Verbal
Job Type: Full-time
Salary: $84,454.31 - $190,806.62 per year
Benefits:
Flexible schedule
Health insurance
Compensation package:
1099 contract
Yearly pay
Experience level:
10 years
9 years
Schedule:
Day shift
Experience:
Azure Data engineer: 9 years (Preferred)
SQL: 9 years (Preferred)
Data warehouse: 10 years (Preferred)
Work Location: Remote","$137,630 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"McDonald's Corporation
3.5",3.5,"Chicago, IL","Data Engineer, Global Data, CRM","Company Description

McDonald’s evolving Accelerating the Arches growth strategy puts our customers and people first, and leverages our competitive advantages to strengthen our brand. We are recognized on lists like Fortune’s Most Admired Companies and Fast Company’s Most Innovative Companies.
Doubling Down on the 4Ds (Delivery, Digital, Drive Thru, and Development)
Our growth pillars emphasize the important role technology plays as the leading, global omni-channel restaurant brand. Technology enables the organization through digital technology, and improving the customer, crew and employee experience each and every day.
Global Technology forging the way
Leading the digitization of our business is the Technology organization made up of intrapreneurs who build industry defining tech using the latest innovations and platforms, like AI and edge computing to deliver on the next set of cutting-edge opportunities for the business. At McDonald’s you get to solve technology innovation challenges at an incredible scale, and work across global teams who are always hungry for a challenge. This provides access to exciting career paths for technologists. It’s bonus points when you get to see your family and friends use the tech you build at their favorite McD restaurant.

Job Description

We are seeking a highly motivated individual for the Data Engineering Manager for the Marketer Customer Data Platform team on Global Data and Architecture. This role will manage, develop, and lead the execution and implementation cloud-based data delivery systems for the MCD Customer Data Platform. This role will specialize in leading and managing the engineering teams in both onshore and offshore locations, as well as working with the relevant product management teams on overall product direction and implementation of the product vision.
What you will do:
Lead and manage the day-to-day operations for the CRM Data Engineering team with both, onshore and offshore, resources to with a focus on CDP/ CRM customer data technologies
Independently drive efforts in marketing technology and automation, collaborating with key stakeholders and business units. Must be able to find marketing solutions alongside building a long-term strategic vision of a marketing ecosystem across a highly technical, complex data stack
Participate in activities of the data engineering team on the solution, design, architect, implementation, support, and monitoring of CRM & CDP related data products
Develop and implement monitoring and remediation structure and applications for critical data flows
Continuously develop and deploy solutions to streamline and enhance the processes, logic, and tactics around customer cohort identification, and targeting via corporate CRM, marketing, and advertising systems. Additionally, develop the overall best practice framework for use by
Gather business and functional requirements and translate into robust, scalable, and user-friendly solutions

Qualifications
Bachelor’s degree in Information Technology or related technical fields such as computer science, marketing tech, mathematics, engineering, etc.
Minimum 2-5 years of relevant work experience in data analytics, engineering, computer science, mathematics or similar logic based fields
Minimum 2 years hands on experience with complex marketing automation tech stacks, leveraging CDP tools such as mParticle, Segment, and Tealium
Strong analytics skills, root cause analysis experiences on internal and external data and processes to answer business questions and improve application performance
Comfortable partnering and working with a wide range of stakeholders at various levels of management and leadership
Ability to focus on details as well as big picture
Competence and comfort focusing on specific detail engineering questions, as well as “big picture” questions
Ability to leverage critical thinking, problem-solving, and analytical skills to improve user experience, adoption, and outcomes
Curious, task-oriented self-starter with a strong bias for action; Proven track record of “getting things done”
Self-motivated with ability to set priorities and mentor others in a performance driven environment.
Passion for technology and innovation & uncompromising attention to detail & passion for problem solving
Desired Skills

Extensive experience working with Agile methodologies and expertise in production deployment processes and continuous deployment best practices.
1-2 years of hands-on experience with Braze and/or similar market automation delivery platforms
1-2 years of hands on experience with marketing automation and technology experience, understanding key components of marketing technology stacks, including CDP tools such as mParticle, Tealium, or Segment
Experience with clickstream tracking architecture implementations, and knowledge of Adobe Analytics and/or Google Analytics
Demonstrated ability to problem solve in complex technology solutioning
Experience developing Confluence pages and working with Jira
Demonstrable knowledge of multiple programming languages leveraged in martech/adtech stacks – javascript, ruby (liquid logic), html/css, python etc.

Additional Information

McDonald’s is committed to providing qualified individuals with disabilities reasonable accommodations to perform the essential functions of their jobs. Additionally, if you (or another applicant of whom you are aware) require assistance accessing or reading this job posting or otherwise seek assistance in the application process, please contact recruiting.supportteam@us.mcd.com
McDonald’s provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to sex, sex stereotyping, pregnancy (including pregnancy, childbirth, and medical conditions related to pregnancy, childbirth, or breastfeeding), race, color, religion, ancestry or national origin, age, disability status, medical condition, marital status, sexual orientation, gender, gender identity, gender expression, transgender status, protected military or veteran status, citizenship status, genetic information, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.
Nothing in this job posting or description should be construed as an offer or guarantee of employment.","$104,720 /yr (est.)",10000+ Employees,Company - Public,Restaurants & Food Service,Restaurants & Cafes,1955,$10+ billion (USD)
Go Intellects Inc,#N/A,"Washington, DC",Data Engineer,"Work Location: REMOTE (1 – 2 Days On-site/Week may require)
Required Skills:
· Collect, manage, and convert raw data accurately and reliably
· Organize data systems for subgroup access and analyses
· Configure and sustain data cloud structures
· Must have expertise in Data Visualization Tools (Tableau)
· Data Modeling/Science as Python/SAS
· Should have AWS cloud native services, security, data pipeline
· Able to work with structured and unstructured data.
· Validate outputs of data pipelines
· Degree in Data Engineering preferred.
Job Type: Full-time
Pay: Up to $150,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Application Question(s):
Do you have, ""Active Secret (or) Top Secret Security Clearance""?
Work Location: In person","$150,000 /yr (est.)",Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Goodship
2.6",2.6,Remote,Senior Infrastructure Engineer - Data,"About GoodShip
GoodShip, the first neutral platform in the $900B US truckload market, offers a comprehensive solution for procurement, performance measurement, and business optimization. Our platform empowers shippers and freight carriers to transition from traditional spreadsheet and email management to a more streamlined, AI-powered system that provides automated reports, insightful analytics, and carrier scorecards. By integrating seamlessly with any shipper TMS, GoodShip enhances collaboration among brokers and carriers.
Backed by leading venture firms such as FUSE, Ironspring Ventures, and Chicago Ventures along with founders of leading freight-tech companies, including Convoy, Stord, Project44, and FreightWaves, GoodShip is positioned for success with significant financial support and strong early traction.
Your Role
As a Senior Data Infrastructure Engineer, you'll join our mission to build a modern freight platform to empower shippers. You will be responsible for building out our data and general infrastructure to support the growing needs of the product and business.
Key Responsibilities:
Own the data ingestion architecture & system from our customers
Design new data pipelines to support the growing needs of the business
Own the strategy for scaling our data infrastructure
Improve our general engineering infrastructure to ensure operational excellence
Establish and promote best engineering practices to facilitate our ongoing growth.
What We're Looking For:
5+ years of industry experience in software development as part of a professional team.
Significant experience maintaining and scaling data infrastructure systems
Strong familiarity with the data technology ecosystem
BS degree or higher in Computer Science/Engineering, or equivalent experience.
A strong sense of ownership and responsibility.
A growth mindset, coupled with the ability to quickly learn new technologies and concepts to meet customer needs.
Proven ability to foster diverse, flexible, and empathetic engineering teams.
Nice To Have
Experience building enterprise SaaS applications
Familiarity with trucking or logistics industry
Experience with AWS, Javascript/Typescript, GraphQL, React, or PostgreSQL
Benefits
Generous equity compensation
100% Employer paid health benefits for employees
Paid time off
Opportunity to work on cutting-edge technology that directly makes an impact on customers and the freight industry",#N/A,1 to 50 Employees,Company - Private,Transportation & Logistics,Shipping & Trucking,#N/A,$5 to $25 million (USD)
"Regions
3.5",3.5,"Atlanta, GA",Data Engineer (REMOTE OPPORTUNITY),"Thank you for your interest in a career at Regions. At Regions, we believe associates deserve more than just a job. We believe in offering performance-driven individuals a place where they can build a career --- a place to expect more opportunities. If you are focused on results, dedicated to quality, strength and integrity, and possess the drive to succeed, then we are your employer of choice.

Regions is dedicated to taking appropriate steps to safeguard and protect private and personally identifiable information you submit. The information that you submit will be collected and reviewed by associates, consultants, and vendors of Regions in order to evaluate your qualifications and experience for job opportunities and will not be used for marketing purposes, sold, or shared outside of Regions unless required by law. Such information will be stored in accordance with regulatory requirements and in conjunction with Regions’ Retention Schedule for a minimum of three years. You may review, modify, or update your information by visiting and logging into the careers section of the system.

Job Description:
At Regions, the Data Engineer focuses on the evaluation, design, and execution of data structures, processes, and logic to deliver business value through operational and analytical data assets. The Data Engineer uses advanced data design and technical skills to work with business subject matter experts to create enterprise data assets utilizing state of the art data techniques and tools.

Primary Responsibilities
Partners with Regions Technology partners to Design, Build, and Maintain the data-based structures and systems in support of Data and Analytics and Data Product use cases
Builds data pipelines to collect and arrange data and manage data storage in Regions’ big data environment
Builds robust, testable programs for moving, transforming, and loading data using big data tools such as Spark.
Coordinates design and development with Data Products Partners, Data Scientists, Data Management, Data Modelers, and other Technical partners to construct strategic and tactical data stores
Ensures data is prepared, arranged and ready for each defined business use case
Designs and deploys frameworks and micro services to serve data assets to data consumers
Collaborates and aligns with technical and non-technical stakeholders to translate customer needs into Data Design requirements, and work to deliver world-class visualizations, data stories while ensuring data quality and integrity
Provides consultation to all areas of the organization that plan to use data to make decisions
Supports any team members in the development of such information delivery and aid in the automation of data products
Acts as trusted adviser and partner to business leads- assisting in the identification of business needs & data opportunities, understanding key drivers of performance, interpreting business case data drivers, turning data into business value, and participating in the guidance of the overall data and analytics strategy

This position is exempt from timekeeping requirements under the Fair Labor Standards Act and is not eligible for overtime pay.

Requirements
Bachelor's degree and five (5) years of experience in a quantitative/analytical/STEM field or technical related field
Or Master’s degree and three (3) years of experience in a quantitative/analytical/STEM field or technical related field
Or Ph.D. and one (1) year of experience in a quantitative/analytical/STEM field
Three (3) years of working programming experience in Python/PySpark, Scala, SQL
Three (3) years of working experience in Big Data Technology in Hadoop, Hive, Impala, Spark, or Kafka

P references
Prior banking or financial Services experience
Experience developing solutions for the financial services industry
Background in Big Data Engineering and Advanced Data Analytics
Experience in Agile Software Development
Experience or exposure to cloud technologies and migrations

Skills and Competencies
Experience building data solutions at scale
Experience designing and building relational data structures in multiple environments
Experience with Airflow, Argo, Luigi, or similar orchestration tool
Experience with DevOps principals and CI/CD.
Experience with Docker and Kubernetes
Experience with No-SQL databases such as HBase, Cassandra, or MongoDB
Experience with streaming technologies such as Kafka, Flink, or Spark Streaming
Experience working with Hadoop ecosystem building Data Assets at an enterprise scale
Proven record of accomplishment of delivering operational Data solutions including Report and Model Ready Data Assets
Significant experience working with senior executives in the use of data, reporting and visualizations to support strategic and operational decision making
Strong ability to transform and integrate complex data from multiple sources into accessible, understandable, and usable data assets and frameworks
Strong background in synthesizing data and analytics in a large (fortune 500), complex, and highly regulated environment
Strong technical background including database and business intelligence skills
Strong communication skills through written and oral presentations

Additional Job Description
Candidates must have experience with DevOps principals and CI/CD.
Candidates must have experience with Docker and Kubernetes
Candidates must have experience with streaming technologies such as Kafka, Flink, or Spark Streaming.
Preferred experience in Snowflake, SQL and Python.
Preferred experience in developing API's.
Preferred experience with AWS Lambda functions.
This position may be filled at a higher level depending on the candidate's qualifications and relevant experience.

Position Type Full time

Compensation Details
Pay ranges are job specific and are provided as a point-of-market reference for compensation decisions. Other factors which directly impact pay for individual associates include: experience, skills, knowledge, contribution, job location and, most importantly, performance in the job role. As these factors vary by individuals, pay will also vary among individual associates within the same job.

The target information listed below is based on the national range and level of the position.

Job Range Target:
Minimum: $85,374.00 USD
Median: $122,800.00 USD
Incentive Pay Plans: This job is not incentive eligible.

Benefits Information
Regions offers a benefits package that is flexible, comprehensive and recognizes that ""one size does not fit all"" for associates. Listed below is a synopsis of the benefits offered by Regions for informational purposes, which is not intended to be a complete summary of plan terms and conditions.
Paid Vacation/Sick Time
401K with Company Match
Medical, Dental and Vision Benefits
Disability Benefits
Health Savings Account
Flexible Spending Account
Life Insurance
Parental Leave
Employee Assistance Program
Associate Volunteer Program

Please note, benefits and plans may be changed, amended, or terminated with respect to all or any class of associate at any time. To learn more about Regions’ benefits, please click or copy the link below to your browser.

https://www.regions.com/welcometour/benefits.rf

Location Details Regions Plaza Atlanta
Location: Atlanta, Georgia

Bring Your Whole Self to Work

We have a passion for creating an inclusive environment that promotes and values diversity of race, color, national origin, religion, age, sexual orientation, gender identity, disability, veteran status, genetic information, sex, pregnancy, and many other primary and secondary dimensions that make each of us unique as individuals and provide valuable perspective that makes us a better company and employer. More importantly, we recognize that creating a workplace where everyone, regardless of background, can do their best work is the right thing to do.

OFCCP Disclosure: Equal Opportunity Employer/Disabled/Veterans","$63,377 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Banking & Lending,1971,$5 to $10 billion (USD)
LandGate Corp,#N/A,Remote,Data Engineer,"LandGate is a fast growing renewable energy and carbon data & analytics platform. We are looking for a Database Engineer with experience using Python and SQL to process data. You will be expected to be a savvy developer who can work tasks from start to finish, applying your in depth experience of data processing using Python and SQL, and be prepared to hit the ground running. Must have excellent attention to detail, communication skills, and willingness to go the extra mile.
Compensation
LandGate offers a very competitive salary (commensurate with experience), commission, bonus, and shares providing ownership in the company. Benefits include 5 weeks PTO, work from home, platinum health/vision/dental insurance 100% paid for employees and dependents, disability/life insurance 100% paid, phone/internet 100% paid, and other benefits.
About LandGate
LandGate is the leading provider of data solutions, and an online marketplace for US commercial land and its resources: solar, wind, carbon, minerals, and water. The company helps investors, developers, real estate agents, and landowners understand energy & environmental resource values and connect on its online marketplace for land-related transactions.
LandGate enables energy and carbon professionals to run economic engineering studies in minutes; access land leads and MLS listings; and manage their leads in a land CRM web app connecting their team. The company opens energy and carbon commission opportunities to real estate agents. LandGate applies its technology to provide the most advanced analytics for renewable energy M&A deals, market & price trends, operators’ benchmark and performance indicators.
Founded in 2016 in Denver, Colorado, LandGate received Series A funding in 2019 from Rice Investment Group, a widely-respected energy technology investor, and Series B funding in 2022 from Nextera Energy, the world’s largest generator of renewable energy, and from Kimmeridge, a leading carbon solutions private equity firm. The company has partnered with the Realtors Land Institute (RLI), the American Association of Professional Landmen (AAPL), and the Texas Engineering Executive Education (TxEEE) from the University of Texas at Austin.
Skills
Experience with Python and SQL (2+ years)
Proficiency in working with PostgreSQL, writing SQL (queries, functions, stored procedures)
Strong math background with ability to apply scientific formulas to large datasets
Experienced using Linux command line
Proven work experience as a Back-end developer
Familiarity with testing and debugging
In-depth understanding of the entire development process (design, development and deployment)
An ability to perform well in a fast-paced environment
Excellent analytical and multitasking skills
BSc degree in Computer Science or relevant field
Job Type: Full-time
Pay: $100,000.00 - $180,000.00 per year
Benefits:
Dental insurance
Health insurance
Life insurance
Paid time off
Retirement plan
Vision insurance
Compensation package:
Performance bonus
Experience level:
4 years
Schedule:
Monday to Friday
Experience:
SQL: 2 years (Required)
Python: 2 years (Required)
Work Location: Remote","$140,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Edelman Financial Engines
3.5",3.5,United States,Data Engineer,"Data Engineer (Remote)
At Edelman Financial Engines (EFE), we believe everyone deserves to move their financial life forward!
We know that assets have the power to fund goals. Those numbers represent individual lifetimes filled with hard work and dreams for our clients and generations to follow. Our clients trust us to guide them forward with empathy, integrity and invention. We uphold that same standard of respect and commitment for clients and colleagues alike.
Founded on the idea that financial education is a fundamental right for everyone, Edelman Financial Engines continues to grow and challenge the status quo. We’re moving forward, together. If our purpose-driven commitment inspires you, we invite you to consider joining our team.
This individual is a hardworking, and outcome focused data engineer with experience working with structured and unstructured data including data modeling and developing data pipelines primarily using Python and SQL on AWS cloud platform.
In addition to an earnest desire to help people, we are looking for the ideal candidate to complement the team’s existing talents. For this Data Engineer role, we are seeking a teammate with proven experience in data engineering, data governance and cloud technologies. If you are passionate about data, analytics and helping business leverage data as an asset, this may be an ideal match for you!
Responsibilities:
Continue to scale our cloud data analytics platform to meet the company’s vision and growth
Follow engineering and industry standards
Contribute to the development of data pipelines to publish high-quality data products for data analysts and data scientists
Flawless communication within internal teams and across the organization
Requirements:
2+ years of total experience with a bachelor's degree in computer science and equivalent experience
1-2+ years’ experience working as a developer in Data Engineering with data pipelines and data processing.
Practical hands-on experience with SQL and GitLab or other file control systems
Experience working with data formats such as Apache AVRO, Apache Parquet, and common methods in data transformation
Experience with AWS technology stack and AWS Certification is a strong plus
Dedicated, and ability to work with a geographically distributed diverse team
Excellent analytical, communication and organizational skills
Strong teamwork, coordination, planning and influencing skills
Experience with Agile framework is a plus
Must have desire to continue learning new technologies, ask questions, and improve skills
About Edelman Financial Engines
Since 1986, Edelman Financial Engines has been committed to always acting in the best interests of our clients. We were founded on the belief that all investors – not just the wealthy – deserve access to personal, comprehensive financial planning and investment advice. Today, we are America’s top independent financial planning and investment advisory firm, recognized by Barron’s,1 with 145+ offices2 across the country and entrusted by more than 1.3 million clients to manage more than $242 billion in assets.3 Our unique approach to serving clients combines our advanced methodology and proprietary technology with the attention of a dedicated personal financial planner. Every client’s situation and goals are unique, and the powerful fusion of high-tech and high-touch allows Edelman Financial Engines to deliver the personal plan and financial confidence that everyone deserves.
For more information, please visit EdelmanFinancialEngines.com.
© 2023 Edelman Financial Engines, LLC. Edelman Financial Engines® is a registered trademark of Edelman Financial Engines, LLC. All advisory services provided by Financial Engines Advisors L.L.C., a federally registered investment advisor. Results are not guaranteed. See EdelmanFinancialEngines.com/patent-information for patent information. AM2789819
For California residents, please see the link for the Privacy Notice for Candidates. California law requires that we provide you this notice about the collection and use of your personal information. Please read it carefully.
Edelman Financial Engines encourages success based on our individual merits and abilities without regard to race, color, religion, creed, sex, gender identity or expression, sexual orientation, pregnancy; marital, domestic partner or civil union status; national origin, citizenship, ancestry, ethnic heritage, genetic information, age, legally recognized disability, military service or veteran status.
Accommodations are modifications or adjustments to the hiring process that would enable you to fully participate in that process. If you need assistance to accommodate a disability, you may request one at any time by either contacting your recruiter or HRQ@EdelmanFinancialEngines.com.
1 The Barron’s 2022 Top 100 RIA Firms list, a seven-year ranking of independent advisory firms, is qualitative and quantitative, including assets managed by the firms, technology spending, staff diversity, succession planning and other metrics. Firms elect to participate but do not pay to be included in the ranking. Ranking awarded each September based on data within a 12-month period. Compensation is paid for use and distribution of the rating. Investor experience and returns are not considered. The 2018 ranking refers to Edelman Financial Services, LLC, which combined its advisory business in its entirety with Financial Engines Advisors L.L.C. (FEA) in November 2018. For the same survey, FEA received a precombination ranking of 12th.
2 Edelman Financial Engines data, as of Dec. 31, 2022.
3 Edelman Financial Engines data, as of Dec. 31, 2022.",#N/A,1001 to 5000 Employees,Company - Private,Financial Services,Investment & Asset Management,1987,Unknown / Non-Applicable
"FinditParts
4.8",4.8,"Los Angeles, CA",Senior Data Warehouse Engineer,"FinditParts is the nation's largest eCommerce provider of heavy-duty truck and trailer parts. From hard-to-find parts to everyday preventative maintenance items, we offer more than 3 million heavy-duty OEM, branded, and aftermarket parts ready to ship. Each month thousands of repair shops, fleets, and owner-operators rely on FinditParts to streamline their part sourcing efforts and keep their trucks on the road.

As the industry leader in parts discovery through visual identification technology and aces/pies, we simplify the complexity of finding the right part to fit any commercial vehicle, reducing the time and frustration associated with parts sourcing.
Founded in 2010, with offices in Los Angeles and the Philippines, FinditParts is well-funded, having recently raised $30 million in Series A funding, and profitable.
SUMMARY
The Senior Data Warehouse Engineer, reporting to the Director of Analytics, will be responsible for leading the complete end-to-end architectural blueprint and development of our Data Warehouse system. A successful candidate will deliver a thoughtful data storage system that will store large data sets through various data intelligence environments. The Senior Data Warehouse Engineer will have a high level of impact, by providing architectural solutions to the design, development, testing, and deployment, and allowing FinditParts to store large volumes of data efficiently and effectively for the organization as a whole.
RESPONSIBILITIES
Lead the architectural design blueprint and development of our data warehouse
End-to-end ownership during the design, development, testing, and deployment
Evaluate architectural and software solutions that will deliver appropriate solutions
Develop and maintain data pipelines as well as API-based or file-based data flows between source systems and the data warehouse
Translate client user requirements into a technical architecture vision and implementation plan
Architect the data intelligence environments, including data lakes, data marts, and metadata repositories
Consult with leadership to define goals and requirements, develop technical requirements, deliver analysis, and thoughtful conclusions with data to provide actionable insights
Build, review, and audit existing ETL jobs and SQL queries
Identify gaps and develop a plan to integrate current systems with a desired future state
Perform root cause analysis of data failures and update existing processes to prevent re-occurring issues
Collaborate with various teams to gather information and create visually appealing, engaging, and informative reports in Tableau
Work closely with the BI team, Product team, and Analytics teams to ensure the Data Warehouse is structured in a way to meet data needs for all business teams
QUALIFICATIONS
5+ years of industry experience in Data Warehousing and/or Data Engineering
Bachelor's Degree in Computer Science, Engineering, Statistics, Mathematics, or another quantitative field
Proven experience managing and transforming data in a data warehouse
Proven experience in manipulating, processing, and extracting value from large data-sets
Familiarity with data schema designs that best support business needs and reporting
High level of experience extracting/cleansing data and generating insights from large transactional data sets using SQL, R, Python, and/or Spark on the cloud
Experience building and managing data pipelines and repositories in cloud environments such as Google Cloud, Microsoft Azure or AWS
Experience in Looker, Redshift, Apache Spark, Spark Structured Streaming, SQL, etc
Experience with cloud computing with Dataproc, Databricks, or similar technologies
Strong problem-solving/analytical abilities
BENEFITS
100% US-based remote
Competitive salary, bonus, and equity
Flexible PTO policy
Paid Medical, dental, and vision insurance options for Employees
Fun and energizing start-up environment
""The US base salary range for this full-time position is $125,000 - $165,000 + bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about specific salary range for your preferred location during the hiring process. Please note that the compensation details listed in the US role postings reflect the base salary only and do not include bonus, equity, or benefits.""
FinditParts is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran or any other characteristic protected by law. FinditParts conforms to the letter of all applicable laws and regulations.","$130,474 /yr (est.)",1 to 50 Employees,Company - Private,Retail & Wholesale,Automotive Parts & Accessories Stores,2010,Unknown / Non-Applicable
"ProIT Inc.
5.0",5.0,"Bellevue, WA",Azure Data Engineer,"Data Engineering experience primarily on Spark. Someone who has worked on Azure cloud with knowledge on Azure DEVOPS (CI/CD & Infrastructure as a code), ADF, ADW & Power BI. Apache Nifi will be good to have.Power BI: Understand business requirements to set functional specifications for reporting applications Build automated reports and dashboards with the help of Power BI reporting tool Be experienced in tools and systems on MS SQL Server BI Stack, including SSRS and TSQL, Power Query, MDX, PowerBI, and DAX Be able to quickly shape data into reporting and analytics solutions Have knowledge of database fundamentals such as multidimensional database design, relational database design, and more Create functional reporting Study, analyze and understand business requirements in context to business intelligence. Design and map data models to shift raw data into meaningful insights. Utilize Power BI to build interactive and visually appealing dashboards and reports. Spot key performance indicators with apt objectives Run DAX queries and functions in Power BI Should have an edge over making DAX queries in Power BI desktop. Developing visual reports, KPI scorecards, and dashboards using Power BI desktop. Connecting data sources, importing data, and transforming data for Business intelligence.
Spark ,Azure DEVOPS (CI/CD & Infrastructure as a code), ADF, ADW & Power
Job Type: Full-time
Pay: $100,154.64 - $104,132.47 per year
Ability to commute/relocate:
Bellevue, WA 98004: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Hybrid remote in Bellevue, WA 98004","$102,144 /yr (est.)",51 to 200 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Wallero
4.4",4.4,"Seattle, WA",Data Engineer,"Hi,
Pretty much interested in your resume, we have an urgent opening for Data Engineer role with one of our clients based in Seattle WA.
Position: Data Engineer
Location: Seattle WA - Remote (100%)
Description:
Good Experience in reporting space including PowerBI, ADF, and possibly standing up a data warehouse/data mart.
The main scope centering is around reporting.
Experience with SQL, and agile framework including requirement gathering and story/feature writing skills.
Need someone strong who is comfortable leading and guiding teams in that capacity.
Migrating data from third party to Azure Cloud
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Seattle, WA 98101: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",$75.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2015,$1 to $5 million (USD)
Provista Software Corporation,#N/A,Remote,Data Engineer,"As a successful candidate, you’ll play a pivotal role in the data conversion and business intelligence solution efforts during project implementation for our state and local clients. We are looking for you to join our software implementation team as a Senior Data Engineer.
Qualification:
5+ years of experience working independently as ETL developer on data migration projects that covered diverse database platforms and complex data conversions.
Must have experience across leading Database Platforms especially Oracle, Azure SQL, Snowflake.
Expert level experience with Extraction Transform Load (ETL) plan, protocols, and tools.
Expert level experience with programming languages SQL, Phyton, Java, C#.
Good at communication with clients and internally.
Expert at problem solving and troubleshooting skills.
Bachelor's degree, preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience
Must be a U.S. Citizen or Green Card Holder.
Functional Responsibility:
Data Conversion -
In collaboration with Data Stewards, discover inventory of legacy data sources and all data the client expects to be migrated to destination Modified Off the Shelf (MOTS) solution.
Complete gap assessment between legacy data and MOTS data requirements, and options to address the gaps.
Develop data migration map to minimally include data dictionary, detailed data map of all elements of the current database, data transformation rules, relationship rules, validation rules, process of migrating images, and process for recovering missing or erroneous data.
Identify issues, risks, and/or barriers that may interfere with data migration and recommendations to resolve or mitigate.
Conduct testing and quality assurance to ensure the data is migrated accurately.
Migrate data from legacy to destination MOTS solution progressively using a sample set of data and the entirety of the data prior to deployment into relevant go-live environments (e.g., staging, production, etc.)
Prepare data migration reports that includes – record counts of source vs migrated vs modified; field level validation of data at row and column; discovered exceptions.
Business Intelligence -
Develop ETL pipelines in and out of Snowflake data warehouse using combination of Python, SnowSQL, and SQL.
In collaboration with Data Stewards, create Snowflake data mart as curated subset of data to support the analytics and business intelligence needs of subgroup of users.
Translate requirements for business intelligence and reporting to database design and reporting design.
Conduct discovery sessions with users to capture business intelligence reports.
Develop Tableau self-service ad hoc reports to support operational users.
Develop Tableau Dashboards to support executive, management, and operational users.
About us:
Provista Software Corporation is fast growing Software Solution provider for US State and Local Government Clients.
Job Type: Full-time
Pay: $67,107.98 - $134,285.66 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Life insurance
Paid time off
Parental leave
Professional development assistance
Vision insurance
Compensation package:
Yearly pay
Experience level:
5 years
Schedule:
8 hour shift
Application Question(s):
What is your most preferred work time zone ?
Experience:
ETL: 5 years (Required)
Data warehouse: 1 year (Preferred)
Tableau: 1 year (Preferred)
Talend: 1 year (Preferred)
Microsoft SQL Server: 5 years (Required)
Work Location: Remote","$100,697 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Codinix Technologies
5.0",5.0,"Boston, MA",Cloud Data Engineer - AWS,"Only US Citizen
Strong in warehouse technologies (Hadoop, Spark, AWS, etc.)
Bachelor's degree in Computer Science, Information Systems, or another related field
5+ years of data engineering, and cloud engineering experience
Experience in Agile project methodologies and DevOps
Experience with AWS (certification preferred; including Athena, Redshift, Glue), Cloudera
Experience building a CI/CD pipeline to deploy applications on AWS (GitLab, Terraform)
Advanced experience programming and scripting on Unix/Linux (i.e. Python, Bash)
Experience with CTRL-M, Cron (other) scheduling of batch jobs
Experience migrating legacy systems to cloud-ready architectures
Job Type: Contract
Pay: Up to $75.00 per hour
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Boston, MA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",$75.00 /hr (est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"MetroStar
3.7",3.7,United States,Data Engineer (Mid),"As a Data Engineer (Mid), you'll bring creative architect solutions to end customers with the goal to make an impact across the federal government.
We know that you can't have great technology services without amazing people. At MetroStar, we are obsessed with our people and have led a two-decade legacy of building the best and brightest teams. Because we know our future relies on our deep understanding and relentless focus on our people, we live by our mission: A passion for our people. Value for our customers.
If you think you can see yourself delivering our mission and pursuing our goals with us, then check out the job description below!
What you'll do:
Work with AI team members to operationalize data pipelines and ML tasks.
Provide day-to-day support of deploying Python-native ML pipelines and perform data engineering tasks to enable AI/ML capabilities.
Present results to a diverse audience in presentation or report form.
Support architectural leadership, technical support, and advisement services to ensure identity management system technologies are integrated and meeting the appropriate security requirements.
Support leadership who engage with senior level executives at a public facing Federal agency and provide subject matter expertise in security architecture and other key domain areas.
What you'll need to succeed:
5+ years of experience in Data/ML engineering (if school experience is used, at most that would contribute to 2 years of actual experience).
Experience with ETL, Data Labeling and Data Prep.
Experience designing, implementing, and maintaining data architecture and services to be used for AI/ML. Additionally, operationalizing and maintaining AI/ML models in production.
The ability to perform data analytics on program related or system related activities. This will include assessing performance and manual processes implementing methods/algorithms to automate/optimize
A bachelor's degree in Computer Science, Information Technology Management or Engineering, or other comparable degree or experience and the ability to obtain and maintain DHS Suitability.

Like we said, we are obsessed with our people. That's why we offer a generous benefits package, professional growth, and valuable time to recharge. Learn more about our company culture code and benefits. Plus, check out our accolades.
Don't meet every single requirement?
Studies have shown that women, people of color and the LGBTQ+ community are less likely to apply to jobs unless they meet every single qualification. At MetroStar we are dedicated to building a diverse, inclusive, and authentic culture, so, if you're excited about this role, but your previous experience doesn't align perfectly with every qualification in the job description, we encourage you to go ahead and apply. We pride ourselves on making great matches, and you may be the perfect match for this role or another one we have. Best of luck! – The MetroStar People & Culture Team
What we want you to know:
In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification form upon hire.
MetroStar Systems is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. The statements herein are intended to describe the general nature and level of work being performed by employees and are not to be construed as an exhaustive list of responsibilities, duties, and skills required of personnel so classified. Furthermore, they do not establish a contract for employment and are subject to change at the discretion of MetroStar Systems.
Not ready to apply now?
Sign up to join our newsletter here.",#N/A,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1999,$25 to $100 million (USD)
"Maven Workforce
4.1",4.1,"McLean, VA",Data Engineer,"MUST HAVES: Hands-on experience with Informatica PowerCenter and/or Informatica Intelligent Cloud Services (IICS), Cloud Data Integration (CDI) , Cloud AWS,UNIX scripting, and any other ETL tools.
Job responsibilities: –
The candidates will be having the IICS module of Informatica more than any other platform modules.
Strong hands-on experience building ETL pipelines and Data Replication
3+ yrs of experience with IICS is required.
Must be hand Ons with AWS services like ECS,S3,EKS etc.
Cloud Data Integration is also required.
Data Engineering Background experience with Spark, Hive is also required.
Preferred:
SQL is preferred, but basic knowledge is required
Job Type: Contract
Salary: $60.00 per hour
Ability to commute/relocate:
Mclean, VA 22101: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica PowerCenter: 7 years (Required)
Informatica Intelligent Cloud Services (IICS): 7 years (Required)
Cloud Data Integration (CDI): 7 years (Required)
UNIX scripting: 7 years (Required)
ETL tools: 7 years (Required)
Work Location: One location
Speak with the employer
+91 7328456015",$60.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2008,Unknown / Non-Applicable
Zenotis Technologies inc,#N/A,"Addison, TX",Big Data Engineer,"Role: Big Data Engineer
Location – Addison, TX/ Charlotte, NC / Chandler, AZ (Hybrid – 3 days onsite 2 days remote)
Position Summary:
Looking for a big data engineer who loves solving complex problems across a full spectrum of technologies. The ideal candidate is excited by experimentation and looking for a new challenge that stretches their talents. The big data engineer will help ensure that our technological infrastructure operates seamlessly in support of business objectives.
The engineer must be able to:
· Translate complex technical and functional requirements into detailed designs
· Guide the development and testing teams in developing and implement pipelines that extract, transform, and load data into an information product that helps the organization reach its strategic goals
· Focus on ingesting, storing, processing, and analyzing large datasets
· Deep understanding on how to create scalable, high-performance web services for tracking data
· Investigate alternatives for data storing and processing to ensure the implementation of the most streamlined solutions
Primary Skill
Hadoop
Secondary Skill
Kafka
Tertiary Skill
Oracle Exadata
Required Skills
· Translate complex technical and functional requirements into detailed designs
· Design data pipelines using design tools and provide solution intent in provided templates
· Work closely with data science team to implement data analytics pipelines
· Analyze vast number of data stores to uncover insights
· Define data governance policies and support data-versioning processes
· Maintain security and data privacy, working closely with data protection guidelines.
· Desired Skills
· Experience with Python, Spark, and Hive
· Understanding of data-warehousing and data-modeling techniques
· Knowledge of industry-wide visualization and analytics tools (ex: Tableau, R)
· Strong data engineering skills with Hadoop platform
· Experience with streaming frameworks such as Kafka
· Knowledge of Core Java, Linux, SQL, and any scripting language
· Good interpersonal skills and positive attitude
Job Type: Full-time
Salary: Up to $70.00 per hour
Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
8 hour shift
Work Location: On the road",$70.00 /hr (est.),201 to 500 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Synovize
5.0",5.0,Remote,Big Data Engineer,"Synovize is a cutting-edge technology company that specializes in providing advanced data analytics and intelligence solutions. Our mission is to empower businesses with the tools and insights they need to make data-driven decisions and drive innovation. We are committed to delivering exceptional results by combining our expertise in big data technologies with our passion for creating impactful solutions.
As a Big Data Engineer at Synovize, you will have the opportunity to work on exciting projects that involve designing and implementing scalable data solutions. You will collaborate with a talented team of professionals, including software developers, data scientists, and domain experts, to tackle complex data engineering challenges and deliver high-quality solutions to our clients.
Job Title:
Big Data Engineer
Company: Synovize
Number of Positions: 2
Location: Remote
Length: 12 Months
Tax Terms: 1099/C2C or W-2
Rate/Range: $70-$90/hr
Work Authorization: US Citizen
Security Clearance: None
Skills Needed:
Proficiency in Java and Kotlin programming languages.
Strong knowledge of AWS (Amazon Web Services) and its big data offerings.
Experience with BigData technologies, frameworks, and tools.
Familiarity with Kafka for streaming data processing.
Proficiency in Python programming language.
Responsibilities:
Design and develop scalable data pipelines to process, integrate, and analyze large volumes of structured and unstructured data.
Implement data ingestion, transformation, and storage mechanisms using big data technologies and frameworks.
Collaborate with data scientists and analysts to understand data requirements and optimize data models for analytics and machine learning.
Utilize AWS services and tools to build and maintain data infrastructure, ensuring data quality, integrity, and security.
Troubleshoot and resolve data-related issues, optimizing performance and efficiency.
Stay up-to-date with emerging trends and advancements in big data technologies, recommending and implementing improvements to existing systems and processes.
Requirements:
Bachelor's or master's degree in computer science, engineering, or a related field.
Proven experience as a Big Data Engineer or similar role, with a strong understanding of big data technologies and architectures.
Proficiency in Java and Kotlin programming languages.
Strong knowledge of AWS and its big data offerings.
Experience with BigData technologies, frameworks, and tools.
Familiarity with Kafka for streaming data processing.
Proficiency in Python programming language.
Strong problem-solving skills and ability to work in a fast-paced, collaborative environment.
Excellent communication and interpersonal skills.
Join Synovize and be part of a dynamic team that is at the forefront of data innovation. Together, we will drive meaningful change and unlock the full potential of data-driven decision-making.
Job Types: Full-time, Contract
Pay: $70.00 - $90.00 per hour
Benefits:
401(k)
Dental insurance
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Vision insurance
Schedule:
10 hour shift
4 hour shift
8 hour shift
Choose your own hours
Application Question(s):
Are you a US Citizen?
Years of experience in Java?
Years of experience in Kotlin?
Years of experience in big data?
Years of experience in AWS?
Security Clearance
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$80.00 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
BIGCLFY,#N/A,"Dallas, TX","W2/ 1099, No C2C - SSIS ETL Data Engineer (Minneapolis, MN/ Dallas-Irving)","No C2C. It's W2/1099 Requirement
Only taking USC, GC or H4 EAD, H1 Transfer, L2 Visa or TN Visa - they will not take OPT EAD or CPT
Need to have confirmation in writing that you will be onsite by Day 1
ALL ROLES ARE HYBRID IN THE OFFICE (2-3 DAYS/WEEK) UNLESS REMOTE IS NOTED
Please submit qualified candidates and include FULL LEGAL NAME as it appears on the passport
If your candidate is relocating, need to have confirmation in writing that they will be onsite by Day 1
Senior SSIS/ETL Data Engineer
12 months – 3 days in office
Minneapolis, MN or Dallas-Irving, TX
5+ years of experience with SQL Server 2016 (or newer) and SSIS
7+ years of data engineering
Must be capable of leading teams
2+ years of experience working with large scale SQL implementations
Experience designing, developing, and maintaining complex stored procedures, functions, and views using T-SQL
Experience designing, developing, and maintaining ETL processes using SSIS
Should understand how to unit test their product
Hands on experience with Visual Studio to document
Experience in financial or regulatory environments highly preferred
Ideal candidates will have experience creating conceptual, logical, and physical database designs
Preference given to candidates with extensive knowledge and experience with performance monitoring, database and SQL tuning
Bachelor’s or Master's in computer science or related field or equivalent work experience
Job Type: Contract
Pay: From $73.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Chandler, AZ 85225: Reliably commute or planning to relocate before starting work (Preferred)
Application Question(s):
Expected Payrate on W2/1099 Tax Term? Please mention. Thanks.
Experience:
Risk Analytics: 5 years (Preferred)
Work Location: In person",$73.00 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"WorkCog
4.3",4.3,"Atlanta, GA",Sr. AWS Data Engineer,"Position: AWS Data Engineer
Location: Atlanta ,GA
Duration: 12+ Months
Interview: Video
JD
Responsibilities
Bachelor’s degree in Computer Science, Software Engineering, or other related field, or equivalent work experience.
Extensive experience in data modeling, writing & performance tuning complex SQL queries & user-defined functions
Hands-on experience in design, implementation and orchestration of data pipelines and ETL batch Jobs.
Hands-on experience on all aspects of data handling: cleansing and standardization, normalization, merging, change data capture, data integration, reconciliation, and transformations
Strong experience in working with Big Data technologies such as Hive/Spark, Data lake and Data warehouse using GCP and/or AWS Services
Ability to write robust code in scripting languages like Python, Scala, PySpark or similar.
Good to Have
Associate Architect or equivalent cloud certification
Experience with processing large datasets using AWS and GCP UDFs (User defined functions)
Experience in ML probability & statistics algorithms, frameworks like Keras or PyTorch and tools like Jupyter notebooks.
Job Type: Contract
Pay: $45.83 - $52.42 per hour
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Atlanta, GA: Reliably commute or planning to relocate before starting work (Preferred)
Experience:
SQL (Preferred)
Informatica (Preferred)
Data warehouse (Preferred)
Work Location: In person",$49.13 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable
Breadboard,#N/A,"New York, NY",Data Engineer,"About Us
We’re on a mission to revolutionize the global electronics supply chain.
During the pandemic we saw how the inefficiencies in electronics manufacturing can have massive global impact. From over $200B lost in car manufacturing to delays on laptops, gaming systems and much more.
At Breadboard we are rethinking the electronics supply chain down to the finest detail. We’re designing the new operating system in which this entire system will run. We’ve already secured seed funding and are now looking to expand our engineering team to help us set the foundation for the future.
Why work with us?
As a part of our team, you will have the unique opportunity to design and implement the data platform for Breadboard, a critical component in our business.
As one of the first data engineers to join the Breadboard team you will be partnering with the CTO, our founding engineers, and will set the foundation for the design of the operating system for the electronics supply chain.
You will be involved in all aspects of the software development process, architectural decisions, recruiting, and building the organization in general.
We’re a tight-knit, intense team and you will have a ton of end to end ownership.
Work with seasoned founders who have had successful exits.
This is not only a unique opportunity for someone who wants to get their hands dirty, but also take that step back and collaborate with the executive team on setting a strategic future.
You’ll be responsible for
Data Management: Develop and automate robust data acquisition, processing, and normalization pipelines from various sources to ensure data quality and consistency.
Database and ETL Architecture: Design, implement, and optimize a database system for efficient data storage, retrieval, and analysis.
Collaboration on Machine Learning: The prospects of working with data scientists to leverage extracted data features, contributing to the development and refinement of a machine learning model for component matching.
API and UI Integration: Collaborate with the software development team to facilitate seamless integration between the database, machine learning models, and user interface.
Continuous Improvement and Compliance: Monitor system performance and user feedback for continuous improvements, while ensuring adherence to data privacy and security standards.

Requirements
Experience: A minimum of 3-5 years of experience in a data engineering role, preferably in the electronics, manufacturing industry or supply chain industry.
Technical Skills: Proficiency in SQL and experience with database design, data modeling, and performance optimization. Strong coding skills in a major programming language, such as Python or TypeScript. Experience with data processing tools and libraries (e.g., pandas, Spark). Experience with pipeline and workflow orchestration tools, such Airflow. Experience with Snowflake or similar cloud-based data warehousing solutions.
Data Acquisition and Processing: Experience with data acquisition methods, such as APIs and web scraping. Familiarity with data cleaning, normalization, and feature extraction techniques.
Machine Learning: Familiarity with machine learning concepts and algorithms, and experience collaborating with data scientists or machine learning engineers.
API Development: Experience with API development and integration, preferably in a microservices architecture.
Data Governance: Knowledge of data privacy and security regulations, and experience implementing data governance measures.
Bonus Points
Entrepreneurial mindset (we encourage all employees to be future founders and this can be a great stepping stone towards that goal).
Customer-centric and passionate about helping small businesses grow.
Previous experience at a high-growth, fast-paced startup.
Experience with our stack (and some others that may be included shortly):
React, Material UI, Typescript, CSS
NestJS, GraphQL, TypeORM, Knex.js, Objection.js, PostgreSQL, gRPC, Kafka
AWS, Docker, Kubernetes, Helm, Github Actions, CI/CD pipelines.
Compensation + Benefits
Competitive cash compensation
Stock ownership at a fast growing company
Health/dental/vision insurance—100% coverage option
The ability to push for other benefits you think are important
Our Engineering Culture
Openness to diverse opinions and backgrounds - Testing our assumptions
Supported career growth with a high emphasis on learning and mentorship
Lean development
End to end ownership and accountability. We’re all going to make mistakes. It’s about how you own up to them and learn from them.

Equal Opportunity Employer
Breadboard Software does not discriminate in employment on the basis of race, color, religion, sex (including pregnancy and gender identity), national origin, political affiliation, sexual orientation, marital status, disability, genetic information, age, membership in an employee organization, retaliation, parental status, military service, or other non-merit factor.",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Ripple Effect Consulting
5.0",5.0,"Denver, CO",Data Engineer - 004/005,"Data Engineer
Hybrid-remote based in Denver, CO | $80-$160k
Job Overview:
We are looking for a savvy Data Engineer to join our growing team of analytics experts. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
Responsibilities for Data Engineer
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Qualifications for Data Engineer
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:
1) Experience with big data tools
2) Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
3) Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
4) Experience with cloud services
5) Experience with stream-processing systems: Storm, Spark-Streaming, etc.
6) Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Job Type: Full-time
Pay: $80,000.00 - $160,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Denver, CO: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person","$120,000 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
IT Customer Service - POST,#N/A,"Newark, NJ",Data Center Plant Engineer,"Job Title: ISP-OSP Data Center Plant Engineer
Location: Newark NJ, (M-F)
Travel Required: As Needed
Overview
UC-ISP/OSP engineer is responsible for cable management including structured cabling, carrier data and voice circuit cables/equipment, rack-and-stack, and oversight of power, backup power, network equipment, and environmental needs to cabinets. Leveraging Clients Data Center Infrastructure Management System, (DCIM) tool to maintain, report and monitor.
Conducting capacity assessments of existing infrastructure to ensure that it can support future growth to ensure compliance with federal regulations of the Data Center infrastructure Standards. Projects include design and build outs of the physical plant within the typical WAN/LAN space within data centers, telco rooms, MDFs and IDFs as needed.
Functions:
Perform initial engineering surveys based on project needs.
Document engineering surveys to determine feasibility and the estimated cost.
Obtain authorization to proceed and confirm funding.
Create detailed engineering drawings of build, floor plans, equipment elevation drawings, structured cabling diagrams, power, and environmental needs.
Designs must conform to state, local, national, and Client standards and codes, and the NJ historical and or environmental commissions approvals if needed.
Based on the engineering documents create a SOW (Scope of Work) and BOM (Bill of Materials)
Submit to Client manager to approve and proceed when funding is validated.
Create a construction package and submit to contractors or in house for RFQ.
Review quotes and choose both low bid and qualified contractor.
Work with all stakeholders to date and time work schedules.
Confirm workers are safety trained and conform to Client, FRA, and OSHA requirements.
Supervise and manage installations and conduct inspections.
Review work to ensure contractor met obligations under terms of contract.
Inform Client managers of status to approve invoices.
Coordinate with Client network engineering to connect, configure, and test equipment.
Review and confirm a successful installation were completed and working as designed prior to cutting over to production.
Assist with Break fix of IT infrastructure troubles or repairs.
Daily travel to location depending on scheduled or unscheduled work
Job Types: Full-time, Contract
Experience level:
5 years
Schedule:
8 hour shift
Work setting:
In-person
Experience:
Data Center: 5 years (Required)
WAN/LAN: 3 years (Required)
cabling: 3 years (Required)
Work Location: One location",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Spartan Technologies
3.4",3.4,"Kansas City, MO",Data Engineer I,"Spartan Technologies, Inc. - Kansas City, MO
Applicants Need to Know
100% remote in East Coast or Central US only
No Sponsorship
Contract to Hire
Hourly Rate: $45 on W2
Data Engineer I
The Data Engineer is responsible for processing structured and unstructured data, validating data quality, and developing and supporting data products for analytics. Brings together data from different sources into a common dataset with common KPIs and dimensions for stakeholders. Understands the analytics challenges that the marketing organization faces in their day-to-day work and partners with them to design viable data solutions.
Your Job
Gathers and processes raw, structured, semi-structured, and unstructured data using batch and real-time data processing frameworks.
Understand the existing source system data models and perform end to end data validation to maintain accuracy.
Implements and optimizes data solutions in enterprise data warehouses and big data repositories.
Ensures data quality and implements tools and frameworks for automating the identification of data quality issues.
Develop re-useable processes that can be leveraged and standardized for multiple instances
Works with internal and external data providers on data validation providing feedback and making customized changes to data feeds and data mappings.
Recommends improvements to processes, technology, and interfaces that improve the effectiveness of the team and reduce technical debt.
Ensure performance by monitoring existing workflows to ensure they are working correctly and that they reconcile with the source.
Design and develop ETL workflows and datasets to be used in extracts by our BI reporting tool Tableau.
Will write complex SQL queries with multiple joins to automate/manipulate these reporting extracts
Works in partnership with marketing stakeholders concerning the accuracy of data and efficiency of processes
Qualifications
Working experience with data modelling, data access, schemas, and data storage techniques within Snowflake
Working experience with batch and real-time data processing frameworks
Working experience working with relational databases such as SQL
Working experience with business intelligence tools and platforms
Working experience with data quality tools
Experience with ETL processes and tools
Desirable:
Working experience with Snowflake
Working experience with Google Analytics BigQuery
Working Experience with Tableau Server
Working experience with SQL Server Integration Services",$45.00 /hr (est.),1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$5 to $25 million (USD)
"AbleTo
3.7",3.7,"New York, NY",Data Engineer,"About AbleTo
Join our mission-driven organization, where your work matters and a diversity of ideas and backgrounds are welcomed. AbleTo is the leading provider of high quality, technology-enabled behavioral health care. We believe that everyone deserves access to high-quality care and offer a suite of technology-enabled services to empower people to lead better lives through better mental health. A proprietary platform connects individuals with AbleTo highly trained licensed providers who deliver weekly sessions by phone or video supported by an integrated digital experience. Members also have access to mental health coaches, and digital support programs. Our outcomes-focused approach is proven to improve both behavioral and physical health and lower medical costs.

Overview
Data is integral to nearly everything that we do at AbleTo, and as a result the data team plays a key role at our company. As a Data Engineer at AbleTo, you will grow and develop your skills as a data engineer to build a world class data platform to support our mental health care mission.

What You Will Do
Build and maintain batch and real-time ETL pipelines in a Google Cloud Platform architecture (BigQuery, Dataproc, Firestore, Informatica, Python, etc.)
Identify code quality issues and implement tests to improve future processes.
Document orchestration work-flows and create run books
Implement data integrity tests to ensure we are ingesting accurate data.
Implement high-quality test-driven code.
Participate in daily team stand ups and other Agile ceremonies

Key Initiatives over the next 12 months
Data Mart client migration
Data Mart enhancements
Automation of manual day to day operational tasks
Decommission legacy pipelines

Who You Are:
1+ years of experience coding in Python.
Experience working with SQL (structured query language).
Familiarity with structuring and writing ETLs.
Experience working with Orchestration tools (Airflow) and Cloud Databases (BigQuery) is a plus.
Experience working in an Agile Development environment is a plus.
BS in Information Systems, Computer Science or related field is preferred.
You are excited to work with data!
Curiosity to dive deeply into issues and feel empowered to make a meaningful impact at a mission-driven company.
Committed to agile development and value delivery and solid engineering principles, as well as continuously improving our craft.
You are a collaborator. You build and maintain strong, productive working relationships with your customers and stakeholders.
You are empathetic and seek to understand each individual's diversity of background and experience contributes to making up a great team.
You drive to improve yourself and others.
You are excited about learning new languages, tools, techniques and technologies.

Why You Should Join Our Team:
We're proud to be a Great Place to Work-Certified™ company. We want you to show up and feel your best at work, and that means respecting your time outside of work. Our inclusive, flexible workspace prioritizes a work/life balance. We offer competitive salaries, comprehensive health benefits (for full-time employees), and professional perks such as 401K matching, fully funded HRA, and generous time off, including mental health days because your well-being is important to us.
At AbleTo, we're empowering people to get better and stay better. Want to join us? Take the next step in your career by applying for this role today.
The salary range is $40,000 to $90,000. Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, AbleTo offers benefits such as a comprehensive benefits package, incentive and recognition programs, and 401k contribution (all benefits are subject to eligibility requirements).

#LI-Remote
Follow AbleTo on LinkedIn, Twitter, and Instagram!
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.
AbleTo is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or veteran status. AbleTo is an E-Verify company.","$65,000 /yr (est.)",501 to 1000 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2008,$25 to $100 million (USD)
"PrizePicks
4.8",4.8,"Atlanta, GA",Data Engineer,"At PrizePicks, we are the fastest growing sports company in North America, as recognized by Inc. 5000. As the leading platform for Daily Fantasy Sports, we cover a diverse range of sports leagues, including the NFL, NBA, and Esports titles like League of Legends and CS : GO. Our team of over 200 employees thrives in an inclusive culture that values individuals from diverse backgrounds, regardless of their level of sports fandom. Ready to reimagine the DFS industry together?
Our Analytics Team is responsible for building and maintaining analytics tools and workflows to support the PrizePicks business. By developing, maintaining, and testing data generation infrastructures, you will enable the PrizePicks business to make smarter, better, and faster data-driven decisions.
What you'll do:
Create, maintain and orchestrate optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Improve and streamline data systems to drive innovation within Prizepicks
What you have:
3+ years of experience building frameworks for data ingestion pipelines but real time and batch using data modeling, ETL/ELT processes
Stellar SQL skills, experience building DBT pipelines in production and know your way around structured, semi-structured and unstructured data.
Have built and optimized 'big data' data pipelines, architectures and data sets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Experience with Python and other object oriented scripting languages
Cloud experience: AWS, EC2, EMR, RDS, Redshift, GCP, etc..
Not required but would be awesome if you have experience with any of the following data pipeline and workflow tools: Azkaban, Luigi, Airflow, Prefect, etc.
Where you'll live:
Anywhere in the US is fine (we are based in Atlanta, GA)
Benefits you'll receive:
In addition to your great compensation package, company subsidized medical/dental/vision coverage plans and matching 401(k), we'll shower you with perks including:
Break room with ping pong, endless snacks and in-office lunch once a week
Unlimited PTO to encourage a healthy work/life balance (2 week min required!)
Modern work schedule focused on getting the job done, not hours clocked
Workplace flexibility
Company and team outings, we encourage a tight-knit workplace
Generous Maternity AND Paternity leave (16 weeks!)
Annual bonus & stock options
Wellness program
Company equipment provided (Windows & Mac options)
Annual performance reviews with opportunity for growth and career development
#LI-REMOTE

You must be authorized to work for any employer in the U.S. We are unable to sponsor or take over sponsorship or an employment Visa at this time.
PrizePicks is an Equal Opportunity Employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.","$97,814 /yr (est.)",1 to 50 Employees,Company - Private,"Arts, Entertainment & Recreation",Sports & Recreation,2015,Unknown / Non-Applicable
"Southern Glazer’s Wine and Spirits
3.6",3.6,"Dallas, TX",Data Engineer,"What You Need To Know
Open the door to a groundbreaking tech career with an industry leader. Southern Glazer’s Wine & Spirits is North America’s preeminent wine and spirits distributor, as well as a family-owned, privately held company with a 50+ year legacy of success. To create a new era in alcohol beverage sales and service, we’re heavily invested in the most transformative new technologies – and the most brilliant tech professionals. Southern Glazer’s was named by Newsweek as a Most Loved Workplace and is included on the Forbes lists for Largest Private Companies and Best Employers for Diversity.
As a full-time employee, you can choose from a full menu of our Top Shelf Benefits, including comprehensive medical and prescription drug coverage, dental and vision plans, tax-saving Flexible Spending Accounts, disability coverage, life insurance plans, and a 401(k) plan. We also offer tuition reimbursement, a wellness program, parental leave, vacation accrual, paid sick leave, and more.
We offer continuous learning and career growth in a fast-paced environment where you are respected, your voice is heard, and technology is part of our strategy for success. If you’re looking to fill your glass with opportunity, come join our FAMILY.
Overview
The Data Engineer's role is to design, develop, maintain and enhance interfaces and connectivity to the Data Warehouse ecosystem by coding with a technical language to meet business requirements and business objectives. This can include taking technical specifications and developing an application or integration of data between applications, testing, as well as, completing the appropriate technical documentation. The Data Engineer will use best practices in software development and adhere to SGWS development standards, as well as, focus on quality and innovation. The Data Engineer may also be responsible for delivering support to end users in the organization for specific code, including troubleshooting code.
Specialized Skills and Technologies
Strong PL/SQL skills
Experience in ETL Tools (Preferrable Informatica)
Data Warehouse techniques will be a plus
Experience in cloud platforms like Azure or AWS will be a plus
Knowledge of UNIX/Linux, shell scripting, Python will be a plus
Experience developing Application Programming Interfaces (API's) will be a plus
Experience in Hadoop will be a plus
Primary Responsibilities
Design, develop, implement, and support software applications
Drive technical validity of solution.
Develop user documentation as well as in-code documentation to explain designs and participate/support user training
Structure requirements to facilitate automation of acceptance tests
In conjunction with Data Management Group, develop routines and procedures that provide data quality checks and balances on data delivery/ingestion
Collaborate across the BI / Analytics, Data Management Group, Enterprise Insights and Analytics teams to establish standards, reusable data models and best practices for delivery/ingestion of data from/to Data Warehouse - This includes Publish/Subscription and API options
Obtain any certifications needed to effectively support applications in scope
Support the development of business and technical process documentation and training materials
Structure requirements to facilitate automation of acceptance tests
Provide support for software applications under area of responsibility
Drive Behavior-Driven-Design (BDD) process
Perform other job-related duties as assigned
Minimum Qualifications
Bachelor’s Degree or a combination of work experience and education
Knowledge in application and software development
Knowledge of software design and programming principles
Proficient oral and written communication skills, ability to influence outcomes, and strong attention to detail
Strong analytical, mathematic, and problem-solving skills
Strong team player with ability to demonstrate Agile delivery values working both within a team and working independently
Strategic thinker – can develop a plan to meet a long-term objective
Agile Delivery Values
Openness – Team and stakeholders agree to be open about all work and challenges
Commitment – Personally commit to achieving the goals of the team
Respect – Respect your team members to be capable and independent
Courage – You have courage to do the right thing and work on tough problems
Focus – Everyone focus on the work in the sprint and the goal of the scrum team. Rise and fall as a team
Physical Demands
Physical demands include a considerable amount of time sitting and typing/keyboarding, using a computer (e.g., keyboard, mouse, and monitor), or mobile device
Physical demands with activity or condition may occasionally include walking, bending, reaching, standing, squatting, and stooping
May require occasional lifting/lowering, pushing, carrying, or pulling up to 20lbs
EEO Statement
Southern Glazer's Wine and Spirits, an Affirmative Action/EEO employer, prohibits discrimination and harassment of any type and provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. Southern Glazer's Wine and Spirits provides competitive compensation based on estimated performance level consistent with the past relevant experience, knowledge, skills, abilities and education of employees. Unless otherwise expressly stated, any pay ranges posted here are estimates from outside of Southern Glazer's Wine and Spirits and do not reflect Southern Glazer's pay bands or ranges.","$97,837 /yr (est.)",10000+ Employees,Company - Private,Retail & Wholesale,Wholesale,1968,$10+ billion (USD)
"Kaizen Analytix
3.9",3.9,"Dallas, TX",Cloud Data Engineer: AWS,"Cloud Data Engineer: AWS
Kaizen Analytix LLC, an analytics services company, is seeking a qualified Cloud Data Engineer Contractor to support data science or data analytics projects. Responsibilities, capabilities for success, and job requirements are below. This role requires an estimated 40 hours/week commitment for approximately 24 weeks.
Responsibilities:
Candidate will be responsible for supporting a data science or data analytics project.
Work with client IT to understand access and use of source-of-truth data systems and downstream data sources.
Work with data scientist or visualization experts to determine data requirements such as structure, latency, availability.
Work with IT Security to assess risk of using sensitive data and how these data can be used throughout the project life cycle.
Develop an as-is and to-be state for data architecture.
Capabilities for Success:
In addition to key responsibilities, Candidate should demonstrate an ability to…
Work with the project manager and the business stakeholders to define upstream requirements for data.
Contribute to estimating input and time required for data engineering development tasks.
Contribute to client demonstrations of solution or presentations on architecture.
Proactively raise risks and issues to internal project manager
Work with data scientist or analysts to proactively assist in data concerns
Job Requirements:
Education: Bachelor's degree required
Extensive Experience in Data Engineering – consuming, wrangling, validating, developing pipelines for data.
Extensive Experience with AWS
Must have Solutions Architect Certification
Strong knowledge of databases, ETL concepts, SQL, and NoSQL sources.
Experience in: Python (data packages such as Pandas, PySpark, SQL Alchemy, pydantic, etc.) and SQL.
Position Type: 1099 Contractor",$57.50 /hr (est.),1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,#N/A,Unknown / Non-Applicable
"PRIMUS Global Services, Inc
4.1",4.1,"Minneapolis, MN","Data Engineer – Hadoop, Python – REMOTE WORK 43198","We have an immediate long-term opportunity with one of our key clients for a position of Data Engineer, to work on a remote basis.

The ideal applicant must have extensive experience with Python, Hadoop, Hive and SQL.

**ALL successful candidates for this position are required to work directly for PRIMUS. No agencies please**

For immediate consideration, please contact:

Tejaswini
PRIMUS Global Services
Direct: 972-798-2662
Desk: 972-753-6500 Ext: 204
Email: jobs@primusglobal.com","$91,633 /yr (est.)",501 to 1000 Employees,Private Practice / Firm,Information Technology,Information Technology Support Services,2002,$5 to $25 million (USD)
"Unilever
4.1",4.1,Remote,Health & Wellbeing Data Engineer,"Health & Well Data Engineer
Remote – USA; Los Angeles
Health & Wellness is a strategic Unilever global business unit established to capture the growth opportunity in the €140bn global consumer health segment defined by Vitamins, Minerals, and Supplements (VMS) product category. Our ambition is to build a €5bn business globally; and be a global top 3 player within this space.
As of today, our brands include Equilibra in Italy; OLLY, SmartyPants, Liquid IV, Onnit, Welly and Nutrafol in the US with ~€1.4bn in turnover. The H&W data and analytics team operates across all these brands in one common data function with the aim of helping these brands to grow through smarter, faster, and better data driven decision making. The data team within H&W was set up in May 2022 and is looking to expand. This is a fantastic opportunity to shape the future of data in a global business unit with the latest technologies in the cloud.
What you will do:
collaborate with other engineering and business teams within H&W and Unilever to solve complex challenges using data.
drive forward best practice by building out data frameworks and design patterns to be utilized across the H&W data infrastructure.
build production ready distributed ETL data pipelines from a wide range of different sources (APIs, flat files, databases, ERP systems etc.)
build performant and reliable data models to democratize the use of data across H&W
build integrations to facilitate the use of other technologies in H&W ecosystem (e.g Kinnaxis, Anaplan)
review and deploy code from other team members as part of a DevOps process, providing coaching and mentoring to junior developers.
manage access within the environments to ensure data security protocols are being met.
contribute to architectural and governance decision marking within the H&W data ecosystem.
Who you are:
Passionate about all things data
Entrepreneurial Self-starter with the ability to thrive in a fast paced start up environment.
Fast learner with the ability to pick up new technologies quickly.
Creative problem solver who thinks outside the box
What you will bring:
3-5 years’ Data engineering experience with strong pyspark and SQL skills
1+ year Distributed computing experience [databricks/Splunk]
1+ year experience working with data in cloud environments [GCP/Azure/AWS]
Bachelor’s degree required.
Experience with streaming workloads is a plus
Experience working within a delta lakehouse in databricks is a plus
Tech Stack:
Cloud agnostic: Databricks (pyspark, scala, SQL)
Azure: Azure Data Factory, Azure Logic Apps, Azure Data Lake Storage (ADLS), Azure Blob Storage, Azure Machine Learning, PBI (data modelling, DAX)
GCP: Google Big Query, Google Cloud Storage
AWS: S3
Pay: The pay range for this position is $83,200 - $124,700. Unilever takes into consideration a wide range of factors that are utilized in making compensation decisions including, but not limited to, skill sets, experience and training, licensure and certifications, qualifications and education, and other business and organizational needs. Bonus: This position is bonus eligible. Long-Term Incentive (LTI): This position is LTI eligible. Benefits: Unilever employees are eligible to participate in our benefits plan. Should the employee choose to participate, they can choose from a range of benefits to include, but is not limited to, health insurance (including prescription drug, dental, and vision coverage), retirement savings benefits, life insurance and disability benefits, parental leave, sick leave, paid vacation, and holidays, as well as access to numerous voluntary benefits. Any coverages for health insurance and retirement benefits will be in accordance with the terms and conditions of the applicable plans and associated governing plan documents.
-
Unilever is an organization committed to diversity and inclusion to drive our business results and create a better future every day for our diverse employees, global consumers, partners, and communities. We believe a diverse workforce allows us to match our growth ambitions and drive inclusion across the business. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, age, national origin, or protected veteran status and will not be discriminated against on the basis of disability.

Employment is subject to verification of pre-screening tests, which may include drug screening, background check, credit check and DMV check.

If you are an individual with a disability in need of assistance at any time during our recruitment process, please contact us at
NA.Accommodations@unilever.com
. Please note: This email is reserved for individuals with disabilities in need of assistance and is not a means of inquiry about positions or application statuses.
#LI-Remote","$103,950 /yr (est.)",10000+ Employees,Company - Public,Manufacturing,Consumer Product Manufacturing,1872,$10+ billion (USD)
"Caterpillar
4.0",4.0,"Peoria, IL","Data Engineer, Cat Digital","Career Area:
Digital
Job Description:
Cat Digital is the digital and technology arm of Caterpillar Inc., responsible for bringing world class digital capabilities to our products and services. With almost one million connected assets worldwide, we're focused on using IoT and other data, technology, advanced analytics and AI capabilities to help our customers build a better world.
This is position is in Connected Data Quality team in Cat Digital. The team is responsible for building tools, dashboards and processes to enable (E2E) telemetry data quality monitoring, finding source of quality issues and work with process partners to resolve the problems at source.

Job Duties: As a Data Engineer you will be responsible for building scalable, high performance infrastructure and data driven and predictive analytics applications that provide actionable insights across all Caterpillar businesses. The position will be part of Caterpillar’s fast-moving and engineering-driven digital organization with highly motivated engineers who tackle challenges and problems that are critical to realizing significant business outcomes. Data engineers work with data scientists, business analysts, and others as part of a team that assembles large, complex data sets that provide competitive advantage.
Build infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Design, develop, and maintain performant and scalable applications
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability
Perform debugging, troubleshooting, modifications and unit testing of integration solutions
Operationalize the developed jobs and processes and processes.
Create databases and infrastructure to processing data at scale
Create solutions and methods to monitor systems and solutions
Automate code testing and pipelines
Engage directly with business partners to participate in design and development of data integration/transformation solutions per functional requirements.
Work in a scaled Agile environment accountable to deliver results in sprints.
Engage and actively seek industry perspectives through external engagements such as hackathons, peer groups, etc.
Generate, prepare, and catalog APIs
Work with UI Designer to build user interfaces per design specifications
Employee is also responsible for performing other job duties as assigned by Caterpillar management from time to time.
Required Skills:
BS or MS degree in computer science or computer engineering
5+ years of software development experience or at least 3 year of experience with master’s degree in object-oriented/object function scripting languages: Python, Java, Javascript, C++, Scala, etc.
3+ years of Python coding experience
Understanding of data structures, algorithms, profiling & optimization.
Understanding of SQL, ETL design, and data modeling techniques
Top candidates will also have:
2+ years of experience developing, deploying, and maintaining software in AWS cloud and working with AWS services: S3, DynamoDB, RDS, SageMaker, ECS, EMR, Lambda, Athena, AWS Glue, CloudFormation
2+ years of experience in developing scripts, procedures in snowflake.
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
Experience with using CI/CD tools such as Jenkins, GoCD, Azure Devops etc.
Experience with automated build automation tools (Maven, etc.).
Advanced level of experience with object oriented programming, data structures and algorithms.
Knowledge of enterprise data sources and uses
Working within an Agile framework (ideally Scrum)
#LI
#B
Relocation is available for this position.Visa sponsorship available for eligible applicants.
EEO/AA Employer. All qualified individuals - Including minorities, females, veterans and individuals with disabilities - are encouraged to apply.
Not ready to apply? Submit your information to our Talent Network here .","$86,643 /yr (est.)",10000+ Employees,Company - Public,Manufacturing,Machinery Manufacturing,1925,$10+ billion (USD)
"McDonald's Corporation
3.5",3.5,"Chicago, IL","Data Engineer, Global Data, CRM","Company Description

McDonald’s evolving Accelerating the Arches growth strategy puts our customers and people first, and leverages our competitive advantages to strengthen our brand. We are recognized on lists like Fortune’s Most Admired Companies and Fast Company’s Most Innovative Companies.
Doubling Down on the 4Ds (Delivery, Digital, Drive Thru, and Development)
Our growth pillars emphasize the important role technology plays as the leading, global omni-channel restaurant brand. Technology enables the organization through digital technology, and improving the customer, crew and employee experience each and every day.
Global Technology forging the way
Leading the digitization of our business is the Technology organization made up of intrapreneurs who build industry defining tech using the latest innovations and platforms, like AI and edge computing to deliver on the next set of cutting-edge opportunities for the business. At McDonald’s you get to solve technology innovation challenges at an incredible scale, and work across global teams who are always hungry for a challenge. This provides access to exciting career paths for technologists. It’s bonus points when you get to see your family and friends use the tech you build at their favorite McD restaurant.

Job Description

We are seeking a highly motivated individual for the Data Engineering Manager for the Marketer Customer Data Platform team on Global Data and Architecture. This role will manage, develop, and lead the execution and implementation cloud-based data delivery systems for the MCD Customer Data Platform. This role will specialize in leading and managing the engineering teams in both onshore and offshore locations, as well as working with the relevant product management teams on overall product direction and implementation of the product vision.
What you will do:
Lead and manage the day-to-day operations for the CRM Data Engineering team with both, onshore and offshore, resources to with a focus on CDP/ CRM customer data technologies
Independently drive efforts in marketing technology and automation, collaborating with key stakeholders and business units. Must be able to find marketing solutions alongside building a long-term strategic vision of a marketing ecosystem across a highly technical, complex data stack
Participate in activities of the data engineering team on the solution, design, architect, implementation, support, and monitoring of CRM & CDP related data products
Develop and implement monitoring and remediation structure and applications for critical data flows
Continuously develop and deploy solutions to streamline and enhance the processes, logic, and tactics around customer cohort identification, and targeting via corporate CRM, marketing, and advertising systems. Additionally, develop the overall best practice framework for use by
Gather business and functional requirements and translate into robust, scalable, and user-friendly solutions

Qualifications
Bachelor’s degree in Information Technology or related technical fields such as computer science, marketing tech, mathematics, engineering, etc.
Minimum 2-5 years of relevant work experience in data analytics, engineering, computer science, mathematics or similar logic based fields
Minimum 2 years hands on experience with complex marketing automation tech stacks, leveraging CDP tools such as mParticle, Segment, and Tealium
Strong analytics skills, root cause analysis experiences on internal and external data and processes to answer business questions and improve application performance
Comfortable partnering and working with a wide range of stakeholders at various levels of management and leadership
Ability to focus on details as well as big picture
Competence and comfort focusing on specific detail engineering questions, as well as “big picture” questions
Ability to leverage critical thinking, problem-solving, and analytical skills to improve user experience, adoption, and outcomes
Curious, task-oriented self-starter with a strong bias for action; Proven track record of “getting things done”
Self-motivated with ability to set priorities and mentor others in a performance driven environment.
Passion for technology and innovation & uncompromising attention to detail & passion for problem solving
Desired Skills

Extensive experience working with Agile methodologies and expertise in production deployment processes and continuous deployment best practices.
1-2 years of hands-on experience with Braze and/or similar market automation delivery platforms
1-2 years of hands on experience with marketing automation and technology experience, understanding key components of marketing technology stacks, including CDP tools such as mParticle, Tealium, or Segment
Experience with clickstream tracking architecture implementations, and knowledge of Adobe Analytics and/or Google Analytics
Demonstrated ability to problem solve in complex technology solutioning
Experience developing Confluence pages and working with Jira
Demonstrable knowledge of multiple programming languages leveraged in martech/adtech stacks – javascript, ruby (liquid logic), html/css, python etc.

Additional Information

McDonald’s is committed to providing qualified individuals with disabilities reasonable accommodations to perform the essential functions of their jobs. Additionally, if you (or another applicant of whom you are aware) require assistance accessing or reading this job posting or otherwise seek assistance in the application process, please contact recruiting.supportteam@us.mcd.com
McDonald’s provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to sex, sex stereotyping, pregnancy (including pregnancy, childbirth, and medical conditions related to pregnancy, childbirth, or breastfeeding), race, color, religion, ancestry or national origin, age, disability status, medical condition, marital status, sexual orientation, gender, gender identity, gender expression, transgender status, protected military or veteran status, citizenship status, genetic information, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.
Nothing in this job posting or description should be construed as an offer or guarantee of employment.","$104,720 /yr (est.)",10000+ Employees,Company - Public,Restaurants & Food Service,Restaurants & Cafes,1955,$10+ billion (USD)
Kanini,#N/A,Remote,Data Engineer,"About Kanini
Kanini provides Agile Software Development, Cloud Computing, Data Science, and Location Intelligence services to public and private organizations. We have successfully served our clients in government, finance, transportation, utility, and software industries since 2003.
Why you should join
Working at Kanini is flexible and personal. We are a highly motivated, collaborative team experimenting with the latest technologies. We are committed to everyone having a healthy work/life balance, and we provide extensive mentorship and training resources to help you succeed.
Kanini is looking for a Big Data Engineer who has a deep experience in Data Engineering, AWS, Python, Data Lakes.
Required Skills
At least 10 years software development experience
At least 5 years leading at least one Scrum team of data engineers building data-intensive products with a modern tech stack.
Significant experience with big data ETL pipeline development with Spark, Hive, and related technologies
Significant experience with a general-purpose programming language such as Python, Scala, or Java
Experience with Spark framework and related tools (PySpark, Scala, SparkR, Spark SQL, Spark UI)
Experience with Hadoop ecosystem using HDFS, ADLS Gen2, or AWS S3
Experience with data visualization development using Python, Tableau, or PowerBI
Experience with Azure, AWS or GCP
Solid understanding of performance tuning concepts for relational and distributed database systems
Familiarity with distributed programming, big data concepts, and cloud computing
Education Qualifications
Bachelor’s degree in computer science/Engineering or Technology related field or possess equivalent work experience.
Preferred Qualifications
Cloud certifications from Azure, AWS or GCP
Big data, data engineering or data science certifications from recognized vendors such as Databricks & Cloudera
Kanini Software Solutions, Inc. does not discriminate in employment matters based on race, gender, religion, age, national origin, citizenship, veteran status, family status, disability status, or any other protected class. We support workplace diversity. If you have a disability, please let us know if there is anything we can do to improve the interview process for you; we’re happy to accommodate.
Kanini Software Solutions, Inc., 25 Century Blvd., Ste. 602, Nashville, TN 37214
Job Types: Full-time, Contract
Pay: $60.00 - $70.00 per hour
Work Location: Remote",$65.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"ProIT Inc.
5.0",5.0,"Bellevue, WA",Azure Data Engineer,"Data Engineering experience primarily on Spark. Someone who has worked on Azure cloud with knowledge on Azure DEVOPS (CI/CD & Infrastructure as a code), ADF, ADW & Power BI. Apache Nifi will be good to have.Power BI: Understand business requirements to set functional specifications for reporting applications Build automated reports and dashboards with the help of Power BI reporting tool Be experienced in tools and systems on MS SQL Server BI Stack, including SSRS and TSQL, Power Query, MDX, PowerBI, and DAX Be able to quickly shape data into reporting and analytics solutions Have knowledge of database fundamentals such as multidimensional database design, relational database design, and more Create functional reporting Study, analyze and understand business requirements in context to business intelligence. Design and map data models to shift raw data into meaningful insights. Utilize Power BI to build interactive and visually appealing dashboards and reports. Spot key performance indicators with apt objectives Run DAX queries and functions in Power BI Should have an edge over making DAX queries in Power BI desktop. Developing visual reports, KPI scorecards, and dashboards using Power BI desktop. Connecting data sources, importing data, and transforming data for Business intelligence.
Spark ,Azure DEVOPS (CI/CD & Infrastructure as a code), ADF, ADW & Power
Job Type: Full-time
Pay: $100,154.64 - $104,132.47 per year
Ability to commute/relocate:
Bellevue, WA 98004: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Hybrid remote in Bellevue, WA 98004","$102,144 /yr (est.)",51 to 200 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Rite Pros
3.4",3.4,"Portland, ME",Big Data Engineer,"Big Data Engineer with Bachelor’s degree in Computer Science, Computer Information Systems, Information Technology, or a combination of education and experience equating to the U.S. equivalent of a Bachelor’s degree in one of the aforementioned subjects.
Job Duties:
Define and manage the data architecture framework, standards, and principles, including modeling, metadata, security, reference, and master data.
Define reference architecture, and models to provide a common vocabulary, reusable components, and industry best practices.
Create solution frameworks integrating large or complex data sets.
Lead all data modeling efforts within DataBricks, including the design of data structures and the identification of business transformation logic.
Analyzing and translating business requirements into conceptual and fully detailed logical data models.
Creating logical data models based on existing applications and databases.
Working with business architects, and data stewards to capture business requirements in a Logical Data Model.
Transform the logical representation of the model into a physical representation and work with data engineering team to instantiate and manage the data.
Contribute to assessment of appropriate data platform(s) for solutioning efforts.
Collaborate with the BI and Analytics teams on creating the optimized, reusable semantic model, complete with metadata and lineage information.
Define and govern data modeling and design standards, tools, best practices, and related development for enterprise data models
Maintain the metadata - the ""data about the data"" describing the data model, its structure, and semantics
Recommend data and design patterns for common business functionality
Assist in multi-phase roadmaps within and across functional teams
Provide insight and technology recommendations that result in enterprise solutions and architectural risk mitigation.
Contribute to innovation strategies by exploring, investigating, recommending, benchmarking and implementing new data centric technologies for the platform.
Identify, design, and implement internal process improvements such as automation of manual processes, optimization of data delivery and re-design of existing infrastructure for greater scalability.
Build the infrastructure and processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and/or other data technologies.
Create and maintain data-related documentation (e.g. data dictionaries, data maps, data models etc.)
Work with data engineering and analytics experts to strive for greater functionality in our data systems and incorporation of industry best practices
Work experience / Technologies required for the position :
Experience building and optimizing Big Data, Data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, meta data, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected data sets.
Working knowledge of message queuing, stream processing, and highly scalable Big Data, data stores.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience in a Data Engineer or similar roles, who has attained a Bachelor’s degree in Bachelor’s degree in Engineering, Computer Science, Information Technology, related field or equivalent work experience.
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Work location is Portland, ME with required travel to client locations throughout USA.

Rite Pros is an equal opportunity employer (EOE).

Please Mail Resumes to:
Rite Pros, Inc.
565 Congress St, Suite # 305
Portland, ME 04101.
Email: resumes@ritepros.com","$97,894 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Less than $1 million (USD)
"Acrisure Technology Group
3.9",3.9,"Austin, TX",Data Engineer,"Data Engineer
Hybrid Position (3 days per week average in Downtown Austin, TX or Grand Rapids, MI office)
Note: This is a full-time, in-house position. We do not offer C2C or C2H employment and are not able to sponsor visas for this position.
Acrisure Technology Group (ATG) is a fast-paced, AI-driven team building innovative software to disrupt the $6T+ insurance industry. Our mission is to help the world share its risk more intelligently to power a more vibrant economy. To do this, we are transforming insurance distribution and underwriting into a science.
At the core of our operating model is our technology: we're building the premier AI Factory in the world for risk and applying it at the center of Acrisure, a privately held company recognized as one of the world's top 10 insurance brokerages and the fastest growing insurance brokerage globally. By using the latest technology and advances in AI to push the boundaries of understanding risk, we are systematically converting data into predictions, insights, and choices, and we believe we can remove the constraints associated with scale, scope, and learning that have existed in the insurance industry for centuries.
We are a small team of extremely high-caliber engineers, technologists, and successful startup founders, with diverse backgrounds across industries and technologies. Our engineers have worked at large companies such as Google and Amazon, hedge funds such as Two Sigma and Jump Trading, and a variety of smaller startups that quickly grew such as Indeed, Bazaarvoice, RetailMeNot, and Vrbo.
The Role
The Business Intelligence team's mission is to unify data across the enterprise to optimize business decisions made at the strategic, tactical, and operational levels of the organization. We accomplish this by providing an enterprise data warehouse, data lake, reporting platform, and business processes that provide quality data, in a timely fashion, from any channel of the company and present them in such a manner as to maximize the value of that data for both internal and external customers.
The Data Engineer is responsible for designing and developing moderate to complex ETL processes required to populate a data lake and structured data warehouse which supply data for the machine learning, AI & BI teams. Responsibility includes working with a team of contracted developers as well as coaching and mentoring junior and mid-level developers. Ensuring high quality and best practices are maintained through the development cycle is key to this position.
You will interact with some of the top technologists on the planet. Our technology runs on Google Cloud and is configured with Kubernetes, leveraging various services in that environment. Our data storage layer includes BigQuery, BigTable, and Postgres. We code primarily in Kotlin, Python, Java, and JavaScript and make use of many frameworks, including Dataflow, Cloud AI Platform, KubeFlow, Spring, and React.
Here are some of the ways in which you'll achieve impact
Leverage established guidelines and custom designs to create complex ETL processes to meet the needs of the business
Develop from strategic and non-strategic data sources including data preparation/ETL and modeling for data visualizations in a self-service platform
Contribute to the definition and development of the overall reporting roadmap
Translate reporting requirements into reporting models, visualizations and reports by having a strong understanding of the enterprise architecture
Standardize reporting that helps generate efficiencies, optimization, and end user standards
Integrate dashboards and reports from a variety of sources, ensuring that they adhere to data quality, usability, and business rule standards
Independently determine methods and procedures for new or existing requirements and functionality
Work closely with analysts and data engineers to identify opportunities and assess improvements of our products and services
Contribute to workshops with the business user community to further their knowledge and use of the data ecosystem
Produce and maintain accurate project documentation
Collaborate with various data providers to resolve dashboard, reporting and data related issues
Perform Data Services reporting benchmarking, enhancements, optimizations, and platform analytics
Participate in the research, development, and adoption of trends in reporting and analytics
Mentor BI Developers and BI Analysts
Other projects as assigned in order to support necessary business goals across teams
You may be fit for this role if you have
Minimum 5 years required, particularly in an Azure environment with Azure Data Bricks, Azure Data Factory, Azure Data Lake
Minimum 5 years designing data warehouses, data modeling, and end-to-end ETL processes in a MS-SQL environment
Minimum 2 years developing machine learning models with Azure ML, ML Flow, BQML
Expert working knowledge of SQL, Python and Spark (and ideally PySpark) with a demonstrated ability to create ad-hoc SQL queries to analyze data, create prototypes, etc required.
Successfully delivered 2+ end to end projects – from Inception to Execution - in Data Engineering / Data Science / Data Integration as a Tech Senior/Principal
Ability to Analyze, summarize, and characterize large or small data sets with varying degrees of fidelity or quality, and identify and explain any insights or patterns within them.
Experience with multi-source data warehouses
Strong skills in in data analytics and reporting, particularly with Power BI
Experience with other cloud environments (GCS, AWS) a definite plus
Strong experience creating reports, dashboards, and/or summarizing large amounts of data into actionable intelligence to drive business decisions required
Strong understanding of core principles of data science and machine learning; experience developing solutions using related tools and libraries
Hands on experience building logical data models and physical data models and using tools like ER/Studio/Idera
Write SQL fluently, recognize and correct inefficient or error-prone SQL, and perform test-driven validation of SQL queries and their results
Proficient in writing Spark sql using complex syntax and logic like analytic functions etc.
Well versed in Data Lake & Delta Lake Concepts
Well versed in Databricks usage in dealing with Delta tables (external \ managed)
Well versed with Key Vault \ create & maintenance and usage of secrets in both Databricks & ADF
Should be knowledgeable in Stored procedures \ functions and be able to use them by ADF & Databricks as this is a widely used Practice internally
Familiar with DevOps process for Azure artifacts and database artifacts
Well versed with ADF concepts like chaining pipelines, passing parameters, using APIs for ADF & Databricks to perform various activities.
Experience creating and sharing standards, best practices, documentation, and reference examples for data warehouse, integration/ETL systems, and end user reporting
Apply disciplined approach to testing software and data, identifying data anomalies, and correcting both data errors and their root causes
Academics: Undergraduate degree preferred or equivalent experience along with a demonstrated desire for continuing education and improvement
Location: Austin, TX or Grand Rapids, MI
We are interested in every qualified candidate who is eligible to work in the United States. We are not able to sponsor visas for this position.","$96,659 /yr (est.)",5001 to 10000 Employees,Company - Private,Insurance,Insurance Agencies & Brokerages,2005,$1 to $5 billion (USD)
"INTELETECH GLOBAL INC
3.7",3.7,"Altamonte Springs, FL",Data Engineer,"Role: Data Engineer
Location: Florida
Type: Contract ( Only w2 )
Visa: Any
Experience level: Mid-level

The Role: · You'll be developing, deploying, and maintaining our production data pipeline which produces risk scores and patient reports vital to the workflows of doctors and care coordinators.· You'll be ensuring product deliverables are executed reliably and accurately on a regular basis.· You'll be managing and monitoring client interfaces to ensure timely delivery of data.

Qualifications :· Bachelor's Degree in computer science, physics, math, or a related field· Minimum 3+ years experience in data engineering in industrial or clinical settings· Experience in deploying data pipelines to production, including large-scale cloud deployments· Adaptability within a dynamic and collaborative environment· Commitment to improve processes and reduce inefficiencies· Deep curiosity to dive into the details of human research studies

We also appreciate if you have familiarity with the following:

Python
SQL
Cloud platforms (e.g. AWS)
Relational and no-SQL database systems
Multi-modal and sensor data","$83,235 /yr (est.)",1 to 50 Employees,Company - Private,Financial Services,Accounting & Tax,#N/A,Unknown / Non-Applicable
"The Bouqs Company
3.1",3.1,"Marina del Rey, CA",Data Analytics Engineer,"The role contributes to The Bouq’s mission of revolutionizing the way we commemorate life’s moments by connecting people to beautifully designed floral experiences and the responsible partners who create them by being a key member of the data team. As an Analytics Engineer, you will work closely with the Product, Engineering, and Data teams to build and maintain the data infrastructure needed to support our data and business needs. You will also be responsible for developing, optimizing, and maintaining the best-in-class data pipelines, data models, and ETL processes to ensure that data is accurate, reliable, and available to stakeholders in a timely manner. The Analytics Engineer will also serve as the liaison between Engineering and Analytics and will serve as an active member in both teams.
Responsibilities:
Lead the transfer of data modeling from legacy systems to DBT
Contribute to building the data modeling layer, which exposes clean, transformed data to the whole company for analytics
Build datasets in DBT (cloud) for data analysts to improve speed and accuracy for the team
Improve current processes, whether that includes modularizing and standardizing a piece of commonly used code
Design and develop new data pipelines and streaming processes that are highly available, scalable, and reliable
Develop review processes for new data models and take charge on implementing SQL standards for the team
Actively strive towards writing performant SQL rather than just SQL that works, while also ensuring the same SQL is easy to understand when new eyes look at it
Optimize data processing and flow within our Snowflake Data Warehouse
Document new datasets and pipelines and the reasoning/story behind their structure
Work closely with engineering to keep track of schema changes in the production database and adjust pipelines, as needed
Support existing data pipelines and systems in production
Apply software engineering best practices to analytics code such as version control and testing
Develop and communicate strong opinions about best practices in analytics
Help explore and evaluate new technologies
Qualifications:
4+ years of experience working within a data team, preferably as an Analyst/Data Engineer
Bachelor's degree in a quantitative field such as statistics, mathematics, economics, or computer science preferred
Strong SQL fluency in both DDL/DML and analytics (Snowflake experience is a plus)
Experience working with JSON, DBT or other data transformation tools
Experience working with an ETL tool such as Fivetran or Stitch
Knowledge of data structures and how to write performant SQL
Experience with ensuring data quality through testing, deltas, lineage, etc
Strong communication and critical thinking skills to deliver solutions that not only solve problems but also serve as tools we didn’t know we needed
Ability to transform raw data into intuitive datasets that serve as building blocks for analytics
Comfortable leading the growth of a data warehouse and maintaining it
Capable of working through uninformative assumptions and built-biases in datasets and are not stalled when data is not perfect/sparse
Compensation & Perks:
Competitive Base Salary Range of $120,000.00 - $180,000.00 USD + Equity Package
Health, Dental & Vision with 100% employee coverage
401k Matching
Three Weeks Paid Vacation
Discounts on The World’s Best Flowers (obviously!)
Work on cutting edge new technologies
About The Bouqs:
Our mission here at The Bouqs is to revolutionize the way we commemorate life’s moments by connecting people to beautifully designed flowers and the responsible partners who create them. Grounded in transparency, responsibility, and simplicity, we create genuine moments of emotional connection for our customers, build meaningful relationships with like-minded farmers and florists while empowering them to thrive, and eliminate unnecessary waste along the way.

Founded in 2012, The Bouqs is a venture-backed online floral retailer that delivers flowers fresh from eco-friendly, sustainable farms to doorsteps nationwide. Headquartered in Marina Del Rey, CA, The Bouqs connects farms and a curated network of artisan florists directly to consumers and disrupts the traditional supply chain by eliminating overhead costs like warehouses, importers, distributors, auctioneers and more. In turn, this model enables a superior product and redefines the experience and economics for both consumers and producers alike.

For more information, visit www.bouqs.com and follow the #BouqLove on Facebook, Instagram and Twitter.
The Bouqs is an Equal Opportunity Employer!","$150,000 /yr (est.)",51 to 200 Employees,Company - Private,Retail & Wholesale,Other Retail Stores,2012,$25 to $100 million (USD)
"Green Worldwide Shipping LLC
3.7",3.7,"Aurora, CO",GEM Data Engineer,"About Green
“Global Reach with a personal touch”, that’s the foundation on which four partners (and long-time friends) agreed to when they started Green Worldwide Shipping back in 2008. This ethos has allowed Green to Grow to a 200+ strong team with 15 offices across the US and growing.At green we are a family of innovative problem-solvers who communicate freely to present creative solutions in an environment of trust, respect, and integrity. We are dedicated to customer service, passionate about logistics, and committed to sustainable growth while inspiring others.
Why this role is special..
GEM (Green Engineered Management) is a technology solutions department that supports various commercial technology products, data management, and development for Green Worldwide Shipping.We are seeking a skilled and experienced Data Engineer to join our dynamic team. As a Data Engineer, you will be responsible for designing, developing, and maintaining our data infrastructure and various technology systems to support efficient data processing and analysis. Your expertise in SQL, CargoWise, database management, Altova Flowforce Server, Altova Mapforce ETL software, Python and C#, along with experience in EDI/API connection protocols integration, will be instrumental in achieving success in this role.
How you will be successful…
Design, develop, and maintain data pipelines and ETL processes to extract, transform, and load data from various sources into our databases using Altova Mapforce ETL software.
Collaborate with cross-functional teams to understand data requirements and provide optimal data solutions.
Develop and optimize SQL queries to retrieve and manipulate data from databases, ensuring efficient data retrieval and storage.
Implement and integrate EDI/API connection protocols to enable seamless data exchange between internal and external systems.
Monitor and maintain the performance, availability, and integrity of the databases and data infrastructure.
Support for reporting, document modification, workflow and database support for internal ERP.
Troubleshoot and resolve data-related issues, including data quality and data integration problems.
Implement data security measures and ensure compliance with data privacy regulations.
Stay updated with industry best practices and emerging technologies in data engineering and incorporate them into the data infrastructure.
Provide technical guidance and support to other team members on data-related projects.
Collaborate with data analysts and data scientists to understand data requirements and support their data exploration and analysis needs.
Create and maintain documentation related to data pipelines, database schemas, and data transformation processes.
What you will bring….
Bachelor's degree in Computer Science, Information Systems, or a related field.
Preferred experience in CargoWise (or related ERP) with reporting, document modification, workflow and database support
Proven experience working with SQL and databases, preferably MySQL.
Prefer strong knowledge and hands-on experience with Altova Flowforce Server and Altova Mapforce ETL software, or related applications.
Experience in integrating and working with EDI/API connection protocols.
Proficiency in Python and/or C# (optional but preferred).
Familiarity with data modeling concepts and techniques.
Experience with data warehousing concepts and technologies.
Excellent problem-solving and analytical skills.
Strong attention to detail and the ability to work in a fast-paced environment.
Effective communication skills to collaborate with cross-functional teams.
Ability to work independently and prioritize tasks effectively.
Preferred Skills:
*
Experience with cloud-based data platforms (e.g., AWS, Azure, Google Cloud Platform).
Knowledge of big data technologies (e.g., Hadoop, Spark) and distributed computing.
Familiarity with data visualization tools (e.g., Tableau, Power BI).
Experience with version control systems (e.g., Git).
Understanding of Agile development methodologies.
What we offer:
Employee High Deductible Health Plan (HDHP) currently paid for 100% by Green.
$1,800 contributed to employee HSA Account, if on HDHP.
Employee Dental provided by Green.
Option to purchase disability insurance.
Paid life insurance and option to buy-up.
401(K) and company match.
Parental Leave Program.
Monthly recognition points to give/receive, which can be converted to gift cards.
Tuition Reimbursement Program.
Weekly fruit baskets in office locations
A tree planted for every new employee to Trees.org
Hybrid work schedule.
Wellbeing Program.
Training and Development.
Access to Mentorship Program.
Green Worldwide Shipping, LLC (Green) is committed to the full inclusion of all qualified individuals. As part of this commitment, Green will ensure that persons with disabilities are provided reasonable accommodations. If reasonable accommodation is needed to participate in the job application or interview process and/or to perform essential job functions, please contact Faye Hughes, Head of People and Culture at faye.hughes@greenworldwide.com.Green is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.
Requirements
Bachelor's degree in Computer Science, Information Systems, or a related field.
Preferred experience in CargoWise (or related ERP) with reporting, document modification, workflow and database support
Proven experience working with SQL and databases, preferably MySQL.
Strong knowledge and hands-on experience with Altova Flowforce Server and Altova Mapforce ETL software, or related applications.
Experience in integrating and working with EDI/API connection protocols.
Proficiency in Python and/or C# (optional but preferred).
Familiarity with data modeling concepts and techniques.
Experience with data warehousing concepts and technologies.
Excellent problem-solving and analytical skills.
Strong attention to detail and the ability to work in a fast-paced environment.
Effective communication skills to collaborate with cross-functional teams.
Ability to work independently and prioritize tasks effectively.
Preferred Skills:
Experience with cloud-based data platforms (e.g., AWS, Azure, Google Cloud Platform).
Knowledge of big data technologies (e.g., Hadoop, Spark) and distributed computing.
Familiarity with data visualization tools (e.g., Tableau, Power BI).
Experience with version control systems (e.g., Git).
Understanding of Agile development methodologies.
Summary
GEM (Green Engineered Management) is a technology solutions department that supports various commercial technology products, data management, and development for Green Worldwide Shipping. We are seeking a skilled and experienced Data Engineer to join our dynamic team. As a Data Engineer, you will be responsible for designing, developing, and maintaining our data infrastructure and various technology systems to support efficient data processing and analysis. Your expertise in SQL, CargoWise, database management, Altova Flowforce Server, Altova Mapforce ETL software, Python and C#, along with experience in EDI/API connection protocols integration, will be instrumental in achieving success in this role.
Job Type: Full-time
Pay: $60,000.00 - $75,000.00 per year
Benefits:
401(k) matching
Dental insurance
Employee discount
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Tuition reimbursement
Vision insurance
Experience level:
4 years
Schedule:
Monday to Friday
Ability to commute/relocate:
Aurora, CO 80014: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL/MySQL (Required)
ETL Software (Preferred)
Data warehouse (Preferred)
Python/C# (Preferred)
Work Location: Hybrid remote in Aurora, CO 80014","$67,500 /yr (est.)",51 to 200 Employees,Company - Private,Transportation & Logistics,Shipping & Trucking,2008,$25 to $100 million (USD)
"FACEBOOK APP
5.0",5.0,Remote,Azure Data Engineer,"Responsibilities:
Create ER diagrams and write relational database queries
Create database objects and maintain referential integrity
Configure, deploy and maintain database
Participate in development and maintenance of Data warehouses
Design, develop and deploy SSIS packages
Creating and deploying reports
Provide technical design, coding assistance to the team to accomplish the project deliverables as planned/scoped.
Ability to talk to client and get the Business Requirements
Skills:
Azure Data Factory
Azure Devops
Azure Storage/ Data Lake
Extraction, Transformation and Loading
Analytics development
Report Development
Relational database and SQL language
Other Requirements:
· Should be well versed with Data Structures & algorithms
· Understanding of software development lifecycle
· Excellent analytical and problem-solving skills.
· Ability to work independently as a self-starter, and within a team environment.
· Good Communication skills- Written and Verbal
Job Type: Full-time
Salary: $84,454.31 - $190,806.62 per year
Benefits:
Flexible schedule
Health insurance
Compensation package:
1099 contract
Yearly pay
Experience level:
10 years
9 years
Schedule:
Day shift
Experience:
Azure Data engineer: 9 years (Preferred)
SQL: 9 years (Preferred)
Data warehouse: 10 years (Preferred)
Work Location: Remote","$137,630 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Regions
3.5",3.5,"Atlanta, GA",Data Engineer (REMOTE OPPORTUNITY),"Thank you for your interest in a career at Regions. At Regions, we believe associates deserve more than just a job. We believe in offering performance-driven individuals a place where they can build a career --- a place to expect more opportunities. If you are focused on results, dedicated to quality, strength and integrity, and possess the drive to succeed, then we are your employer of choice.

Regions is dedicated to taking appropriate steps to safeguard and protect private and personally identifiable information you submit. The information that you submit will be collected and reviewed by associates, consultants, and vendors of Regions in order to evaluate your qualifications and experience for job opportunities and will not be used for marketing purposes, sold, or shared outside of Regions unless required by law. Such information will be stored in accordance with regulatory requirements and in conjunction with Regions’ Retention Schedule for a minimum of three years. You may review, modify, or update your information by visiting and logging into the careers section of the system.

Job Description:
At Regions, the Data Engineer focuses on the evaluation, design, and execution of data structures, processes, and logic to deliver business value through operational and analytical data assets. The Data Engineer uses advanced data design and technical skills to work with business subject matter experts to create enterprise data assets utilizing state of the art data techniques and tools.

Primary Responsibilities
Partners with Regions Technology partners to Design, Build, and Maintain the data-based structures and systems in support of Data and Analytics and Data Product use cases
Builds data pipelines to collect and arrange data and manage data storage in Regions’ big data environment
Builds robust, testable programs for moving, transforming, and loading data using big data tools such as Spark.
Coordinates design and development with Data Products Partners, Data Scientists, Data Management, Data Modelers, and other Technical partners to construct strategic and tactical data stores
Ensures data is prepared, arranged and ready for each defined business use case
Designs and deploys frameworks and micro services to serve data assets to data consumers
Collaborates and aligns with technical and non-technical stakeholders to translate customer needs into Data Design requirements, and work to deliver world-class visualizations, data stories while ensuring data quality and integrity
Provides consultation to all areas of the organization that plan to use data to make decisions
Supports any team members in the development of such information delivery and aid in the automation of data products
Acts as trusted adviser and partner to business leads- assisting in the identification of business needs & data opportunities, understanding key drivers of performance, interpreting business case data drivers, turning data into business value, and participating in the guidance of the overall data and analytics strategy

This position is exempt from timekeeping requirements under the Fair Labor Standards Act and is not eligible for overtime pay.

Requirements
Bachelor's degree and five (5) years of experience in a quantitative/analytical/STEM field or technical related field
Or Master’s degree and three (3) years of experience in a quantitative/analytical/STEM field or technical related field
Or Ph.D. and one (1) year of experience in a quantitative/analytical/STEM field
Three (3) years of working programming experience in Python/PySpark, Scala, SQL
Three (3) years of working experience in Big Data Technology in Hadoop, Hive, Impala, Spark, or Kafka

P references
Prior banking or financial Services experience
Experience developing solutions for the financial services industry
Background in Big Data Engineering and Advanced Data Analytics
Experience in Agile Software Development
Experience or exposure to cloud technologies and migrations

Skills and Competencies
Experience building data solutions at scale
Experience designing and building relational data structures in multiple environments
Experience with Airflow, Argo, Luigi, or similar orchestration tool
Experience with DevOps principals and CI/CD.
Experience with Docker and Kubernetes
Experience with No-SQL databases such as HBase, Cassandra, or MongoDB
Experience with streaming technologies such as Kafka, Flink, or Spark Streaming
Experience working with Hadoop ecosystem building Data Assets at an enterprise scale
Proven record of accomplishment of delivering operational Data solutions including Report and Model Ready Data Assets
Significant experience working with senior executives in the use of data, reporting and visualizations to support strategic and operational decision making
Strong ability to transform and integrate complex data from multiple sources into accessible, understandable, and usable data assets and frameworks
Strong background in synthesizing data and analytics in a large (fortune 500), complex, and highly regulated environment
Strong technical background including database and business intelligence skills
Strong communication skills through written and oral presentations

Additional Job Description
Candidates must have experience with DevOps principals and CI/CD.
Candidates must have experience with Docker and Kubernetes
Candidates must have experience with streaming technologies such as Kafka, Flink, or Spark Streaming.
Preferred experience in Snowflake, SQL and Python.
Preferred experience in developing API's.
Preferred experience with AWS Lambda functions.
This position may be filled at a higher level depending on the candidate's qualifications and relevant experience.

Position Type Full time

Compensation Details
Pay ranges are job specific and are provided as a point-of-market reference for compensation decisions. Other factors which directly impact pay for individual associates include: experience, skills, knowledge, contribution, job location and, most importantly, performance in the job role. As these factors vary by individuals, pay will also vary among individual associates within the same job.

The target information listed below is based on the national range and level of the position.

Job Range Target:
Minimum: $85,374.00 USD
Median: $122,800.00 USD
Incentive Pay Plans: This job is not incentive eligible.

Benefits Information
Regions offers a benefits package that is flexible, comprehensive and recognizes that ""one size does not fit all"" for associates. Listed below is a synopsis of the benefits offered by Regions for informational purposes, which is not intended to be a complete summary of plan terms and conditions.
Paid Vacation/Sick Time
401K with Company Match
Medical, Dental and Vision Benefits
Disability Benefits
Health Savings Account
Flexible Spending Account
Life Insurance
Parental Leave
Employee Assistance Program
Associate Volunteer Program

Please note, benefits and plans may be changed, amended, or terminated with respect to all or any class of associate at any time. To learn more about Regions’ benefits, please click or copy the link below to your browser.

https://www.regions.com/welcometour/benefits.rf

Location Details Regions Plaza Atlanta
Location: Atlanta, Georgia

Bring Your Whole Self to Work

We have a passion for creating an inclusive environment that promotes and values diversity of race, color, national origin, religion, age, sexual orientation, gender identity, disability, veteran status, genetic information, sex, pregnancy, and many other primary and secondary dimensions that make each of us unique as individuals and provide valuable perspective that makes us a better company and employer. More importantly, we recognize that creating a workplace where everyone, regardless of background, can do their best work is the right thing to do.

OFCCP Disclosure: Equal Opportunity Employer/Disabled/Veterans","$63,377 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Banking & Lending,1971,$5 to $10 billion (USD)
Provista Software Corporation,#N/A,Remote,Data Engineer,"As a successful candidate, you’ll play a pivotal role in the data conversion and business intelligence solution efforts during project implementation for our state and local clients. We are looking for you to join our software implementation team as a Senior Data Engineer.
Qualification:
5+ years of experience working independently as ETL developer on data migration projects that covered diverse database platforms and complex data conversions.
Must have experience across leading Database Platforms especially Oracle, Azure SQL, Snowflake.
Expert level experience with Extraction Transform Load (ETL) plan, protocols, and tools.
Expert level experience with programming languages SQL, Phyton, Java, C#.
Good at communication with clients and internally.
Expert at problem solving and troubleshooting skills.
Bachelor's degree, preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience
Must be a U.S. Citizen or Green Card Holder.
Functional Responsibility:
Data Conversion -
In collaboration with Data Stewards, discover inventory of legacy data sources and all data the client expects to be migrated to destination Modified Off the Shelf (MOTS) solution.
Complete gap assessment between legacy data and MOTS data requirements, and options to address the gaps.
Develop data migration map to minimally include data dictionary, detailed data map of all elements of the current database, data transformation rules, relationship rules, validation rules, process of migrating images, and process for recovering missing or erroneous data.
Identify issues, risks, and/or barriers that may interfere with data migration and recommendations to resolve or mitigate.
Conduct testing and quality assurance to ensure the data is migrated accurately.
Migrate data from legacy to destination MOTS solution progressively using a sample set of data and the entirety of the data prior to deployment into relevant go-live environments (e.g., staging, production, etc.)
Prepare data migration reports that includes – record counts of source vs migrated vs modified; field level validation of data at row and column; discovered exceptions.
Business Intelligence -
Develop ETL pipelines in and out of Snowflake data warehouse using combination of Python, SnowSQL, and SQL.
In collaboration with Data Stewards, create Snowflake data mart as curated subset of data to support the analytics and business intelligence needs of subgroup of users.
Translate requirements for business intelligence and reporting to database design and reporting design.
Conduct discovery sessions with users to capture business intelligence reports.
Develop Tableau self-service ad hoc reports to support operational users.
Develop Tableau Dashboards to support executive, management, and operational users.
About us:
Provista Software Corporation is fast growing Software Solution provider for US State and Local Government Clients.
Job Type: Full-time
Pay: $67,107.98 - $134,285.66 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Life insurance
Paid time off
Parental leave
Professional development assistance
Vision insurance
Compensation package:
Yearly pay
Experience level:
5 years
Schedule:
8 hour shift
Application Question(s):
What is your most preferred work time zone ?
Experience:
ETL: 5 years (Required)
Data warehouse: 1 year (Preferred)
Tableau: 1 year (Preferred)
Talend: 1 year (Preferred)
Microsoft SQL Server: 5 years (Required)
Work Location: Remote","$100,697 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Codinix Technologies
5.0",5.0,"Boston, MA",Cloud Data Engineer - AWS,"Only US Citizen
Strong in warehouse technologies (Hadoop, Spark, AWS, etc.)
Bachelor's degree in Computer Science, Information Systems, or another related field
5+ years of data engineering, and cloud engineering experience
Experience in Agile project methodologies and DevOps
Experience with AWS (certification preferred; including Athena, Redshift, Glue), Cloudera
Experience building a CI/CD pipeline to deploy applications on AWS (GitLab, Terraform)
Advanced experience programming and scripting on Unix/Linux (i.e. Python, Bash)
Experience with CTRL-M, Cron (other) scheduling of batch jobs
Experience migrating legacy systems to cloud-ready architectures
Job Type: Contract
Pay: Up to $75.00 per hour
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Boston, MA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",$75.00 /hr (est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Wallero
4.4",4.4,"Seattle, WA",Data Engineer,"Hi,
Pretty much interested in your resume, we have an urgent opening for Data Engineer role with one of our clients based in Seattle WA.
Position: Data Engineer
Location: Seattle WA - Remote (100%)
Description:
Good Experience in reporting space including PowerBI, ADF, and possibly standing up a data warehouse/data mart.
The main scope centering is around reporting.
Experience with SQL, and agile framework including requirement gathering and story/feature writing skills.
Need someone strong who is comfortable leading and guiding teams in that capacity.
Migrating data from third party to Azure Cloud
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Seattle, WA 98101: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",$75.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2015,$1 to $5 million (USD)
Zenotis Technologies inc,#N/A,"Addison, TX",Big Data Engineer,"Role: Big Data Engineer
Location – Addison, TX/ Charlotte, NC / Chandler, AZ (Hybrid – 3 days onsite 2 days remote)
Position Summary:
Looking for a big data engineer who loves solving complex problems across a full spectrum of technologies. The ideal candidate is excited by experimentation and looking for a new challenge that stretches their talents. The big data engineer will help ensure that our technological infrastructure operates seamlessly in support of business objectives.
The engineer must be able to:
· Translate complex technical and functional requirements into detailed designs
· Guide the development and testing teams in developing and implement pipelines that extract, transform, and load data into an information product that helps the organization reach its strategic goals
· Focus on ingesting, storing, processing, and analyzing large datasets
· Deep understanding on how to create scalable, high-performance web services for tracking data
· Investigate alternatives for data storing and processing to ensure the implementation of the most streamlined solutions
Primary Skill
Hadoop
Secondary Skill
Kafka
Tertiary Skill
Oracle Exadata
Required Skills
· Translate complex technical and functional requirements into detailed designs
· Design data pipelines using design tools and provide solution intent in provided templates
· Work closely with data science team to implement data analytics pipelines
· Analyze vast number of data stores to uncover insights
· Define data governance policies and support data-versioning processes
· Maintain security and data privacy, working closely with data protection guidelines.
· Desired Skills
· Experience with Python, Spark, and Hive
· Understanding of data-warehousing and data-modeling techniques
· Knowledge of industry-wide visualization and analytics tools (ex: Tableau, R)
· Strong data engineering skills with Hadoop platform
· Experience with streaming frameworks such as Kafka
· Knowledge of Core Java, Linux, SQL, and any scripting language
· Good interpersonal skills and positive attitude
Job Type: Full-time
Salary: Up to $70.00 per hour
Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
8 hour shift
Work Location: On the road",$70.00 /hr (est.),201 to 500 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Synovize
5.0",5.0,Remote,Big Data Engineer,"Synovize is a cutting-edge technology company that specializes in providing advanced data analytics and intelligence solutions. Our mission is to empower businesses with the tools and insights they need to make data-driven decisions and drive innovation. We are committed to delivering exceptional results by combining our expertise in big data technologies with our passion for creating impactful solutions.
As a Big Data Engineer at Synovize, you will have the opportunity to work on exciting projects that involve designing and implementing scalable data solutions. You will collaborate with a talented team of professionals, including software developers, data scientists, and domain experts, to tackle complex data engineering challenges and deliver high-quality solutions to our clients.
Job Title:
Big Data Engineer
Company: Synovize
Number of Positions: 2
Location: Remote
Length: 12 Months
Tax Terms: 1099/C2C or W-2
Rate/Range: $70-$90/hr
Work Authorization: US Citizen
Security Clearance: None
Skills Needed:
Proficiency in Java and Kotlin programming languages.
Strong knowledge of AWS (Amazon Web Services) and its big data offerings.
Experience with BigData technologies, frameworks, and tools.
Familiarity with Kafka for streaming data processing.
Proficiency in Python programming language.
Responsibilities:
Design and develop scalable data pipelines to process, integrate, and analyze large volumes of structured and unstructured data.
Implement data ingestion, transformation, and storage mechanisms using big data technologies and frameworks.
Collaborate with data scientists and analysts to understand data requirements and optimize data models for analytics and machine learning.
Utilize AWS services and tools to build and maintain data infrastructure, ensuring data quality, integrity, and security.
Troubleshoot and resolve data-related issues, optimizing performance and efficiency.
Stay up-to-date with emerging trends and advancements in big data technologies, recommending and implementing improvements to existing systems and processes.
Requirements:
Bachelor's or master's degree in computer science, engineering, or a related field.
Proven experience as a Big Data Engineer or similar role, with a strong understanding of big data technologies and architectures.
Proficiency in Java and Kotlin programming languages.
Strong knowledge of AWS and its big data offerings.
Experience with BigData technologies, frameworks, and tools.
Familiarity with Kafka for streaming data processing.
Proficiency in Python programming language.
Strong problem-solving skills and ability to work in a fast-paced, collaborative environment.
Excellent communication and interpersonal skills.
Join Synovize and be part of a dynamic team that is at the forefront of data innovation. Together, we will drive meaningful change and unlock the full potential of data-driven decision-making.
Job Types: Full-time, Contract
Pay: $70.00 - $90.00 per hour
Benefits:
401(k)
Dental insurance
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Vision insurance
Schedule:
10 hour shift
4 hour shift
8 hour shift
Choose your own hours
Application Question(s):
Are you a US Citizen?
Years of experience in Java?
Years of experience in Kotlin?
Years of experience in big data?
Years of experience in AWS?
Security Clearance
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$80.00 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Breadboard,#N/A,"New York, NY",Data Engineer,"About Us
We’re on a mission to revolutionize the global electronics supply chain.
During the pandemic we saw how the inefficiencies in electronics manufacturing can have massive global impact. From over $200B lost in car manufacturing to delays on laptops, gaming systems and much more.
At Breadboard we are rethinking the electronics supply chain down to the finest detail. We’re designing the new operating system in which this entire system will run. We’ve already secured seed funding and are now looking to expand our engineering team to help us set the foundation for the future.
Why work with us?
As a part of our team, you will have the unique opportunity to design and implement the data platform for Breadboard, a critical component in our business.
As one of the first data engineers to join the Breadboard team you will be partnering with the CTO, our founding engineers, and will set the foundation for the design of the operating system for the electronics supply chain.
You will be involved in all aspects of the software development process, architectural decisions, recruiting, and building the organization in general.
We’re a tight-knit, intense team and you will have a ton of end to end ownership.
Work with seasoned founders who have had successful exits.
This is not only a unique opportunity for someone who wants to get their hands dirty, but also take that step back and collaborate with the executive team on setting a strategic future.
You’ll be responsible for
Data Management: Develop and automate robust data acquisition, processing, and normalization pipelines from various sources to ensure data quality and consistency.
Database and ETL Architecture: Design, implement, and optimize a database system for efficient data storage, retrieval, and analysis.
Collaboration on Machine Learning: The prospects of working with data scientists to leverage extracted data features, contributing to the development and refinement of a machine learning model for component matching.
API and UI Integration: Collaborate with the software development team to facilitate seamless integration between the database, machine learning models, and user interface.
Continuous Improvement and Compliance: Monitor system performance and user feedback for continuous improvements, while ensuring adherence to data privacy and security standards.

Requirements
Experience: A minimum of 3-5 years of experience in a data engineering role, preferably in the electronics, manufacturing industry or supply chain industry.
Technical Skills: Proficiency in SQL and experience with database design, data modeling, and performance optimization. Strong coding skills in a major programming language, such as Python or TypeScript. Experience with data processing tools and libraries (e.g., pandas, Spark). Experience with pipeline and workflow orchestration tools, such Airflow. Experience with Snowflake or similar cloud-based data warehousing solutions.
Data Acquisition and Processing: Experience with data acquisition methods, such as APIs and web scraping. Familiarity with data cleaning, normalization, and feature extraction techniques.
Machine Learning: Familiarity with machine learning concepts and algorithms, and experience collaborating with data scientists or machine learning engineers.
API Development: Experience with API development and integration, preferably in a microservices architecture.
Data Governance: Knowledge of data privacy and security regulations, and experience implementing data governance measures.
Bonus Points
Entrepreneurial mindset (we encourage all employees to be future founders and this can be a great stepping stone towards that goal).
Customer-centric and passionate about helping small businesses grow.
Previous experience at a high-growth, fast-paced startup.
Experience with our stack (and some others that may be included shortly):
React, Material UI, Typescript, CSS
NestJS, GraphQL, TypeORM, Knex.js, Objection.js, PostgreSQL, gRPC, Kafka
AWS, Docker, Kubernetes, Helm, Github Actions, CI/CD pipelines.
Compensation + Benefits
Competitive cash compensation
Stock ownership at a fast growing company
Health/dental/vision insurance—100% coverage option
The ability to push for other benefits you think are important
Our Engineering Culture
Openness to diverse opinions and backgrounds - Testing our assumptions
Supported career growth with a high emphasis on learning and mentorship
Lean development
End to end ownership and accountability. We’re all going to make mistakes. It’s about how you own up to them and learn from them.

Equal Opportunity Employer
Breadboard Software does not discriminate in employment on the basis of race, color, religion, sex (including pregnancy and gender identity), national origin, political affiliation, sexual orientation, marital status, disability, genetic information, age, membership in an employee organization, retaliation, parental status, military service, or other non-merit factor.",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Maven Workforce
4.1",4.1,"McLean, VA",Data Engineer,"MUST HAVES: Hands-on experience with Informatica PowerCenter and/or Informatica Intelligent Cloud Services (IICS), Cloud Data Integration (CDI) , Cloud AWS,UNIX scripting, and any other ETL tools.
Job responsibilities: –
The candidates will be having the IICS module of Informatica more than any other platform modules.
Strong hands-on experience building ETL pipelines and Data Replication
3+ yrs of experience with IICS is required.
Must be hand Ons with AWS services like ECS,S3,EKS etc.
Cloud Data Integration is also required.
Data Engineering Background experience with Spark, Hive is also required.
Preferred:
SQL is preferred, but basic knowledge is required
Job Type: Contract
Salary: $60.00 per hour
Ability to commute/relocate:
Mclean, VA 22101: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica PowerCenter: 7 years (Required)
Informatica Intelligent Cloud Services (IICS): 7 years (Required)
Cloud Data Integration (CDI): 7 years (Required)
UNIX scripting: 7 years (Required)
ETL tools: 7 years (Required)
Work Location: One location
Speak with the employer
+91 7328456015",$60.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2008,Unknown / Non-Applicable
"WorkCog
4.3",4.3,"Atlanta, GA",Sr. AWS Data Engineer,"Position: AWS Data Engineer
Location: Atlanta ,GA
Duration: 12+ Months
Interview: Video
JD
Responsibilities
Bachelor’s degree in Computer Science, Software Engineering, or other related field, or equivalent work experience.
Extensive experience in data modeling, writing & performance tuning complex SQL queries & user-defined functions
Hands-on experience in design, implementation and orchestration of data pipelines and ETL batch Jobs.
Hands-on experience on all aspects of data handling: cleansing and standardization, normalization, merging, change data capture, data integration, reconciliation, and transformations
Strong experience in working with Big Data technologies such as Hive/Spark, Data lake and Data warehouse using GCP and/or AWS Services
Ability to write robust code in scripting languages like Python, Scala, PySpark or similar.
Good to Have
Associate Architect or equivalent cloud certification
Experience with processing large datasets using AWS and GCP UDFs (User defined functions)
Experience in ML probability & statistics algorithms, frameworks like Keras or PyTorch and tools like Jupyter notebooks.
Job Type: Contract
Pay: $45.83 - $52.42 per hour
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Atlanta, GA: Reliably commute or planning to relocate before starting work (Preferred)
Experience:
SQL (Preferred)
Informatica (Preferred)
Data warehouse (Preferred)
Work Location: In person",$49.13 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable
"Ripple Effect Consulting
5.0",5.0,"Denver, CO",Data Engineer - 004/005,"Data Engineer
Hybrid-remote based in Denver, CO | $80-$160k
Job Overview:
We are looking for a savvy Data Engineer to join our growing team of analytics experts. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
Responsibilities for Data Engineer
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Qualifications for Data Engineer
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:
1) Experience with big data tools
2) Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
3) Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
4) Experience with cloud services
5) Experience with stream-processing systems: Storm, Spark-Streaming, etc.
6) Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Job Type: Full-time
Pay: $80,000.00 - $160,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Denver, CO: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person","$120,000 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
BIGCLFY,#N/A,"Dallas, TX","W2/ 1099, No C2C - SSIS ETL Data Engineer (Minneapolis, MN/ Dallas-Irving)","No C2C. It's W2/1099 Requirement
Only taking USC, GC or H4 EAD, H1 Transfer, L2 Visa or TN Visa - they will not take OPT EAD or CPT
Need to have confirmation in writing that you will be onsite by Day 1
ALL ROLES ARE HYBRID IN THE OFFICE (2-3 DAYS/WEEK) UNLESS REMOTE IS NOTED
Please submit qualified candidates and include FULL LEGAL NAME as it appears on the passport
If your candidate is relocating, need to have confirmation in writing that they will be onsite by Day 1
Senior SSIS/ETL Data Engineer
12 months – 3 days in office
Minneapolis, MN or Dallas-Irving, TX
5+ years of experience with SQL Server 2016 (or newer) and SSIS
7+ years of data engineering
Must be capable of leading teams
2+ years of experience working with large scale SQL implementations
Experience designing, developing, and maintaining complex stored procedures, functions, and views using T-SQL
Experience designing, developing, and maintaining ETL processes using SSIS
Should understand how to unit test their product
Hands on experience with Visual Studio to document
Experience in financial or regulatory environments highly preferred
Ideal candidates will have experience creating conceptual, logical, and physical database designs
Preference given to candidates with extensive knowledge and experience with performance monitoring, database and SQL tuning
Bachelor’s or Master's in computer science or related field or equivalent work experience
Job Type: Contract
Pay: From $73.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Chandler, AZ 85225: Reliably commute or planning to relocate before starting work (Preferred)
Application Question(s):
Expected Payrate on W2/1099 Tax Term? Please mention. Thanks.
Experience:
Risk Analytics: 5 years (Preferred)
Work Location: In person",$73.00 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
IT Customer Service - POST,#N/A,"Newark, NJ",Data Center Plant Engineer,"Job Title: ISP-OSP Data Center Plant Engineer
Location: Newark NJ, (M-F)
Travel Required: As Needed
Overview
UC-ISP/OSP engineer is responsible for cable management including structured cabling, carrier data and voice circuit cables/equipment, rack-and-stack, and oversight of power, backup power, network equipment, and environmental needs to cabinets. Leveraging Clients Data Center Infrastructure Management System, (DCIM) tool to maintain, report and monitor.
Conducting capacity assessments of existing infrastructure to ensure that it can support future growth to ensure compliance with federal regulations of the Data Center infrastructure Standards. Projects include design and build outs of the physical plant within the typical WAN/LAN space within data centers, telco rooms, MDFs and IDFs as needed.
Functions:
Perform initial engineering surveys based on project needs.
Document engineering surveys to determine feasibility and the estimated cost.
Obtain authorization to proceed and confirm funding.
Create detailed engineering drawings of build, floor plans, equipment elevation drawings, structured cabling diagrams, power, and environmental needs.
Designs must conform to state, local, national, and Client standards and codes, and the NJ historical and or environmental commissions approvals if needed.
Based on the engineering documents create a SOW (Scope of Work) and BOM (Bill of Materials)
Submit to Client manager to approve and proceed when funding is validated.
Create a construction package and submit to contractors or in house for RFQ.
Review quotes and choose both low bid and qualified contractor.
Work with all stakeholders to date and time work schedules.
Confirm workers are safety trained and conform to Client, FRA, and OSHA requirements.
Supervise and manage installations and conduct inspections.
Review work to ensure contractor met obligations under terms of contract.
Inform Client managers of status to approve invoices.
Coordinate with Client network engineering to connect, configure, and test equipment.
Review and confirm a successful installation were completed and working as designed prior to cutting over to production.
Assist with Break fix of IT infrastructure troubles or repairs.
Daily travel to location depending on scheduled or unscheduled work
Job Types: Full-time, Contract
Experience level:
5 years
Schedule:
8 hour shift
Work setting:
In-person
Experience:
Data Center: 5 years (Required)
WAN/LAN: 3 years (Required)
cabling: 3 years (Required)
Work Location: One location",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Spartan Technologies
3.4",3.4,"Kansas City, MO",Data Engineer I,"Spartan Technologies, Inc. - Kansas City, MO
Applicants Need to Know
100% remote in East Coast or Central US only
No Sponsorship
Contract to Hire
Hourly Rate: $45 on W2
Data Engineer I
The Data Engineer is responsible for processing structured and unstructured data, validating data quality, and developing and supporting data products for analytics. Brings together data from different sources into a common dataset with common KPIs and dimensions for stakeholders. Understands the analytics challenges that the marketing organization faces in their day-to-day work and partners with them to design viable data solutions.
Your Job
Gathers and processes raw, structured, semi-structured, and unstructured data using batch and real-time data processing frameworks.
Understand the existing source system data models and perform end to end data validation to maintain accuracy.
Implements and optimizes data solutions in enterprise data warehouses and big data repositories.
Ensures data quality and implements tools and frameworks for automating the identification of data quality issues.
Develop re-useable processes that can be leveraged and standardized for multiple instances
Works with internal and external data providers on data validation providing feedback and making customized changes to data feeds and data mappings.
Recommends improvements to processes, technology, and interfaces that improve the effectiveness of the team and reduce technical debt.
Ensure performance by monitoring existing workflows to ensure they are working correctly and that they reconcile with the source.
Design and develop ETL workflows and datasets to be used in extracts by our BI reporting tool Tableau.
Will write complex SQL queries with multiple joins to automate/manipulate these reporting extracts
Works in partnership with marketing stakeholders concerning the accuracy of data and efficiency of processes
Qualifications
Working experience with data modelling, data access, schemas, and data storage techniques within Snowflake
Working experience with batch and real-time data processing frameworks
Working experience working with relational databases such as SQL
Working experience with business intelligence tools and platforms
Working experience with data quality tools
Experience with ETL processes and tools
Desirable:
Working experience with Snowflake
Working experience with Google Analytics BigQuery
Working Experience with Tableau Server
Working experience with SQL Server Integration Services",$45.00 /hr (est.),1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$5 to $25 million (USD)
"PRIMUS Global Services, Inc
4.1",4.1,"Minneapolis, MN","Data Engineer – Hadoop, Python – REMOTE WORK 43198","We have an immediate long-term opportunity with one of our key clients for a position of Data Engineer, to work on a remote basis.

The ideal applicant must have extensive experience with Python, Hadoop, Hive and SQL.

**ALL successful candidates for this position are required to work directly for PRIMUS. No agencies please**

For immediate consideration, please contact:

Tejaswini
PRIMUS Global Services
Direct: 972-798-2662
Desk: 972-753-6500 Ext: 204
Email: jobs@primusglobal.com","$91,633 /yr (est.)",501 to 1000 Employees,Private Practice / Firm,Information Technology,Information Technology Support Services,2002,$5 to $25 million (USD)
"AbleTo
3.7",3.7,"New York, NY",Data Engineer,"About AbleTo
Join our mission-driven organization, where your work matters and a diversity of ideas and backgrounds are welcomed. AbleTo is the leading provider of high quality, technology-enabled behavioral health care. We believe that everyone deserves access to high-quality care and offer a suite of technology-enabled services to empower people to lead better lives through better mental health. A proprietary platform connects individuals with AbleTo highly trained licensed providers who deliver weekly sessions by phone or video supported by an integrated digital experience. Members also have access to mental health coaches, and digital support programs. Our outcomes-focused approach is proven to improve both behavioral and physical health and lower medical costs.

Overview
Data is integral to nearly everything that we do at AbleTo, and as a result the data team plays a key role at our company. As a Data Engineer at AbleTo, you will grow and develop your skills as a data engineer to build a world class data platform to support our mental health care mission.

What You Will Do
Build and maintain batch and real-time ETL pipelines in a Google Cloud Platform architecture (BigQuery, Dataproc, Firestore, Informatica, Python, etc.)
Identify code quality issues and implement tests to improve future processes.
Document orchestration work-flows and create run books
Implement data integrity tests to ensure we are ingesting accurate data.
Implement high-quality test-driven code.
Participate in daily team stand ups and other Agile ceremonies

Key Initiatives over the next 12 months
Data Mart client migration
Data Mart enhancements
Automation of manual day to day operational tasks
Decommission legacy pipelines

Who You Are:
1+ years of experience coding in Python.
Experience working with SQL (structured query language).
Familiarity with structuring and writing ETLs.
Experience working with Orchestration tools (Airflow) and Cloud Databases (BigQuery) is a plus.
Experience working in an Agile Development environment is a plus.
BS in Information Systems, Computer Science or related field is preferred.
You are excited to work with data!
Curiosity to dive deeply into issues and feel empowered to make a meaningful impact at a mission-driven company.
Committed to agile development and value delivery and solid engineering principles, as well as continuously improving our craft.
You are a collaborator. You build and maintain strong, productive working relationships with your customers and stakeholders.
You are empathetic and seek to understand each individual's diversity of background and experience contributes to making up a great team.
You drive to improve yourself and others.
You are excited about learning new languages, tools, techniques and technologies.

Why You Should Join Our Team:
We're proud to be a Great Place to Work-Certified™ company. We want you to show up and feel your best at work, and that means respecting your time outside of work. Our inclusive, flexible workspace prioritizes a work/life balance. We offer competitive salaries, comprehensive health benefits (for full-time employees), and professional perks such as 401K matching, fully funded HRA, and generous time off, including mental health days because your well-being is important to us.
At AbleTo, we're empowering people to get better and stay better. Want to join us? Take the next step in your career by applying for this role today.
The salary range is $40,000 to $90,000. Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, AbleTo offers benefits such as a comprehensive benefits package, incentive and recognition programs, and 401k contribution (all benefits are subject to eligibility requirements).

#LI-Remote
Follow AbleTo on LinkedIn, Twitter, and Instagram!
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.
AbleTo is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or veteran status. AbleTo is an E-Verify company.","$65,000 /yr (est.)",501 to 1000 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2008,$25 to $100 million (USD)
"PrizePicks
4.8",4.8,"Atlanta, GA",Data Engineer,"At PrizePicks, we are the fastest growing sports company in North America, as recognized by Inc. 5000. As the leading platform for Daily Fantasy Sports, we cover a diverse range of sports leagues, including the NFL, NBA, and Esports titles like League of Legends and CS : GO. Our team of over 200 employees thrives in an inclusive culture that values individuals from diverse backgrounds, regardless of their level of sports fandom. Ready to reimagine the DFS industry together?
Our Analytics Team is responsible for building and maintaining analytics tools and workflows to support the PrizePicks business. By developing, maintaining, and testing data generation infrastructures, you will enable the PrizePicks business to make smarter, better, and faster data-driven decisions.
What you'll do:
Create, maintain and orchestrate optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Improve and streamline data systems to drive innovation within Prizepicks
What you have:
3+ years of experience building frameworks for data ingestion pipelines but real time and batch using data modeling, ETL/ELT processes
Stellar SQL skills, experience building DBT pipelines in production and know your way around structured, semi-structured and unstructured data.
Have built and optimized 'big data' data pipelines, architectures and data sets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Experience with Python and other object oriented scripting languages
Cloud experience: AWS, EC2, EMR, RDS, Redshift, GCP, etc..
Not required but would be awesome if you have experience with any of the following data pipeline and workflow tools: Azkaban, Luigi, Airflow, Prefect, etc.
Where you'll live:
Anywhere in the US is fine (we are based in Atlanta, GA)
Benefits you'll receive:
In addition to your great compensation package, company subsidized medical/dental/vision coverage plans and matching 401(k), we'll shower you with perks including:
Break room with ping pong, endless snacks and in-office lunch once a week
Unlimited PTO to encourage a healthy work/life balance (2 week min required!)
Modern work schedule focused on getting the job done, not hours clocked
Workplace flexibility
Company and team outings, we encourage a tight-knit workplace
Generous Maternity AND Paternity leave (16 weeks!)
Annual bonus & stock options
Wellness program
Company equipment provided (Windows & Mac options)
Annual performance reviews with opportunity for growth and career development
#LI-REMOTE

You must be authorized to work for any employer in the U.S. We are unable to sponsor or take over sponsorship or an employment Visa at this time.
PrizePicks is an Equal Opportunity Employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.","$97,814 /yr (est.)",1 to 50 Employees,Company - Private,"Arts, Entertainment & Recreation",Sports & Recreation,2015,Unknown / Non-Applicable
"Kaizen Analytix
3.9",3.9,"Dallas, TX",Cloud Data Engineer: AWS,"Cloud Data Engineer: AWS
Kaizen Analytix LLC, an analytics services company, is seeking a qualified Cloud Data Engineer Contractor to support data science or data analytics projects. Responsibilities, capabilities for success, and job requirements are below. This role requires an estimated 40 hours/week commitment for approximately 24 weeks.
Responsibilities:
Candidate will be responsible for supporting a data science or data analytics project.
Work with client IT to understand access and use of source-of-truth data systems and downstream data sources.
Work with data scientist or visualization experts to determine data requirements such as structure, latency, availability.
Work with IT Security to assess risk of using sensitive data and how these data can be used throughout the project life cycle.
Develop an as-is and to-be state for data architecture.
Capabilities for Success:
In addition to key responsibilities, Candidate should demonstrate an ability to…
Work with the project manager and the business stakeholders to define upstream requirements for data.
Contribute to estimating input and time required for data engineering development tasks.
Contribute to client demonstrations of solution or presentations on architecture.
Proactively raise risks and issues to internal project manager
Work with data scientist or analysts to proactively assist in data concerns
Job Requirements:
Education: Bachelor's degree required
Extensive Experience in Data Engineering – consuming, wrangling, validating, developing pipelines for data.
Extensive Experience with AWS
Must have Solutions Architect Certification
Strong knowledge of databases, ETL concepts, SQL, and NoSQL sources.
Experience in: Python (data packages such as Pandas, PySpark, SQL Alchemy, pydantic, etc.) and SQL.
Position Type: 1099 Contractor",$57.50 /hr (est.),1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,#N/A,Unknown / Non-Applicable
"Southern Glazer’s Wine and Spirits
3.6",3.6,"Dallas, TX",Data Engineer,"What You Need To Know
Open the door to a groundbreaking tech career with an industry leader. Southern Glazer’s Wine & Spirits is North America’s preeminent wine and spirits distributor, as well as a family-owned, privately held company with a 50+ year legacy of success. To create a new era in alcohol beverage sales and service, we’re heavily invested in the most transformative new technologies – and the most brilliant tech professionals. Southern Glazer’s was named by Newsweek as a Most Loved Workplace and is included on the Forbes lists for Largest Private Companies and Best Employers for Diversity.
As a full-time employee, you can choose from a full menu of our Top Shelf Benefits, including comprehensive medical and prescription drug coverage, dental and vision plans, tax-saving Flexible Spending Accounts, disability coverage, life insurance plans, and a 401(k) plan. We also offer tuition reimbursement, a wellness program, parental leave, vacation accrual, paid sick leave, and more.
We offer continuous learning and career growth in a fast-paced environment where you are respected, your voice is heard, and technology is part of our strategy for success. If you’re looking to fill your glass with opportunity, come join our FAMILY.
Overview
The Data Engineer's role is to design, develop, maintain and enhance interfaces and connectivity to the Data Warehouse ecosystem by coding with a technical language to meet business requirements and business objectives. This can include taking technical specifications and developing an application or integration of data between applications, testing, as well as, completing the appropriate technical documentation. The Data Engineer will use best practices in software development and adhere to SGWS development standards, as well as, focus on quality and innovation. The Data Engineer may also be responsible for delivering support to end users in the organization for specific code, including troubleshooting code.
Specialized Skills and Technologies
Strong PL/SQL skills
Experience in ETL Tools (Preferrable Informatica)
Data Warehouse techniques will be a plus
Experience in cloud platforms like Azure or AWS will be a plus
Knowledge of UNIX/Linux, shell scripting, Python will be a plus
Experience developing Application Programming Interfaces (API's) will be a plus
Experience in Hadoop will be a plus
Primary Responsibilities
Design, develop, implement, and support software applications
Drive technical validity of solution.
Develop user documentation as well as in-code documentation to explain designs and participate/support user training
Structure requirements to facilitate automation of acceptance tests
In conjunction with Data Management Group, develop routines and procedures that provide data quality checks and balances on data delivery/ingestion
Collaborate across the BI / Analytics, Data Management Group, Enterprise Insights and Analytics teams to establish standards, reusable data models and best practices for delivery/ingestion of data from/to Data Warehouse - This includes Publish/Subscription and API options
Obtain any certifications needed to effectively support applications in scope
Support the development of business and technical process documentation and training materials
Structure requirements to facilitate automation of acceptance tests
Provide support for software applications under area of responsibility
Drive Behavior-Driven-Design (BDD) process
Perform other job-related duties as assigned
Minimum Qualifications
Bachelor’s Degree or a combination of work experience and education
Knowledge in application and software development
Knowledge of software design and programming principles
Proficient oral and written communication skills, ability to influence outcomes, and strong attention to detail
Strong analytical, mathematic, and problem-solving skills
Strong team player with ability to demonstrate Agile delivery values working both within a team and working independently
Strategic thinker – can develop a plan to meet a long-term objective
Agile Delivery Values
Openness – Team and stakeholders agree to be open about all work and challenges
Commitment – Personally commit to achieving the goals of the team
Respect – Respect your team members to be capable and independent
Courage – You have courage to do the right thing and work on tough problems
Focus – Everyone focus on the work in the sprint and the goal of the scrum team. Rise and fall as a team
Physical Demands
Physical demands include a considerable amount of time sitting and typing/keyboarding, using a computer (e.g., keyboard, mouse, and monitor), or mobile device
Physical demands with activity or condition may occasionally include walking, bending, reaching, standing, squatting, and stooping
May require occasional lifting/lowering, pushing, carrying, or pulling up to 20lbs
EEO Statement
Southern Glazer's Wine and Spirits, an Affirmative Action/EEO employer, prohibits discrimination and harassment of any type and provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. Southern Glazer's Wine and Spirits provides competitive compensation based on estimated performance level consistent with the past relevant experience, knowledge, skills, abilities and education of employees. Unless otherwise expressly stated, any pay ranges posted here are estimates from outside of Southern Glazer's Wine and Spirits and do not reflect Southern Glazer's pay bands or ranges.","$97,837 /yr (est.)",10000+ Employees,Company - Private,Retail & Wholesale,Wholesale,1968,$10+ billion (USD)
"Rite Pros
3.4",3.4,"Portland, ME",Big Data Engineer,"Big Data Engineer with Bachelor’s degree in Computer Science, Computer Information Systems, Information Technology, or a combination of education and experience equating to the U.S. equivalent of a Bachelor’s degree in one of the aforementioned subjects.
Job Duties:
Define and manage the data architecture framework, standards, and principles, including modeling, metadata, security, reference, and master data.
Define reference architecture, and models to provide a common vocabulary, reusable components, and industry best practices.
Create solution frameworks integrating large or complex data sets.
Lead all data modeling efforts within DataBricks, including the design of data structures and the identification of business transformation logic.
Analyzing and translating business requirements into conceptual and fully detailed logical data models.
Creating logical data models based on existing applications and databases.
Working with business architects, and data stewards to capture business requirements in a Logical Data Model.
Transform the logical representation of the model into a physical representation and work with data engineering team to instantiate and manage the data.
Contribute to assessment of appropriate data platform(s) for solutioning efforts.
Collaborate with the BI and Analytics teams on creating the optimized, reusable semantic model, complete with metadata and lineage information.
Define and govern data modeling and design standards, tools, best practices, and related development for enterprise data models
Maintain the metadata - the ""data about the data"" describing the data model, its structure, and semantics
Recommend data and design patterns for common business functionality
Assist in multi-phase roadmaps within and across functional teams
Provide insight and technology recommendations that result in enterprise solutions and architectural risk mitigation.
Contribute to innovation strategies by exploring, investigating, recommending, benchmarking and implementing new data centric technologies for the platform.
Identify, design, and implement internal process improvements such as automation of manual processes, optimization of data delivery and re-design of existing infrastructure for greater scalability.
Build the infrastructure and processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and/or other data technologies.
Create and maintain data-related documentation (e.g. data dictionaries, data maps, data models etc.)
Work with data engineering and analytics experts to strive for greater functionality in our data systems and incorporation of industry best practices
Work experience / Technologies required for the position :
Experience building and optimizing Big Data, Data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, meta data, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected data sets.
Working knowledge of message queuing, stream processing, and highly scalable Big Data, data stores.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience in a Data Engineer or similar roles, who has attained a Bachelor’s degree in Bachelor’s degree in Engineering, Computer Science, Information Technology, related field or equivalent work experience.
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Work location is Portland, ME with required travel to client locations throughout USA.

Rite Pros is an equal opportunity employer (EOE).

Please Mail Resumes to:
Rite Pros, Inc.
565 Congress St, Suite # 305
Portland, ME 04101.
Email: resumes@ritepros.com","$97,894 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Less than $1 million (USD)
"FinditParts
4.8",4.8,"Los Angeles, CA",Senior Data Warehouse Engineer,"FinditParts is the nation's largest eCommerce provider of heavy-duty truck and trailer parts. From hard-to-find parts to everyday preventative maintenance items, we offer more than 3 million heavy-duty OEM, branded, and aftermarket parts ready to ship. Each month thousands of repair shops, fleets, and owner-operators rely on FinditParts to streamline their part sourcing efforts and keep their trucks on the road.

As the industry leader in parts discovery through visual identification technology and aces/pies, we simplify the complexity of finding the right part to fit any commercial vehicle, reducing the time and frustration associated with parts sourcing.
Founded in 2010, with offices in Los Angeles and the Philippines, FinditParts is well-funded, having recently raised $30 million in Series A funding, and profitable.
SUMMARY
The Senior Data Warehouse Engineer, reporting to the Director of Analytics, will be responsible for leading the complete end-to-end architectural blueprint and development of our Data Warehouse system. A successful candidate will deliver a thoughtful data storage system that will store large data sets through various data intelligence environments. The Senior Data Warehouse Engineer will have a high level of impact, by providing architectural solutions to the design, development, testing, and deployment, and allowing FinditParts to store large volumes of data efficiently and effectively for the organization as a whole.
RESPONSIBILITIES
Lead the architectural design blueprint and development of our data warehouse
End-to-end ownership during the design, development, testing, and deployment
Evaluate architectural and software solutions that will deliver appropriate solutions
Develop and maintain data pipelines as well as API-based or file-based data flows between source systems and the data warehouse
Translate client user requirements into a technical architecture vision and implementation plan
Architect the data intelligence environments, including data lakes, data marts, and metadata repositories
Consult with leadership to define goals and requirements, develop technical requirements, deliver analysis, and thoughtful conclusions with data to provide actionable insights
Build, review, and audit existing ETL jobs and SQL queries
Identify gaps and develop a plan to integrate current systems with a desired future state
Perform root cause analysis of data failures and update existing processes to prevent re-occurring issues
Collaborate with various teams to gather information and create visually appealing, engaging, and informative reports in Tableau
Work closely with the BI team, Product team, and Analytics teams to ensure the Data Warehouse is structured in a way to meet data needs for all business teams
QUALIFICATIONS
5+ years of industry experience in Data Warehousing and/or Data Engineering
Bachelor's Degree in Computer Science, Engineering, Statistics, Mathematics, or another quantitative field
Proven experience managing and transforming data in a data warehouse
Proven experience in manipulating, processing, and extracting value from large data-sets
Familiarity with data schema designs that best support business needs and reporting
High level of experience extracting/cleansing data and generating insights from large transactional data sets using SQL, R, Python, and/or Spark on the cloud
Experience building and managing data pipelines and repositories in cloud environments such as Google Cloud, Microsoft Azure or AWS
Experience in Looker, Redshift, Apache Spark, Spark Structured Streaming, SQL, etc
Experience with cloud computing with Dataproc, Databricks, or similar technologies
Strong problem-solving/analytical abilities
BENEFITS
100% US-based remote
Competitive salary, bonus, and equity
Flexible PTO policy
Paid Medical, dental, and vision insurance options for Employees
Fun and energizing start-up environment
""The US base salary range for this full-time position is $125,000 - $165,000 + bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about specific salary range for your preferred location during the hiring process. Please note that the compensation details listed in the US role postings reflect the base salary only and do not include bonus, equity, or benefits.""
FinditParts is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran or any other characteristic protected by law. FinditParts conforms to the letter of all applicable laws and regulations.","$130,474 /yr (est.)",1 to 50 Employees,Company - Private,Retail & Wholesale,Automotive Parts & Accessories Stores,2010,Unknown / Non-Applicable
Kanini,#N/A,Remote,Data Engineer,"About Kanini
Kanini provides Agile Software Development, Cloud Computing, Data Science, and Location Intelligence services to public and private organizations. We have successfully served our clients in government, finance, transportation, utility, and software industries since 2003.
Why you should join
Working at Kanini is flexible and personal. We are a highly motivated, collaborative team experimenting with the latest technologies. We are committed to everyone having a healthy work/life balance, and we provide extensive mentorship and training resources to help you succeed.
Kanini is looking for a Big Data Engineer who has a deep experience in Data Engineering, AWS, Python, Data Lakes.
Required Skills
At least 10 years software development experience
At least 5 years leading at least one Scrum team of data engineers building data-intensive products with a modern tech stack.
Significant experience with big data ETL pipeline development with Spark, Hive, and related technologies
Significant experience with a general-purpose programming language such as Python, Scala, or Java
Experience with Spark framework and related tools (PySpark, Scala, SparkR, Spark SQL, Spark UI)
Experience with Hadoop ecosystem using HDFS, ADLS Gen2, or AWS S3
Experience with data visualization development using Python, Tableau, or PowerBI
Experience with Azure, AWS or GCP
Solid understanding of performance tuning concepts for relational and distributed database systems
Familiarity with distributed programming, big data concepts, and cloud computing
Education Qualifications
Bachelor’s degree in computer science/Engineering or Technology related field or possess equivalent work experience.
Preferred Qualifications
Cloud certifications from Azure, AWS or GCP
Big data, data engineering or data science certifications from recognized vendors such as Databricks & Cloudera
Kanini Software Solutions, Inc. does not discriminate in employment matters based on race, gender, religion, age, national origin, citizenship, veteran status, family status, disability status, or any other protected class. We support workplace diversity. If you have a disability, please let us know if there is anything we can do to improve the interview process for you; we’re happy to accommodate.
Kanini Software Solutions, Inc., 25 Century Blvd., Ste. 602, Nashville, TN 37214
Job Types: Full-time, Contract
Pay: $60.00 - $70.00 per hour
Work Location: Remote",$65.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"FACEBOOK APP
5.0",5.0,Remote,Azure Data Engineer,"Responsibilities:
Create ER diagrams and write relational database queries
Create database objects and maintain referential integrity
Configure, deploy and maintain database
Participate in development and maintenance of Data warehouses
Design, develop and deploy SSIS packages
Creating and deploying reports
Provide technical design, coding assistance to the team to accomplish the project deliverables as planned/scoped.
Ability to talk to client and get the Business Requirements
Skills:
Azure Data Factory
Azure Devops
Azure Storage/ Data Lake
Extraction, Transformation and Loading
Analytics development
Report Development
Relational database and SQL language
Other Requirements:
· Should be well versed with Data Structures & algorithms
· Understanding of software development lifecycle
· Excellent analytical and problem-solving skills.
· Ability to work independently as a self-starter, and within a team environment.
· Good Communication skills- Written and Verbal
Job Type: Full-time
Salary: $84,454.31 - $190,806.62 per year
Benefits:
Flexible schedule
Health insurance
Compensation package:
1099 contract
Yearly pay
Experience level:
10 years
9 years
Schedule:
Day shift
Experience:
Azure Data engineer: 9 years (Preferred)
SQL: 9 years (Preferred)
Data warehouse: 10 years (Preferred)
Work Location: Remote","$137,630 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Green Worldwide Shipping LLC
3.7",3.7,"Aurora, CO",GEM Data Engineer,"About Green
“Global Reach with a personal touch”, that’s the foundation on which four partners (and long-time friends) agreed to when they started Green Worldwide Shipping back in 2008. This ethos has allowed Green to Grow to a 200+ strong team with 15 offices across the US and growing.At green we are a family of innovative problem-solvers who communicate freely to present creative solutions in an environment of trust, respect, and integrity. We are dedicated to customer service, passionate about logistics, and committed to sustainable growth while inspiring others.
Why this role is special..
GEM (Green Engineered Management) is a technology solutions department that supports various commercial technology products, data management, and development for Green Worldwide Shipping.We are seeking a skilled and experienced Data Engineer to join our dynamic team. As a Data Engineer, you will be responsible for designing, developing, and maintaining our data infrastructure and various technology systems to support efficient data processing and analysis. Your expertise in SQL, CargoWise, database management, Altova Flowforce Server, Altova Mapforce ETL software, Python and C#, along with experience in EDI/API connection protocols integration, will be instrumental in achieving success in this role.
How you will be successful…
Design, develop, and maintain data pipelines and ETL processes to extract, transform, and load data from various sources into our databases using Altova Mapforce ETL software.
Collaborate with cross-functional teams to understand data requirements and provide optimal data solutions.
Develop and optimize SQL queries to retrieve and manipulate data from databases, ensuring efficient data retrieval and storage.
Implement and integrate EDI/API connection protocols to enable seamless data exchange between internal and external systems.
Monitor and maintain the performance, availability, and integrity of the databases and data infrastructure.
Support for reporting, document modification, workflow and database support for internal ERP.
Troubleshoot and resolve data-related issues, including data quality and data integration problems.
Implement data security measures and ensure compliance with data privacy regulations.
Stay updated with industry best practices and emerging technologies in data engineering and incorporate them into the data infrastructure.
Provide technical guidance and support to other team members on data-related projects.
Collaborate with data analysts and data scientists to understand data requirements and support their data exploration and analysis needs.
Create and maintain documentation related to data pipelines, database schemas, and data transformation processes.
What you will bring….
Bachelor's degree in Computer Science, Information Systems, or a related field.
Preferred experience in CargoWise (or related ERP) with reporting, document modification, workflow and database support
Proven experience working with SQL and databases, preferably MySQL.
Prefer strong knowledge and hands-on experience with Altova Flowforce Server and Altova Mapforce ETL software, or related applications.
Experience in integrating and working with EDI/API connection protocols.
Proficiency in Python and/or C# (optional but preferred).
Familiarity with data modeling concepts and techniques.
Experience with data warehousing concepts and technologies.
Excellent problem-solving and analytical skills.
Strong attention to detail and the ability to work in a fast-paced environment.
Effective communication skills to collaborate with cross-functional teams.
Ability to work independently and prioritize tasks effectively.
Preferred Skills:
*
Experience with cloud-based data platforms (e.g., AWS, Azure, Google Cloud Platform).
Knowledge of big data technologies (e.g., Hadoop, Spark) and distributed computing.
Familiarity with data visualization tools (e.g., Tableau, Power BI).
Experience with version control systems (e.g., Git).
Understanding of Agile development methodologies.
What we offer:
Employee High Deductible Health Plan (HDHP) currently paid for 100% by Green.
$1,800 contributed to employee HSA Account, if on HDHP.
Employee Dental provided by Green.
Option to purchase disability insurance.
Paid life insurance and option to buy-up.
401(K) and company match.
Parental Leave Program.
Monthly recognition points to give/receive, which can be converted to gift cards.
Tuition Reimbursement Program.
Weekly fruit baskets in office locations
A tree planted for every new employee to Trees.org
Hybrid work schedule.
Wellbeing Program.
Training and Development.
Access to Mentorship Program.
Green Worldwide Shipping, LLC (Green) is committed to the full inclusion of all qualified individuals. As part of this commitment, Green will ensure that persons with disabilities are provided reasonable accommodations. If reasonable accommodation is needed to participate in the job application or interview process and/or to perform essential job functions, please contact Faye Hughes, Head of People and Culture at faye.hughes@greenworldwide.com.Green is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.
Requirements
Bachelor's degree in Computer Science, Information Systems, or a related field.
Preferred experience in CargoWise (or related ERP) with reporting, document modification, workflow and database support
Proven experience working with SQL and databases, preferably MySQL.
Strong knowledge and hands-on experience with Altova Flowforce Server and Altova Mapforce ETL software, or related applications.
Experience in integrating and working with EDI/API connection protocols.
Proficiency in Python and/or C# (optional but preferred).
Familiarity with data modeling concepts and techniques.
Experience with data warehousing concepts and technologies.
Excellent problem-solving and analytical skills.
Strong attention to detail and the ability to work in a fast-paced environment.
Effective communication skills to collaborate with cross-functional teams.
Ability to work independently and prioritize tasks effectively.
Preferred Skills:
Experience with cloud-based data platforms (e.g., AWS, Azure, Google Cloud Platform).
Knowledge of big data technologies (e.g., Hadoop, Spark) and distributed computing.
Familiarity with data visualization tools (e.g., Tableau, Power BI).
Experience with version control systems (e.g., Git).
Understanding of Agile development methodologies.
Summary
GEM (Green Engineered Management) is a technology solutions department that supports various commercial technology products, data management, and development for Green Worldwide Shipping. We are seeking a skilled and experienced Data Engineer to join our dynamic team. As a Data Engineer, you will be responsible for designing, developing, and maintaining our data infrastructure and various technology systems to support efficient data processing and analysis. Your expertise in SQL, CargoWise, database management, Altova Flowforce Server, Altova Mapforce ETL software, Python and C#, along with experience in EDI/API connection protocols integration, will be instrumental in achieving success in this role.
Job Type: Full-time
Pay: $60,000.00 - $75,000.00 per year
Benefits:
401(k) matching
Dental insurance
Employee discount
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Tuition reimbursement
Vision insurance
Experience level:
4 years
Schedule:
Monday to Friday
Ability to commute/relocate:
Aurora, CO 80014: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL/MySQL (Required)
ETL Software (Preferred)
Data warehouse (Preferred)
Python/C# (Preferred)
Work Location: Hybrid remote in Aurora, CO 80014","$67,500 /yr (est.)",51 to 200 Employees,Company - Private,Transportation & Logistics,Shipping & Trucking,2008,$25 to $100 million (USD)
"McDonald's Corporation
3.5",3.5,"Chicago, IL","Data Engineer, Global Data, CRM","Company Description

McDonald’s evolving Accelerating the Arches growth strategy puts our customers and people first, and leverages our competitive advantages to strengthen our brand. We are recognized on lists like Fortune’s Most Admired Companies and Fast Company’s Most Innovative Companies.
Doubling Down on the 4Ds (Delivery, Digital, Drive Thru, and Development)
Our growth pillars emphasize the important role technology plays as the leading, global omni-channel restaurant brand. Technology enables the organization through digital technology, and improving the customer, crew and employee experience each and every day.
Global Technology forging the way
Leading the digitization of our business is the Technology organization made up of intrapreneurs who build industry defining tech using the latest innovations and platforms, like AI and edge computing to deliver on the next set of cutting-edge opportunities for the business. At McDonald’s you get to solve technology innovation challenges at an incredible scale, and work across global teams who are always hungry for a challenge. This provides access to exciting career paths for technologists. It’s bonus points when you get to see your family and friends use the tech you build at their favorite McD restaurant.

Job Description

We are seeking a highly motivated individual for the Data Engineering Manager for the Marketer Customer Data Platform team on Global Data and Architecture. This role will manage, develop, and lead the execution and implementation cloud-based data delivery systems for the MCD Customer Data Platform. This role will specialize in leading and managing the engineering teams in both onshore and offshore locations, as well as working with the relevant product management teams on overall product direction and implementation of the product vision.
What you will do:
Lead and manage the day-to-day operations for the CRM Data Engineering team with both, onshore and offshore, resources to with a focus on CDP/ CRM customer data technologies
Independently drive efforts in marketing technology and automation, collaborating with key stakeholders and business units. Must be able to find marketing solutions alongside building a long-term strategic vision of a marketing ecosystem across a highly technical, complex data stack
Participate in activities of the data engineering team on the solution, design, architect, implementation, support, and monitoring of CRM & CDP related data products
Develop and implement monitoring and remediation structure and applications for critical data flows
Continuously develop and deploy solutions to streamline and enhance the processes, logic, and tactics around customer cohort identification, and targeting via corporate CRM, marketing, and advertising systems. Additionally, develop the overall best practice framework for use by
Gather business and functional requirements and translate into robust, scalable, and user-friendly solutions

Qualifications
Bachelor’s degree in Information Technology or related technical fields such as computer science, marketing tech, mathematics, engineering, etc.
Minimum 2-5 years of relevant work experience in data analytics, engineering, computer science, mathematics or similar logic based fields
Minimum 2 years hands on experience with complex marketing automation tech stacks, leveraging CDP tools such as mParticle, Segment, and Tealium
Strong analytics skills, root cause analysis experiences on internal and external data and processes to answer business questions and improve application performance
Comfortable partnering and working with a wide range of stakeholders at various levels of management and leadership
Ability to focus on details as well as big picture
Competence and comfort focusing on specific detail engineering questions, as well as “big picture” questions
Ability to leverage critical thinking, problem-solving, and analytical skills to improve user experience, adoption, and outcomes
Curious, task-oriented self-starter with a strong bias for action; Proven track record of “getting things done”
Self-motivated with ability to set priorities and mentor others in a performance driven environment.
Passion for technology and innovation & uncompromising attention to detail & passion for problem solving
Desired Skills

Extensive experience working with Agile methodologies and expertise in production deployment processes and continuous deployment best practices.
1-2 years of hands-on experience with Braze and/or similar market automation delivery platforms
1-2 years of hands on experience with marketing automation and technology experience, understanding key components of marketing technology stacks, including CDP tools such as mParticle, Tealium, or Segment
Experience with clickstream tracking architecture implementations, and knowledge of Adobe Analytics and/or Google Analytics
Demonstrated ability to problem solve in complex technology solutioning
Experience developing Confluence pages and working with Jira
Demonstrable knowledge of multiple programming languages leveraged in martech/adtech stacks – javascript, ruby (liquid logic), html/css, python etc.

Additional Information

McDonald’s is committed to providing qualified individuals with disabilities reasonable accommodations to perform the essential functions of their jobs. Additionally, if you (or another applicant of whom you are aware) require assistance accessing or reading this job posting or otherwise seek assistance in the application process, please contact recruiting.supportteam@us.mcd.com
McDonald’s provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to sex, sex stereotyping, pregnancy (including pregnancy, childbirth, and medical conditions related to pregnancy, childbirth, or breastfeeding), race, color, religion, ancestry or national origin, age, disability status, medical condition, marital status, sexual orientation, gender, gender identity, gender expression, transgender status, protected military or veteran status, citizenship status, genetic information, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.
Nothing in this job posting or description should be construed as an offer or guarantee of employment.","$104,720 /yr (est.)",10000+ Employees,Company - Public,Restaurants & Food Service,Restaurants & Cafes,1955,$10+ billion (USD)
Go Intellects Inc,#N/A,"Washington, DC",Data Engineer,"Work Location: REMOTE (1 – 2 Days On-site/Week may require)
Required Skills:
· Collect, manage, and convert raw data accurately and reliably
· Organize data systems for subgroup access and analyses
· Configure and sustain data cloud structures
· Must have expertise in Data Visualization Tools (Tableau)
· Data Modeling/Science as Python/SAS
· Should have AWS cloud native services, security, data pipeline
· Able to work with structured and unstructured data.
· Validate outputs of data pipelines
· Degree in Data Engineering preferred.
Job Type: Full-time
Pay: Up to $150,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Application Question(s):
Do you have, ""Active Secret (or) Top Secret Security Clearance""?
Work Location: In person","$150,000 /yr (est.)",Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Comcast
3.8",3.8,"West Chester, PA",Data Engineer 2,"Make your mark at Comcast - a Fortune 30 global media and technology company. From the connectivity and platforms we provide, to the content and experiences we create, we reach hundreds of millions of customers, viewers, and guests worldwide. Become part of our award-winning technology team that turns big ideas into cutting-edge products, platforms, and solutions that our customers love. We create space to innovate, and we recognize, reward, and invest in your ideas, while ensuring you can proudly bring your authentic self to the workplace. Join us. You’ll do the best work of your career right here at Comcast. (In most cases, Comcast prefers to have employees on-site collaborating unless the team has been designated as virtual due to the nature of their work. If a position is listed with both office locations and virtual offerings, Comcast may be willing to consider candidates who live greater than 100 miles from the office for the remote option.)

Job Summary
The Data Experience Team (dx) has the responsibility of data engineering and data governance for Comcast data platforms, focused on gathering, organizing, and making sense of Comcast data. Within the Big Data domain, this role is responsible for software development including planning, designing, developing, testing, implementation and management of big data applications focused on video usage and viewership, both on premise and in the cloud. Responsible for design to implementation, including new programs, enhancements, and modifications. Contribute to functional strategy development. The candidate should have experience operating in a DevSecOps team and will be expected to contribute to an internal DevSecOps culture encompassing end-to-end responsibility for development, deployment, production support, monitoring, data quality and automation of their applications.
Job Description
Core Responsibilities
Optimize data ingest, filtering and improvement
Knowledge of data structures, design patterns, and algorithms
Experience with OOO programming languages and SQL
An open mind and a passion for coding excellence
Building data products using AWS cloud and Spark processing technologies.
Experience building End to End Datalake data product ingestions and optimize the pipelines.
Great design and problem-solving skills, with a strong bias for architecting at scale
Strong troubleshooting and problem-solving skills, adaptable, proactive, and willing to take ownership
Analyzes and determines integration needs.
Evaluates and plans software designs, test results and technical manuals.
Reviews literature, patents and current practices relevant to the solution of assigned projects.
Programs new software, web applications and supports new applications under development and the customization of current applications.
Edits and reviews technical requirements documentation.
Works with Quality Assurance team to determine if applications fit specification and technical requirements.
Displays knowledge of engineering methodologies, concepts, skills and their application in the area of specified engineering specialty.
Displays knowledge of and ability to apply, process design and redesign skills.
Displays in-depth knowledge of and ability to apply, project management skills.
Consistent exercise of independent judgment and discretion in matters of significance.
Regular, consistent and punctual attendance. Must be able to work nights and weekends, variable schedule(s) and overtime as vital.
Other duties and responsibilities as assigned.
Required Technical Skills:
Programming languages like Scala, Python.
Databricks, Apache Spark, Spark Streaming, Pyspark, Spark Sql.
AWS Cloud Computing like Glue, Lambada, CloudWatch, Athena.
Big Data Architecture, Solutions & Technologies.
Relational databases like Hive, Oracle, and No-Sql Databases like Dynamodb.
CI/CD tools like GoCD, Jenkins, Concourse.
Employees at all levels are expected to:
Understand our Operating Principles; make them the guidelines for how you do your job.
Be responsible for the customer experience - think and act in ways that put our customers first, give them seamless digital options at every touchpoint, and make them promoters of our products and services.
Know your stuff - be hard-working learners, users and advocates of our groundbreaking technology, products and services, especially our digital tools and experiences.
Win as a team - make big things happen by working together and being open to new insights!
Be an active part of the Net Promoter System - a way of working that brings more employee and customer feedback into the company - by joining huddles, making call backs and helping us elevate opportunities to do better for our customers.
Get results and growth!
Respect and promote inclusion & diversity.
Do what's right for each other, our customers, investors and our communities.
Disclaimer:
This information has been designed to indicate the general nature and level of work performed by employees in this role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications.
This position is ineligible for visa sponsorship. To be considered for this role, you must be legally authorized to work in the United States and not require sponsorship for employment now or in the future.
Comcast is an EOE/Veterans/Disabled/LGBT employer.
Comcast is proud to be an equal opportunity workplace. We will consider all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, or any other basis protected by applicable law.

Education
Bachelor's Degree
While possessing the stated degree is preferred, Comcast also may consider applicants who hold some combination of coursework and experience, or who have extensive related professional experience.
Relevant Work Experience
2-5 Years

Salary:
Pay Range: This job can be performed in Denver Campus, with a Pay Range of $87,529.82 USD - $131,294.73 USD
Comcast intends to offer the selected candidate base pay within this range, dependent on job-related, non-discriminatory factors such as experience.

Base pay is one part of the Total Rewards that Comcast provides to compensate and recognize employees for their work. Most sales positions are eligible for a Commission under the terms of an applicable plan, while most non-sales positions are eligible for a Bonus. Additionally, Comcast provides best-in-class Benefits. We believe that benefits should connect you to the support you need when it matters most, and should help you care for those who matter most. That’s why we provide an array of options, expert guidance and always-on tools, that are personalized to meet the needs of your reality – to help support you physically, financially and emotionally through the big milestones and in your everyday life. Please visit the compensation and benefits summary on our careers site for more details.","$109,412 /yr (est.)",10000+ Employees,Company - Public,Telecommunications,Telecommunications Services,1963,$10+ billion (USD)
Arcon Group Inc,#N/A,"Mooresville, NC",Data Engineer,"Job Title: Dot NET Full Stack
Location: Charlotte, NC (Day 1 Onsite)
Duration: 12+ Months
Client: Wells Fargo
only on W2
Interview Mode: Phone & TEAMS
Note: We are looking only at OPT & H1-B
Minimum 3-5 years of experience with C#, .NET
Familiarity with the ASP.NET framework, SQL Server, and design/architectural patterns (e.g. Model-View-Controller (MVC))Experienced in implementing niche solutions with C# and .NET
Abundant experience in designing and writing reusable code with C# and .NET
Experienced with SQL/Oracle/Linux/Windows Servers
Work experience with Oracle, SQL, MySQL Database
Good to Have
Familiarity with Any Cloud Functions
C++/Java/Perl
Power Shell script
SAFE Agile Development
Job Type: Full-time
Salary: $45.00 - $55.00 per hour
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Mooresville, NC 28115: Reliably commute or planning to relocate before starting work (Required)
Education:
Master's (Preferred)
Experience:
Informatica: 1 year (Preferred)
SQL: 2 years (Preferred)
ASP.NET: 1 year (Preferred)
License/Certification:
Driver's License (Preferred)
Work Location: One location",$50.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Core4ce
4.3",4.3,"Herndon, VA",Data Engineer,"The Data Engineer will provide the engineering support to data science and software engineering team members.
Design and optimize Data Pipelines using Spark, Hudi, EMR cloud services, and Kubernetes containers
Make sure pedigree and provenance of the data is maintained such that the access to data is protected
Clean and preprocess data to enable analytic access
Architects complex, repeatable ETL processes
Provide Advanced Database Administration support in Oracle, MySQL, MariaDB, MongoDB, Elastic and others
Supports Experience with Targeting using Sponsor Tools, Reverse Engineering
Develop API connectors to enable ingest of new data catalog entries from databases and files
Ensure that data mappings will provide the best performance for expected user experience
Augmenting technical support in the area of data science, data engineering, and systems engineering, and reviewing and providing technical assessments.
Provides current system architecture documentation, engineering/web development programming support for program/project requirements defined tasks, data science/data engineering related technical assessments, effective communication with users and managers, and server administration, to include hardware and software support to existing servers.
Support ad-hoc data analysis requirements defined by the client's Leadership.
Reporting solutions will encompass multiple technology platforms used by the client and will drive business process re-design and raining/change management initiatives.
Will work on Data cleaning and transformation efforts in delivery of CDRLs.
Supports Deliverables and Reports

Requirements
5+ years experience working with Data Sets
Experienced in extracting and aggregating structured and unstructured data.
Experienced in data programming languages and tools such as Python and R.
Experience with SQL or similar database language.
Experience designing and implementing data models to enable, sustain and enhance the value of information they contain
Knowledge of business intelligence reporting tools and data visualization software including Tableau.
Strong analytical and critical thinking skills.
Ability to work collaboratively and effectively in a team environment.
TS/SCI with Full Scope Poly Required

All qualified applicants will receive consideration for employment without regard to race, color, sex, sexual orientation, gender identity, religion, national origin, disability, veteran status, age, marital status, pregnancy, genetic information, or other legally protected status

Required Skills

Required Experience","$103,496 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2018,$100 to $500 million (USD)
"Unilever
4.1",4.1,Remote,Health & Wellbeing Data Engineer,"Health & Well Data Engineer
Remote – USA; Los Angeles
Health & Wellness is a strategic Unilever global business unit established to capture the growth opportunity in the €140bn global consumer health segment defined by Vitamins, Minerals, and Supplements (VMS) product category. Our ambition is to build a €5bn business globally; and be a global top 3 player within this space.
As of today, our brands include Equilibra in Italy; OLLY, SmartyPants, Liquid IV, Onnit, Welly and Nutrafol in the US with ~€1.4bn in turnover. The H&W data and analytics team operates across all these brands in one common data function with the aim of helping these brands to grow through smarter, faster, and better data driven decision making. The data team within H&W was set up in May 2022 and is looking to expand. This is a fantastic opportunity to shape the future of data in a global business unit with the latest technologies in the cloud.
What you will do:
collaborate with other engineering and business teams within H&W and Unilever to solve complex challenges using data.
drive forward best practice by building out data frameworks and design patterns to be utilized across the H&W data infrastructure.
build production ready distributed ETL data pipelines from a wide range of different sources (APIs, flat files, databases, ERP systems etc.)
build performant and reliable data models to democratize the use of data across H&W
build integrations to facilitate the use of other technologies in H&W ecosystem (e.g Kinnaxis, Anaplan)
review and deploy code from other team members as part of a DevOps process, providing coaching and mentoring to junior developers.
manage access within the environments to ensure data security protocols are being met.
contribute to architectural and governance decision marking within the H&W data ecosystem.
Who you are:
Passionate about all things data
Entrepreneurial Self-starter with the ability to thrive in a fast paced start up environment.
Fast learner with the ability to pick up new technologies quickly.
Creative problem solver who thinks outside the box
What you will bring:
3-5 years’ Data engineering experience with strong pyspark and SQL skills
1+ year Distributed computing experience [databricks/Splunk]
1+ year experience working with data in cloud environments [GCP/Azure/AWS]
Bachelor’s degree required.
Experience with streaming workloads is a plus
Experience working within a delta lakehouse in databricks is a plus
Tech Stack:
Cloud agnostic: Databricks (pyspark, scala, SQL)
Azure: Azure Data Factory, Azure Logic Apps, Azure Data Lake Storage (ADLS), Azure Blob Storage, Azure Machine Learning, PBI (data modelling, DAX)
GCP: Google Big Query, Google Cloud Storage
AWS: S3
Pay: The pay range for this position is $83,200 - $124,700. Unilever takes into consideration a wide range of factors that are utilized in making compensation decisions including, but not limited to, skill sets, experience and training, licensure and certifications, qualifications and education, and other business and organizational needs. Bonus: This position is bonus eligible. Long-Term Incentive (LTI): This position is LTI eligible. Benefits: Unilever employees are eligible to participate in our benefits plan. Should the employee choose to participate, they can choose from a range of benefits to include, but is not limited to, health insurance (including prescription drug, dental, and vision coverage), retirement savings benefits, life insurance and disability benefits, parental leave, sick leave, paid vacation, and holidays, as well as access to numerous voluntary benefits. Any coverages for health insurance and retirement benefits will be in accordance with the terms and conditions of the applicable plans and associated governing plan documents.
-
Unilever is an organization committed to diversity and inclusion to drive our business results and create a better future every day for our diverse employees, global consumers, partners, and communities. We believe a diverse workforce allows us to match our growth ambitions and drive inclusion across the business. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, age, national origin, or protected veteran status and will not be discriminated against on the basis of disability.

Employment is subject to verification of pre-screening tests, which may include drug screening, background check, credit check and DMV check.

If you are an individual with a disability in need of assistance at any time during our recruitment process, please contact us at
NA.Accommodations@unilever.com
. Please note: This email is reserved for individuals with disabilities in need of assistance and is not a means of inquiry about positions or application statuses.
#LI-Remote","$103,950 /yr (est.)",10000+ Employees,Company - Public,Manufacturing,Consumer Product Manufacturing,1872,$10+ billion (USD)
"Bright Health
2.8",2.8,"Austin, TX",Senior Data Software Engineer,"Back to Career Site

Our Mission is to Make Healthcare Right. Together. Built upon the belief that by connecting and aligning the best local resources in healthcare delivery with the financing of care, we can deliver a superior consumer experience, lower costs, and optimized clinical outcomes.
What drives our mission? The company values we live and breathe every day. We keep it simple: Be Brave. Be Brilliant. Be Accountable. Be Inclusive. Be Collaborative.
If you share our passion for changing healthcare so all people can live healthy, brighter lives – apply to join our team.

SCOPE OF ROLE
Senior Data Software Engineers spend most of their time hands-on in the creation, feature development, and subsequent maintenance of Bright Health properties. The more senior their level, the more that individual will assist in the continuous improvement of the team's productivity and overall design of our systems. Additionally, senior talent plays a key role in building and maintaining a culture that focuses on making sure every engineer progresses in their career in line with personal goals/expectations.
ROLE RESPONSIBILITIES
The Senior Data Software Engineer description is intended to point out major responsibilities within the role, but it is not limited to these items:
Collaborate directly with teammates to solve business needs and drive solutions to solve key business priorities
Contribute across the complete development lifecycle, including technical design, work estimation, implementation, testing, and addressing feedback or bugs
Lead and contribute to continually improve the team's productivity, culture, and code quality
Peer review team member's code and provide constructive feedback to improve quality
Create documentation and processes as needed to improve team productivity and transparency
EDUCATION, TRAINING, AND PROFESSIONAL EXPERIENCE
Bachelor's degree in Computer Science, Computer Engineering, Information Systems, or equivalent experience required
Five (5) or more years of software development experience and/or a reviewable portfolio of work
Deep experience building/deploying/maintaining scalable and always available consumer-facing products
Working knowledge of Agile methodologies (SCRUM)
PROFESSIONAL COMPETENCIES
Experience with Databricks, Spark, and Scala
Experience data modeling for scalability, and efficiency while meeting business needs
Seasoned estimation skills — can reasonably judge the complexity of a feature and appropriately break it down (if needed) and size it
Strong work ethic plus follow-through on commitments (works smartly to get the job done right and on time)
Proven track record of high standards for self, team, company, and ultimately clients
Proactive approach to problem-solving (acting without being told, driving for measurable results, etc.)
WORK ENVIRONMENT
This is a remote position
We're Making Healthcare Right. Together.
We are realizing a completely different healthcare experience where payors, providers, doctors, and patients can all feel connected, aligned and unified on the same team. By eradicating the frictions of competing needs, we are making it possible to give everyone more of what they want and deserve. We do this by:
Focusing on Consumers
We understand patient pain points, eliminating complexity while increasing transparency, for greater access and easier navigation.
Building on Alignment
We integrate and align individual incentives at all levels, from financing to optimization to delivery of care.
Powered by Technology
We employ our purpose built, integrated data platform to connect clinical, financial, and social data, to deliver exceptional outcomes.

As an Equal Opportunity Employer, we welcome and employ a diverse employee group committed to meeting the needs of Bright Health, our consumers, and the communities we serve. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.","$117,199 /yr (est.)",1001 to 5000 Employees,Company - Public,Healthcare,Health Care Services & Hospitals,2015,$1 to $5 billion (USD)
"HCA Healthcare
3.3",3.3,"Nashville, TN",Data Engineer,"Introduction
Last year our HCA Healthcare colleagues invested over 156,000 hours volunteering in our communities. As a Staff Data Engineer with HCA Healthcare you can be a part of an organization that is devoted to giving back!
Benefits
HCA Healthcare, offers a total rewards package that supports the health, life, career and retirement of our colleagues. The available plans and programs include:
Comprehensive medical coverage that covers many common services at no cost or for a low copay. Plans include prescription drug and behavioral health coverage as well as free telemedicine services and free AirMed medical transportation.
Additional options for dental and vision benefits, life and disability coverage, flexible spending accounts, supplemental health protection plans (accident, critical illness, hospital indemnity), auto and home insurance, identity theft protection, legal counseling, long-term care coverage, moving assistance, pet insurance and more.
Free counseling services and resources for emotional, physical and financial wellbeing
401(k) Plan with a 100% match on 3% to 9% of pay (based on years of service)
Employee Stock Purchase Plan with 10% off HCA Healthcare stock
Family support through fertility and family building benefits with Progyny and adoption assistance.
Referral services for child, elder and pet care, home and auto repair, event planning and more
Consumer discounts through Abenity and Consumer Discounts
Retirement readiness, rollover assistance services and preferred banking partnerships
Education assistance (tuition, student loan, certification support, dependent scholarships)
Colleague recognition program
Time Away From Work Program (paid time off, paid family leave, long- and short-term disability coverage and leaves of absence)
Employee Health Assistance Fund that offers free employee-only coverage to full-time and part-time colleagues based on income.
Learn more about Employee Benefits
Note: Eligibility for benefits may vary by location.

Would you like to unlock your potential with a leading healthcare provider dedicated to the growth and development of our colleagues? Join the HCA Healthcare family! We will give you the tools and resources you need to succeed in our organization. We are looking for an enthusiastic Staff Data Engineer to help us reach our goals. Unlock your potential!
Job Summary and Qualifications
Data Engineers within HCA’s Information and Analytics organization are responsible for defining and implementing data management practices across the enterprise. This full-time position will focus primarily on enterprise data management and migrating of data to the cloud. Data Engineers are expected to source and incorporate new data sources into the Enterprise Data Ecosystem. The responsibilities will include writing, testing, and reviewing ETL pipelines for defining and implementing data management practices across the enterprise. Due to the emerging and fast-evolving nature of Cloud technology and practice, the position requires that one stay well-informed of technological advancements and be proficient at putting new innovations into effective practice.
As a Data Engineer, you will work closely with all team members to create a modular, scalable solution that addresses current needs, but will also serve as a foundation for future success. The position will be critical in building the team’s engineering practices in test driven development, continuous integration, and automated deployment and is a hands-on team member who actively coaches the team to solve complex problems. This is a leadership position that assumes the responsibility for project success and the upward development of team members. They are the development team's point of contact that must interface with business partners of varying roles ranging from technical staff to executive leadership.
As a Staff Data Engineer level, the role requires 'self-starters' who are proficient in problem solving and capable of bringing clarity to complex situations. It requires contributing to strategic technical direction and system architecture approaches for individual projects and platform migrations. It also requires working closely with others, frequently in a matrixed environment, and with little supervision. This candidate will have a history of increasing responsibility in a small multi-role team. This position requires a candidate who can analyze business requirements, perform design tasks, construct, test, and implement solutions with minimal supervision.
Our Purpose
Applied to this position, your skills will help transform healthcare through technology and solutions that dramatically improve patient care and business operations.
Core Competencies
At HCA ITG, your deliverables will influence patient care. Every process, technology, and decision matters. This role will provide leadership and deep technical expertise in all aspects of solution design and application development for specific business environments. It will focus on setting technical direction on groups of applications and similar technologies as well as taking responsibility for technically robust solutions encompassing all business, architecture, and technology constraints.
Responsible for building and supporting a Cloud based ecosystem designed for enterprise-wide analysis of structured, semi-structured, and unstructured data. Direct the transformation from HCA Healthcare’s current on premise Teradata platform to Google Cloud Platform to enable analytics and machine learning at scale.
Design the cloud environment from a comprehensive perspective, ensuring that it satisfies all the company’s needs.
Develop, manage, and own full data lifecycle from raw data acquisition through transformation to end user consumption
Share knowledge and experience to contribute to growth of overall team capabilities
Perform activities such as deployment, maintenance, monitoring, and management inside the cloud framework that has been created
Provide guidance on technology choices and design considerations for migrating data to the Cloud
Maintain a holistic view of information assets by creating and maintaining artifacts that illustrate how information is stored, processed, and accessed
Demonstrate deep understanding and act as a leader in the team’s continuous integration and continuous delivery automation pipeline
Collaborate with business analysts, project lead, management, and customers on requirements
Design fit-for-purpose products to ensure products align to the customer's strategic plans and technology road maps
Actively participate in technical group discussions and adopt any new technologies to improve the development and operations.
Assist team members with production issues and offer support, guidance, and assist in communicating issues with appropriate stakeholders when necessary.
Provide leadership on key technology choices for Enterprise Data Ecosystem including data warehouse, analytical and big data platforms.
Ensure architectural, quality, and governance adherence through design reviews.
Education & Experience
Bachelor's degree in computer science or related field - Required
Master's degree in computer science or related field - Preferred
3+ years of experience in Data Engineer/Architect- Required
1+ year(s) of experience in Healthcare - Preferred
8+ years of experience in Information Technology - Required
Knowledge, Skills, Abilities, Behaviors
A successful candidate will have:
Experience developing and supporting data pipelines from various source types (on-prem rdbms, AWS, GCS bucket, flat file) to Big Query utilizing Google Cloud Platform native technologies
Knowledge and experience using the following technologies
o Big Query
o Dataflow, Data Proc, Data Fusion, Cloud Composer
o GSUTIL, GCS, Kafka, Pub/Sub
o Data Catalog/Dataplex
o Python, Unix, Linux
Strong understanding of best practices and standards for cloud application design and implementation.
Extensive experience with relational database management systems; Teradata, Oracle or SQL Server:
o Advanced SQL skills
o Write, tune, and interpret SQL queries
o BTEQs
o Stored procedures
Experience with Unstructured Data
Ability to troubleshoot, maintain, reverse engineer, and optimize existing ETL pipelines.
Requires strong practical experience in agile application development and DevOps discipline, including deployment of CI/CD pipelines in Git
Ability to multitask and to balance competing priorities.
Expertise in planning, implementing, supporting, and tuning Cloud ecosystem environments using a variety of tools and techniques.
Ability to define and utilize best practice techniques and to impose order in a fast-changing environment. Must have strong problem-solving skills.
Strong verbal, written, and interpersonal skills, including a desire to work within a highly-matrixed, team-oriented environment.
A successful candidate may have:
o Experience in Healthcare Domain
o Experience in Patient Data
Certifications (a plus, but not required)
GCP Cloud Professional Data Engineer
HCA Healthcare has been recognized as one of the World’s Most Ethical Companies® by the Ethisphere Institute more than ten times. In recent years, HCA Healthcare spent an estimated $3.7 billion in cost for the delivery of charitable care, uninsured discounts, and other uncompensated expenses.
""There is so much good to do in the world and so many different ways to do it.""- Dr. Thomas Frist, Sr.
HCA Healthcare Co-Founder
Be a part of an organization that invests in you! We are reviewing applications for our Staff Data Engineer opening. Qualified candidates will be contacted for interviews. Submit your application and help us raise the bar in patient care!
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.","$93,734 /yr (est.)",10000+ Employees,Company - Public,Healthcare,Health Care Services & Hospitals,1968,$10+ billion (USD)
Zenotis Technologies inc,#N/A,"Addison, TX",Big Data Engineer,"Role: Big Data Engineer
Location – Addison, TX/ Charlotte, NC / Chandler, AZ (Hybrid – 3 days onsite 2 days remote)
Position Summary:
Looking for a big data engineer who loves solving complex problems across a full spectrum of technologies. The ideal candidate is excited by experimentation and looking for a new challenge that stretches their talents. The big data engineer will help ensure that our technological infrastructure operates seamlessly in support of business objectives.
The engineer must be able to:
· Translate complex technical and functional requirements into detailed designs
· Guide the development and testing teams in developing and implement pipelines that extract, transform, and load data into an information product that helps the organization reach its strategic goals
· Focus on ingesting, storing, processing, and analyzing large datasets
· Deep understanding on how to create scalable, high-performance web services for tracking data
· Investigate alternatives for data storing and processing to ensure the implementation of the most streamlined solutions
Primary Skill
Hadoop
Secondary Skill
Kafka
Tertiary Skill
Oracle Exadata
Required Skills
· Translate complex technical and functional requirements into detailed designs
· Design data pipelines using design tools and provide solution intent in provided templates
· Work closely with data science team to implement data analytics pipelines
· Analyze vast number of data stores to uncover insights
· Define data governance policies and support data-versioning processes
· Maintain security and data privacy, working closely with data protection guidelines.
· Desired Skills
· Experience with Python, Spark, and Hive
· Understanding of data-warehousing and data-modeling techniques
· Knowledge of industry-wide visualization and analytics tools (ex: Tableau, R)
· Strong data engineering skills with Hadoop platform
· Experience with streaming frameworks such as Kafka
· Knowledge of Core Java, Linux, SQL, and any scripting language
· Good interpersonal skills and positive attitude
Job Type: Full-time
Salary: Up to $70.00 per hour
Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
8 hour shift
Work Location: On the road",$70.00 /hr (est.),201 to 500 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Synovize
5.0",5.0,Remote,Big Data Engineer,"Synovize is a cutting-edge technology company that specializes in providing advanced data analytics and intelligence solutions. Our mission is to empower businesses with the tools and insights they need to make data-driven decisions and drive innovation. We are committed to delivering exceptional results by combining our expertise in big data technologies with our passion for creating impactful solutions.
As a Big Data Engineer at Synovize, you will have the opportunity to work on exciting projects that involve designing and implementing scalable data solutions. You will collaborate with a talented team of professionals, including software developers, data scientists, and domain experts, to tackle complex data engineering challenges and deliver high-quality solutions to our clients.
Job Title:
Big Data Engineer
Company: Synovize
Number of Positions: 2
Location: Remote
Length: 12 Months
Tax Terms: 1099/C2C or W-2
Rate/Range: $70-$90/hr
Work Authorization: US Citizen
Security Clearance: None
Skills Needed:
Proficiency in Java and Kotlin programming languages.
Strong knowledge of AWS (Amazon Web Services) and its big data offerings.
Experience with BigData technologies, frameworks, and tools.
Familiarity with Kafka for streaming data processing.
Proficiency in Python programming language.
Responsibilities:
Design and develop scalable data pipelines to process, integrate, and analyze large volumes of structured and unstructured data.
Implement data ingestion, transformation, and storage mechanisms using big data technologies and frameworks.
Collaborate with data scientists and analysts to understand data requirements and optimize data models for analytics and machine learning.
Utilize AWS services and tools to build and maintain data infrastructure, ensuring data quality, integrity, and security.
Troubleshoot and resolve data-related issues, optimizing performance and efficiency.
Stay up-to-date with emerging trends and advancements in big data technologies, recommending and implementing improvements to existing systems and processes.
Requirements:
Bachelor's or master's degree in computer science, engineering, or a related field.
Proven experience as a Big Data Engineer or similar role, with a strong understanding of big data technologies and architectures.
Proficiency in Java and Kotlin programming languages.
Strong knowledge of AWS and its big data offerings.
Experience with BigData technologies, frameworks, and tools.
Familiarity with Kafka for streaming data processing.
Proficiency in Python programming language.
Strong problem-solving skills and ability to work in a fast-paced, collaborative environment.
Excellent communication and interpersonal skills.
Join Synovize and be part of a dynamic team that is at the forefront of data innovation. Together, we will drive meaningful change and unlock the full potential of data-driven decision-making.
Job Types: Full-time, Contract
Pay: $70.00 - $90.00 per hour
Benefits:
401(k)
Dental insurance
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Vision insurance
Schedule:
10 hour shift
4 hour shift
8 hour shift
Choose your own hours
Application Question(s):
Are you a US Citizen?
Years of experience in Java?
Years of experience in Kotlin?
Years of experience in big data?
Years of experience in AWS?
Security Clearance
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$80.00 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"MetroStar
3.7",3.7,United States,Data Engineer (Mid),"As a Data Engineer (Mid), you'll bring creative architect solutions to end customers with the goal to make an impact across the federal government.
We know that you can't have great technology services without amazing people. At MetroStar, we are obsessed with our people and have led a two-decade legacy of building the best and brightest teams. Because we know our future relies on our deep understanding and relentless focus on our people, we live by our mission: A passion for our people. Value for our customers.
If you think you can see yourself delivering our mission and pursuing our goals with us, then check out the job description below!
What you'll do:
Work with AI team members to operationalize data pipelines and ML tasks.
Provide day-to-day support of deploying Python-native ML pipelines and perform data engineering tasks to enable AI/ML capabilities.
Present results to a diverse audience in presentation or report form.
Support architectural leadership, technical support, and advisement services to ensure identity management system technologies are integrated and meeting the appropriate security requirements.
Support leadership who engage with senior level executives at a public facing Federal agency and provide subject matter expertise in security architecture and other key domain areas.
What you'll need to succeed:
5+ years of experience in Data/ML engineering (if school experience is used, at most that would contribute to 2 years of actual experience).
Experience with ETL, Data Labeling and Data Prep.
Experience designing, implementing, and maintaining data architecture and services to be used for AI/ML. Additionally, operationalizing and maintaining AI/ML models in production.
The ability to perform data analytics on program related or system related activities. This will include assessing performance and manual processes implementing methods/algorithms to automate/optimize
A bachelor's degree in Computer Science, Information Technology Management or Engineering, or other comparable degree or experience and the ability to obtain and maintain DHS Suitability.

Like we said, we are obsessed with our people. That's why we offer a generous benefits package, professional growth, and valuable time to recharge. Learn more about our company culture code and benefits. Plus, check out our accolades.
Don't meet every single requirement?
Studies have shown that women, people of color and the LGBTQ+ community are less likely to apply to jobs unless they meet every single qualification. At MetroStar we are dedicated to building a diverse, inclusive, and authentic culture, so, if you're excited about this role, but your previous experience doesn't align perfectly with every qualification in the job description, we encourage you to go ahead and apply. We pride ourselves on making great matches, and you may be the perfect match for this role or another one we have. Best of luck! – The MetroStar People & Culture Team
What we want you to know:
In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification form upon hire.
MetroStar Systems is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. The statements herein are intended to describe the general nature and level of work being performed by employees and are not to be construed as an exhaustive list of responsibilities, duties, and skills required of personnel so classified. Furthermore, they do not establish a contract for employment and are subject to change at the discretion of MetroStar Systems.
Not ready to apply now?
Sign up to join our newsletter here.",#N/A,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1999,$25 to $100 million (USD)
Provista Software Corporation,#N/A,Remote,Data Engineer,"As a successful candidate, you’ll play a pivotal role in the data conversion and business intelligence solution efforts during project implementation for our state and local clients. We are looking for you to join our software implementation team as a Senior Data Engineer.
Qualification:
5+ years of experience working independently as ETL developer on data migration projects that covered diverse database platforms and complex data conversions.
Must have experience across leading Database Platforms especially Oracle, Azure SQL, Snowflake.
Expert level experience with Extraction Transform Load (ETL) plan, protocols, and tools.
Expert level experience with programming languages SQL, Phyton, Java, C#.
Good at communication with clients and internally.
Expert at problem solving and troubleshooting skills.
Bachelor's degree, preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience
Must be a U.S. Citizen or Green Card Holder.
Functional Responsibility:
Data Conversion -
In collaboration with Data Stewards, discover inventory of legacy data sources and all data the client expects to be migrated to destination Modified Off the Shelf (MOTS) solution.
Complete gap assessment between legacy data and MOTS data requirements, and options to address the gaps.
Develop data migration map to minimally include data dictionary, detailed data map of all elements of the current database, data transformation rules, relationship rules, validation rules, process of migrating images, and process for recovering missing or erroneous data.
Identify issues, risks, and/or barriers that may interfere with data migration and recommendations to resolve or mitigate.
Conduct testing and quality assurance to ensure the data is migrated accurately.
Migrate data from legacy to destination MOTS solution progressively using a sample set of data and the entirety of the data prior to deployment into relevant go-live environments (e.g., staging, production, etc.)
Prepare data migration reports that includes – record counts of source vs migrated vs modified; field level validation of data at row and column; discovered exceptions.
Business Intelligence -
Develop ETL pipelines in and out of Snowflake data warehouse using combination of Python, SnowSQL, and SQL.
In collaboration with Data Stewards, create Snowflake data mart as curated subset of data to support the analytics and business intelligence needs of subgroup of users.
Translate requirements for business intelligence and reporting to database design and reporting design.
Conduct discovery sessions with users to capture business intelligence reports.
Develop Tableau self-service ad hoc reports to support operational users.
Develop Tableau Dashboards to support executive, management, and operational users.
About us:
Provista Software Corporation is fast growing Software Solution provider for US State and Local Government Clients.
Job Type: Full-time
Pay: $67,107.98 - $134,285.66 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Life insurance
Paid time off
Parental leave
Professional development assistance
Vision insurance
Compensation package:
Yearly pay
Experience level:
5 years
Schedule:
8 hour shift
Application Question(s):
What is your most preferred work time zone ?
Experience:
ETL: 5 years (Required)
Data warehouse: 1 year (Preferred)
Tableau: 1 year (Preferred)
Talend: 1 year (Preferred)
Microsoft SQL Server: 5 years (Required)
Work Location: Remote","$100,697 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Ripple Effect Consulting
5.0",5.0,"Denver, CO",Data Engineer - 004/005,"Data Engineer
Hybrid-remote based in Denver, CO | $80-$160k
Job Overview:
We are looking for a savvy Data Engineer to join our growing team of analytics experts. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
Responsibilities for Data Engineer
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Qualifications for Data Engineer
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:
1) Experience with big data tools
2) Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
3) Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
4) Experience with cloud services
5) Experience with stream-processing systems: Storm, Spark-Streaming, etc.
6) Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Job Type: Full-time
Pay: $80,000.00 - $160,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Denver, CO: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person","$120,000 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Codinix Technologies
5.0",5.0,"Boston, MA",Cloud Data Engineer - AWS,"Only US Citizen
Strong in warehouse technologies (Hadoop, Spark, AWS, etc.)
Bachelor's degree in Computer Science, Information Systems, or another related field
5+ years of data engineering, and cloud engineering experience
Experience in Agile project methodologies and DevOps
Experience with AWS (certification preferred; including Athena, Redshift, Glue), Cloudera
Experience building a CI/CD pipeline to deploy applications on AWS (GitLab, Terraform)
Advanced experience programming and scripting on Unix/Linux (i.e. Python, Bash)
Experience with CTRL-M, Cron (other) scheduling of batch jobs
Experience migrating legacy systems to cloud-ready architectures
Job Type: Contract
Pay: Up to $75.00 per hour
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Boston, MA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",$75.00 /hr (est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
BIGCLFY,#N/A,"Dallas, TX","W2/ 1099, No C2C - SSIS ETL Data Engineer (Minneapolis, MN/ Dallas-Irving)","No C2C. It's W2/1099 Requirement
Only taking USC, GC or H4 EAD, H1 Transfer, L2 Visa or TN Visa - they will not take OPT EAD or CPT
Need to have confirmation in writing that you will be onsite by Day 1
ALL ROLES ARE HYBRID IN THE OFFICE (2-3 DAYS/WEEK) UNLESS REMOTE IS NOTED
Please submit qualified candidates and include FULL LEGAL NAME as it appears on the passport
If your candidate is relocating, need to have confirmation in writing that they will be onsite by Day 1
Senior SSIS/ETL Data Engineer
12 months – 3 days in office
Minneapolis, MN or Dallas-Irving, TX
5+ years of experience with SQL Server 2016 (or newer) and SSIS
7+ years of data engineering
Must be capable of leading teams
2+ years of experience working with large scale SQL implementations
Experience designing, developing, and maintaining complex stored procedures, functions, and views using T-SQL
Experience designing, developing, and maintaining ETL processes using SSIS
Should understand how to unit test their product
Hands on experience with Visual Studio to document
Experience in financial or regulatory environments highly preferred
Ideal candidates will have experience creating conceptual, logical, and physical database designs
Preference given to candidates with extensive knowledge and experience with performance monitoring, database and SQL tuning
Bachelor’s or Master's in computer science or related field or equivalent work experience
Job Type: Contract
Pay: From $73.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Chandler, AZ 85225: Reliably commute or planning to relocate before starting work (Preferred)
Application Question(s):
Expected Payrate on W2/1099 Tax Term? Please mention. Thanks.
Experience:
Risk Analytics: 5 years (Preferred)
Work Location: In person",$73.00 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Wallero
4.4",4.4,"Seattle, WA",Data Engineer,"Hi,
Pretty much interested in your resume, we have an urgent opening for Data Engineer role with one of our clients based in Seattle WA.
Position: Data Engineer
Location: Seattle WA - Remote (100%)
Description:
Good Experience in reporting space including PowerBI, ADF, and possibly standing up a data warehouse/data mart.
The main scope centering is around reporting.
Experience with SQL, and agile framework including requirement gathering and story/feature writing skills.
Need someone strong who is comfortable leading and guiding teams in that capacity.
Migrating data from third party to Azure Cloud
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Seattle, WA 98101: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",$75.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2015,$1 to $5 million (USD)
"Maven Workforce
4.1",4.1,"McLean, VA",Data Engineer,"MUST HAVES: Hands-on experience with Informatica PowerCenter and/or Informatica Intelligent Cloud Services (IICS), Cloud Data Integration (CDI) , Cloud AWS,UNIX scripting, and any other ETL tools.
Job responsibilities: –
The candidates will be having the IICS module of Informatica more than any other platform modules.
Strong hands-on experience building ETL pipelines and Data Replication
3+ yrs of experience with IICS is required.
Must be hand Ons with AWS services like ECS,S3,EKS etc.
Cloud Data Integration is also required.
Data Engineering Background experience with Spark, Hive is also required.
Preferred:
SQL is preferred, but basic knowledge is required
Job Type: Contract
Salary: $60.00 per hour
Ability to commute/relocate:
Mclean, VA 22101: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica PowerCenter: 7 years (Required)
Informatica Intelligent Cloud Services (IICS): 7 years (Required)
Cloud Data Integration (CDI): 7 years (Required)
UNIX scripting: 7 years (Required)
ETL tools: 7 years (Required)
Work Location: One location
Speak with the employer
+91 7328456015",$60.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2008,Unknown / Non-Applicable
"Spartan Technologies
3.4",3.4,"Kansas City, MO",Data Engineer I,"Spartan Technologies, Inc. - Kansas City, MO
Applicants Need to Know
100% remote in East Coast or Central US only
No Sponsorship
Contract to Hire
Hourly Rate: $45 on W2
Data Engineer I
The Data Engineer is responsible for processing structured and unstructured data, validating data quality, and developing and supporting data products for analytics. Brings together data from different sources into a common dataset with common KPIs and dimensions for stakeholders. Understands the analytics challenges that the marketing organization faces in their day-to-day work and partners with them to design viable data solutions.
Your Job
Gathers and processes raw, structured, semi-structured, and unstructured data using batch and real-time data processing frameworks.
Understand the existing source system data models and perform end to end data validation to maintain accuracy.
Implements and optimizes data solutions in enterprise data warehouses and big data repositories.
Ensures data quality and implements tools and frameworks for automating the identification of data quality issues.
Develop re-useable processes that can be leveraged and standardized for multiple instances
Works with internal and external data providers on data validation providing feedback and making customized changes to data feeds and data mappings.
Recommends improvements to processes, technology, and interfaces that improve the effectiveness of the team and reduce technical debt.
Ensure performance by monitoring existing workflows to ensure they are working correctly and that they reconcile with the source.
Design and develop ETL workflows and datasets to be used in extracts by our BI reporting tool Tableau.
Will write complex SQL queries with multiple joins to automate/manipulate these reporting extracts
Works in partnership with marketing stakeholders concerning the accuracy of data and efficiency of processes
Qualifications
Working experience with data modelling, data access, schemas, and data storage techniques within Snowflake
Working experience with batch and real-time data processing frameworks
Working experience working with relational databases such as SQL
Working experience with business intelligence tools and platforms
Working experience with data quality tools
Experience with ETL processes and tools
Desirable:
Working experience with Snowflake
Working experience with Google Analytics BigQuery
Working Experience with Tableau Server
Working experience with SQL Server Integration Services",$45.00 /hr (est.),1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$5 to $25 million (USD)
"PRIMUS Global Services, Inc
4.1",4.1,"Minneapolis, MN","Data Engineer – Hadoop, Python – REMOTE WORK 43198","We have an immediate long-term opportunity with one of our key clients for a position of Data Engineer, to work on a remote basis.

The ideal applicant must have extensive experience with Python, Hadoop, Hive and SQL.

**ALL successful candidates for this position are required to work directly for PRIMUS. No agencies please**

For immediate consideration, please contact:

Tejaswini
PRIMUS Global Services
Direct: 972-798-2662
Desk: 972-753-6500 Ext: 204
Email: jobs@primusglobal.com","$91,633 /yr (est.)",501 to 1000 Employees,Private Practice / Firm,Information Technology,Information Technology Support Services,2002,$5 to $25 million (USD)
"Rite Pros
3.4",3.4,"Portland, ME",Big Data Engineer,"Big Data Engineer with Bachelor’s degree in Computer Science, Computer Information Systems, Information Technology, or a combination of education and experience equating to the U.S. equivalent of a Bachelor’s degree in one of the aforementioned subjects.
Job Duties:
Define and manage the data architecture framework, standards, and principles, including modeling, metadata, security, reference, and master data.
Define reference architecture, and models to provide a common vocabulary, reusable components, and industry best practices.
Create solution frameworks integrating large or complex data sets.
Lead all data modeling efforts within DataBricks, including the design of data structures and the identification of business transformation logic.
Analyzing and translating business requirements into conceptual and fully detailed logical data models.
Creating logical data models based on existing applications and databases.
Working with business architects, and data stewards to capture business requirements in a Logical Data Model.
Transform the logical representation of the model into a physical representation and work with data engineering team to instantiate and manage the data.
Contribute to assessment of appropriate data platform(s) for solutioning efforts.
Collaborate with the BI and Analytics teams on creating the optimized, reusable semantic model, complete with metadata and lineage information.
Define and govern data modeling and design standards, tools, best practices, and related development for enterprise data models
Maintain the metadata - the ""data about the data"" describing the data model, its structure, and semantics
Recommend data and design patterns for common business functionality
Assist in multi-phase roadmaps within and across functional teams
Provide insight and technology recommendations that result in enterprise solutions and architectural risk mitigation.
Contribute to innovation strategies by exploring, investigating, recommending, benchmarking and implementing new data centric technologies for the platform.
Identify, design, and implement internal process improvements such as automation of manual processes, optimization of data delivery and re-design of existing infrastructure for greater scalability.
Build the infrastructure and processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and/or other data technologies.
Create and maintain data-related documentation (e.g. data dictionaries, data maps, data models etc.)
Work with data engineering and analytics experts to strive for greater functionality in our data systems and incorporation of industry best practices
Work experience / Technologies required for the position :
Experience building and optimizing Big Data, Data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, meta data, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected data sets.
Working knowledge of message queuing, stream processing, and highly scalable Big Data, data stores.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience in a Data Engineer or similar roles, who has attained a Bachelor’s degree in Bachelor’s degree in Engineering, Computer Science, Information Technology, related field or equivalent work experience.
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Work location is Portland, ME with required travel to client locations throughout USA.

Rite Pros is an equal opportunity employer (EOE).

Please Mail Resumes to:
Rite Pros, Inc.
565 Congress St, Suite # 305
Portland, ME 04101.
Email: resumes@ritepros.com","$97,894 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Less than $1 million (USD)
Breadboard,#N/A,"New York, NY",Data Engineer,"About Us
We’re on a mission to revolutionize the global electronics supply chain.
During the pandemic we saw how the inefficiencies in electronics manufacturing can have massive global impact. From over $200B lost in car manufacturing to delays on laptops, gaming systems and much more.
At Breadboard we are rethinking the electronics supply chain down to the finest detail. We’re designing the new operating system in which this entire system will run. We’ve already secured seed funding and are now looking to expand our engineering team to help us set the foundation for the future.
Why work with us?
As a part of our team, you will have the unique opportunity to design and implement the data platform for Breadboard, a critical component in our business.
As one of the first data engineers to join the Breadboard team you will be partnering with the CTO, our founding engineers, and will set the foundation for the design of the operating system for the electronics supply chain.
You will be involved in all aspects of the software development process, architectural decisions, recruiting, and building the organization in general.
We’re a tight-knit, intense team and you will have a ton of end to end ownership.
Work with seasoned founders who have had successful exits.
This is not only a unique opportunity for someone who wants to get their hands dirty, but also take that step back and collaborate with the executive team on setting a strategic future.
You’ll be responsible for
Data Management: Develop and automate robust data acquisition, processing, and normalization pipelines from various sources to ensure data quality and consistency.
Database and ETL Architecture: Design, implement, and optimize a database system for efficient data storage, retrieval, and analysis.
Collaboration on Machine Learning: The prospects of working with data scientists to leverage extracted data features, contributing to the development and refinement of a machine learning model for component matching.
API and UI Integration: Collaborate with the software development team to facilitate seamless integration between the database, machine learning models, and user interface.
Continuous Improvement and Compliance: Monitor system performance and user feedback for continuous improvements, while ensuring adherence to data privacy and security standards.

Requirements
Experience: A minimum of 3-5 years of experience in a data engineering role, preferably in the electronics, manufacturing industry or supply chain industry.
Technical Skills: Proficiency in SQL and experience with database design, data modeling, and performance optimization. Strong coding skills in a major programming language, such as Python or TypeScript. Experience with data processing tools and libraries (e.g., pandas, Spark). Experience with pipeline and workflow orchestration tools, such Airflow. Experience with Snowflake or similar cloud-based data warehousing solutions.
Data Acquisition and Processing: Experience with data acquisition methods, such as APIs and web scraping. Familiarity with data cleaning, normalization, and feature extraction techniques.
Machine Learning: Familiarity with machine learning concepts and algorithms, and experience collaborating with data scientists or machine learning engineers.
API Development: Experience with API development and integration, preferably in a microservices architecture.
Data Governance: Knowledge of data privacy and security regulations, and experience implementing data governance measures.
Bonus Points
Entrepreneurial mindset (we encourage all employees to be future founders and this can be a great stepping stone towards that goal).
Customer-centric and passionate about helping small businesses grow.
Previous experience at a high-growth, fast-paced startup.
Experience with our stack (and some others that may be included shortly):
React, Material UI, Typescript, CSS
NestJS, GraphQL, TypeORM, Knex.js, Objection.js, PostgreSQL, gRPC, Kafka
AWS, Docker, Kubernetes, Helm, Github Actions, CI/CD pipelines.
Compensation + Benefits
Competitive cash compensation
Stock ownership at a fast growing company
Health/dental/vision insurance—100% coverage option
The ability to push for other benefits you think are important
Our Engineering Culture
Openness to diverse opinions and backgrounds - Testing our assumptions
Supported career growth with a high emphasis on learning and mentorship
Lean development
End to end ownership and accountability. We’re all going to make mistakes. It’s about how you own up to them and learn from them.

Equal Opportunity Employer
Breadboard Software does not discriminate in employment on the basis of race, color, religion, sex (including pregnancy and gender identity), national origin, political affiliation, sexual orientation, marital status, disability, genetic information, age, membership in an employee organization, retaliation, parental status, military service, or other non-merit factor.",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Green Worldwide Shipping LLC
3.7",3.7,"Aurora, CO",GEM Data Engineer,"About Green
“Global Reach with a personal touch”, that’s the foundation on which four partners (and long-time friends) agreed to when they started Green Worldwide Shipping back in 2008. This ethos has allowed Green to Grow to a 200+ strong team with 15 offices across the US and growing.At green we are a family of innovative problem-solvers who communicate freely to present creative solutions in an environment of trust, respect, and integrity. We are dedicated to customer service, passionate about logistics, and committed to sustainable growth while inspiring others.
Why this role is special..
GEM (Green Engineered Management) is a technology solutions department that supports various commercial technology products, data management, and development for Green Worldwide Shipping.We are seeking a skilled and experienced Data Engineer to join our dynamic team. As a Data Engineer, you will be responsible for designing, developing, and maintaining our data infrastructure and various technology systems to support efficient data processing and analysis. Your expertise in SQL, CargoWise, database management, Altova Flowforce Server, Altova Mapforce ETL software, Python and C#, along with experience in EDI/API connection protocols integration, will be instrumental in achieving success in this role.
How you will be successful…
Design, develop, and maintain data pipelines and ETL processes to extract, transform, and load data from various sources into our databases using Altova Mapforce ETL software.
Collaborate with cross-functional teams to understand data requirements and provide optimal data solutions.
Develop and optimize SQL queries to retrieve and manipulate data from databases, ensuring efficient data retrieval and storage.
Implement and integrate EDI/API connection protocols to enable seamless data exchange between internal and external systems.
Monitor and maintain the performance, availability, and integrity of the databases and data infrastructure.
Support for reporting, document modification, workflow and database support for internal ERP.
Troubleshoot and resolve data-related issues, including data quality and data integration problems.
Implement data security measures and ensure compliance with data privacy regulations.
Stay updated with industry best practices and emerging technologies in data engineering and incorporate them into the data infrastructure.
Provide technical guidance and support to other team members on data-related projects.
Collaborate with data analysts and data scientists to understand data requirements and support their data exploration and analysis needs.
Create and maintain documentation related to data pipelines, database schemas, and data transformation processes.
What you will bring….
Bachelor's degree in Computer Science, Information Systems, or a related field.
Preferred experience in CargoWise (or related ERP) with reporting, document modification, workflow and database support
Proven experience working with SQL and databases, preferably MySQL.
Prefer strong knowledge and hands-on experience with Altova Flowforce Server and Altova Mapforce ETL software, or related applications.
Experience in integrating and working with EDI/API connection protocols.
Proficiency in Python and/or C# (optional but preferred).
Familiarity with data modeling concepts and techniques.
Experience with data warehousing concepts and technologies.
Excellent problem-solving and analytical skills.
Strong attention to detail and the ability to work in a fast-paced environment.
Effective communication skills to collaborate with cross-functional teams.
Ability to work independently and prioritize tasks effectively.
Preferred Skills:
*
Experience with cloud-based data platforms (e.g., AWS, Azure, Google Cloud Platform).
Knowledge of big data technologies (e.g., Hadoop, Spark) and distributed computing.
Familiarity with data visualization tools (e.g., Tableau, Power BI).
Experience with version control systems (e.g., Git).
Understanding of Agile development methodologies.
What we offer:
Employee High Deductible Health Plan (HDHP) currently paid for 100% by Green.
$1,800 contributed to employee HSA Account, if on HDHP.
Employee Dental provided by Green.
Option to purchase disability insurance.
Paid life insurance and option to buy-up.
401(K) and company match.
Parental Leave Program.
Monthly recognition points to give/receive, which can be converted to gift cards.
Tuition Reimbursement Program.
Weekly fruit baskets in office locations
A tree planted for every new employee to Trees.org
Hybrid work schedule.
Wellbeing Program.
Training and Development.
Access to Mentorship Program.
Green Worldwide Shipping, LLC (Green) is committed to the full inclusion of all qualified individuals. As part of this commitment, Green will ensure that persons with disabilities are provided reasonable accommodations. If reasonable accommodation is needed to participate in the job application or interview process and/or to perform essential job functions, please contact Faye Hughes, Head of People and Culture at faye.hughes@greenworldwide.com.Green is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.
Requirements
Bachelor's degree in Computer Science, Information Systems, or a related field.
Preferred experience in CargoWise (or related ERP) with reporting, document modification, workflow and database support
Proven experience working with SQL and databases, preferably MySQL.
Strong knowledge and hands-on experience with Altova Flowforce Server and Altova Mapforce ETL software, or related applications.
Experience in integrating and working with EDI/API connection protocols.
Proficiency in Python and/or C# (optional but preferred).
Familiarity with data modeling concepts and techniques.
Experience with data warehousing concepts and technologies.
Excellent problem-solving and analytical skills.
Strong attention to detail and the ability to work in a fast-paced environment.
Effective communication skills to collaborate with cross-functional teams.
Ability to work independently and prioritize tasks effectively.
Preferred Skills:
Experience with cloud-based data platforms (e.g., AWS, Azure, Google Cloud Platform).
Knowledge of big data technologies (e.g., Hadoop, Spark) and distributed computing.
Familiarity with data visualization tools (e.g., Tableau, Power BI).
Experience with version control systems (e.g., Git).
Understanding of Agile development methodologies.
Summary
GEM (Green Engineered Management) is a technology solutions department that supports various commercial technology products, data management, and development for Green Worldwide Shipping. We are seeking a skilled and experienced Data Engineer to join our dynamic team. As a Data Engineer, you will be responsible for designing, developing, and maintaining our data infrastructure and various technology systems to support efficient data processing and analysis. Your expertise in SQL, CargoWise, database management, Altova Flowforce Server, Altova Mapforce ETL software, Python and C#, along with experience in EDI/API connection protocols integration, will be instrumental in achieving success in this role.
Job Type: Full-time
Pay: $60,000.00 - $75,000.00 per year
Benefits:
401(k) matching
Dental insurance
Employee discount
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Tuition reimbursement
Vision insurance
Experience level:
4 years
Schedule:
Monday to Friday
Ability to commute/relocate:
Aurora, CO 80014: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL/MySQL (Required)
ETL Software (Preferred)
Data warehouse (Preferred)
Python/C# (Preferred)
Work Location: Hybrid remote in Aurora, CO 80014","$67,500 /yr (est.)",51 to 200 Employees,Company - Private,Transportation & Logistics,Shipping & Trucking,2008,$25 to $100 million (USD)
"ProIT Inc.
5.0",5.0,"Bellevue, WA",Azure Data Engineer,"Data Engineering experience primarily on Spark. Someone who has worked on Azure cloud with knowledge on Azure DEVOPS (CI/CD & Infrastructure as a code), ADF, ADW & Power BI. Apache Nifi will be good to have.Power BI: Understand business requirements to set functional specifications for reporting applications Build automated reports and dashboards with the help of Power BI reporting tool Be experienced in tools and systems on MS SQL Server BI Stack, including SSRS and TSQL, Power Query, MDX, PowerBI, and DAX Be able to quickly shape data into reporting and analytics solutions Have knowledge of database fundamentals such as multidimensional database design, relational database design, and more Create functional reporting Study, analyze and understand business requirements in context to business intelligence. Design and map data models to shift raw data into meaningful insights. Utilize Power BI to build interactive and visually appealing dashboards and reports. Spot key performance indicators with apt objectives Run DAX queries and functions in Power BI Should have an edge over making DAX queries in Power BI desktop. Developing visual reports, KPI scorecards, and dashboards using Power BI desktop. Connecting data sources, importing data, and transforming data for Business intelligence.
Spark ,Azure DEVOPS (CI/CD & Infrastructure as a code), ADF, ADW & Power
Job Type: Full-time
Pay: $100,154.64 - $104,132.47 per year
Ability to commute/relocate:
Bellevue, WA 98004: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Hybrid remote in Bellevue, WA 98004","$102,144 /yr (est.)",51 to 200 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"AbleTo
3.7",3.7,"New York, NY",Data Engineer,"About AbleTo
Join our mission-driven organization, where your work matters and a diversity of ideas and backgrounds are welcomed. AbleTo is the leading provider of high quality, technology-enabled behavioral health care. We believe that everyone deserves access to high-quality care and offer a suite of technology-enabled services to empower people to lead better lives through better mental health. A proprietary platform connects individuals with AbleTo highly trained licensed providers who deliver weekly sessions by phone or video supported by an integrated digital experience. Members also have access to mental health coaches, and digital support programs. Our outcomes-focused approach is proven to improve both behavioral and physical health and lower medical costs.

Overview
Data is integral to nearly everything that we do at AbleTo, and as a result the data team plays a key role at our company. As a Data Engineer at AbleTo, you will grow and develop your skills as a data engineer to build a world class data platform to support our mental health care mission.

What You Will Do
Build and maintain batch and real-time ETL pipelines in a Google Cloud Platform architecture (BigQuery, Dataproc, Firestore, Informatica, Python, etc.)
Identify code quality issues and implement tests to improve future processes.
Document orchestration work-flows and create run books
Implement data integrity tests to ensure we are ingesting accurate data.
Implement high-quality test-driven code.
Participate in daily team stand ups and other Agile ceremonies

Key Initiatives over the next 12 months
Data Mart client migration
Data Mart enhancements
Automation of manual day to day operational tasks
Decommission legacy pipelines

Who You Are:
1+ years of experience coding in Python.
Experience working with SQL (structured query language).
Familiarity with structuring and writing ETLs.
Experience working with Orchestration tools (Airflow) and Cloud Databases (BigQuery) is a plus.
Experience working in an Agile Development environment is a plus.
BS in Information Systems, Computer Science or related field is preferred.
You are excited to work with data!
Curiosity to dive deeply into issues and feel empowered to make a meaningful impact at a mission-driven company.
Committed to agile development and value delivery and solid engineering principles, as well as continuously improving our craft.
You are a collaborator. You build and maintain strong, productive working relationships with your customers and stakeholders.
You are empathetic and seek to understand each individual's diversity of background and experience contributes to making up a great team.
You drive to improve yourself and others.
You are excited about learning new languages, tools, techniques and technologies.

Why You Should Join Our Team:
We're proud to be a Great Place to Work-Certified™ company. We want you to show up and feel your best at work, and that means respecting your time outside of work. Our inclusive, flexible workspace prioritizes a work/life balance. We offer competitive salaries, comprehensive health benefits (for full-time employees), and professional perks such as 401K matching, fully funded HRA, and generous time off, including mental health days because your well-being is important to us.
At AbleTo, we're empowering people to get better and stay better. Want to join us? Take the next step in your career by applying for this role today.
The salary range is $40,000 to $90,000. Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, AbleTo offers benefits such as a comprehensive benefits package, incentive and recognition programs, and 401k contribution (all benefits are subject to eligibility requirements).

#LI-Remote
Follow AbleTo on LinkedIn, Twitter, and Instagram!
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.
AbleTo is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or veteran status. AbleTo is an E-Verify company.","$65,000 /yr (est.)",501 to 1000 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2008,$25 to $100 million (USD)
"Evergreen Residential Holdings, LLC
5.0",5.0,"Dallas, TX",Engineer Data & Analytics,"We are Evergreen Residential, a high growth early-stage institutional investment platform in the single-family residential sector. Our team is collaborative, open-minded and curious. Transparency is a core value, we speak our minds, are responsible for our actions and celebrate our wins. We are serious about the business without taking ourselves too seriously. We look for people who thrive in an entrepreneurial and fast paced environment. If you are self-motivated and mission driven with a 'can do' mindset and see solutions where others may see problems, come and grow with us!
We offer a flexible, empowering culture, competitive compensation and benefits, and potential for career growth through working closely with, and learning from, our experienced leadership team.
As a technical/engineering expert, you pride yourself on being able to quickly build strong business relationships both internally and externally e.g., with the leadership team, current and potential investors. With a passion for keeping current with advancements of the field, you deploy technology and data resources to provide innovative solutions to business needs.
The Role: Priorities can often change in a fast-paced environment like ours. Initial focus is to work with external data purchased, and to harvest internal data and work within Snowflake warehouse for use in our 3rd party property mgt system and BI reporting tool. Overall ensure there is one source of truth.

The role includes, but is not limited to, the following responsibilities:
Designing and implementing data pipelines to extract, transform, and load data from various sources into a centralized data repository
Developing and maintaining data processing and storage infrastructure
Establish productive relationships and effective communications with Company leadership to understand business drivers and align on required outcomes
Collaborating with data analysts to ensure that data is readily available for analysis and modeling
Optimizing database performance and troubleshooting issues as they arise
Implementing data security and access controls to protect sensitive data
Highlight key trends derived from data analysis and be a resource for improving data proficiency throughout the organization
Staying up-to-date with emerging trends and technologies in data engineering
Leverage historical data and predictive models to identify key historical factors that impact critical KPIs, and recommend actions to drive future performance
Ensure scientific method and research are key drivers of the product roadmap
What You Will Bring to the Table:
Knowledge of data modeling, database design, and ETL best practices
At least 3-5 years of experience in data engineering or a related field
Proficiency in one or more programming languages such as Python, Java, or Scala
Experience with data processing and storage technologies such as Hadoop, Spark, Kafka, Snowflake, and NoSQL databases
Experience in real estate investment and/or rental sector highly desirable
Prior experience managing a team of direct reports within the Data Science, Data Engineering, Analytics space in the SFR or Multifamily industry
Significant Experience building, motivating, and retaining a high- performing, flexible and collaborative data and analytics function
Proven hands-on technical background in data science, business intelligence or data engineering with demonstrated strategic impact at an executive level
A strong problem solver with experience building technical strategy and understanding technical tradeoffs and risk
Collaborative team player, you are truly a ""do-er"", happy to be a hands-on problem-solver to move the data program forward
Excellent communication skills – verbal and written
About Evergreen Residential
Founded in 2021, Evergreen Residential is a full-service SFR platform leveraging proven operational practices and the latest technological advances to optimize investor returns and achieve positive outcomes for our residents and the communities in which we operate. We offer a full suite of services, including Investment Management, Asset Origination, and Advisory Services. The firm is headquartered in Dallas with offices in New York City.
The leadership team has extensive experience dating back to the early institutionalization of SFR and unrivaled depth of experience in the complete asset life cycle. We are built to withstand changing market conditions, and our business produces resilient, predictable cash flows and margins. We are committed to charting new paths and using data to achieve best-in-class results. Our business is evergreen.
Beyond financial returns, the Company is committed to measurable impact objectives. We believe that inclusive and equitable management, environmentally sustainable long-term strategies, and resident-focused policies are good business - for our residents, our investors, and our team. We are committed to using environmentally sustainable practices and empowering our residents to improve their financial health.
Our cornerstone values - Accountability, Transparency and Partnership - are built on a foundation of Integrity and provide the roadmap for our daily actions, interactions and decisions.

Equal Opportunities and Other Employment Statements
We are deeply committed to building a workplace and community where inclusion is not only valued, but prioritized. We take pride in being an equal opportunity employer and seek to create a welcoming environment based on mutual respect, and to recruit, develop and retain the most talented people from a diverse candidate pool. All employment decisions shall be made without regard to race, color, religion, gender, gender identity or expression, family status, marital status, sexual orientation, national origin, genetics, neuro-diversity, disability, age, or veteran status, or any other basis as protected by federal, state, or local law.","$98,425 /yr (est.)",51 to 200 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"The Bouqs Company
3.1",3.1,"Marina del Rey, CA",Data Analytics Engineer,"The role contributes to The Bouq’s mission of revolutionizing the way we commemorate life’s moments by connecting people to beautifully designed floral experiences and the responsible partners who create them by being a key member of the data team. As an Analytics Engineer, you will work closely with the Product, Engineering, and Data teams to build and maintain the data infrastructure needed to support our data and business needs. You will also be responsible for developing, optimizing, and maintaining the best-in-class data pipelines, data models, and ETL processes to ensure that data is accurate, reliable, and available to stakeholders in a timely manner. The Analytics Engineer will also serve as the liaison between Engineering and Analytics and will serve as an active member in both teams.
Responsibilities:
Lead the transfer of data modeling from legacy systems to DBT
Contribute to building the data modeling layer, which exposes clean, transformed data to the whole company for analytics
Build datasets in DBT (cloud) for data analysts to improve speed and accuracy for the team
Improve current processes, whether that includes modularizing and standardizing a piece of commonly used code
Design and develop new data pipelines and streaming processes that are highly available, scalable, and reliable
Develop review processes for new data models and take charge on implementing SQL standards for the team
Actively strive towards writing performant SQL rather than just SQL that works, while also ensuring the same SQL is easy to understand when new eyes look at it
Optimize data processing and flow within our Snowflake Data Warehouse
Document new datasets and pipelines and the reasoning/story behind their structure
Work closely with engineering to keep track of schema changes in the production database and adjust pipelines, as needed
Support existing data pipelines and systems in production
Apply software engineering best practices to analytics code such as version control and testing
Develop and communicate strong opinions about best practices in analytics
Help explore and evaluate new technologies
Qualifications:
4+ years of experience working within a data team, preferably as an Analyst/Data Engineer
Bachelor's degree in a quantitative field such as statistics, mathematics, economics, or computer science preferred
Strong SQL fluency in both DDL/DML and analytics (Snowflake experience is a plus)
Experience working with JSON, DBT or other data transformation tools
Experience working with an ETL tool such as Fivetran or Stitch
Knowledge of data structures and how to write performant SQL
Experience with ensuring data quality through testing, deltas, lineage, etc
Strong communication and critical thinking skills to deliver solutions that not only solve problems but also serve as tools we didn’t know we needed
Ability to transform raw data into intuitive datasets that serve as building blocks for analytics
Comfortable leading the growth of a data warehouse and maintaining it
Capable of working through uninformative assumptions and built-biases in datasets and are not stalled when data is not perfect/sparse
Compensation & Perks:
Competitive Base Salary Range of $120,000.00 - $180,000.00 USD + Equity Package
Health, Dental & Vision with 100% employee coverage
401k Matching
Three Weeks Paid Vacation
Discounts on The World’s Best Flowers (obviously!)
Work on cutting edge new technologies
About The Bouqs:
Our mission here at The Bouqs is to revolutionize the way we commemorate life’s moments by connecting people to beautifully designed flowers and the responsible partners who create them. Grounded in transparency, responsibility, and simplicity, we create genuine moments of emotional connection for our customers, build meaningful relationships with like-minded farmers and florists while empowering them to thrive, and eliminate unnecessary waste along the way.

Founded in 2012, The Bouqs is a venture-backed online floral retailer that delivers flowers fresh from eco-friendly, sustainable farms to doorsteps nationwide. Headquartered in Marina Del Rey, CA, The Bouqs connects farms and a curated network of artisan florists directly to consumers and disrupts the traditional supply chain by eliminating overhead costs like warehouses, importers, distributors, auctioneers and more. In turn, this model enables a superior product and redefines the experience and economics for both consumers and producers alike.

For more information, visit www.bouqs.com and follow the #BouqLove on Facebook, Instagram and Twitter.
The Bouqs is an Equal Opportunity Employer!","$150,000 /yr (est.)",51 to 200 Employees,Company - Private,Retail & Wholesale,Other Retail Stores,2012,$25 to $100 million (USD)
"PrizePicks
4.8",4.8,"Atlanta, GA",Data Engineer,"At PrizePicks, we are the fastest growing sports company in North America, as recognized by Inc. 5000. As the leading platform for Daily Fantasy Sports, we cover a diverse range of sports leagues, including the NFL, NBA, and Esports titles like League of Legends and CS : GO. Our team of over 200 employees thrives in an inclusive culture that values individuals from diverse backgrounds, regardless of their level of sports fandom. Ready to reimagine the DFS industry together?
Our Analytics Team is responsible for building and maintaining analytics tools and workflows to support the PrizePicks business. By developing, maintaining, and testing data generation infrastructures, you will enable the PrizePicks business to make smarter, better, and faster data-driven decisions.
What you'll do:
Create, maintain and orchestrate optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Improve and streamline data systems to drive innovation within Prizepicks
What you have:
3+ years of experience building frameworks for data ingestion pipelines but real time and batch using data modeling, ETL/ELT processes
Stellar SQL skills, experience building DBT pipelines in production and know your way around structured, semi-structured and unstructured data.
Have built and optimized 'big data' data pipelines, architectures and data sets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Experience with Python and other object oriented scripting languages
Cloud experience: AWS, EC2, EMR, RDS, Redshift, GCP, etc..
Not required but would be awesome if you have experience with any of the following data pipeline and workflow tools: Azkaban, Luigi, Airflow, Prefect, etc.
Where you'll live:
Anywhere in the US is fine (we are based in Atlanta, GA)
Benefits you'll receive:
In addition to your great compensation package, company subsidized medical/dental/vision coverage plans and matching 401(k), we'll shower you with perks including:
Break room with ping pong, endless snacks and in-office lunch once a week
Unlimited PTO to encourage a healthy work/life balance (2 week min required!)
Modern work schedule focused on getting the job done, not hours clocked
Workplace flexibility
Company and team outings, we encourage a tight-knit workplace
Generous Maternity AND Paternity leave (16 weeks!)
Annual bonus & stock options
Wellness program
Company equipment provided (Windows & Mac options)
Annual performance reviews with opportunity for growth and career development
#LI-REMOTE

You must be authorized to work for any employer in the U.S. We are unable to sponsor or take over sponsorship or an employment Visa at this time.
PrizePicks is an Equal Opportunity Employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.","$97,814 /yr (est.)",1 to 50 Employees,Company - Private,"Arts, Entertainment & Recreation",Sports & Recreation,2015,Unknown / Non-Applicable
"WorkCog
4.3",4.3,"Atlanta, GA",Sr. AWS Data Engineer,"Position: AWS Data Engineer
Location: Atlanta ,GA
Duration: 12+ Months
Interview: Video
JD
Responsibilities
Bachelor’s degree in Computer Science, Software Engineering, or other related field, or equivalent work experience.
Extensive experience in data modeling, writing & performance tuning complex SQL queries & user-defined functions
Hands-on experience in design, implementation and orchestration of data pipelines and ETL batch Jobs.
Hands-on experience on all aspects of data handling: cleansing and standardization, normalization, merging, change data capture, data integration, reconciliation, and transformations
Strong experience in working with Big Data technologies such as Hive/Spark, Data lake and Data warehouse using GCP and/or AWS Services
Ability to write robust code in scripting languages like Python, Scala, PySpark or similar.
Good to Have
Associate Architect or equivalent cloud certification
Experience with processing large datasets using AWS and GCP UDFs (User defined functions)
Experience in ML probability & statistics algorithms, frameworks like Keras or PyTorch and tools like Jupyter notebooks.
Job Type: Contract
Pay: $45.83 - $52.42 per hour
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Atlanta, GA: Reliably commute or planning to relocate before starting work (Preferred)
Experience:
SQL (Preferred)
Informatica (Preferred)
Data warehouse (Preferred)
Work Location: In person",$49.13 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable
"Overhaul
3.5",3.5,"Austin, TX",Data Platform Engineer,"Who We Are
Overhaul is a supply chain integrity solutions company that allows shippers to connect disparate sources of data into the first fully transparent situational analysis engine designed for the logistics industry. Data that is transformed into critical insights can instantly trigger corrective actions, impacting everything from temperature control to handling requirements or package-level tracking, ensuring cargo arrives at its destination safely, undamaged, and on time. We are a dynamic, innovative, and fun team who is highly committed to our customers' experiences and our Mission and Vision.
The Role
At Overhaul, we're building the future of supply chain monitoring technology. As a Data Platform Engineer, you'll work with our platform and data analytics teams to understand and improve our data model, research and implement business intelligence tools to allow us to better serve our data internally and externally, and work with other engineers to design better systems for data ingestion and processing. Our company is growing fast, and we need you to help us deliver the best results to our users as we continue that trend.
Responsibilities:
Analyze existing tools, including business intelligence, databases, messaging services, etc. and provide solution recommendations
Deploy and manage data software solutions in a cloud environment (We use AWS and Azure)
Analyze custom data models and provide solutions for data ingestion/ETL
Create custom data software solutions for our internal and external customers using workflow tools such as Argo Workflows
Required Skills and Qualifications:
Experience doing data model analysis
Experience using cloud based technologies for data ingestion, analysis, and extraction (AWS Lambda, Workflow solutions, Snowflake, etc)
Excellent written and oral communication skills
Preferred Qualifications:
Software Engineering experience beyond writing simple scripts, preferably in Python
Experience using workflow solutions such as Argo Workflows, Prefect, Kubeflow, or Jenkins to implement data pipelines
Our Core Values and how they benefit you as an ""Overhauler""
Authenticity, Receptivity and Trust
Extremely competitive base salary package
401(k) with Overhaul match
Flexible working schedules
Remote, hybrid, and/or In-office*
Encouragement and Learning
Progressive advancement opportunity & career mobility
Paid development personal stipend
Monthly lunch and learns
2 Unique learning systems w/Instructor led content
Wellness and Integrity
Rotating Overhaul ""Perks @ work"" (Discounts and Freebies)
Overhaul fully provided healthcare plan
Employee assistance & wellbeing programs
New Parent/Family/Caregiver leave(s)
Daily BAMM time (body and mind movement)
Life by design vacation policy
Diversity and Inclusivity Statement:
Overhaul has always been, and always will be, committed to diversity and inclusion. Our Overhaul Culture Code's top listed commitment is to ""Diversity and Synergy."" All aspects of employment will be based on merit, competence, performance, and business needs. We do not discriminate on the basis of race, color, religion, marital status, age, national origin, ancestry, physical or mental disability, medical condition, pregnancy, genetic information, gender, sexual orientation, gender identity or expression, veteran status, or any other status protected under federal, state, or local law. We strongly encourage people from underrepresented groups to apply!
#BI-Remote","$102,068 /yr (est.)",51 to 200 Employees,Company - Private,Transportation & Logistics,Shipping & Trucking,2016,$25 to $100 million (USD)
"DiamondPick
4.5",4.5,"Edison, NJ",Senior Data Engineer,"Hi ,
Greetings from Diamond pick inc.
We are currently looking for the below position for one of our clients... Please let me know your interest along with your updated resume ASAP..
Role:Data engineer
Location: Berkley Heights, NJ(Onsite)(locals only)
9+ years of experience is must
Description
Skills: strong Java,Azure,Spark & sql
Company Description:
Apexon is a digital-first technology services firm backed by Goldman Sachs Asset Management and Everstone Capital. We specialize in accelerating business transformation and delivering human-centric digital experiences. For over 17 years, Apexon has been meeting customers wherever they are in the digital lifecycle and helping them outperform their competition through speed and innovation.
RESPONSIBILITIES
Basic Qualifications for consideration:
5+ Overall industry experience
3+ years' experience with building large scale big data applications development
Bachelors in Computer Science or related field
Provide technical leadership in developing data solutions and building frameworks
Experience building Data Lake, data applications and micro-services using Azure, AWS and Hands-on experience in cloud Data stack (preference is Azure)
Hands-on experience with Springboot framework, MVC architecture, Hibernate, API creation, Restful services, SOAP using Java
Java experience with OOPS concepts, multithreading
Experience deploying code on containers
Hands on experience with leveraging CI/CD to rapidly build & test application code
Conduct code reviews and strive for improvement in software engineering quality
Review data architecture and develop detailed implementation design
Hands-on experience in production rollout and infrastructure configuration
Demonstrable experience of successfully delivering big data projects using Kafka, Spark
Experience working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search
Experience working with PCI Data and working with data scientists is a plus
In depth knowledge of design principles and patterns
Able to tune big data solutions to improve performance
QualificationsBachelor's Degree in Computer Science or Computer Engineering is required
Job Type: Contract
Salary: $43.96 - $70.65 per hour
Ability to commute/relocate:
Edison, NJ 08817: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL (Required)
Java (Required)
Azure (Required)
Work Location: One location",$57.30 /hr (est.),1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,#N/A,Unknown / Non-Applicable
Kanini,#N/A,Remote,Data Engineer,"About Kanini
Kanini provides Agile Software Development, Cloud Computing, Data Science, and Location Intelligence services to public and private organizations. We have successfully served our clients in government, finance, transportation, utility, and software industries since 2003.
Why you should join
Working at Kanini is flexible and personal. We are a highly motivated, collaborative team experimenting with the latest technologies. We are committed to everyone having a healthy work/life balance, and we provide extensive mentorship and training resources to help you succeed.
Kanini is looking for a Big Data Engineer who has a deep experience in Data Engineering, AWS, Python, Data Lakes.
Required Skills
At least 10 years software development experience
At least 5 years leading at least one Scrum team of data engineers building data-intensive products with a modern tech stack.
Significant experience with big data ETL pipeline development with Spark, Hive, and related technologies
Significant experience with a general-purpose programming language such as Python, Scala, or Java
Experience with Spark framework and related tools (PySpark, Scala, SparkR, Spark SQL, Spark UI)
Experience with Hadoop ecosystem using HDFS, ADLS Gen2, or AWS S3
Experience with data visualization development using Python, Tableau, or PowerBI
Experience with Azure, AWS or GCP
Solid understanding of performance tuning concepts for relational and distributed database systems
Familiarity with distributed programming, big data concepts, and cloud computing
Education Qualifications
Bachelor’s degree in computer science/Engineering or Technology related field or possess equivalent work experience.
Preferred Qualifications
Cloud certifications from Azure, AWS or GCP
Big data, data engineering or data science certifications from recognized vendors such as Databricks & Cloudera
Kanini Software Solutions, Inc. does not discriminate in employment matters based on race, gender, religion, age, national origin, citizenship, veteran status, family status, disability status, or any other protected class. We support workplace diversity. If you have a disability, please let us know if there is anything we can do to improve the interview process for you; we’re happy to accommodate.
Kanini Software Solutions, Inc., 25 Century Blvd., Ste. 602, Nashville, TN 37214
Job Types: Full-time, Contract
Pay: $60.00 - $70.00 per hour
Work Location: Remote",$65.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Global Enterprise Services, LLC
4.0",4.0,"Atlanta, GA",Data Harmonization Engineer,"Data Harmonization Engineer

Collects data models for disparate datasets within a domain. Collaborates with CDC & STLT stakeholders to develop a conceptual model that defines the atomic concepts for that domain.
Constructs semantic mappings from the data elements in the source datasets to atomic or compound concepts in the conceptual model.
Develops sample transformation code to translate among representations.
Configures and manages vocabulary services to allow data modernization users to improve understanding of the relationships among data representations.

Data Harmonization Engineer - Junior [YoE, Edu, Certs: 0-3 yrs & BA/BS] [Salary Range: $81k - $91k]
Data Harmonization Engineer - Journeyman [YoE, Edu, Certs: 4-9 yrs & BA/BS] [Salary Range: $95k - $105k]
Data Harmonization Engineer - Senior [YoE, Edu, Certs: 10+ yrs & MA/MS] [Salary Range: $108k - $108k]","$100,000 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
IT Customer Service - POST,#N/A,"Newark, NJ",Data Center Plant Engineer,"Job Title: ISP-OSP Data Center Plant Engineer
Location: Newark NJ, (M-F)
Travel Required: As Needed
Overview
UC-ISP/OSP engineer is responsible for cable management including structured cabling, carrier data and voice circuit cables/equipment, rack-and-stack, and oversight of power, backup power, network equipment, and environmental needs to cabinets. Leveraging Clients Data Center Infrastructure Management System, (DCIM) tool to maintain, report and monitor.
Conducting capacity assessments of existing infrastructure to ensure that it can support future growth to ensure compliance with federal regulations of the Data Center infrastructure Standards. Projects include design and build outs of the physical plant within the typical WAN/LAN space within data centers, telco rooms, MDFs and IDFs as needed.
Functions:
Perform initial engineering surveys based on project needs.
Document engineering surveys to determine feasibility and the estimated cost.
Obtain authorization to proceed and confirm funding.
Create detailed engineering drawings of build, floor plans, equipment elevation drawings, structured cabling diagrams, power, and environmental needs.
Designs must conform to state, local, national, and Client standards and codes, and the NJ historical and or environmental commissions approvals if needed.
Based on the engineering documents create a SOW (Scope of Work) and BOM (Bill of Materials)
Submit to Client manager to approve and proceed when funding is validated.
Create a construction package and submit to contractors or in house for RFQ.
Review quotes and choose both low bid and qualified contractor.
Work with all stakeholders to date and time work schedules.
Confirm workers are safety trained and conform to Client, FRA, and OSHA requirements.
Supervise and manage installations and conduct inspections.
Review work to ensure contractor met obligations under terms of contract.
Inform Client managers of status to approve invoices.
Coordinate with Client network engineering to connect, configure, and test equipment.
Review and confirm a successful installation were completed and working as designed prior to cutting over to production.
Assist with Break fix of IT infrastructure troubles or repairs.
Daily travel to location depending on scheduled or unscheduled work
Job Types: Full-time, Contract
Experience level:
5 years
Schedule:
8 hour shift
Work setting:
In-person
Experience:
Data Center: 5 years (Required)
WAN/LAN: 3 years (Required)
cabling: 3 years (Required)
Work Location: One location",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Premier Consulting Group,#N/A,"Boca Raton, FL",Senior Data Engineer,"* The right person could be located in Florida or Massachusetts as company has office locations in Boca Roton and Boston.
* Must be a US Citizen
Overview Summary of the Work:
ETL Azure Data Factory work: A Data Engineer who is to be able to import data from a variety of different sources (SFTP, API, Fileshare) and file types (CSV, Excel, JSON). Company also captures files via email now, but they have a pattern set up to be able to do that, so the person would just repeat how they’ve done that. Check existing Azure Data Factory pipelines for failures and troubleshoot. Familiar with dynamic expression and syntax in Azure Data Factory pipelines.
Backlog of stored procedures: Be skilled in stored procedure development and SQL skills.
Data quality and data cleansing: Be able to evaluate data (incoming and existing). Look for issues and be able to resolve them. In general, someone who can look at the data and mentally make the leap of “hey, this looks wrong.” Look at data and determine how it might be better utilized (examples: 2.5% as a string or 0.025 as a number, identify values as empty strings and save as nulls instead, etc).
Technical Experience Required/Preferred:
SQL, Azure SQL, Azure Data Factory, Data warehousing environments (dims and facts), Snowflake (highly preferred), Python (highly preferred).
Job Types: Full-time, Permanent
Pay: $140,000.00 - $160,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Bonus pay
Yearly bonus
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Weekend availability
Work Location: Hybrid remote in Boca Raton, FL 33431","$150,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Dave & Buster's
3.6",3.6,"Dallas, TX",Data Lakehouse Engineer,"THE JIST:
The Data Lakehouse Engineer will be tasked with managing data movement and data storage for Dave & Buster’s, from a variety of data sources to a consolidated data lake/ODS/warehouse, intended to support a variety of business intelligence offerings. Works closely with members of the Information Technology departments to support, design and implement data solutions through an iterative process with end users.

THE ESSENTIALS:
Works across Technology, Vendor, and D&B operational and executive teams for data-related and insight needs.
Familiarity with data structures, storage systems, cloud infrastructure, and other technical tools.
Ability to work effectively in teams of technical and non-technical individuals.
Ability to continuously learn, work independently, and make decisions with minimal supervision
Demonstrate accountability, prioritize tasks, and consistently meet deadlines.
Create, maintain, and optimize data environments needed to satisfy defined business requirements in on-premise and hosted environments.
Identifying, designing and implementing internal process improvements, including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes.
Familiarity with Agile/SCRUM practices

SCOPE:
Understanding of how technical decisions impact the user of what you’re building
Prioritization of work is typically determined by the team
Will frequently makes recommendations to end users to drive better outcomes
Has some authority to modify processes, interpret policies or recommend programs that will directly impact the functional area
Coordinate communications with IT Support of updates, changes, and issues.
Monitor, maintain, and provide on-call support for production data environments.

CREDENTIALS:
1 -3 years of data engineering or relevant industry experience
Preferred Computer Science, Mathematics, and Software Engineering degree

THE GOODS:
(Qualifications - The minimum level of specific skills or abilities one must possess to be considered for this job, such as computer skills, communication skills, leadership ability, analytical ability, etc.)
Proficiency with programming languages such as Python, SQL, MDX, DAX or similar languages
Proficiency with reporting tools such as Power BI, SSRS, Tableau, QlikView, etc.
Experience with SQL database and design including stored procedures, triggers, and views (SQL Server, Oracle, mysql, Pyspark).
Windows Server knowledge is helpful (powershell, scripting, performance metrics checks, event viewer)
Experience with data warehousing, Azure and the Azure Stack
Exposure to DataBricks

PHYSICAL DEMANDS:
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.

While performing the duties of this job, the employee is regularly required to use hands to finger, handle, or feel and talk or hear. The employee frequently is required to sit and reach with hands and arms. The employee is occasionally required to stand and walk. The employee must occasionally lift and/or move up to 10 pounds. Specific vision abilities required by this job include close vision, distance vision, color vision, peripheral vision, depth perception, and ability to adjust focus.

WORK ENVIRONMENT:
The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.

The noise level in the work environment is usually moderate.","$58,253 /yr (est.)",10000+ Employees,Company - Public,Restaurants & Food Service,Restaurants & Cafes,1982,$1 to $5 billion (USD)
"Inovalon
3.1",3.1,United States,Software Data Engineer,"Inovalon was founded in 1998 on the belief that technology, and data specifically, would empower the transformation of the entire healthcare ecosystem for the better, improving both outcomes and economics. At Inovalon, we believe that when our customers are successful in their missions, healthcare improves. Therefore, we focus on empowering them with data-driven solutions. And the momentum is building.
Together, as ONE Inovalon, we are a united force delivering solutions that address healthcare's greatest needs. Through our mission-based culture of inclusion and innovation, our organization brings value not just to our customers, but to the millions of patients and members they serve.
Overview: The Software Data Engineer is responsible for contributing to data pipelines, ETL, data warehouse, jobs, data operations. They will be part of the team which is building next generation data & reporting platform. This platform will cater internal business stakeholders and external customers to provide insights & forecasting to understand current state of business, improve decision-making for their tactical and strategic goals & KPIs. This position may require independent work, sharing information and assisting others with work request.
Duties and Responsibilities:
Work with the agile team to participate in agile ceremonies like grooming, planning, standup, retrospective, demos
Actively contribute to grooming, and standup, create & update tasks, estimate and status
Work with data architects and business analysts to create a logical data model and create DDL scripts for physical database creation
Work on large data to ensure ingestion of data, dynamic rule & validation of data, cleansing, transforming, and loading into the data warehouse
Write complex queries, stored procedures, functions, SSIS Packages for various job execution
Develop modern ETL framework utilizing tools like ADF (Azure Data Factory), MS-SSIS etc
Develop STAR or SNOWFLAKE database schema utilizing industry best practices to build Data warehouse, data marts, views, and data sets/products
Develop ETL pipelines, using SQL, Stored procedures/functions to extract data from various sources and load into warehouse
Develop Symantec layer and data export frameworks to extract the data from the warehouse, transform, pre-aggregate, perform calculations and load into various data marts for Analytics use
Develop configurable export framework to extract data from Data warehouse and data marts to generate reports for internal and external customers in .csv, flat files and
Design and implement data validation and quality checks to ensure the accuracy and completeness of the data in the data warehouse
Perform performance of queries and data processing, identify and resolve any issues
Work and communicate in a cross-functional geographically dispersed team environment comprised of software engineers and product managers; and
Ensure compliance to company procedures when making changes and implementing code.
Maintain compliance with Inovalon's policies, procedures and mission statement;
Adhere to all confidentiality and HIPAA requirements as outlined within Inovalon's Operating Policies and Procedures in all ways and at all times with respect to any aspect of the data handled or services rendered in the undertaking of the position; and
Fulfill those responsibilities and/or duties that may be reasonably provided by Inovalon for the purpose of achieving operational and financial success of the Employer.
Job Requirements:
Minimum two (2) years related experience required; healthcare industry experience preferred.
Strong understanding to develop SQL queries for data analysis.
Experience working on Azure Cloud is preferred
3+ experience in MS SQL, T-SQL, ETL Jobs
3+ experience in Microsoft tools like SSMS, SSIS, SQL Server
Strong understanding of database concepts and schema (like star, snowflake schema)
Ability to learn quickly and independently
Ability to effectively communicate with internal and external customers
Experience with test driven development methodologies.
Education:
Bachelor's degree in Computer Science, Software Engineering, or Information Technology.
Physical Demands and Work Environment:
Sedentary work (i.e., sitting for long periods of time);
Exerting up to 10 pounds of force occasionally and/or negligible amount of force
Frequently or constantly to lift, carry push, pull or otherwise move objects and repetitive motions
Subject to inside environmental conditions; and
Studies have shown that women and people of color are less likely to apply for jobs unless they believe they meet every one of the qualifications listed in a job description. If you don't meet every qualification listed but are excited about our mission and the work described, we encourage you to apply regardless. Inovalon is most interested in finding the best candidate for the job and you may be just the right person for this or other roles.
By embracing diversity, equity and inclusion we enhance our work environment and drive business success. Inovalon strives to reflect the diversity of the communities where we operate and of our clients and everyone whom we serve. We endeavor to create a culture of inclusion in which our associates feel empowered to bring their full, authentic selves to work and pursue their professional goals in an equitable setting. We understand that by fostering this type of culture, and welcoming different perspectives, we generate innovation and growth.
Inovalon is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirement.
The Company maintains a drug free work environment for all of its associates, which includes employees, contractors and vendors. It is unlawful for associates to manufacture, sell, distribute, dispense, possess or use any controlled substance or marijuana in the workplace and doing so will result in disciplinary action, up to and including termination of employment or the contracted relationship.",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Software Development,1998,$500 million to $1 billion (USD)
"Fathom Management LLC
2.0",2.0,Remote,Sr. Data Engineer Remote Opportunity,"Sr. Data Engineer

seeking a Senior Data Engineer who possesses expert level knowledge of appropriate data sources to address the specific requirements of projects for data modeling. Understand business requirements and translating into technical work. Design and implement features in collaboration with team engineers, product owners, data analysts, business partners using Agile/SCRUM Methodology.

This is a full- time position / 100% Remote.
The salary range of $140,000 - $160,000 will be based on technical experience and technical interview.

Responsibilities:

Ability to build programs or systems that can take data and turn it into meaningful information that can be studied.
Build ETL/ELT jobs and workflows to combine data from disparate sources.
Install continuous pipelines of huge pools of filtered information so that data analyst/scientists can pull relevant data sets for their analyses.
Build data workflows using SQL Server Integration Services (SSIS)
Build data workflows using Microsoft Azure (Azure Data Factory, Storage Accounts, Synapse)
Build data workflows using Databricks
Collaborate with business stakeholders, business operations, and product engineering teams in analyzing business problems, in building and testing solutions and data models.
Experience implementing and operating analytic models and services.
Document the current-state and target-state software architecture and create roadmap plans for success on various software components.
Assist in the design, implementation, and maintenance of complex solutions.
Build systems that collect, manage, and convert raw data into usable information for business analysts to interpret.
Make data accessible for evaluation and optimization
Collaborate with business stakeholders, business operations, and product engineering teams.
Coordinate activities with other technical personnel as appropriate.
Works with back-end data and develops tables using SQL scripts, SSIS, and SSMS.
Experience with Azure cloud platforms and Data bricks

Required Experience and Education:
Master's degree in computer science, systems engineering, or related technical discipline is preferred with 7-10 years of experience as a Data Engineer/Administrator or similar role. OR , B.S. in Computer Science with 15 years of relevant experience.

Benefits Overview: Full-time employees are offered comprehensive and competitive benefits package including paid vacation, sick leave, holidays, health insurance, life insurance, military leave, training, tuition reimbursement, a wellness program, short- and long-term disability, 401(k) retirement plan with company matches/immediate vesting, commuter benefits, and more.
EEO Policy: It is our policy to promote equal employment opportunities. All personnel decisions, including, but not limited to, recruiting, hiring, training, promotion, compensation, benefits, and termination, are made without regard to race, creed, color, religion, national origin, sex, age, marital status, sexual orientation, gender identity, citizenship status, veteran status, disability, or any other characteristic protected by applicable federal, state or local law.","$150,000 /yr (est.)",1 to 50 Employees,Self-employed,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"FACEBOOK APP
5.0",5.0,Remote,Azure Data Engineer,"Responsibilities:
Create ER diagrams and write relational database queries
Create database objects and maintain referential integrity
Configure, deploy and maintain database
Participate in development and maintenance of Data warehouses
Design, develop and deploy SSIS packages
Creating and deploying reports
Provide technical design, coding assistance to the team to accomplish the project deliverables as planned/scoped.
Ability to talk to client and get the Business Requirements
Skills:
Azure Data Factory
Azure Devops
Azure Storage/ Data Lake
Extraction, Transformation and Loading
Analytics development
Report Development
Relational database and SQL language
Other Requirements:
· Should be well versed with Data Structures & algorithms
· Understanding of software development lifecycle
· Excellent analytical and problem-solving skills.
· Ability to work independently as a self-starter, and within a team environment.
· Good Communication skills- Written and Verbal
Job Type: Full-time
Salary: $84,454.31 - $190,806.62 per year
Benefits:
Flexible schedule
Health insurance
Compensation package:
1099 contract
Yearly pay
Experience level:
10 years
9 years
Schedule:
Day shift
Experience:
Azure Data engineer: 9 years (Preferred)
SQL: 9 years (Preferred)
Data warehouse: 10 years (Preferred)
Work Location: Remote","$137,630 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Zenotis Technologies inc,#N/A,"Addison, TX",Big Data Engineer,"Role: Big Data Engineer
Location – Addison, TX/ Charlotte, NC / Chandler, AZ (Hybrid – 3 days onsite 2 days remote)
Position Summary:
Looking for a big data engineer who loves solving complex problems across a full spectrum of technologies. The ideal candidate is excited by experimentation and looking for a new challenge that stretches their talents. The big data engineer will help ensure that our technological infrastructure operates seamlessly in support of business objectives.
The engineer must be able to:
· Translate complex technical and functional requirements into detailed designs
· Guide the development and testing teams in developing and implement pipelines that extract, transform, and load data into an information product that helps the organization reach its strategic goals
· Focus on ingesting, storing, processing, and analyzing large datasets
· Deep understanding on how to create scalable, high-performance web services for tracking data
· Investigate alternatives for data storing and processing to ensure the implementation of the most streamlined solutions
Primary Skill
Hadoop
Secondary Skill
Kafka
Tertiary Skill
Oracle Exadata
Required Skills
· Translate complex technical and functional requirements into detailed designs
· Design data pipelines using design tools and provide solution intent in provided templates
· Work closely with data science team to implement data analytics pipelines
· Analyze vast number of data stores to uncover insights
· Define data governance policies and support data-versioning processes
· Maintain security and data privacy, working closely with data protection guidelines.
· Desired Skills
· Experience with Python, Spark, and Hive
· Understanding of data-warehousing and data-modeling techniques
· Knowledge of industry-wide visualization and analytics tools (ex: Tableau, R)
· Strong data engineering skills with Hadoop platform
· Experience with streaming frameworks such as Kafka
· Knowledge of Core Java, Linux, SQL, and any scripting language
· Good interpersonal skills and positive attitude
Job Type: Full-time
Salary: Up to $70.00 per hour
Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
8 hour shift
Work Location: On the road",$70.00 /hr (est.),201 to 500 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Ripple Effect Consulting
5.0",5.0,"Denver, CO",Data Engineer - 004/005,"Data Engineer
Hybrid-remote based in Denver, CO | $80-$160k
Job Overview:
We are looking for a savvy Data Engineer to join our growing team of analytics experts. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
Responsibilities for Data Engineer
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Qualifications for Data Engineer
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:
1) Experience with big data tools
2) Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
3) Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
4) Experience with cloud services
5) Experience with stream-processing systems: Storm, Spark-Streaming, etc.
6) Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Job Type: Full-time
Pay: $80,000.00 - $160,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Denver, CO: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person","$120,000 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Green Worldwide Shipping LLC
3.7",3.7,"Aurora, CO",GEM Data Engineer,"About Green
“Global Reach with a personal touch”, that’s the foundation on which four partners (and long-time friends) agreed to when they started Green Worldwide Shipping back in 2008. This ethos has allowed Green to Grow to a 200+ strong team with 15 offices across the US and growing.At green we are a family of innovative problem-solvers who communicate freely to present creative solutions in an environment of trust, respect, and integrity. We are dedicated to customer service, passionate about logistics, and committed to sustainable growth while inspiring others.
Why this role is special..
GEM (Green Engineered Management) is a technology solutions department that supports various commercial technology products, data management, and development for Green Worldwide Shipping.We are seeking a skilled and experienced Data Engineer to join our dynamic team. As a Data Engineer, you will be responsible for designing, developing, and maintaining our data infrastructure and various technology systems to support efficient data processing and analysis. Your expertise in SQL, CargoWise, database management, Altova Flowforce Server, Altova Mapforce ETL software, Python and C#, along with experience in EDI/API connection protocols integration, will be instrumental in achieving success in this role.
How you will be successful…
Design, develop, and maintain data pipelines and ETL processes to extract, transform, and load data from various sources into our databases using Altova Mapforce ETL software.
Collaborate with cross-functional teams to understand data requirements and provide optimal data solutions.
Develop and optimize SQL queries to retrieve and manipulate data from databases, ensuring efficient data retrieval and storage.
Implement and integrate EDI/API connection protocols to enable seamless data exchange between internal and external systems.
Monitor and maintain the performance, availability, and integrity of the databases and data infrastructure.
Support for reporting, document modification, workflow and database support for internal ERP.
Troubleshoot and resolve data-related issues, including data quality and data integration problems.
Implement data security measures and ensure compliance with data privacy regulations.
Stay updated with industry best practices and emerging technologies in data engineering and incorporate them into the data infrastructure.
Provide technical guidance and support to other team members on data-related projects.
Collaborate with data analysts and data scientists to understand data requirements and support their data exploration and analysis needs.
Create and maintain documentation related to data pipelines, database schemas, and data transformation processes.
What you will bring….
Bachelor's degree in Computer Science, Information Systems, or a related field.
Preferred experience in CargoWise (or related ERP) with reporting, document modification, workflow and database support
Proven experience working with SQL and databases, preferably MySQL.
Prefer strong knowledge and hands-on experience with Altova Flowforce Server and Altova Mapforce ETL software, or related applications.
Experience in integrating and working with EDI/API connection protocols.
Proficiency in Python and/or C# (optional but preferred).
Familiarity with data modeling concepts and techniques.
Experience with data warehousing concepts and technologies.
Excellent problem-solving and analytical skills.
Strong attention to detail and the ability to work in a fast-paced environment.
Effective communication skills to collaborate with cross-functional teams.
Ability to work independently and prioritize tasks effectively.
Preferred Skills:
*
Experience with cloud-based data platforms (e.g., AWS, Azure, Google Cloud Platform).
Knowledge of big data technologies (e.g., Hadoop, Spark) and distributed computing.
Familiarity with data visualization tools (e.g., Tableau, Power BI).
Experience with version control systems (e.g., Git).
Understanding of Agile development methodologies.
What we offer:
Employee High Deductible Health Plan (HDHP) currently paid for 100% by Green.
$1,800 contributed to employee HSA Account, if on HDHP.
Employee Dental provided by Green.
Option to purchase disability insurance.
Paid life insurance and option to buy-up.
401(K) and company match.
Parental Leave Program.
Monthly recognition points to give/receive, which can be converted to gift cards.
Tuition Reimbursement Program.
Weekly fruit baskets in office locations
A tree planted for every new employee to Trees.org
Hybrid work schedule.
Wellbeing Program.
Training and Development.
Access to Mentorship Program.
Green Worldwide Shipping, LLC (Green) is committed to the full inclusion of all qualified individuals. As part of this commitment, Green will ensure that persons with disabilities are provided reasonable accommodations. If reasonable accommodation is needed to participate in the job application or interview process and/or to perform essential job functions, please contact Faye Hughes, Head of People and Culture at faye.hughes@greenworldwide.com.Green is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.
Requirements
Bachelor's degree in Computer Science, Information Systems, or a related field.
Preferred experience in CargoWise (or related ERP) with reporting, document modification, workflow and database support
Proven experience working with SQL and databases, preferably MySQL.
Strong knowledge and hands-on experience with Altova Flowforce Server and Altova Mapforce ETL software, or related applications.
Experience in integrating and working with EDI/API connection protocols.
Proficiency in Python and/or C# (optional but preferred).
Familiarity with data modeling concepts and techniques.
Experience with data warehousing concepts and technologies.
Excellent problem-solving and analytical skills.
Strong attention to detail and the ability to work in a fast-paced environment.
Effective communication skills to collaborate with cross-functional teams.
Ability to work independently and prioritize tasks effectively.
Preferred Skills:
Experience with cloud-based data platforms (e.g., AWS, Azure, Google Cloud Platform).
Knowledge of big data technologies (e.g., Hadoop, Spark) and distributed computing.
Familiarity with data visualization tools (e.g., Tableau, Power BI).
Experience with version control systems (e.g., Git).
Understanding of Agile development methodologies.
Summary
GEM (Green Engineered Management) is a technology solutions department that supports various commercial technology products, data management, and development for Green Worldwide Shipping. We are seeking a skilled and experienced Data Engineer to join our dynamic team. As a Data Engineer, you will be responsible for designing, developing, and maintaining our data infrastructure and various technology systems to support efficient data processing and analysis. Your expertise in SQL, CargoWise, database management, Altova Flowforce Server, Altova Mapforce ETL software, Python and C#, along with experience in EDI/API connection protocols integration, will be instrumental in achieving success in this role.
Job Type: Full-time
Pay: $60,000.00 - $75,000.00 per year
Benefits:
401(k) matching
Dental insurance
Employee discount
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Tuition reimbursement
Vision insurance
Experience level:
4 years
Schedule:
Monday to Friday
Ability to commute/relocate:
Aurora, CO 80014: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL/MySQL (Required)
ETL Software (Preferred)
Data warehouse (Preferred)
Python/C# (Preferred)
Work Location: Hybrid remote in Aurora, CO 80014","$67,500 /yr (est.)",51 to 200 Employees,Company - Private,Transportation & Logistics,Shipping & Trucking,2008,$25 to $100 million (USD)
"Maven Workforce
4.1",4.1,"McLean, VA",Data Engineer,"MUST HAVES: Hands-on experience with Informatica PowerCenter and/or Informatica Intelligent Cloud Services (IICS), Cloud Data Integration (CDI) , Cloud AWS,UNIX scripting, and any other ETL tools.
Job responsibilities: –
The candidates will be having the IICS module of Informatica more than any other platform modules.
Strong hands-on experience building ETL pipelines and Data Replication
3+ yrs of experience with IICS is required.
Must be hand Ons with AWS services like ECS,S3,EKS etc.
Cloud Data Integration is also required.
Data Engineering Background experience with Spark, Hive is also required.
Preferred:
SQL is preferred, but basic knowledge is required
Job Type: Contract
Salary: $60.00 per hour
Ability to commute/relocate:
Mclean, VA 22101: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica PowerCenter: 7 years (Required)
Informatica Intelligent Cloud Services (IICS): 7 years (Required)
Cloud Data Integration (CDI): 7 years (Required)
UNIX scripting: 7 years (Required)
ETL tools: 7 years (Required)
Work Location: One location
Speak with the employer
+91 7328456015",$60.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2008,Unknown / Non-Applicable
Breadboard,#N/A,"New York, NY",Data Engineer,"About Us
We’re on a mission to revolutionize the global electronics supply chain.
During the pandemic we saw how the inefficiencies in electronics manufacturing can have massive global impact. From over $200B lost in car manufacturing to delays on laptops, gaming systems and much more.
At Breadboard we are rethinking the electronics supply chain down to the finest detail. We’re designing the new operating system in which this entire system will run. We’ve already secured seed funding and are now looking to expand our engineering team to help us set the foundation for the future.
Why work with us?
As a part of our team, you will have the unique opportunity to design and implement the data platform for Breadboard, a critical component in our business.
As one of the first data engineers to join the Breadboard team you will be partnering with the CTO, our founding engineers, and will set the foundation for the design of the operating system for the electronics supply chain.
You will be involved in all aspects of the software development process, architectural decisions, recruiting, and building the organization in general.
We’re a tight-knit, intense team and you will have a ton of end to end ownership.
Work with seasoned founders who have had successful exits.
This is not only a unique opportunity for someone who wants to get their hands dirty, but also take that step back and collaborate with the executive team on setting a strategic future.
You’ll be responsible for
Data Management: Develop and automate robust data acquisition, processing, and normalization pipelines from various sources to ensure data quality and consistency.
Database and ETL Architecture: Design, implement, and optimize a database system for efficient data storage, retrieval, and analysis.
Collaboration on Machine Learning: The prospects of working with data scientists to leverage extracted data features, contributing to the development and refinement of a machine learning model for component matching.
API and UI Integration: Collaborate with the software development team to facilitate seamless integration between the database, machine learning models, and user interface.
Continuous Improvement and Compliance: Monitor system performance and user feedback for continuous improvements, while ensuring adherence to data privacy and security standards.

Requirements
Experience: A minimum of 3-5 years of experience in a data engineering role, preferably in the electronics, manufacturing industry or supply chain industry.
Technical Skills: Proficiency in SQL and experience with database design, data modeling, and performance optimization. Strong coding skills in a major programming language, such as Python or TypeScript. Experience with data processing tools and libraries (e.g., pandas, Spark). Experience with pipeline and workflow orchestration tools, such Airflow. Experience with Snowflake or similar cloud-based data warehousing solutions.
Data Acquisition and Processing: Experience with data acquisition methods, such as APIs and web scraping. Familiarity with data cleaning, normalization, and feature extraction techniques.
Machine Learning: Familiarity with machine learning concepts and algorithms, and experience collaborating with data scientists or machine learning engineers.
API Development: Experience with API development and integration, preferably in a microservices architecture.
Data Governance: Knowledge of data privacy and security regulations, and experience implementing data governance measures.
Bonus Points
Entrepreneurial mindset (we encourage all employees to be future founders and this can be a great stepping stone towards that goal).
Customer-centric and passionate about helping small businesses grow.
Previous experience at a high-growth, fast-paced startup.
Experience with our stack (and some others that may be included shortly):
React, Material UI, Typescript, CSS
NestJS, GraphQL, TypeORM, Knex.js, Objection.js, PostgreSQL, gRPC, Kafka
AWS, Docker, Kubernetes, Helm, Github Actions, CI/CD pipelines.
Compensation + Benefits
Competitive cash compensation
Stock ownership at a fast growing company
Health/dental/vision insurance—100% coverage option
The ability to push for other benefits you think are important
Our Engineering Culture
Openness to diverse opinions and backgrounds - Testing our assumptions
Supported career growth with a high emphasis on learning and mentorship
Lean development
End to end ownership and accountability. We’re all going to make mistakes. It’s about how you own up to them and learn from them.

Equal Opportunity Employer
Breadboard Software does not discriminate in employment on the basis of race, color, religion, sex (including pregnancy and gender identity), national origin, political affiliation, sexual orientation, marital status, disability, genetic information, age, membership in an employee organization, retaliation, parental status, military service, or other non-merit factor.",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Provista Software Corporation,#N/A,Remote,Data Engineer,"As a successful candidate, you’ll play a pivotal role in the data conversion and business intelligence solution efforts during project implementation for our state and local clients. We are looking for you to join our software implementation team as a Senior Data Engineer.
Qualification:
5+ years of experience working independently as ETL developer on data migration projects that covered diverse database platforms and complex data conversions.
Must have experience across leading Database Platforms especially Oracle, Azure SQL, Snowflake.
Expert level experience with Extraction Transform Load (ETL) plan, protocols, and tools.
Expert level experience with programming languages SQL, Phyton, Java, C#.
Good at communication with clients and internally.
Expert at problem solving and troubleshooting skills.
Bachelor's degree, preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience
Must be a U.S. Citizen or Green Card Holder.
Functional Responsibility:
Data Conversion -
In collaboration with Data Stewards, discover inventory of legacy data sources and all data the client expects to be migrated to destination Modified Off the Shelf (MOTS) solution.
Complete gap assessment between legacy data and MOTS data requirements, and options to address the gaps.
Develop data migration map to minimally include data dictionary, detailed data map of all elements of the current database, data transformation rules, relationship rules, validation rules, process of migrating images, and process for recovering missing or erroneous data.
Identify issues, risks, and/or barriers that may interfere with data migration and recommendations to resolve or mitigate.
Conduct testing and quality assurance to ensure the data is migrated accurately.
Migrate data from legacy to destination MOTS solution progressively using a sample set of data and the entirety of the data prior to deployment into relevant go-live environments (e.g., staging, production, etc.)
Prepare data migration reports that includes – record counts of source vs migrated vs modified; field level validation of data at row and column; discovered exceptions.
Business Intelligence -
Develop ETL pipelines in and out of Snowflake data warehouse using combination of Python, SnowSQL, and SQL.
In collaboration with Data Stewards, create Snowflake data mart as curated subset of data to support the analytics and business intelligence needs of subgroup of users.
Translate requirements for business intelligence and reporting to database design and reporting design.
Conduct discovery sessions with users to capture business intelligence reports.
Develop Tableau self-service ad hoc reports to support operational users.
Develop Tableau Dashboards to support executive, management, and operational users.
About us:
Provista Software Corporation is fast growing Software Solution provider for US State and Local Government Clients.
Job Type: Full-time
Pay: $67,107.98 - $134,285.66 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Life insurance
Paid time off
Parental leave
Professional development assistance
Vision insurance
Compensation package:
Yearly pay
Experience level:
5 years
Schedule:
8 hour shift
Application Question(s):
What is your most preferred work time zone ?
Experience:
ETL: 5 years (Required)
Data warehouse: 1 year (Preferred)
Tableau: 1 year (Preferred)
Talend: 1 year (Preferred)
Microsoft SQL Server: 5 years (Required)
Work Location: Remote","$100,697 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Synovize
5.0",5.0,Remote,Big Data Engineer,"Synovize is a cutting-edge technology company that specializes in providing advanced data analytics and intelligence solutions. Our mission is to empower businesses with the tools and insights they need to make data-driven decisions and drive innovation. We are committed to delivering exceptional results by combining our expertise in big data technologies with our passion for creating impactful solutions.
As a Big Data Engineer at Synovize, you will have the opportunity to work on exciting projects that involve designing and implementing scalable data solutions. You will collaborate with a talented team of professionals, including software developers, data scientists, and domain experts, to tackle complex data engineering challenges and deliver high-quality solutions to our clients.
Job Title:
Big Data Engineer
Company: Synovize
Number of Positions: 2
Location: Remote
Length: 12 Months
Tax Terms: 1099/C2C or W-2
Rate/Range: $70-$90/hr
Work Authorization: US Citizen
Security Clearance: None
Skills Needed:
Proficiency in Java and Kotlin programming languages.
Strong knowledge of AWS (Amazon Web Services) and its big data offerings.
Experience with BigData technologies, frameworks, and tools.
Familiarity with Kafka for streaming data processing.
Proficiency in Python programming language.
Responsibilities:
Design and develop scalable data pipelines to process, integrate, and analyze large volumes of structured and unstructured data.
Implement data ingestion, transformation, and storage mechanisms using big data technologies and frameworks.
Collaborate with data scientists and analysts to understand data requirements and optimize data models for analytics and machine learning.
Utilize AWS services and tools to build and maintain data infrastructure, ensuring data quality, integrity, and security.
Troubleshoot and resolve data-related issues, optimizing performance and efficiency.
Stay up-to-date with emerging trends and advancements in big data technologies, recommending and implementing improvements to existing systems and processes.
Requirements:
Bachelor's or master's degree in computer science, engineering, or a related field.
Proven experience as a Big Data Engineer or similar role, with a strong understanding of big data technologies and architectures.
Proficiency in Java and Kotlin programming languages.
Strong knowledge of AWS and its big data offerings.
Experience with BigData technologies, frameworks, and tools.
Familiarity with Kafka for streaming data processing.
Proficiency in Python programming language.
Strong problem-solving skills and ability to work in a fast-paced, collaborative environment.
Excellent communication and interpersonal skills.
Join Synovize and be part of a dynamic team that is at the forefront of data innovation. Together, we will drive meaningful change and unlock the full potential of data-driven decision-making.
Job Types: Full-time, Contract
Pay: $70.00 - $90.00 per hour
Benefits:
401(k)
Dental insurance
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Vision insurance
Schedule:
10 hour shift
4 hour shift
8 hour shift
Choose your own hours
Application Question(s):
Are you a US Citizen?
Years of experience in Java?
Years of experience in Kotlin?
Years of experience in big data?
Years of experience in AWS?
Security Clearance
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$80.00 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"ProIT Inc.
5.0",5.0,"Bellevue, WA",Azure Data Engineer,"Data Engineering experience primarily on Spark. Someone who has worked on Azure cloud with knowledge on Azure DEVOPS (CI/CD & Infrastructure as a code), ADF, ADW & Power BI. Apache Nifi will be good to have.Power BI: Understand business requirements to set functional specifications for reporting applications Build automated reports and dashboards with the help of Power BI reporting tool Be experienced in tools and systems on MS SQL Server BI Stack, including SSRS and TSQL, Power Query, MDX, PowerBI, and DAX Be able to quickly shape data into reporting and analytics solutions Have knowledge of database fundamentals such as multidimensional database design, relational database design, and more Create functional reporting Study, analyze and understand business requirements in context to business intelligence. Design and map data models to shift raw data into meaningful insights. Utilize Power BI to build interactive and visually appealing dashboards and reports. Spot key performance indicators with apt objectives Run DAX queries and functions in Power BI Should have an edge over making DAX queries in Power BI desktop. Developing visual reports, KPI scorecards, and dashboards using Power BI desktop. Connecting data sources, importing data, and transforming data for Business intelligence.
Spark ,Azure DEVOPS (CI/CD & Infrastructure as a code), ADF, ADW & Power
Job Type: Full-time
Pay: $100,154.64 - $104,132.47 per year
Ability to commute/relocate:
Bellevue, WA 98004: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Hybrid remote in Bellevue, WA 98004","$102,144 /yr (est.)",51 to 200 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Codinix Technologies
5.0",5.0,"Boston, MA",Cloud Data Engineer - AWS,"Only US Citizen
Strong in warehouse technologies (Hadoop, Spark, AWS, etc.)
Bachelor's degree in Computer Science, Information Systems, or another related field
5+ years of data engineering, and cloud engineering experience
Experience in Agile project methodologies and DevOps
Experience with AWS (certification preferred; including Athena, Redshift, Glue), Cloudera
Experience building a CI/CD pipeline to deploy applications on AWS (GitLab, Terraform)
Advanced experience programming and scripting on Unix/Linux (i.e. Python, Bash)
Experience with CTRL-M, Cron (other) scheduling of batch jobs
Experience migrating legacy systems to cloud-ready architectures
Job Type: Contract
Pay: Up to $75.00 per hour
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Boston, MA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",$75.00 /hr (est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Wallero
4.4",4.4,"Seattle, WA",Data Engineer,"Hi,
Pretty much interested in your resume, we have an urgent opening for Data Engineer role with one of our clients based in Seattle WA.
Position: Data Engineer
Location: Seattle WA - Remote (100%)
Description:
Good Experience in reporting space including PowerBI, ADF, and possibly standing up a data warehouse/data mart.
The main scope centering is around reporting.
Experience with SQL, and agile framework including requirement gathering and story/feature writing skills.
Need someone strong who is comfortable leading and guiding teams in that capacity.
Migrating data from third party to Azure Cloud
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Seattle, WA 98101: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",$75.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2015,$1 to $5 million (USD)
"Rite Pros
3.4",3.4,"Portland, ME",Big Data Engineer,"Big Data Engineer with Bachelor’s degree in Computer Science, Computer Information Systems, Information Technology, or a combination of education and experience equating to the U.S. equivalent of a Bachelor’s degree in one of the aforementioned subjects.
Job Duties:
Define and manage the data architecture framework, standards, and principles, including modeling, metadata, security, reference, and master data.
Define reference architecture, and models to provide a common vocabulary, reusable components, and industry best practices.
Create solution frameworks integrating large or complex data sets.
Lead all data modeling efforts within DataBricks, including the design of data structures and the identification of business transformation logic.
Analyzing and translating business requirements into conceptual and fully detailed logical data models.
Creating logical data models based on existing applications and databases.
Working with business architects, and data stewards to capture business requirements in a Logical Data Model.
Transform the logical representation of the model into a physical representation and work with data engineering team to instantiate and manage the data.
Contribute to assessment of appropriate data platform(s) for solutioning efforts.
Collaborate with the BI and Analytics teams on creating the optimized, reusable semantic model, complete with metadata and lineage information.
Define and govern data modeling and design standards, tools, best practices, and related development for enterprise data models
Maintain the metadata - the ""data about the data"" describing the data model, its structure, and semantics
Recommend data and design patterns for common business functionality
Assist in multi-phase roadmaps within and across functional teams
Provide insight and technology recommendations that result in enterprise solutions and architectural risk mitigation.
Contribute to innovation strategies by exploring, investigating, recommending, benchmarking and implementing new data centric technologies for the platform.
Identify, design, and implement internal process improvements such as automation of manual processes, optimization of data delivery and re-design of existing infrastructure for greater scalability.
Build the infrastructure and processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and/or other data technologies.
Create and maintain data-related documentation (e.g. data dictionaries, data maps, data models etc.)
Work with data engineering and analytics experts to strive for greater functionality in our data systems and incorporation of industry best practices
Work experience / Technologies required for the position :
Experience building and optimizing Big Data, Data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, meta data, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected data sets.
Working knowledge of message queuing, stream processing, and highly scalable Big Data, data stores.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience in a Data Engineer or similar roles, who has attained a Bachelor’s degree in Bachelor’s degree in Engineering, Computer Science, Information Technology, related field or equivalent work experience.
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Work location is Portland, ME with required travel to client locations throughout USA.

Rite Pros is an equal opportunity employer (EOE).

Please Mail Resumes to:
Rite Pros, Inc.
565 Congress St, Suite # 305
Portland, ME 04101.
Email: resumes@ritepros.com","$97,894 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Less than $1 million (USD)
"The Bouqs Company
3.1",3.1,"Marina del Rey, CA",Data Analytics Engineer,"The role contributes to The Bouq’s mission of revolutionizing the way we commemorate life’s moments by connecting people to beautifully designed floral experiences and the responsible partners who create them by being a key member of the data team. As an Analytics Engineer, you will work closely with the Product, Engineering, and Data teams to build and maintain the data infrastructure needed to support our data and business needs. You will also be responsible for developing, optimizing, and maintaining the best-in-class data pipelines, data models, and ETL processes to ensure that data is accurate, reliable, and available to stakeholders in a timely manner. The Analytics Engineer will also serve as the liaison between Engineering and Analytics and will serve as an active member in both teams.
Responsibilities:
Lead the transfer of data modeling from legacy systems to DBT
Contribute to building the data modeling layer, which exposes clean, transformed data to the whole company for analytics
Build datasets in DBT (cloud) for data analysts to improve speed and accuracy for the team
Improve current processes, whether that includes modularizing and standardizing a piece of commonly used code
Design and develop new data pipelines and streaming processes that are highly available, scalable, and reliable
Develop review processes for new data models and take charge on implementing SQL standards for the team
Actively strive towards writing performant SQL rather than just SQL that works, while also ensuring the same SQL is easy to understand when new eyes look at it
Optimize data processing and flow within our Snowflake Data Warehouse
Document new datasets and pipelines and the reasoning/story behind their structure
Work closely with engineering to keep track of schema changes in the production database and adjust pipelines, as needed
Support existing data pipelines and systems in production
Apply software engineering best practices to analytics code such as version control and testing
Develop and communicate strong opinions about best practices in analytics
Help explore and evaluate new technologies
Qualifications:
4+ years of experience working within a data team, preferably as an Analyst/Data Engineer
Bachelor's degree in a quantitative field such as statistics, mathematics, economics, or computer science preferred
Strong SQL fluency in both DDL/DML and analytics (Snowflake experience is a plus)
Experience working with JSON, DBT or other data transformation tools
Experience working with an ETL tool such as Fivetran or Stitch
Knowledge of data structures and how to write performant SQL
Experience with ensuring data quality through testing, deltas, lineage, etc
Strong communication and critical thinking skills to deliver solutions that not only solve problems but also serve as tools we didn’t know we needed
Ability to transform raw data into intuitive datasets that serve as building blocks for analytics
Comfortable leading the growth of a data warehouse and maintaining it
Capable of working through uninformative assumptions and built-biases in datasets and are not stalled when data is not perfect/sparse
Compensation & Perks:
Competitive Base Salary Range of $120,000.00 - $180,000.00 USD + Equity Package
Health, Dental & Vision with 100% employee coverage
401k Matching
Three Weeks Paid Vacation
Discounts on The World’s Best Flowers (obviously!)
Work on cutting edge new technologies
About The Bouqs:
Our mission here at The Bouqs is to revolutionize the way we commemorate life’s moments by connecting people to beautifully designed flowers and the responsible partners who create them. Grounded in transparency, responsibility, and simplicity, we create genuine moments of emotional connection for our customers, build meaningful relationships with like-minded farmers and florists while empowering them to thrive, and eliminate unnecessary waste along the way.

Founded in 2012, The Bouqs is a venture-backed online floral retailer that delivers flowers fresh from eco-friendly, sustainable farms to doorsteps nationwide. Headquartered in Marina Del Rey, CA, The Bouqs connects farms and a curated network of artisan florists directly to consumers and disrupts the traditional supply chain by eliminating overhead costs like warehouses, importers, distributors, auctioneers and more. In turn, this model enables a superior product and redefines the experience and economics for both consumers and producers alike.

For more information, visit www.bouqs.com and follow the #BouqLove on Facebook, Instagram and Twitter.
The Bouqs is an Equal Opportunity Employer!","$150,000 /yr (est.)",51 to 200 Employees,Company - Private,Retail & Wholesale,Other Retail Stores,2012,$25 to $100 million (USD)
"DiamondPick
4.5",4.5,"Edison, NJ",Senior Data Engineer,"Hi ,
Greetings from Diamond pick inc.
We are currently looking for the below position for one of our clients... Please let me know your interest along with your updated resume ASAP..
Role:Data engineer
Location: Berkley Heights, NJ(Onsite)(locals only)
9+ years of experience is must
Description
Skills: strong Java,Azure,Spark & sql
Company Description:
Apexon is a digital-first technology services firm backed by Goldman Sachs Asset Management and Everstone Capital. We specialize in accelerating business transformation and delivering human-centric digital experiences. For over 17 years, Apexon has been meeting customers wherever they are in the digital lifecycle and helping them outperform their competition through speed and innovation.
RESPONSIBILITIES
Basic Qualifications for consideration:
5+ Overall industry experience
3+ years' experience with building large scale big data applications development
Bachelors in Computer Science or related field
Provide technical leadership in developing data solutions and building frameworks
Experience building Data Lake, data applications and micro-services using Azure, AWS and Hands-on experience in cloud Data stack (preference is Azure)
Hands-on experience with Springboot framework, MVC architecture, Hibernate, API creation, Restful services, SOAP using Java
Java experience with OOPS concepts, multithreading
Experience deploying code on containers
Hands on experience with leveraging CI/CD to rapidly build & test application code
Conduct code reviews and strive for improvement in software engineering quality
Review data architecture and develop detailed implementation design
Hands-on experience in production rollout and infrastructure configuration
Demonstrable experience of successfully delivering big data projects using Kafka, Spark
Experience working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search
Experience working with PCI Data and working with data scientists is a plus
In depth knowledge of design principles and patterns
Able to tune big data solutions to improve performance
QualificationsBachelor's Degree in Computer Science or Computer Engineering is required
Job Type: Contract
Salary: $43.96 - $70.65 per hour
Ability to commute/relocate:
Edison, NJ 08817: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL (Required)
Java (Required)
Azure (Required)
Work Location: One location",$57.30 /hr (est.),1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,#N/A,Unknown / Non-Applicable
Schober Consulting LLC,#N/A,"Minneapolis, MN",Data Engineer (Junior),"We are seeking a talented and experienced Data Engineer to join our team. The ideal candidate will have 2-3 years of professional experience in data engineering, with a bonus if they have worked in the real estate industry. Additionally, an interest in machine learning is highly desirable.
Responsibilities:
Design, develop, and maintain scalable and efficient data pipelines and ETL processes
Build and optimize data models and data storage solutions for large-scale datasets
Collaborate with cross-functional teams, including data scientists and analysts, to gather requirements and design data-driven solutions
Develop and maintain data infrastructure to support advanced analytics and machine learning initiatives
Ensure data quality and integrity by implementing data validation and cleansing processes
Continuously monitor and optimize data pipelines and systems for performance and reliability
Stay updated with the latest industry trends and technologies in data engineering and machine learning
Qualifications:
Bachelor's or Master's degree in Computer Science, Data Engineering, or a related field
2-3 years of professional experience as a Data Engineer, preferably with exposure to the real estate industry
Strong understanding of data engineering principles, ETL processes, and data warehousing concepts
Proficiency in SQL and experience with relational databases
Solid programming skills in Python, Java, or other relevant languages
Familiarity with cloud computing platforms, preferably AWS or Azure
Experience with data modeling and designing efficient database schemas
Knowledge of machine learning concepts and a strong interest in applying machine learning to data engineering tasks
Excellent problem-solving and analytical abilities
Strong communication and collaboration skills
If you are a self-motivated individual with a passion for data engineering and an interest in the real estate industry and machine learning, we encourage you to apply. We offer a competitive salary, comprehensive benefits package, and a collaborative work environment that fosters professional growth and development.
Job Type: Full-time
Pay: $70,000.00 - $100,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Compensation package:
Employee stock ownership plan
Experience level:
2 years
Schedule:
Monday to Friday
Ability to commute/relocate:
Minneapolis, MN 55402: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
SQL (Preferred)
Data warehouse (Preferred)
Python (Required)
Work Location: Hybrid remote in Minneapolis, MN 55402","$85,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
BIGCLFY,#N/A,"Dallas, TX","W2/ 1099, No C2C - SSIS ETL Data Engineer (Minneapolis, MN/ Dallas-Irving)","No C2C. It's W2/1099 Requirement
Only taking USC, GC or H4 EAD, H1 Transfer, L2 Visa or TN Visa - they will not take OPT EAD or CPT
Need to have confirmation in writing that you will be onsite by Day 1
ALL ROLES ARE HYBRID IN THE OFFICE (2-3 DAYS/WEEK) UNLESS REMOTE IS NOTED
Please submit qualified candidates and include FULL LEGAL NAME as it appears on the passport
If your candidate is relocating, need to have confirmation in writing that they will be onsite by Day 1
Senior SSIS/ETL Data Engineer
12 months – 3 days in office
Minneapolis, MN or Dallas-Irving, TX
5+ years of experience with SQL Server 2016 (or newer) and SSIS
7+ years of data engineering
Must be capable of leading teams
2+ years of experience working with large scale SQL implementations
Experience designing, developing, and maintaining complex stored procedures, functions, and views using T-SQL
Experience designing, developing, and maintaining ETL processes using SSIS
Should understand how to unit test their product
Hands on experience with Visual Studio to document
Experience in financial or regulatory environments highly preferred
Ideal candidates will have experience creating conceptual, logical, and physical database designs
Preference given to candidates with extensive knowledge and experience with performance monitoring, database and SQL tuning
Bachelor’s or Master's in computer science or related field or equivalent work experience
Job Type: Contract
Pay: From $73.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Chandler, AZ 85225: Reliably commute or planning to relocate before starting work (Preferred)
Application Question(s):
Expected Payrate on W2/1099 Tax Term? Please mention. Thanks.
Experience:
Risk Analytics: 5 years (Preferred)
Work Location: In person",$73.00 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Evergreen Residential Holdings, LLC
5.0",5.0,"Dallas, TX",Engineer Data & Analytics,"We are Evergreen Residential, a high growth early-stage institutional investment platform in the single-family residential sector. Our team is collaborative, open-minded and curious. Transparency is a core value, we speak our minds, are responsible for our actions and celebrate our wins. We are serious about the business without taking ourselves too seriously. We look for people who thrive in an entrepreneurial and fast paced environment. If you are self-motivated and mission driven with a 'can do' mindset and see solutions where others may see problems, come and grow with us!
We offer a flexible, empowering culture, competitive compensation and benefits, and potential for career growth through working closely with, and learning from, our experienced leadership team.
As a technical/engineering expert, you pride yourself on being able to quickly build strong business relationships both internally and externally e.g., with the leadership team, current and potential investors. With a passion for keeping current with advancements of the field, you deploy technology and data resources to provide innovative solutions to business needs.
The Role: Priorities can often change in a fast-paced environment like ours. Initial focus is to work with external data purchased, and to harvest internal data and work within Snowflake warehouse for use in our 3rd party property mgt system and BI reporting tool. Overall ensure there is one source of truth.

The role includes, but is not limited to, the following responsibilities:
Designing and implementing data pipelines to extract, transform, and load data from various sources into a centralized data repository
Developing and maintaining data processing and storage infrastructure
Establish productive relationships and effective communications with Company leadership to understand business drivers and align on required outcomes
Collaborating with data analysts to ensure that data is readily available for analysis and modeling
Optimizing database performance and troubleshooting issues as they arise
Implementing data security and access controls to protect sensitive data
Highlight key trends derived from data analysis and be a resource for improving data proficiency throughout the organization
Staying up-to-date with emerging trends and technologies in data engineering
Leverage historical data and predictive models to identify key historical factors that impact critical KPIs, and recommend actions to drive future performance
Ensure scientific method and research are key drivers of the product roadmap
What You Will Bring to the Table:
Knowledge of data modeling, database design, and ETL best practices
At least 3-5 years of experience in data engineering or a related field
Proficiency in one or more programming languages such as Python, Java, or Scala
Experience with data processing and storage technologies such as Hadoop, Spark, Kafka, Snowflake, and NoSQL databases
Experience in real estate investment and/or rental sector highly desirable
Prior experience managing a team of direct reports within the Data Science, Data Engineering, Analytics space in the SFR or Multifamily industry
Significant Experience building, motivating, and retaining a high- performing, flexible and collaborative data and analytics function
Proven hands-on technical background in data science, business intelligence or data engineering with demonstrated strategic impact at an executive level
A strong problem solver with experience building technical strategy and understanding technical tradeoffs and risk
Collaborative team player, you are truly a ""do-er"", happy to be a hands-on problem-solver to move the data program forward
Excellent communication skills – verbal and written
About Evergreen Residential
Founded in 2021, Evergreen Residential is a full-service SFR platform leveraging proven operational practices and the latest technological advances to optimize investor returns and achieve positive outcomes for our residents and the communities in which we operate. We offer a full suite of services, including Investment Management, Asset Origination, and Advisory Services. The firm is headquartered in Dallas with offices in New York City.
The leadership team has extensive experience dating back to the early institutionalization of SFR and unrivaled depth of experience in the complete asset life cycle. We are built to withstand changing market conditions, and our business produces resilient, predictable cash flows and margins. We are committed to charting new paths and using data to achieve best-in-class results. Our business is evergreen.
Beyond financial returns, the Company is committed to measurable impact objectives. We believe that inclusive and equitable management, environmentally sustainable long-term strategies, and resident-focused policies are good business - for our residents, our investors, and our team. We are committed to using environmentally sustainable practices and empowering our residents to improve their financial health.
Our cornerstone values - Accountability, Transparency and Partnership - are built on a foundation of Integrity and provide the roadmap for our daily actions, interactions and decisions.

Equal Opportunities and Other Employment Statements
We are deeply committed to building a workplace and community where inclusion is not only valued, but prioritized. We take pride in being an equal opportunity employer and seek to create a welcoming environment based on mutual respect, and to recruit, develop and retain the most talented people from a diverse candidate pool. All employment decisions shall be made without regard to race, color, religion, gender, gender identity or expression, family status, marital status, sexual orientation, national origin, genetics, neuro-diversity, disability, age, or veteran status, or any other basis as protected by federal, state, or local law.","$98,425 /yr (est.)",51 to 200 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Spartan Technologies
3.4",3.4,"Kansas City, MO",Data Engineer I,"Spartan Technologies, Inc. - Kansas City, MO
Applicants Need to Know
100% remote in East Coast or Central US only
No Sponsorship
Contract to Hire
Hourly Rate: $45 on W2
Data Engineer I
The Data Engineer is responsible for processing structured and unstructured data, validating data quality, and developing and supporting data products for analytics. Brings together data from different sources into a common dataset with common KPIs and dimensions for stakeholders. Understands the analytics challenges that the marketing organization faces in their day-to-day work and partners with them to design viable data solutions.
Your Job
Gathers and processes raw, structured, semi-structured, and unstructured data using batch and real-time data processing frameworks.
Understand the existing source system data models and perform end to end data validation to maintain accuracy.
Implements and optimizes data solutions in enterprise data warehouses and big data repositories.
Ensures data quality and implements tools and frameworks for automating the identification of data quality issues.
Develop re-useable processes that can be leveraged and standardized for multiple instances
Works with internal and external data providers on data validation providing feedback and making customized changes to data feeds and data mappings.
Recommends improvements to processes, technology, and interfaces that improve the effectiveness of the team and reduce technical debt.
Ensure performance by monitoring existing workflows to ensure they are working correctly and that they reconcile with the source.
Design and develop ETL workflows and datasets to be used in extracts by our BI reporting tool Tableau.
Will write complex SQL queries with multiple joins to automate/manipulate these reporting extracts
Works in partnership with marketing stakeholders concerning the accuracy of data and efficiency of processes
Qualifications
Working experience with data modelling, data access, schemas, and data storage techniques within Snowflake
Working experience with batch and real-time data processing frameworks
Working experience working with relational databases such as SQL
Working experience with business intelligence tools and platforms
Working experience with data quality tools
Experience with ETL processes and tools
Desirable:
Working experience with Snowflake
Working experience with Google Analytics BigQuery
Working Experience with Tableau Server
Working experience with SQL Server Integration Services",$45.00 /hr (est.),1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$5 to $25 million (USD)
"AbleTo
3.7",3.7,"New York, NY",Data Engineer,"About AbleTo
Join our mission-driven organization, where your work matters and a diversity of ideas and backgrounds are welcomed. AbleTo is the leading provider of high quality, technology-enabled behavioral health care. We believe that everyone deserves access to high-quality care and offer a suite of technology-enabled services to empower people to lead better lives through better mental health. A proprietary platform connects individuals with AbleTo highly trained licensed providers who deliver weekly sessions by phone or video supported by an integrated digital experience. Members also have access to mental health coaches, and digital support programs. Our outcomes-focused approach is proven to improve both behavioral and physical health and lower medical costs.

Overview
Data is integral to nearly everything that we do at AbleTo, and as a result the data team plays a key role at our company. As a Data Engineer at AbleTo, you will grow and develop your skills as a data engineer to build a world class data platform to support our mental health care mission.

What You Will Do
Build and maintain batch and real-time ETL pipelines in a Google Cloud Platform architecture (BigQuery, Dataproc, Firestore, Informatica, Python, etc.)
Identify code quality issues and implement tests to improve future processes.
Document orchestration work-flows and create run books
Implement data integrity tests to ensure we are ingesting accurate data.
Implement high-quality test-driven code.
Participate in daily team stand ups and other Agile ceremonies

Key Initiatives over the next 12 months
Data Mart client migration
Data Mart enhancements
Automation of manual day to day operational tasks
Decommission legacy pipelines

Who You Are:
1+ years of experience coding in Python.
Experience working with SQL (structured query language).
Familiarity with structuring and writing ETLs.
Experience working with Orchestration tools (Airflow) and Cloud Databases (BigQuery) is a plus.
Experience working in an Agile Development environment is a plus.
BS in Information Systems, Computer Science or related field is preferred.
You are excited to work with data!
Curiosity to dive deeply into issues and feel empowered to make a meaningful impact at a mission-driven company.
Committed to agile development and value delivery and solid engineering principles, as well as continuously improving our craft.
You are a collaborator. You build and maintain strong, productive working relationships with your customers and stakeholders.
You are empathetic and seek to understand each individual's diversity of background and experience contributes to making up a great team.
You drive to improve yourself and others.
You are excited about learning new languages, tools, techniques and technologies.

Why You Should Join Our Team:
We're proud to be a Great Place to Work-Certified™ company. We want you to show up and feel your best at work, and that means respecting your time outside of work. Our inclusive, flexible workspace prioritizes a work/life balance. We offer competitive salaries, comprehensive health benefits (for full-time employees), and professional perks such as 401K matching, fully funded HRA, and generous time off, including mental health days because your well-being is important to us.
At AbleTo, we're empowering people to get better and stay better. Want to join us? Take the next step in your career by applying for this role today.
The salary range is $40,000 to $90,000. Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, AbleTo offers benefits such as a comprehensive benefits package, incentive and recognition programs, and 401k contribution (all benefits are subject to eligibility requirements).

#LI-Remote
Follow AbleTo on LinkedIn, Twitter, and Instagram!
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.
AbleTo is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or veteran status. AbleTo is an E-Verify company.","$65,000 /yr (est.)",501 to 1000 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2008,$25 to $100 million (USD)
"Global Enterprise Services, LLC
4.0",4.0,"Atlanta, GA",Data Harmonization Engineer,"Data Harmonization Engineer

Collects data models for disparate datasets within a domain. Collaborates with CDC & STLT stakeholders to develop a conceptual model that defines the atomic concepts for that domain.
Constructs semantic mappings from the data elements in the source datasets to atomic or compound concepts in the conceptual model.
Develops sample transformation code to translate among representations.
Configures and manages vocabulary services to allow data modernization users to improve understanding of the relationships among data representations.

Data Harmonization Engineer - Junior [YoE, Edu, Certs: 0-3 yrs & BA/BS] [Salary Range: $81k - $91k]
Data Harmonization Engineer - Journeyman [YoE, Edu, Certs: 4-9 yrs & BA/BS] [Salary Range: $95k - $105k]
Data Harmonization Engineer - Senior [YoE, Edu, Certs: 10+ yrs & MA/MS] [Salary Range: $108k - $108k]","$100,000 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
IT Customer Service - POST,#N/A,"Newark, NJ",Data Center Plant Engineer,"Job Title: ISP-OSP Data Center Plant Engineer
Location: Newark NJ, (M-F)
Travel Required: As Needed
Overview
UC-ISP/OSP engineer is responsible for cable management including structured cabling, carrier data and voice circuit cables/equipment, rack-and-stack, and oversight of power, backup power, network equipment, and environmental needs to cabinets. Leveraging Clients Data Center Infrastructure Management System, (DCIM) tool to maintain, report and monitor.
Conducting capacity assessments of existing infrastructure to ensure that it can support future growth to ensure compliance with federal regulations of the Data Center infrastructure Standards. Projects include design and build outs of the physical plant within the typical WAN/LAN space within data centers, telco rooms, MDFs and IDFs as needed.
Functions:
Perform initial engineering surveys based on project needs.
Document engineering surveys to determine feasibility and the estimated cost.
Obtain authorization to proceed and confirm funding.
Create detailed engineering drawings of build, floor plans, equipment elevation drawings, structured cabling diagrams, power, and environmental needs.
Designs must conform to state, local, national, and Client standards and codes, and the NJ historical and or environmental commissions approvals if needed.
Based on the engineering documents create a SOW (Scope of Work) and BOM (Bill of Materials)
Submit to Client manager to approve and proceed when funding is validated.
Create a construction package and submit to contractors or in house for RFQ.
Review quotes and choose both low bid and qualified contractor.
Work with all stakeholders to date and time work schedules.
Confirm workers are safety trained and conform to Client, FRA, and OSHA requirements.
Supervise and manage installations and conduct inspections.
Review work to ensure contractor met obligations under terms of contract.
Inform Client managers of status to approve invoices.
Coordinate with Client network engineering to connect, configure, and test equipment.
Review and confirm a successful installation were completed and working as designed prior to cutting over to production.
Assist with Break fix of IT infrastructure troubles or repairs.
Daily travel to location depending on scheduled or unscheduled work
Job Types: Full-time, Contract
Experience level:
5 years
Schedule:
8 hour shift
Work setting:
In-person
Experience:
Data Center: 5 years (Required)
WAN/LAN: 3 years (Required)
cabling: 3 years (Required)
Work Location: One location",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"WorkCog
4.3",4.3,"Atlanta, GA",Sr. AWS Data Engineer,"Position: AWS Data Engineer
Location: Atlanta ,GA
Duration: 12+ Months
Interview: Video
JD
Responsibilities
Bachelor’s degree in Computer Science, Software Engineering, or other related field, or equivalent work experience.
Extensive experience in data modeling, writing & performance tuning complex SQL queries & user-defined functions
Hands-on experience in design, implementation and orchestration of data pipelines and ETL batch Jobs.
Hands-on experience on all aspects of data handling: cleansing and standardization, normalization, merging, change data capture, data integration, reconciliation, and transformations
Strong experience in working with Big Data technologies such as Hive/Spark, Data lake and Data warehouse using GCP and/or AWS Services
Ability to write robust code in scripting languages like Python, Scala, PySpark or similar.
Good to Have
Associate Architect or equivalent cloud certification
Experience with processing large datasets using AWS and GCP UDFs (User defined functions)
Experience in ML probability & statistics algorithms, frameworks like Keras or PyTorch and tools like Jupyter notebooks.
Job Type: Contract
Pay: $45.83 - $52.42 per hour
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Atlanta, GA: Reliably commute or planning to relocate before starting work (Preferred)
Experience:
SQL (Preferred)
Informatica (Preferred)
Data warehouse (Preferred)
Work Location: In person",$49.13 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable
"Jane Technologies
4.8",4.8,"Santa Cruz, CA",Data Migration Engineer,"Company Overview:
Jane is an MIT-founded, high-growth, and quickly-growing technology company in the cannabis industry. We believe in the cannabis industry's ability to bring well-being, health, and love into this world, and it is our mission to bring confidence to the online cannabis shopping experience. As the cannabis industry's first complete real-time marketplace, we aim to provide consumers with a confident, safe, and simple shopping experience.
To learn more about who we are, our culture, and whether this is the right place for you, read our Key Values profile: https://www.keyvalues.com/jane. Check out our product at: https://www.iheartjane.com/.

What You'll do:
Jane is looking for a Data Migration Engineer to help with the development, documentation and implementation of data migration services for Jane Point of Sale. As a Data Migration Engineer you will work within the Operations Department and collaborate with Engineering, Partner Success, Content and Tech Support. The Data Migration Engineer will collaborate with cross-functional teams and directly with our partners during the onboarding process to extract, transform and load (ETL) data from diverse sources into Jane POS. You will help ensure our partners experience a smooth transition from their legacy system to our industry-leading point of sale. Culture is the single most important part of Jane's success to date. You will report to our Program Manager.
Responsibilities
Take lead responsibility for researching, implementing, testing, validating, and documenting data migration services for Jane POS
Take lead responsibility for executing data migration services for each Jane POS onboarding, which may include personal health information (PHI) of dispensary customers
Validate and cleanse data to ensure accuracy and integrity during the migration process
Develop data mapping rules and perform data transformations to align source data with target system requirements
Conduct data profiling and quality assessments to resolve any issues or discrepancies
Help create test plans to validate the accuracy of migrated data
Collaborate with cross-functional teams, including developers, project managers, and data owners, to ensure seamless data migration
Follow data protection and privacy regulations, ensuring the secure handling of sensitive customer information
Document migration processes, procedures, and any encountered issues for future reference
Provide ad-hoc support to Deal Desk team migrating internal data within existing Jane systems
Work with Engineering to identify challenges in the Jane POS data model and import tool
Work with Partner Success and Technical Support to identify and remedy issues that occur during or because of the data migration process
Stay updated on industry best practices and latest trends in data migration techniques and technologies
Qualifications
Familiar with Python, software engineering concepts, and current technologies
Familiarity with working with relational databases and API endpoints
Communicate challenges and progress with team members through presentations, group calls.
Ability to leverage and create technical documentation
Bachelor's degree in software engineering or similar field or 2+ years of experience working in a similar role
Our Benefits:
Great compensation package and equity
Remote friendly work environment with employees throughout the US and Canada
Health, Dental, Vision, 401k, Unlimited PTO, and home office stipend

Jane Technologies is proud to be an equal opportunity employer and committed to providing employment opportunities regardless of race, religious creed, color, national origin, ancestry, physical disability, mental disability, medical condition, genetic information, marital status, sex, gender, gender identity, gender expression, pregnancy, childbirth and breastfeeding, age, sexual orientation, military or veteran status, or any other protected classification, in accordance with applicable federal, state, and local laws. EOE, including disability/vets.","$90,292 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Software Development,2017,Unknown / Non-Applicable
"PrizePicks
4.8",4.8,"Atlanta, GA",Data Engineer,"At PrizePicks, we are the fastest growing sports company in North America, as recognized by Inc. 5000. As the leading platform for Daily Fantasy Sports, we cover a diverse range of sports leagues, including the NFL, NBA, and Esports titles like League of Legends and CS : GO. Our team of over 200 employees thrives in an inclusive culture that values individuals from diverse backgrounds, regardless of their level of sports fandom. Ready to reimagine the DFS industry together?
Our Analytics Team is responsible for building and maintaining analytics tools and workflows to support the PrizePicks business. By developing, maintaining, and testing data generation infrastructures, you will enable the PrizePicks business to make smarter, better, and faster data-driven decisions.
What you'll do:
Create, maintain and orchestrate optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Improve and streamline data systems to drive innovation within Prizepicks
What you have:
3+ years of experience building frameworks for data ingestion pipelines but real time and batch using data modeling, ETL/ELT processes
Stellar SQL skills, experience building DBT pipelines in production and know your way around structured, semi-structured and unstructured data.
Have built and optimized 'big data' data pipelines, architectures and data sets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Experience with Python and other object oriented scripting languages
Cloud experience: AWS, EC2, EMR, RDS, Redshift, GCP, etc..
Not required but would be awesome if you have experience with any of the following data pipeline and workflow tools: Azkaban, Luigi, Airflow, Prefect, etc.
Where you'll live:
Anywhere in the US is fine (we are based in Atlanta, GA)
Benefits you'll receive:
In addition to your great compensation package, company subsidized medical/dental/vision coverage plans and matching 401(k), we'll shower you with perks including:
Break room with ping pong, endless snacks and in-office lunch once a week
Unlimited PTO to encourage a healthy work/life balance (2 week min required!)
Modern work schedule focused on getting the job done, not hours clocked
Workplace flexibility
Company and team outings, we encourage a tight-knit workplace
Generous Maternity AND Paternity leave (16 weeks!)
Annual bonus & stock options
Wellness program
Company equipment provided (Windows & Mac options)
Annual performance reviews with opportunity for growth and career development
#LI-REMOTE

You must be authorized to work for any employer in the U.S. We are unable to sponsor or take over sponsorship or an employment Visa at this time.
PrizePicks is an Equal Opportunity Employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.","$97,814 /yr (est.)",1 to 50 Employees,Company - Private,"Arts, Entertainment & Recreation",Sports & Recreation,2015,Unknown / Non-Applicable
"PRIMUS Global Services, Inc
4.1",4.1,"Minneapolis, MN","Data Engineer – Hadoop, Python – REMOTE WORK 43198","We have an immediate long-term opportunity with one of our key clients for a position of Data Engineer, to work on a remote basis.

The ideal applicant must have extensive experience with Python, Hadoop, Hive and SQL.

**ALL successful candidates for this position are required to work directly for PRIMUS. No agencies please**

For immediate consideration, please contact:

Tejaswini
PRIMUS Global Services
Direct: 972-798-2662
Desk: 972-753-6500 Ext: 204
Email: jobs@primusglobal.com","$91,633 /yr (est.)",501 to 1000 Employees,Private Practice / Firm,Information Technology,Information Technology Support Services,2002,$5 to $25 million (USD)
"Acrisure Technology Group
3.9",3.9,"Austin, TX",Data Engineer,"Data Engineer
Hybrid Position (3 days per week average in Downtown Austin, TX or Grand Rapids, MI office)
Note: This is a full-time, in-house position. We do not offer C2C or C2H employment and are not able to sponsor visas for this position.
Acrisure Technology Group (ATG) is a fast-paced, AI-driven team building innovative software to disrupt the $6T+ insurance industry. Our mission is to help the world share its risk more intelligently to power a more vibrant economy. To do this, we are transforming insurance distribution and underwriting into a science.
At the core of our operating model is our technology: we're building the premier AI Factory in the world for risk and applying it at the center of Acrisure, a privately held company recognized as one of the world's top 10 insurance brokerages and the fastest growing insurance brokerage globally. By using the latest technology and advances in AI to push the boundaries of understanding risk, we are systematically converting data into predictions, insights, and choices, and we believe we can remove the constraints associated with scale, scope, and learning that have existed in the insurance industry for centuries.
We are a small team of extremely high-caliber engineers, technologists, and successful startup founders, with diverse backgrounds across industries and technologies. Our engineers have worked at large companies such as Google and Amazon, hedge funds such as Two Sigma and Jump Trading, and a variety of smaller startups that quickly grew such as Indeed, Bazaarvoice, RetailMeNot, and Vrbo.
The Role
The Business Intelligence team's mission is to unify data across the enterprise to optimize business decisions made at the strategic, tactical, and operational levels of the organization. We accomplish this by providing an enterprise data warehouse, data lake, reporting platform, and business processes that provide quality data, in a timely fashion, from any channel of the company and present them in such a manner as to maximize the value of that data for both internal and external customers.
The Data Engineer is responsible for designing and developing moderate to complex ETL processes required to populate a data lake and structured data warehouse which supply data for the machine learning, AI & BI teams. Responsibility includes working with a team of contracted developers as well as coaching and mentoring junior and mid-level developers. Ensuring high quality and best practices are maintained through the development cycle is key to this position.
You will interact with some of the top technologists on the planet. Our technology runs on Google Cloud and is configured with Kubernetes, leveraging various services in that environment. Our data storage layer includes BigQuery, BigTable, and Postgres. We code primarily in Kotlin, Python, Java, and JavaScript and make use of many frameworks, including Dataflow, Cloud AI Platform, KubeFlow, Spring, and React.
Here are some of the ways in which you'll achieve impact
Leverage established guidelines and custom designs to create complex ETL processes to meet the needs of the business
Develop from strategic and non-strategic data sources including data preparation/ETL and modeling for data visualizations in a self-service platform
Contribute to the definition and development of the overall reporting roadmap
Translate reporting requirements into reporting models, visualizations and reports by having a strong understanding of the enterprise architecture
Standardize reporting that helps generate efficiencies, optimization, and end user standards
Integrate dashboards and reports from a variety of sources, ensuring that they adhere to data quality, usability, and business rule standards
Independently determine methods and procedures for new or existing requirements and functionality
Work closely with analysts and data engineers to identify opportunities and assess improvements of our products and services
Contribute to workshops with the business user community to further their knowledge and use of the data ecosystem
Produce and maintain accurate project documentation
Collaborate with various data providers to resolve dashboard, reporting and data related issues
Perform Data Services reporting benchmarking, enhancements, optimizations, and platform analytics
Participate in the research, development, and adoption of trends in reporting and analytics
Mentor BI Developers and BI Analysts
Other projects as assigned in order to support necessary business goals across teams
You may be fit for this role if you have
Minimum 5 years required, particularly in an Azure environment with Azure Data Bricks, Azure Data Factory, Azure Data Lake
Minimum 5 years designing data warehouses, data modeling, and end-to-end ETL processes in a MS-SQL environment
Minimum 2 years developing machine learning models with Azure ML, ML Flow, BQML
Expert working knowledge of SQL, Python and Spark (and ideally PySpark) with a demonstrated ability to create ad-hoc SQL queries to analyze data, create prototypes, etc required.
Successfully delivered 2+ end to end projects – from Inception to Execution - in Data Engineering / Data Science / Data Integration as a Tech Senior/Principal
Ability to Analyze, summarize, and characterize large or small data sets with varying degrees of fidelity or quality, and identify and explain any insights or patterns within them.
Experience with multi-source data warehouses
Strong skills in in data analytics and reporting, particularly with Power BI
Experience with other cloud environments (GCS, AWS) a definite plus
Strong experience creating reports, dashboards, and/or summarizing large amounts of data into actionable intelligence to drive business decisions required
Strong understanding of core principles of data science and machine learning; experience developing solutions using related tools and libraries
Hands on experience building logical data models and physical data models and using tools like ER/Studio/Idera
Write SQL fluently, recognize and correct inefficient or error-prone SQL, and perform test-driven validation of SQL queries and their results
Proficient in writing Spark sql using complex syntax and logic like analytic functions etc.
Well versed in Data Lake & Delta Lake Concepts
Well versed in Databricks usage in dealing with Delta tables (external \ managed)
Well versed with Key Vault \ create & maintenance and usage of secrets in both Databricks & ADF
Should be knowledgeable in Stored procedures \ functions and be able to use them by ADF & Databricks as this is a widely used Practice internally
Familiar with DevOps process for Azure artifacts and database artifacts
Well versed with ADF concepts like chaining pipelines, passing parameters, using APIs for ADF & Databricks to perform various activities.
Experience creating and sharing standards, best practices, documentation, and reference examples for data warehouse, integration/ETL systems, and end user reporting
Apply disciplined approach to testing software and data, identifying data anomalies, and correcting both data errors and their root causes
Academics: Undergraduate degree preferred or equivalent experience along with a demonstrated desire for continuing education and improvement
Location: Austin, TX or Grand Rapids, MI
We are interested in every qualified candidate who is eligible to work in the United States. We are not able to sponsor visas for this position.","$96,659 /yr (est.)",5001 to 10000 Employees,Company - Private,Insurance,Insurance Agencies & Brokerages,2005,$1 to $5 billion (USD)
Premier Consulting Group,#N/A,"Boca Raton, FL",Senior Data Engineer,"* The right person could be located in Florida or Massachusetts as company has office locations in Boca Roton and Boston.
* Must be a US Citizen
Overview Summary of the Work:
ETL Azure Data Factory work: A Data Engineer who is to be able to import data from a variety of different sources (SFTP, API, Fileshare) and file types (CSV, Excel, JSON). Company also captures files via email now, but they have a pattern set up to be able to do that, so the person would just repeat how they’ve done that. Check existing Azure Data Factory pipelines for failures and troubleshoot. Familiar with dynamic expression and syntax in Azure Data Factory pipelines.
Backlog of stored procedures: Be skilled in stored procedure development and SQL skills.
Data quality and data cleansing: Be able to evaluate data (incoming and existing). Look for issues and be able to resolve them. In general, someone who can look at the data and mentally make the leap of “hey, this looks wrong.” Look at data and determine how it might be better utilized (examples: 2.5% as a string or 0.025 as a number, identify values as empty strings and save as nulls instead, etc).
Technical Experience Required/Preferred:
SQL, Azure SQL, Azure Data Factory, Data warehousing environments (dims and facts), Snowflake (highly preferred), Python (highly preferred).
Job Types: Full-time, Permanent
Pay: $140,000.00 - $160,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Bonus pay
Yearly bonus
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Weekend availability
Work Location: Hybrid remote in Boca Raton, FL 33431","$150,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Inovalon
3.1",3.1,United States,Software Data Engineer,"Inovalon was founded in 1998 on the belief that technology, and data specifically, would empower the transformation of the entire healthcare ecosystem for the better, improving both outcomes and economics. At Inovalon, we believe that when our customers are successful in their missions, healthcare improves. Therefore, we focus on empowering them with data-driven solutions. And the momentum is building.
Together, as ONE Inovalon, we are a united force delivering solutions that address healthcare's greatest needs. Through our mission-based culture of inclusion and innovation, our organization brings value not just to our customers, but to the millions of patients and members they serve.
Overview: The Software Data Engineer is responsible for contributing to data pipelines, ETL, data warehouse, jobs, data operations. They will be part of the team which is building next generation data & reporting platform. This platform will cater internal business stakeholders and external customers to provide insights & forecasting to understand current state of business, improve decision-making for their tactical and strategic goals & KPIs. This position may require independent work, sharing information and assisting others with work request.
Duties and Responsibilities:
Work with the agile team to participate in agile ceremonies like grooming, planning, standup, retrospective, demos
Actively contribute to grooming, and standup, create & update tasks, estimate and status
Work with data architects and business analysts to create a logical data model and create DDL scripts for physical database creation
Work on large data to ensure ingestion of data, dynamic rule & validation of data, cleansing, transforming, and loading into the data warehouse
Write complex queries, stored procedures, functions, SSIS Packages for various job execution
Develop modern ETL framework utilizing tools like ADF (Azure Data Factory), MS-SSIS etc
Develop STAR or SNOWFLAKE database schema utilizing industry best practices to build Data warehouse, data marts, views, and data sets/products
Develop ETL pipelines, using SQL, Stored procedures/functions to extract data from various sources and load into warehouse
Develop Symantec layer and data export frameworks to extract the data from the warehouse, transform, pre-aggregate, perform calculations and load into various data marts for Analytics use
Develop configurable export framework to extract data from Data warehouse and data marts to generate reports for internal and external customers in .csv, flat files and
Design and implement data validation and quality checks to ensure the accuracy and completeness of the data in the data warehouse
Perform performance of queries and data processing, identify and resolve any issues
Work and communicate in a cross-functional geographically dispersed team environment comprised of software engineers and product managers; and
Ensure compliance to company procedures when making changes and implementing code.
Maintain compliance with Inovalon's policies, procedures and mission statement;
Adhere to all confidentiality and HIPAA requirements as outlined within Inovalon's Operating Policies and Procedures in all ways and at all times with respect to any aspect of the data handled or services rendered in the undertaking of the position; and
Fulfill those responsibilities and/or duties that may be reasonably provided by Inovalon for the purpose of achieving operational and financial success of the Employer.
Job Requirements:
Minimum two (2) years related experience required; healthcare industry experience preferred.
Strong understanding to develop SQL queries for data analysis.
Experience working on Azure Cloud is preferred
3+ experience in MS SQL, T-SQL, ETL Jobs
3+ experience in Microsoft tools like SSMS, SSIS, SQL Server
Strong understanding of database concepts and schema (like star, snowflake schema)
Ability to learn quickly and independently
Ability to effectively communicate with internal and external customers
Experience with test driven development methodologies.
Education:
Bachelor's degree in Computer Science, Software Engineering, or Information Technology.
Physical Demands and Work Environment:
Sedentary work (i.e., sitting for long periods of time);
Exerting up to 10 pounds of force occasionally and/or negligible amount of force
Frequently or constantly to lift, carry push, pull or otherwise move objects and repetitive motions
Subject to inside environmental conditions; and
Studies have shown that women and people of color are less likely to apply for jobs unless they believe they meet every one of the qualifications listed in a job description. If you don't meet every qualification listed but are excited about our mission and the work described, we encourage you to apply regardless. Inovalon is most interested in finding the best candidate for the job and you may be just the right person for this or other roles.
By embracing diversity, equity and inclusion we enhance our work environment and drive business success. Inovalon strives to reflect the diversity of the communities where we operate and of our clients and everyone whom we serve. We endeavor to create a culture of inclusion in which our associates feel empowered to bring their full, authentic selves to work and pursue their professional goals in an equitable setting. We understand that by fostering this type of culture, and welcoming different perspectives, we generate innovation and growth.
Inovalon is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirement.
The Company maintains a drug free work environment for all of its associates, which includes employees, contractors and vendors. It is unlawful for associates to manufacture, sell, distribute, dispense, possess or use any controlled substance or marijuana in the workplace and doing so will result in disciplinary action, up to and including termination of employment or the contracted relationship.",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Software Development,1998,$500 million to $1 billion (USD)
Kanini,#N/A,Remote,Data Engineer,"About Kanini
Kanini provides Agile Software Development, Cloud Computing, Data Science, and Location Intelligence services to public and private organizations. We have successfully served our clients in government, finance, transportation, utility, and software industries since 2003.
Why you should join
Working at Kanini is flexible and personal. We are a highly motivated, collaborative team experimenting with the latest technologies. We are committed to everyone having a healthy work/life balance, and we provide extensive mentorship and training resources to help you succeed.
Kanini is looking for a Big Data Engineer who has a deep experience in Data Engineering, AWS, Python, Data Lakes.
Required Skills
At least 10 years software development experience
At least 5 years leading at least one Scrum team of data engineers building data-intensive products with a modern tech stack.
Significant experience with big data ETL pipeline development with Spark, Hive, and related technologies
Significant experience with a general-purpose programming language such as Python, Scala, or Java
Experience with Spark framework and related tools (PySpark, Scala, SparkR, Spark SQL, Spark UI)
Experience with Hadoop ecosystem using HDFS, ADLS Gen2, or AWS S3
Experience with data visualization development using Python, Tableau, or PowerBI
Experience with Azure, AWS or GCP
Solid understanding of performance tuning concepts for relational and distributed database systems
Familiarity with distributed programming, big data concepts, and cloud computing
Education Qualifications
Bachelor’s degree in computer science/Engineering or Technology related field or possess equivalent work experience.
Preferred Qualifications
Cloud certifications from Azure, AWS or GCP
Big data, data engineering or data science certifications from recognized vendors such as Databricks & Cloudera
Kanini Software Solutions, Inc. does not discriminate in employment matters based on race, gender, religion, age, national origin, citizenship, veteran status, family status, disability status, or any other protected class. We support workplace diversity. If you have a disability, please let us know if there is anything we can do to improve the interview process for you; we’re happy to accommodate.
Kanini Software Solutions, Inc., 25 Century Blvd., Ste. 602, Nashville, TN 37214
Job Types: Full-time, Contract
Pay: $60.00 - $70.00 per hour
Work Location: Remote",$65.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Core4ce
4.3",4.3,"Herndon, VA",Data Engineer,"The Data Engineer will provide the engineering support to data science and software engineering team members.
Design and optimize Data Pipelines using Spark, Hudi, EMR cloud services, and Kubernetes containers
Make sure pedigree and provenance of the data is maintained such that the access to data is protected
Clean and preprocess data to enable analytic access
Architects complex, repeatable ETL processes
Provide Advanced Database Administration support in Oracle, MySQL, MariaDB, MongoDB, Elastic and others
Supports Experience with Targeting using Sponsor Tools, Reverse Engineering
Develop API connectors to enable ingest of new data catalog entries from databases and files
Ensure that data mappings will provide the best performance for expected user experience
Augmenting technical support in the area of data science, data engineering, and systems engineering, and reviewing and providing technical assessments.
Provides current system architecture documentation, engineering/web development programming support for program/project requirements defined tasks, data science/data engineering related technical assessments, effective communication with users and managers, and server administration, to include hardware and software support to existing servers.
Support ad-hoc data analysis requirements defined by the client's Leadership.
Reporting solutions will encompass multiple technology platforms used by the client and will drive business process re-design and raining/change management initiatives.
Will work on Data cleaning and transformation efforts in delivery of CDRLs.
Supports Deliverables and Reports

Requirements
5+ years experience working with Data Sets
Experienced in extracting and aggregating structured and unstructured data.
Experienced in data programming languages and tools such as Python and R.
Experience with SQL or similar database language.
Experience designing and implementing data models to enable, sustain and enhance the value of information they contain
Knowledge of business intelligence reporting tools and data visualization software including Tableau.
Strong analytical and critical thinking skills.
Ability to work collaboratively and effectively in a team environment.
TS/SCI with Full Scope Poly Required

All qualified applicants will receive consideration for employment without regard to race, color, sex, sexual orientation, gender identity, religion, national origin, disability, veteran status, age, marital status, pregnancy, genetic information, or other legally protected status

Required Skills

Required Experience","$103,496 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2018,$100 to $500 million (USD)
"FACEBOOK APP
5.0",5.0,Remote,Azure Data Engineer,"Responsibilities:
Create ER diagrams and write relational database queries
Create database objects and maintain referential integrity
Configure, deploy and maintain database
Participate in development and maintenance of Data warehouses
Design, develop and deploy SSIS packages
Creating and deploying reports
Provide technical design, coding assistance to the team to accomplish the project deliverables as planned/scoped.
Ability to talk to client and get the Business Requirements
Skills:
Azure Data Factory
Azure Devops
Azure Storage/ Data Lake
Extraction, Transformation and Loading
Analytics development
Report Development
Relational database and SQL language
Other Requirements:
· Should be well versed with Data Structures & algorithms
· Understanding of software development lifecycle
· Excellent analytical and problem-solving skills.
· Ability to work independently as a self-starter, and within a team environment.
· Good Communication skills- Written and Verbal
Job Type: Full-time
Salary: $84,454.31 - $190,806.62 per year
Benefits:
Flexible schedule
Health insurance
Compensation package:
1099 contract
Yearly pay
Experience level:
10 years
9 years
Schedule:
Day shift
Experience:
Azure Data engineer: 9 years (Preferred)
SQL: 9 years (Preferred)
Data warehouse: 10 years (Preferred)
Work Location: Remote","$137,630 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Zenotis Technologies inc,#N/A,"Addison, TX",Big Data Engineer,"Role: Big Data Engineer
Location – Addison, TX/ Charlotte, NC / Chandler, AZ (Hybrid – 3 days onsite 2 days remote)
Position Summary:
Looking for a big data engineer who loves solving complex problems across a full spectrum of technologies. The ideal candidate is excited by experimentation and looking for a new challenge that stretches their talents. The big data engineer will help ensure that our technological infrastructure operates seamlessly in support of business objectives.
The engineer must be able to:
· Translate complex technical and functional requirements into detailed designs
· Guide the development and testing teams in developing and implement pipelines that extract, transform, and load data into an information product that helps the organization reach its strategic goals
· Focus on ingesting, storing, processing, and analyzing large datasets
· Deep understanding on how to create scalable, high-performance web services for tracking data
· Investigate alternatives for data storing and processing to ensure the implementation of the most streamlined solutions
Primary Skill
Hadoop
Secondary Skill
Kafka
Tertiary Skill
Oracle Exadata
Required Skills
· Translate complex technical and functional requirements into detailed designs
· Design data pipelines using design tools and provide solution intent in provided templates
· Work closely with data science team to implement data analytics pipelines
· Analyze vast number of data stores to uncover insights
· Define data governance policies and support data-versioning processes
· Maintain security and data privacy, working closely with data protection guidelines.
· Desired Skills
· Experience with Python, Spark, and Hive
· Understanding of data-warehousing and data-modeling techniques
· Knowledge of industry-wide visualization and analytics tools (ex: Tableau, R)
· Strong data engineering skills with Hadoop platform
· Experience with streaming frameworks such as Kafka
· Knowledge of Core Java, Linux, SQL, and any scripting language
· Good interpersonal skills and positive attitude
Job Type: Full-time
Salary: Up to $70.00 per hour
Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
8 hour shift
Work Location: On the road",$70.00 /hr (est.),201 to 500 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Ripple Effect Consulting
5.0",5.0,"Denver, CO",Data Engineer - 004/005,"Data Engineer
Hybrid-remote based in Denver, CO | $80-$160k
Job Overview:
We are looking for a savvy Data Engineer to join our growing team of analytics experts. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
Responsibilities for Data Engineer
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Qualifications for Data Engineer
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:
1) Experience with big data tools
2) Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
3) Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
4) Experience with cloud services
5) Experience with stream-processing systems: Storm, Spark-Streaming, etc.
6) Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Job Type: Full-time
Pay: $80,000.00 - $160,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Denver, CO: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person","$120,000 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Wallero
4.4",4.4,"Seattle, WA",Data Engineer,"Hi,
Pretty much interested in your resume, we have an urgent opening for Data Engineer role with one of our clients based in Seattle WA.
Position: Data Engineer
Location: Seattle WA - Remote (100%)
Description:
Good Experience in reporting space including PowerBI, ADF, and possibly standing up a data warehouse/data mart.
The main scope centering is around reporting.
Experience with SQL, and agile framework including requirement gathering and story/feature writing skills.
Need someone strong who is comfortable leading and guiding teams in that capacity.
Migrating data from third party to Azure Cloud
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Seattle, WA 98101: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",$75.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2015,$1 to $5 million (USD)
"Maven Workforce
4.1",4.1,"McLean, VA",Data Engineer,"MUST HAVES: Hands-on experience with Informatica PowerCenter and/or Informatica Intelligent Cloud Services (IICS), Cloud Data Integration (CDI) , Cloud AWS,UNIX scripting, and any other ETL tools.
Job responsibilities: –
The candidates will be having the IICS module of Informatica more than any other platform modules.
Strong hands-on experience building ETL pipelines and Data Replication
3+ yrs of experience with IICS is required.
Must be hand Ons with AWS services like ECS,S3,EKS etc.
Cloud Data Integration is also required.
Data Engineering Background experience with Spark, Hive is also required.
Preferred:
SQL is preferred, but basic knowledge is required
Job Type: Contract
Salary: $60.00 per hour
Ability to commute/relocate:
Mclean, VA 22101: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica PowerCenter: 7 years (Required)
Informatica Intelligent Cloud Services (IICS): 7 years (Required)
Cloud Data Integration (CDI): 7 years (Required)
UNIX scripting: 7 years (Required)
ETL tools: 7 years (Required)
Work Location: One location
Speak with the employer
+91 7328456015",$60.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2008,Unknown / Non-Applicable
"Codinix Technologies
5.0",5.0,"Boston, MA",Cloud Data Engineer - AWS,"Only US Citizen
Strong in warehouse technologies (Hadoop, Spark, AWS, etc.)
Bachelor's degree in Computer Science, Information Systems, or another related field
5+ years of data engineering, and cloud engineering experience
Experience in Agile project methodologies and DevOps
Experience with AWS (certification preferred; including Athena, Redshift, Glue), Cloudera
Experience building a CI/CD pipeline to deploy applications on AWS (GitLab, Terraform)
Advanced experience programming and scripting on Unix/Linux (i.e. Python, Bash)
Experience with CTRL-M, Cron (other) scheduling of batch jobs
Experience migrating legacy systems to cloud-ready architectures
Job Type: Contract
Pay: Up to $75.00 per hour
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Boston, MA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",$75.00 /hr (est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Green Worldwide Shipping LLC
3.7",3.7,"Aurora, CO",GEM Data Engineer,"About Green
“Global Reach with a personal touch”, that’s the foundation on which four partners (and long-time friends) agreed to when they started Green Worldwide Shipping back in 2008. This ethos has allowed Green to Grow to a 200+ strong team with 15 offices across the US and growing.At green we are a family of innovative problem-solvers who communicate freely to present creative solutions in an environment of trust, respect, and integrity. We are dedicated to customer service, passionate about logistics, and committed to sustainable growth while inspiring others.
Why this role is special..
GEM (Green Engineered Management) is a technology solutions department that supports various commercial technology products, data management, and development for Green Worldwide Shipping.We are seeking a skilled and experienced Data Engineer to join our dynamic team. As a Data Engineer, you will be responsible for designing, developing, and maintaining our data infrastructure and various technology systems to support efficient data processing and analysis. Your expertise in SQL, CargoWise, database management, Altova Flowforce Server, Altova Mapforce ETL software, Python and C#, along with experience in EDI/API connection protocols integration, will be instrumental in achieving success in this role.
How you will be successful…
Design, develop, and maintain data pipelines and ETL processes to extract, transform, and load data from various sources into our databases using Altova Mapforce ETL software.
Collaborate with cross-functional teams to understand data requirements and provide optimal data solutions.
Develop and optimize SQL queries to retrieve and manipulate data from databases, ensuring efficient data retrieval and storage.
Implement and integrate EDI/API connection protocols to enable seamless data exchange between internal and external systems.
Monitor and maintain the performance, availability, and integrity of the databases and data infrastructure.
Support for reporting, document modification, workflow and database support for internal ERP.
Troubleshoot and resolve data-related issues, including data quality and data integration problems.
Implement data security measures and ensure compliance with data privacy regulations.
Stay updated with industry best practices and emerging technologies in data engineering and incorporate them into the data infrastructure.
Provide technical guidance and support to other team members on data-related projects.
Collaborate with data analysts and data scientists to understand data requirements and support their data exploration and analysis needs.
Create and maintain documentation related to data pipelines, database schemas, and data transformation processes.
What you will bring….
Bachelor's degree in Computer Science, Information Systems, or a related field.
Preferred experience in CargoWise (or related ERP) with reporting, document modification, workflow and database support
Proven experience working with SQL and databases, preferably MySQL.
Prefer strong knowledge and hands-on experience with Altova Flowforce Server and Altova Mapforce ETL software, or related applications.
Experience in integrating and working with EDI/API connection protocols.
Proficiency in Python and/or C# (optional but preferred).
Familiarity with data modeling concepts and techniques.
Experience with data warehousing concepts and technologies.
Excellent problem-solving and analytical skills.
Strong attention to detail and the ability to work in a fast-paced environment.
Effective communication skills to collaborate with cross-functional teams.
Ability to work independently and prioritize tasks effectively.
Preferred Skills:
*
Experience with cloud-based data platforms (e.g., AWS, Azure, Google Cloud Platform).
Knowledge of big data technologies (e.g., Hadoop, Spark) and distributed computing.
Familiarity with data visualization tools (e.g., Tableau, Power BI).
Experience with version control systems (e.g., Git).
Understanding of Agile development methodologies.
What we offer:
Employee High Deductible Health Plan (HDHP) currently paid for 100% by Green.
$1,800 contributed to employee HSA Account, if on HDHP.
Employee Dental provided by Green.
Option to purchase disability insurance.
Paid life insurance and option to buy-up.
401(K) and company match.
Parental Leave Program.
Monthly recognition points to give/receive, which can be converted to gift cards.
Tuition Reimbursement Program.
Weekly fruit baskets in office locations
A tree planted for every new employee to Trees.org
Hybrid work schedule.
Wellbeing Program.
Training and Development.
Access to Mentorship Program.
Green Worldwide Shipping, LLC (Green) is committed to the full inclusion of all qualified individuals. As part of this commitment, Green will ensure that persons with disabilities are provided reasonable accommodations. If reasonable accommodation is needed to participate in the job application or interview process and/or to perform essential job functions, please contact Faye Hughes, Head of People and Culture at faye.hughes@greenworldwide.com.Green is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.
Requirements
Bachelor's degree in Computer Science, Information Systems, or a related field.
Preferred experience in CargoWise (or related ERP) with reporting, document modification, workflow and database support
Proven experience working with SQL and databases, preferably MySQL.
Strong knowledge and hands-on experience with Altova Flowforce Server and Altova Mapforce ETL software, or related applications.
Experience in integrating and working with EDI/API connection protocols.
Proficiency in Python and/or C# (optional but preferred).
Familiarity with data modeling concepts and techniques.
Experience with data warehousing concepts and technologies.
Excellent problem-solving and analytical skills.
Strong attention to detail and the ability to work in a fast-paced environment.
Effective communication skills to collaborate with cross-functional teams.
Ability to work independently and prioritize tasks effectively.
Preferred Skills:
Experience with cloud-based data platforms (e.g., AWS, Azure, Google Cloud Platform).
Knowledge of big data technologies (e.g., Hadoop, Spark) and distributed computing.
Familiarity with data visualization tools (e.g., Tableau, Power BI).
Experience with version control systems (e.g., Git).
Understanding of Agile development methodologies.
Summary
GEM (Green Engineered Management) is a technology solutions department that supports various commercial technology products, data management, and development for Green Worldwide Shipping. We are seeking a skilled and experienced Data Engineer to join our dynamic team. As a Data Engineer, you will be responsible for designing, developing, and maintaining our data infrastructure and various technology systems to support efficient data processing and analysis. Your expertise in SQL, CargoWise, database management, Altova Flowforce Server, Altova Mapforce ETL software, Python and C#, along with experience in EDI/API connection protocols integration, will be instrumental in achieving success in this role.
Job Type: Full-time
Pay: $60,000.00 - $75,000.00 per year
Benefits:
401(k) matching
Dental insurance
Employee discount
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Tuition reimbursement
Vision insurance
Experience level:
4 years
Schedule:
Monday to Friday
Ability to commute/relocate:
Aurora, CO 80014: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL/MySQL (Required)
ETL Software (Preferred)
Data warehouse (Preferred)
Python/C# (Preferred)
Work Location: Hybrid remote in Aurora, CO 80014","$67,500 /yr (est.)",51 to 200 Employees,Company - Private,Transportation & Logistics,Shipping & Trucking,2008,$25 to $100 million (USD)
"Synovize
5.0",5.0,Remote,Big Data Engineer,"Synovize is a cutting-edge technology company that specializes in providing advanced data analytics and intelligence solutions. Our mission is to empower businesses with the tools and insights they need to make data-driven decisions and drive innovation. We are committed to delivering exceptional results by combining our expertise in big data technologies with our passion for creating impactful solutions.
As a Big Data Engineer at Synovize, you will have the opportunity to work on exciting projects that involve designing and implementing scalable data solutions. You will collaborate with a talented team of professionals, including software developers, data scientists, and domain experts, to tackle complex data engineering challenges and deliver high-quality solutions to our clients.
Job Title:
Big Data Engineer
Company: Synovize
Number of Positions: 2
Location: Remote
Length: 12 Months
Tax Terms: 1099/C2C or W-2
Rate/Range: $70-$90/hr
Work Authorization: US Citizen
Security Clearance: None
Skills Needed:
Proficiency in Java and Kotlin programming languages.
Strong knowledge of AWS (Amazon Web Services) and its big data offerings.
Experience with BigData technologies, frameworks, and tools.
Familiarity with Kafka for streaming data processing.
Proficiency in Python programming language.
Responsibilities:
Design and develop scalable data pipelines to process, integrate, and analyze large volumes of structured and unstructured data.
Implement data ingestion, transformation, and storage mechanisms using big data technologies and frameworks.
Collaborate with data scientists and analysts to understand data requirements and optimize data models for analytics and machine learning.
Utilize AWS services and tools to build and maintain data infrastructure, ensuring data quality, integrity, and security.
Troubleshoot and resolve data-related issues, optimizing performance and efficiency.
Stay up-to-date with emerging trends and advancements in big data technologies, recommending and implementing improvements to existing systems and processes.
Requirements:
Bachelor's or master's degree in computer science, engineering, or a related field.
Proven experience as a Big Data Engineer or similar role, with a strong understanding of big data technologies and architectures.
Proficiency in Java and Kotlin programming languages.
Strong knowledge of AWS and its big data offerings.
Experience with BigData technologies, frameworks, and tools.
Familiarity with Kafka for streaming data processing.
Proficiency in Python programming language.
Strong problem-solving skills and ability to work in a fast-paced, collaborative environment.
Excellent communication and interpersonal skills.
Join Synovize and be part of a dynamic team that is at the forefront of data innovation. Together, we will drive meaningful change and unlock the full potential of data-driven decision-making.
Job Types: Full-time, Contract
Pay: $70.00 - $90.00 per hour
Benefits:
401(k)
Dental insurance
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Vision insurance
Schedule:
10 hour shift
4 hour shift
8 hour shift
Choose your own hours
Application Question(s):
Are you a US Citizen?
Years of experience in Java?
Years of experience in Kotlin?
Years of experience in big data?
Years of experience in AWS?
Security Clearance
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$80.00 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Breadboard,#N/A,"New York, NY",Data Engineer,"About Us
We’re on a mission to revolutionize the global electronics supply chain.
During the pandemic we saw how the inefficiencies in electronics manufacturing can have massive global impact. From over $200B lost in car manufacturing to delays on laptops, gaming systems and much more.
At Breadboard we are rethinking the electronics supply chain down to the finest detail. We’re designing the new operating system in which this entire system will run. We’ve already secured seed funding and are now looking to expand our engineering team to help us set the foundation for the future.
Why work with us?
As a part of our team, you will have the unique opportunity to design and implement the data platform for Breadboard, a critical component in our business.
As one of the first data engineers to join the Breadboard team you will be partnering with the CTO, our founding engineers, and will set the foundation for the design of the operating system for the electronics supply chain.
You will be involved in all aspects of the software development process, architectural decisions, recruiting, and building the organization in general.
We’re a tight-knit, intense team and you will have a ton of end to end ownership.
Work with seasoned founders who have had successful exits.
This is not only a unique opportunity for someone who wants to get their hands dirty, but also take that step back and collaborate with the executive team on setting a strategic future.
You’ll be responsible for
Data Management: Develop and automate robust data acquisition, processing, and normalization pipelines from various sources to ensure data quality and consistency.
Database and ETL Architecture: Design, implement, and optimize a database system for efficient data storage, retrieval, and analysis.
Collaboration on Machine Learning: The prospects of working with data scientists to leverage extracted data features, contributing to the development and refinement of a machine learning model for component matching.
API and UI Integration: Collaborate with the software development team to facilitate seamless integration between the database, machine learning models, and user interface.
Continuous Improvement and Compliance: Monitor system performance and user feedback for continuous improvements, while ensuring adherence to data privacy and security standards.

Requirements
Experience: A minimum of 3-5 years of experience in a data engineering role, preferably in the electronics, manufacturing industry or supply chain industry.
Technical Skills: Proficiency in SQL and experience with database design, data modeling, and performance optimization. Strong coding skills in a major programming language, such as Python or TypeScript. Experience with data processing tools and libraries (e.g., pandas, Spark). Experience with pipeline and workflow orchestration tools, such Airflow. Experience with Snowflake or similar cloud-based data warehousing solutions.
Data Acquisition and Processing: Experience with data acquisition methods, such as APIs and web scraping. Familiarity with data cleaning, normalization, and feature extraction techniques.
Machine Learning: Familiarity with machine learning concepts and algorithms, and experience collaborating with data scientists or machine learning engineers.
API Development: Experience with API development and integration, preferably in a microservices architecture.
Data Governance: Knowledge of data privacy and security regulations, and experience implementing data governance measures.
Bonus Points
Entrepreneurial mindset (we encourage all employees to be future founders and this can be a great stepping stone towards that goal).
Customer-centric and passionate about helping small businesses grow.
Previous experience at a high-growth, fast-paced startup.
Experience with our stack (and some others that may be included shortly):
React, Material UI, Typescript, CSS
NestJS, GraphQL, TypeORM, Knex.js, Objection.js, PostgreSQL, gRPC, Kafka
AWS, Docker, Kubernetes, Helm, Github Actions, CI/CD pipelines.
Compensation + Benefits
Competitive cash compensation
Stock ownership at a fast growing company
Health/dental/vision insurance—100% coverage option
The ability to push for other benefits you think are important
Our Engineering Culture
Openness to diverse opinions and backgrounds - Testing our assumptions
Supported career growth with a high emphasis on learning and mentorship
Lean development
End to end ownership and accountability. We’re all going to make mistakes. It’s about how you own up to them and learn from them.

Equal Opportunity Employer
Breadboard Software does not discriminate in employment on the basis of race, color, religion, sex (including pregnancy and gender identity), national origin, political affiliation, sexual orientation, marital status, disability, genetic information, age, membership in an employee organization, retaliation, parental status, military service, or other non-merit factor.",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Provista Software Corporation,#N/A,Remote,Data Engineer,"As a successful candidate, you’ll play a pivotal role in the data conversion and business intelligence solution efforts during project implementation for our state and local clients. We are looking for you to join our software implementation team as a Senior Data Engineer.
Qualification:
5+ years of experience working independently as ETL developer on data migration projects that covered diverse database platforms and complex data conversions.
Must have experience across leading Database Platforms especially Oracle, Azure SQL, Snowflake.
Expert level experience with Extraction Transform Load (ETL) plan, protocols, and tools.
Expert level experience with programming languages SQL, Phyton, Java, C#.
Good at communication with clients and internally.
Expert at problem solving and troubleshooting skills.
Bachelor's degree, preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience
Must be a U.S. Citizen or Green Card Holder.
Functional Responsibility:
Data Conversion -
In collaboration with Data Stewards, discover inventory of legacy data sources and all data the client expects to be migrated to destination Modified Off the Shelf (MOTS) solution.
Complete gap assessment between legacy data and MOTS data requirements, and options to address the gaps.
Develop data migration map to minimally include data dictionary, detailed data map of all elements of the current database, data transformation rules, relationship rules, validation rules, process of migrating images, and process for recovering missing or erroneous data.
Identify issues, risks, and/or barriers that may interfere with data migration and recommendations to resolve or mitigate.
Conduct testing and quality assurance to ensure the data is migrated accurately.
Migrate data from legacy to destination MOTS solution progressively using a sample set of data and the entirety of the data prior to deployment into relevant go-live environments (e.g., staging, production, etc.)
Prepare data migration reports that includes – record counts of source vs migrated vs modified; field level validation of data at row and column; discovered exceptions.
Business Intelligence -
Develop ETL pipelines in and out of Snowflake data warehouse using combination of Python, SnowSQL, and SQL.
In collaboration with Data Stewards, create Snowflake data mart as curated subset of data to support the analytics and business intelligence needs of subgroup of users.
Translate requirements for business intelligence and reporting to database design and reporting design.
Conduct discovery sessions with users to capture business intelligence reports.
Develop Tableau self-service ad hoc reports to support operational users.
Develop Tableau Dashboards to support executive, management, and operational users.
About us:
Provista Software Corporation is fast growing Software Solution provider for US State and Local Government Clients.
Job Type: Full-time
Pay: $67,107.98 - $134,285.66 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Life insurance
Paid time off
Parental leave
Professional development assistance
Vision insurance
Compensation package:
Yearly pay
Experience level:
5 years
Schedule:
8 hour shift
Application Question(s):
What is your most preferred work time zone ?
Experience:
ETL: 5 years (Required)
Data warehouse: 1 year (Preferred)
Tableau: 1 year (Preferred)
Talend: 1 year (Preferred)
Microsoft SQL Server: 5 years (Required)
Work Location: Remote","$100,697 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"ProIT Inc.
5.0",5.0,"Bellevue, WA",Azure Data Engineer,"Data Engineering experience primarily on Spark. Someone who has worked on Azure cloud with knowledge on Azure DEVOPS (CI/CD & Infrastructure as a code), ADF, ADW & Power BI. Apache Nifi will be good to have.Power BI: Understand business requirements to set functional specifications for reporting applications Build automated reports and dashboards with the help of Power BI reporting tool Be experienced in tools and systems on MS SQL Server BI Stack, including SSRS and TSQL, Power Query, MDX, PowerBI, and DAX Be able to quickly shape data into reporting and analytics solutions Have knowledge of database fundamentals such as multidimensional database design, relational database design, and more Create functional reporting Study, analyze and understand business requirements in context to business intelligence. Design and map data models to shift raw data into meaningful insights. Utilize Power BI to build interactive and visually appealing dashboards and reports. Spot key performance indicators with apt objectives Run DAX queries and functions in Power BI Should have an edge over making DAX queries in Power BI desktop. Developing visual reports, KPI scorecards, and dashboards using Power BI desktop. Connecting data sources, importing data, and transforming data for Business intelligence.
Spark ,Azure DEVOPS (CI/CD & Infrastructure as a code), ADF, ADW & Power
Job Type: Full-time
Pay: $100,154.64 - $104,132.47 per year
Ability to commute/relocate:
Bellevue, WA 98004: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Hybrid remote in Bellevue, WA 98004","$102,144 /yr (est.)",51 to 200 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
IT Customer Service - POST,#N/A,"Newark, NJ",Data Center Plant Engineer,"Job Title: ISP-OSP Data Center Plant Engineer
Location: Newark NJ, (M-F)
Travel Required: As Needed
Overview
UC-ISP/OSP engineer is responsible for cable management including structured cabling, carrier data and voice circuit cables/equipment, rack-and-stack, and oversight of power, backup power, network equipment, and environmental needs to cabinets. Leveraging Clients Data Center Infrastructure Management System, (DCIM) tool to maintain, report and monitor.
Conducting capacity assessments of existing infrastructure to ensure that it can support future growth to ensure compliance with federal regulations of the Data Center infrastructure Standards. Projects include design and build outs of the physical plant within the typical WAN/LAN space within data centers, telco rooms, MDFs and IDFs as needed.
Functions:
Perform initial engineering surveys based on project needs.
Document engineering surveys to determine feasibility and the estimated cost.
Obtain authorization to proceed and confirm funding.
Create detailed engineering drawings of build, floor plans, equipment elevation drawings, structured cabling diagrams, power, and environmental needs.
Designs must conform to state, local, national, and Client standards and codes, and the NJ historical and or environmental commissions approvals if needed.
Based on the engineering documents create a SOW (Scope of Work) and BOM (Bill of Materials)
Submit to Client manager to approve and proceed when funding is validated.
Create a construction package and submit to contractors or in house for RFQ.
Review quotes and choose both low bid and qualified contractor.
Work with all stakeholders to date and time work schedules.
Confirm workers are safety trained and conform to Client, FRA, and OSHA requirements.
Supervise and manage installations and conduct inspections.
Review work to ensure contractor met obligations under terms of contract.
Inform Client managers of status to approve invoices.
Coordinate with Client network engineering to connect, configure, and test equipment.
Review and confirm a successful installation were completed and working as designed prior to cutting over to production.
Assist with Break fix of IT infrastructure troubles or repairs.
Daily travel to location depending on scheduled or unscheduled work
Job Types: Full-time, Contract
Experience level:
5 years
Schedule:
8 hour shift
Work setting:
In-person
Experience:
Data Center: 5 years (Required)
WAN/LAN: 3 years (Required)
cabling: 3 years (Required)
Work Location: One location",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"WorkCog
4.3",4.3,"Atlanta, GA",Sr. AWS Data Engineer,"Position: AWS Data Engineer
Location: Atlanta ,GA
Duration: 12+ Months
Interview: Video
JD
Responsibilities
Bachelor’s degree in Computer Science, Software Engineering, or other related field, or equivalent work experience.
Extensive experience in data modeling, writing & performance tuning complex SQL queries & user-defined functions
Hands-on experience in design, implementation and orchestration of data pipelines and ETL batch Jobs.
Hands-on experience on all aspects of data handling: cleansing and standardization, normalization, merging, change data capture, data integration, reconciliation, and transformations
Strong experience in working with Big Data technologies such as Hive/Spark, Data lake and Data warehouse using GCP and/or AWS Services
Ability to write robust code in scripting languages like Python, Scala, PySpark or similar.
Good to Have
Associate Architect or equivalent cloud certification
Experience with processing large datasets using AWS and GCP UDFs (User defined functions)
Experience in ML probability & statistics algorithms, frameworks like Keras or PyTorch and tools like Jupyter notebooks.
Job Type: Contract
Pay: $45.83 - $52.42 per hour
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Atlanta, GA: Reliably commute or planning to relocate before starting work (Preferred)
Experience:
SQL (Preferred)
Informatica (Preferred)
Data warehouse (Preferred)
Work Location: In person",$49.13 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable
"DiamondPick
4.5",4.5,"Edison, NJ",Senior Data Engineer,"Hi ,
Greetings from Diamond pick inc.
We are currently looking for the below position for one of our clients... Please let me know your interest along with your updated resume ASAP..
Role:Data engineer
Location: Berkley Heights, NJ(Onsite)(locals only)
9+ years of experience is must
Description
Skills: strong Java,Azure,Spark & sql
Company Description:
Apexon is a digital-first technology services firm backed by Goldman Sachs Asset Management and Everstone Capital. We specialize in accelerating business transformation and delivering human-centric digital experiences. For over 17 years, Apexon has been meeting customers wherever they are in the digital lifecycle and helping them outperform their competition through speed and innovation.
RESPONSIBILITIES
Basic Qualifications for consideration:
5+ Overall industry experience
3+ years' experience with building large scale big data applications development
Bachelors in Computer Science or related field
Provide technical leadership in developing data solutions and building frameworks
Experience building Data Lake, data applications and micro-services using Azure, AWS and Hands-on experience in cloud Data stack (preference is Azure)
Hands-on experience with Springboot framework, MVC architecture, Hibernate, API creation, Restful services, SOAP using Java
Java experience with OOPS concepts, multithreading
Experience deploying code on containers
Hands on experience with leveraging CI/CD to rapidly build & test application code
Conduct code reviews and strive for improvement in software engineering quality
Review data architecture and develop detailed implementation design
Hands-on experience in production rollout and infrastructure configuration
Demonstrable experience of successfully delivering big data projects using Kafka, Spark
Experience working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search
Experience working with PCI Data and working with data scientists is a plus
In depth knowledge of design principles and patterns
Able to tune big data solutions to improve performance
QualificationsBachelor's Degree in Computer Science or Computer Engineering is required
Job Type: Contract
Salary: $43.96 - $70.65 per hour
Ability to commute/relocate:
Edison, NJ 08817: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL (Required)
Java (Required)
Azure (Required)
Work Location: One location",$57.30 /hr (est.),1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,#N/A,Unknown / Non-Applicable
"Rite Pros
3.4",3.4,"Portland, ME",Big Data Engineer,"Big Data Engineer with Bachelor’s degree in Computer Science, Computer Information Systems, Information Technology, or a combination of education and experience equating to the U.S. equivalent of a Bachelor’s degree in one of the aforementioned subjects.
Job Duties:
Define and manage the data architecture framework, standards, and principles, including modeling, metadata, security, reference, and master data.
Define reference architecture, and models to provide a common vocabulary, reusable components, and industry best practices.
Create solution frameworks integrating large or complex data sets.
Lead all data modeling efforts within DataBricks, including the design of data structures and the identification of business transformation logic.
Analyzing and translating business requirements into conceptual and fully detailed logical data models.
Creating logical data models based on existing applications and databases.
Working with business architects, and data stewards to capture business requirements in a Logical Data Model.
Transform the logical representation of the model into a physical representation and work with data engineering team to instantiate and manage the data.
Contribute to assessment of appropriate data platform(s) for solutioning efforts.
Collaborate with the BI and Analytics teams on creating the optimized, reusable semantic model, complete with metadata and lineage information.
Define and govern data modeling and design standards, tools, best practices, and related development for enterprise data models
Maintain the metadata - the ""data about the data"" describing the data model, its structure, and semantics
Recommend data and design patterns for common business functionality
Assist in multi-phase roadmaps within and across functional teams
Provide insight and technology recommendations that result in enterprise solutions and architectural risk mitigation.
Contribute to innovation strategies by exploring, investigating, recommending, benchmarking and implementing new data centric technologies for the platform.
Identify, design, and implement internal process improvements such as automation of manual processes, optimization of data delivery and re-design of existing infrastructure for greater scalability.
Build the infrastructure and processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and/or other data technologies.
Create and maintain data-related documentation (e.g. data dictionaries, data maps, data models etc.)
Work with data engineering and analytics experts to strive for greater functionality in our data systems and incorporation of industry best practices
Work experience / Technologies required for the position :
Experience building and optimizing Big Data, Data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, meta data, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected data sets.
Working knowledge of message queuing, stream processing, and highly scalable Big Data, data stores.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience in a Data Engineer or similar roles, who has attained a Bachelor’s degree in Bachelor’s degree in Engineering, Computer Science, Information Technology, related field or equivalent work experience.
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Work location is Portland, ME with required travel to client locations throughout USA.

Rite Pros is an equal opportunity employer (EOE).

Please Mail Resumes to:
Rite Pros, Inc.
565 Congress St, Suite # 305
Portland, ME 04101.
Email: resumes@ritepros.com","$97,894 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Less than $1 million (USD)
BIGCLFY,#N/A,"Dallas, TX","W2/ 1099, No C2C - SSIS ETL Data Engineer (Minneapolis, MN/ Dallas-Irving)","No C2C. It's W2/1099 Requirement
Only taking USC, GC or H4 EAD, H1 Transfer, L2 Visa or TN Visa - they will not take OPT EAD or CPT
Need to have confirmation in writing that you will be onsite by Day 1
ALL ROLES ARE HYBRID IN THE OFFICE (2-3 DAYS/WEEK) UNLESS REMOTE IS NOTED
Please submit qualified candidates and include FULL LEGAL NAME as it appears on the passport
If your candidate is relocating, need to have confirmation in writing that they will be onsite by Day 1
Senior SSIS/ETL Data Engineer
12 months – 3 days in office
Minneapolis, MN or Dallas-Irving, TX
5+ years of experience with SQL Server 2016 (or newer) and SSIS
7+ years of data engineering
Must be capable of leading teams
2+ years of experience working with large scale SQL implementations
Experience designing, developing, and maintaining complex stored procedures, functions, and views using T-SQL
Experience designing, developing, and maintaining ETL processes using SSIS
Should understand how to unit test their product
Hands on experience with Visual Studio to document
Experience in financial or regulatory environments highly preferred
Ideal candidates will have experience creating conceptual, logical, and physical database designs
Preference given to candidates with extensive knowledge and experience with performance monitoring, database and SQL tuning
Bachelor’s or Master's in computer science or related field or equivalent work experience
Job Type: Contract
Pay: From $73.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Chandler, AZ 85225: Reliably commute or planning to relocate before starting work (Preferred)
Application Question(s):
Expected Payrate on W2/1099 Tax Term? Please mention. Thanks.
Experience:
Risk Analytics: 5 years (Preferred)
Work Location: In person",$73.00 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Spartan Technologies
3.4",3.4,"Kansas City, MO",Data Engineer I,"Spartan Technologies, Inc. - Kansas City, MO
Applicants Need to Know
100% remote in East Coast or Central US only
No Sponsorship
Contract to Hire
Hourly Rate: $45 on W2
Data Engineer I
The Data Engineer is responsible for processing structured and unstructured data, validating data quality, and developing and supporting data products for analytics. Brings together data from different sources into a common dataset with common KPIs and dimensions for stakeholders. Understands the analytics challenges that the marketing organization faces in their day-to-day work and partners with them to design viable data solutions.
Your Job
Gathers and processes raw, structured, semi-structured, and unstructured data using batch and real-time data processing frameworks.
Understand the existing source system data models and perform end to end data validation to maintain accuracy.
Implements and optimizes data solutions in enterprise data warehouses and big data repositories.
Ensures data quality and implements tools and frameworks for automating the identification of data quality issues.
Develop re-useable processes that can be leveraged and standardized for multiple instances
Works with internal and external data providers on data validation providing feedback and making customized changes to data feeds and data mappings.
Recommends improvements to processes, technology, and interfaces that improve the effectiveness of the team and reduce technical debt.
Ensure performance by monitoring existing workflows to ensure they are working correctly and that they reconcile with the source.
Design and develop ETL workflows and datasets to be used in extracts by our BI reporting tool Tableau.
Will write complex SQL queries with multiple joins to automate/manipulate these reporting extracts
Works in partnership with marketing stakeholders concerning the accuracy of data and efficiency of processes
Qualifications
Working experience with data modelling, data access, schemas, and data storage techniques within Snowflake
Working experience with batch and real-time data processing frameworks
Working experience working with relational databases such as SQL
Working experience with business intelligence tools and platforms
Working experience with data quality tools
Experience with ETL processes and tools
Desirable:
Working experience with Snowflake
Working experience with Google Analytics BigQuery
Working Experience with Tableau Server
Working experience with SQL Server Integration Services",$45.00 /hr (est.),1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$5 to $25 million (USD)
Schober Consulting LLC,#N/A,"Minneapolis, MN",Data Engineer (Junior),"We are seeking a talented and experienced Data Engineer to join our team. The ideal candidate will have 2-3 years of professional experience in data engineering, with a bonus if they have worked in the real estate industry. Additionally, an interest in machine learning is highly desirable.
Responsibilities:
Design, develop, and maintain scalable and efficient data pipelines and ETL processes
Build and optimize data models and data storage solutions for large-scale datasets
Collaborate with cross-functional teams, including data scientists and analysts, to gather requirements and design data-driven solutions
Develop and maintain data infrastructure to support advanced analytics and machine learning initiatives
Ensure data quality and integrity by implementing data validation and cleansing processes
Continuously monitor and optimize data pipelines and systems for performance and reliability
Stay updated with the latest industry trends and technologies in data engineering and machine learning
Qualifications:
Bachelor's or Master's degree in Computer Science, Data Engineering, or a related field
2-3 years of professional experience as a Data Engineer, preferably with exposure to the real estate industry
Strong understanding of data engineering principles, ETL processes, and data warehousing concepts
Proficiency in SQL and experience with relational databases
Solid programming skills in Python, Java, or other relevant languages
Familiarity with cloud computing platforms, preferably AWS or Azure
Experience with data modeling and designing efficient database schemas
Knowledge of machine learning concepts and a strong interest in applying machine learning to data engineering tasks
Excellent problem-solving and analytical abilities
Strong communication and collaboration skills
If you are a self-motivated individual with a passion for data engineering and an interest in the real estate industry and machine learning, we encourage you to apply. We offer a competitive salary, comprehensive benefits package, and a collaborative work environment that fosters professional growth and development.
Job Type: Full-time
Pay: $70,000.00 - $100,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Compensation package:
Employee stock ownership plan
Experience level:
2 years
Schedule:
Monday to Friday
Ability to commute/relocate:
Minneapolis, MN 55402: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
SQL (Preferred)
Data warehouse (Preferred)
Python (Required)
Work Location: Hybrid remote in Minneapolis, MN 55402","$85,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"The Bouqs Company
3.1",3.1,"Marina del Rey, CA",Data Analytics Engineer,"The role contributes to The Bouq’s mission of revolutionizing the way we commemorate life’s moments by connecting people to beautifully designed floral experiences and the responsible partners who create them by being a key member of the data team. As an Analytics Engineer, you will work closely with the Product, Engineering, and Data teams to build and maintain the data infrastructure needed to support our data and business needs. You will also be responsible for developing, optimizing, and maintaining the best-in-class data pipelines, data models, and ETL processes to ensure that data is accurate, reliable, and available to stakeholders in a timely manner. The Analytics Engineer will also serve as the liaison between Engineering and Analytics and will serve as an active member in both teams.
Responsibilities:
Lead the transfer of data modeling from legacy systems to DBT
Contribute to building the data modeling layer, which exposes clean, transformed data to the whole company for analytics
Build datasets in DBT (cloud) for data analysts to improve speed and accuracy for the team
Improve current processes, whether that includes modularizing and standardizing a piece of commonly used code
Design and develop new data pipelines and streaming processes that are highly available, scalable, and reliable
Develop review processes for new data models and take charge on implementing SQL standards for the team
Actively strive towards writing performant SQL rather than just SQL that works, while also ensuring the same SQL is easy to understand when new eyes look at it
Optimize data processing and flow within our Snowflake Data Warehouse
Document new datasets and pipelines and the reasoning/story behind their structure
Work closely with engineering to keep track of schema changes in the production database and adjust pipelines, as needed
Support existing data pipelines and systems in production
Apply software engineering best practices to analytics code such as version control and testing
Develop and communicate strong opinions about best practices in analytics
Help explore and evaluate new technologies
Qualifications:
4+ years of experience working within a data team, preferably as an Analyst/Data Engineer
Bachelor's degree in a quantitative field such as statistics, mathematics, economics, or computer science preferred
Strong SQL fluency in both DDL/DML and analytics (Snowflake experience is a plus)
Experience working with JSON, DBT or other data transformation tools
Experience working with an ETL tool such as Fivetran or Stitch
Knowledge of data structures and how to write performant SQL
Experience with ensuring data quality through testing, deltas, lineage, etc
Strong communication and critical thinking skills to deliver solutions that not only solve problems but also serve as tools we didn’t know we needed
Ability to transform raw data into intuitive datasets that serve as building blocks for analytics
Comfortable leading the growth of a data warehouse and maintaining it
Capable of working through uninformative assumptions and built-biases in datasets and are not stalled when data is not perfect/sparse
Compensation & Perks:
Competitive Base Salary Range of $120,000.00 - $180,000.00 USD + Equity Package
Health, Dental & Vision with 100% employee coverage
401k Matching
Three Weeks Paid Vacation
Discounts on The World’s Best Flowers (obviously!)
Work on cutting edge new technologies
About The Bouqs:
Our mission here at The Bouqs is to revolutionize the way we commemorate life’s moments by connecting people to beautifully designed flowers and the responsible partners who create them. Grounded in transparency, responsibility, and simplicity, we create genuine moments of emotional connection for our customers, build meaningful relationships with like-minded farmers and florists while empowering them to thrive, and eliminate unnecessary waste along the way.

Founded in 2012, The Bouqs is a venture-backed online floral retailer that delivers flowers fresh from eco-friendly, sustainable farms to doorsteps nationwide. Headquartered in Marina Del Rey, CA, The Bouqs connects farms and a curated network of artisan florists directly to consumers and disrupts the traditional supply chain by eliminating overhead costs like warehouses, importers, distributors, auctioneers and more. In turn, this model enables a superior product and redefines the experience and economics for both consumers and producers alike.

For more information, visit www.bouqs.com and follow the #BouqLove on Facebook, Instagram and Twitter.
The Bouqs is an Equal Opportunity Employer!","$150,000 /yr (est.)",51 to 200 Employees,Company - Private,Retail & Wholesale,Other Retail Stores,2012,$25 to $100 million (USD)
"AbleTo
3.7",3.7,"New York, NY",Data Engineer,"About AbleTo
Join our mission-driven organization, where your work matters and a diversity of ideas and backgrounds are welcomed. AbleTo is the leading provider of high quality, technology-enabled behavioral health care. We believe that everyone deserves access to high-quality care and offer a suite of technology-enabled services to empower people to lead better lives through better mental health. A proprietary platform connects individuals with AbleTo highly trained licensed providers who deliver weekly sessions by phone or video supported by an integrated digital experience. Members also have access to mental health coaches, and digital support programs. Our outcomes-focused approach is proven to improve both behavioral and physical health and lower medical costs.

Overview
Data is integral to nearly everything that we do at AbleTo, and as a result the data team plays a key role at our company. As a Data Engineer at AbleTo, you will grow and develop your skills as a data engineer to build a world class data platform to support our mental health care mission.

What You Will Do
Build and maintain batch and real-time ETL pipelines in a Google Cloud Platform architecture (BigQuery, Dataproc, Firestore, Informatica, Python, etc.)
Identify code quality issues and implement tests to improve future processes.
Document orchestration work-flows and create run books
Implement data integrity tests to ensure we are ingesting accurate data.
Implement high-quality test-driven code.
Participate in daily team stand ups and other Agile ceremonies

Key Initiatives over the next 12 months
Data Mart client migration
Data Mart enhancements
Automation of manual day to day operational tasks
Decommission legacy pipelines

Who You Are:
1+ years of experience coding in Python.
Experience working with SQL (structured query language).
Familiarity with structuring and writing ETLs.
Experience working with Orchestration tools (Airflow) and Cloud Databases (BigQuery) is a plus.
Experience working in an Agile Development environment is a plus.
BS in Information Systems, Computer Science or related field is preferred.
You are excited to work with data!
Curiosity to dive deeply into issues and feel empowered to make a meaningful impact at a mission-driven company.
Committed to agile development and value delivery and solid engineering principles, as well as continuously improving our craft.
You are a collaborator. You build and maintain strong, productive working relationships with your customers and stakeholders.
You are empathetic and seek to understand each individual's diversity of background and experience contributes to making up a great team.
You drive to improve yourself and others.
You are excited about learning new languages, tools, techniques and technologies.

Why You Should Join Our Team:
We're proud to be a Great Place to Work-Certified™ company. We want you to show up and feel your best at work, and that means respecting your time outside of work. Our inclusive, flexible workspace prioritizes a work/life balance. We offer competitive salaries, comprehensive health benefits (for full-time employees), and professional perks such as 401K matching, fully funded HRA, and generous time off, including mental health days because your well-being is important to us.
At AbleTo, we're empowering people to get better and stay better. Want to join us? Take the next step in your career by applying for this role today.
The salary range is $40,000 to $90,000. Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, AbleTo offers benefits such as a comprehensive benefits package, incentive and recognition programs, and 401k contribution (all benefits are subject to eligibility requirements).

#LI-Remote
Follow AbleTo on LinkedIn, Twitter, and Instagram!
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.
AbleTo is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or veteran status. AbleTo is an E-Verify company.","$65,000 /yr (est.)",501 to 1000 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2008,$25 to $100 million (USD)
NextGenITSupport,#N/A,"Raleigh, NC",Network data engineer - onsite,"Job Title: Network Data Engineer.
Must have Skill: HP ARUBA experience.
Rate: $45/hour
Location: On-Site in Raleigh, North Carolina
Job Description:
Experience with HP ARUBA is a must.
Develop and install network infrastructure, configurations and equipment such as routers and switches
Monitor networks and troubleshoot issues or outages.
LAN, WAN, SWITCH & ROUTING experience is needed.
Consult with clients to suggest network solutions
Manage junior employees and provide training resources for team members
Test and install new computer systems, hardware, software and applications
Develop engineering design packages to integrate new processes into existing ones
Collaborate with clients, other tech support services and network providers to ensure the quality of networks
Team player and support on-call responsibilities as HP ARUBA NMS team member
Coordinate upgrades/updates and maintenance of HP ARUBA NMS tool solutions
Collaborate across multiple technical teams and lines of business with focus on implementing enterprise NMS solutions.
Strong problem-solving and troubleshooting skills
Excellent communication and interpersonal abilities
Ability to work independently and as part of a team
Ability to multi-task, prioritize, and manage time efficiently
Highly organized and detail-oriented
Certifications:-
CCNA minimum CCNP preferred.
· 7+ years experience with network planning, design, and implementing LAN, WAN, network security, and wireless network infrastructures.
Job Type: Contract
Pay: $45.00 - $48.00 per hour
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",$46.50 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Southern Glazer’s Wine and Spirits
3.6",3.6,"Dallas, TX",Data Engineer,"What You Need To Know
Open the door to a groundbreaking tech career with an industry leader. Southern Glazer’s Wine & Spirits is North America’s preeminent wine and spirits distributor, as well as a family-owned, privately held company with a 50+ year legacy of success. To create a new era in alcohol beverage sales and service, we’re heavily invested in the most transformative new technologies – and the most brilliant tech professionals. Southern Glazer’s was named by Newsweek as a Most Loved Workplace and is included on the Forbes lists for Largest Private Companies and Best Employers for Diversity.
As a full-time employee, you can choose from a full menu of our Top Shelf Benefits, including comprehensive medical and prescription drug coverage, dental and vision plans, tax-saving Flexible Spending Accounts, disability coverage, life insurance plans, and a 401(k) plan. We also offer tuition reimbursement, a wellness program, parental leave, vacation accrual, paid sick leave, and more.
We offer continuous learning and career growth in a fast-paced environment where you are respected, your voice is heard, and technology is part of our strategy for success. If you’re looking to fill your glass with opportunity, come join our FAMILY.
Overview
The Data Engineer's role is to design, develop, maintain and enhance interfaces and connectivity to the Data Warehouse ecosystem by coding with a technical language to meet business requirements and business objectives. This can include taking technical specifications and developing an application or integration of data between applications, testing, as well as, completing the appropriate technical documentation. The Data Engineer will use best practices in software development and adhere to SGWS development standards, as well as, focus on quality and innovation. The Data Engineer may also be responsible for delivering support to end users in the organization for specific code, including troubleshooting code.
Specialized Skills and Technologies
Strong PL/SQL skills
Experience in ETL Tools (Preferrable Informatica)
Data Warehouse techniques will be a plus
Experience in cloud platforms like Azure or AWS will be a plus
Knowledge of UNIX/Linux, shell scripting, Python will be a plus
Experience developing Application Programming Interfaces (API's) will be a plus
Experience in Hadoop will be a plus
Primary Responsibilities
Design, develop, implement, and support software applications
Drive technical validity of solution.
Develop user documentation as well as in-code documentation to explain designs and participate/support user training
Structure requirements to facilitate automation of acceptance tests
In conjunction with Data Management Group, develop routines and procedures that provide data quality checks and balances on data delivery/ingestion
Collaborate across the BI / Analytics, Data Management Group, Enterprise Insights and Analytics teams to establish standards, reusable data models and best practices for delivery/ingestion of data from/to Data Warehouse - This includes Publish/Subscription and API options
Obtain any certifications needed to effectively support applications in scope
Support the development of business and technical process documentation and training materials
Structure requirements to facilitate automation of acceptance tests
Provide support for software applications under area of responsibility
Drive Behavior-Driven-Design (BDD) process
Perform other job-related duties as assigned
Minimum Qualifications
Bachelor’s Degree or a combination of work experience and education
Knowledge in application and software development
Knowledge of software design and programming principles
Proficient oral and written communication skills, ability to influence outcomes, and strong attention to detail
Strong analytical, mathematic, and problem-solving skills
Strong team player with ability to demonstrate Agile delivery values working both within a team and working independently
Strategic thinker – can develop a plan to meet a long-term objective
Agile Delivery Values
Openness – Team and stakeholders agree to be open about all work and challenges
Commitment – Personally commit to achieving the goals of the team
Respect – Respect your team members to be capable and independent
Courage – You have courage to do the right thing and work on tough problems
Focus – Everyone focus on the work in the sprint and the goal of the scrum team. Rise and fall as a team
Physical Demands
Physical demands include a considerable amount of time sitting and typing/keyboarding, using a computer (e.g., keyboard, mouse, and monitor), or mobile device
Physical demands with activity or condition may occasionally include walking, bending, reaching, standing, squatting, and stooping
May require occasional lifting/lowering, pushing, carrying, or pulling up to 20lbs
EEO Statement
Southern Glazer's Wine and Spirits, an Affirmative Action/EEO employer, prohibits discrimination and harassment of any type and provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. Southern Glazer's Wine and Spirits provides competitive compensation based on estimated performance level consistent with the past relevant experience, knowledge, skills, abilities and education of employees. Unless otherwise expressly stated, any pay ranges posted here are estimates from outside of Southern Glazer's Wine and Spirits and do not reflect Southern Glazer's pay bands or ranges.","$97,837 /yr (est.)",10000+ Employees,Company - Private,Retail & Wholesale,Wholesale,1968,$10+ billion (USD)
"Global Enterprise Services, LLC
4.0",4.0,"Atlanta, GA",Data Harmonization Engineer,"Data Harmonization Engineer

Collects data models for disparate datasets within a domain. Collaborates with CDC & STLT stakeholders to develop a conceptual model that defines the atomic concepts for that domain.
Constructs semantic mappings from the data elements in the source datasets to atomic or compound concepts in the conceptual model.
Develops sample transformation code to translate among representations.
Configures and manages vocabulary services to allow data modernization users to improve understanding of the relationships among data representations.

Data Harmonization Engineer - Junior [YoE, Edu, Certs: 0-3 yrs & BA/BS] [Salary Range: $81k - $91k]
Data Harmonization Engineer - Journeyman [YoE, Edu, Certs: 4-9 yrs & BA/BS] [Salary Range: $95k - $105k]
Data Harmonization Engineer - Senior [YoE, Edu, Certs: 10+ yrs & MA/MS] [Salary Range: $108k - $108k]","$100,000 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Premier Consulting Group,#N/A,"Boca Raton, FL",Senior Data Engineer,"* The right person could be located in Florida or Massachusetts as company has office locations in Boca Roton and Boston.
* Must be a US Citizen
Overview Summary of the Work:
ETL Azure Data Factory work: A Data Engineer who is to be able to import data from a variety of different sources (SFTP, API, Fileshare) and file types (CSV, Excel, JSON). Company also captures files via email now, but they have a pattern set up to be able to do that, so the person would just repeat how they’ve done that. Check existing Azure Data Factory pipelines for failures and troubleshoot. Familiar with dynamic expression and syntax in Azure Data Factory pipelines.
Backlog of stored procedures: Be skilled in stored procedure development and SQL skills.
Data quality and data cleansing: Be able to evaluate data (incoming and existing). Look for issues and be able to resolve them. In general, someone who can look at the data and mentally make the leap of “hey, this looks wrong.” Look at data and determine how it might be better utilized (examples: 2.5% as a string or 0.025 as a number, identify values as empty strings and save as nulls instead, etc).
Technical Experience Required/Preferred:
SQL, Azure SQL, Azure Data Factory, Data warehousing environments (dims and facts), Snowflake (highly preferred), Python (highly preferred).
Job Types: Full-time, Permanent
Pay: $140,000.00 - $160,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Bonus pay
Yearly bonus
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Weekend availability
Work Location: Hybrid remote in Boca Raton, FL 33431","$150,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Charles Schwab
4.1",4.1,"Westlake, TX",Associate - Data Engineer,"Your Opportunity

This full-time role is part of a nine-month NERD (New Employee Recruitment and Development) program that blends on-the-job experience with an extensive training curriculum that covers tools, technologies, processes, and soft skills required to be successful in Schwab Technology Services. By pairing the curriculum, on-the-job experience, and a web-of-support from others, NERDs are well prepared to
succeed in their role which sets the table for future opportunities at Schwab.

What you’ll do
As a member of the NERD program, you will be working in our Data and Rep Technology (DaRT) organization that governs the strategy and implementation of the enterprise data warehouse and emerging data platforms. You will collaborate with partners across the firm to build the next generation analytics platform and capabilities for Charles Schwab. DaRT supports executive leadership, Sales, Marketing, and Finance teams by integrating and analyzing data to help them make data-based decisions through four primary opportunities:

Data Science Frameworks
This team partners with our data scientists and business stakeholders to design intuitive data solutions and create the best-suited serving pipelines for their modeling and analysis needs. You will help drive the deployment of cross-functional applications, determine platform design and architecture, and set the vision to build and scale our machine learning and experimentation platforms.

Data Engineering
This team designs, develops, and implements enterprise data integration solutions. You will have the opportunity to partner with other developers to set the future of the Data Warehouse through exciting and challenging projects and learning and using emerging technologies.

Data Analytics
This team performs data analysis of enterprise integration solutions to transform data into actionable insights leveraging best-in-class technologies, analytics, and visualization tools.

Platform and Production Support
This team builds next-gen enterprise data platforms, manages currency upkeep of existing platform assets, and matures those investments. The core functions include platform engineering, tenant-focused governance, capacity management, software upgrades, performance optimization, administration, lifecycle management, and maintenance.

What you will get out of the program
Mentorship
Challenging Career Opportunities
Continuous Training and Development
Certification Opportunities
Hands-on Technology Experience
Knowledge Sharing and Presentation Opportunities
Exposure to Leadership
Community of Dedicated and Welcoming Peers

Workplace Flexibility Program
We're proud to support our employees in a working approach that allows you to bring your best self to work – whether that’s in the office or remote.
Employees will have the flexibility of a hybrid work environment, spending some time working remotely and sometimes in the office.
Employees and managers can discuss and decide what works best for them, with flexibility available based on their role, business needs, and individual circumstances.
What you are good at

You enjoy problem solving
You have a hunger for knowledge
You work well with others
You are a good communicator
What you have

Undergraduate or graduate degree in Computer Science, Management Information Systems, or related discipline with a graduation date of August 2023 or earlier and/or
Workforce training certifications through coding bootcamps with a graduation date of August 2023 or earlier
Ability to start full-time with the program on September 18, 2023
Basic understanding of data modeling
Understanding of SQL development principles
Experience with algorithm design
Basic understanding of object-oriented analysis and design
Familiarity with data structures
Experience with data engineering, Extract, Transform, Load (ETL), Hadoop, MongoDB, Unix, Sqoop, HiveQL, or Pig Scripting is a plus
Inventiveness and eagerness to work with experimental technology
Demonstrated leadership potential
Passion and interest in solving problems applying innovation and experimentation","$88,000 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Investment & Asset Management,1973,$10+ billion (USD)
Arcon Group Inc,#N/A,"Mooresville, NC",Data Engineer,"Job Title: Dot NET Full Stack
Location: Charlotte, NC (Day 1 Onsite)
Duration: 12+ Months
Client: Wells Fargo
only on W2
Interview Mode: Phone & TEAMS
Note: We are looking only at OPT & H1-B
Minimum 3-5 years of experience with C#, .NET
Familiarity with the ASP.NET framework, SQL Server, and design/architectural patterns (e.g. Model-View-Controller (MVC))Experienced in implementing niche solutions with C# and .NET
Abundant experience in designing and writing reusable code with C# and .NET
Experienced with SQL/Oracle/Linux/Windows Servers
Work experience with Oracle, SQL, MySQL Database
Good to Have
Familiarity with Any Cloud Functions
C++/Java/Perl
Power Shell script
SAFE Agile Development
Job Type: Full-time
Salary: $45.00 - $55.00 per hour
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Mooresville, NC 28115: Reliably commute or planning to relocate before starting work (Required)
Education:
Master's (Preferred)
Experience:
Informatica: 1 year (Preferred)
SQL: 2 years (Preferred)
ASP.NET: 1 year (Preferred)
License/Certification:
Driver's License (Preferred)
Work Location: One location",$50.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"PRIMUS Global Services, Inc
4.1",4.1,"Minneapolis, MN","Data Engineer – Hadoop, Python – REMOTE WORK 43198","We have an immediate long-term opportunity with one of our key clients for a position of Data Engineer, to work on a remote basis.

The ideal applicant must have extensive experience with Python, Hadoop, Hive and SQL.

**ALL successful candidates for this position are required to work directly for PRIMUS. No agencies please**

For immediate consideration, please contact:

Tejaswini
PRIMUS Global Services
Direct: 972-798-2662
Desk: 972-753-6500 Ext: 204
Email: jobs@primusglobal.com","$91,633 /yr (est.)",501 to 1000 Employees,Private Practice / Firm,Information Technology,Information Technology Support Services,2002,$5 to $25 million (USD)
Kanini,#N/A,Remote,Data Engineer,"About Kanini
Kanini provides Agile Software Development, Cloud Computing, Data Science, and Location Intelligence services to public and private organizations. We have successfully served our clients in government, finance, transportation, utility, and software industries since 2003.
Why you should join
Working at Kanini is flexible and personal. We are a highly motivated, collaborative team experimenting with the latest technologies. We are committed to everyone having a healthy work/life balance, and we provide extensive mentorship and training resources to help you succeed.
Kanini is looking for a Big Data Engineer who has a deep experience in Data Engineering, AWS, Python, Data Lakes.
Required Skills
At least 10 years software development experience
At least 5 years leading at least one Scrum team of data engineers building data-intensive products with a modern tech stack.
Significant experience with big data ETL pipeline development with Spark, Hive, and related technologies
Significant experience with a general-purpose programming language such as Python, Scala, or Java
Experience with Spark framework and related tools (PySpark, Scala, SparkR, Spark SQL, Spark UI)
Experience with Hadoop ecosystem using HDFS, ADLS Gen2, or AWS S3
Experience with data visualization development using Python, Tableau, or PowerBI
Experience with Azure, AWS or GCP
Solid understanding of performance tuning concepts for relational and distributed database systems
Familiarity with distributed programming, big data concepts, and cloud computing
Education Qualifications
Bachelor’s degree in computer science/Engineering or Technology related field or possess equivalent work experience.
Preferred Qualifications
Cloud certifications from Azure, AWS or GCP
Big data, data engineering or data science certifications from recognized vendors such as Databricks & Cloudera
Kanini Software Solutions, Inc. does not discriminate in employment matters based on race, gender, religion, age, national origin, citizenship, veteran status, family status, disability status, or any other protected class. We support workplace diversity. If you have a disability, please let us know if there is anything we can do to improve the interview process for you; we’re happy to accommodate.
Kanini Software Solutions, Inc., 25 Century Blvd., Ste. 602, Nashville, TN 37214
Job Types: Full-time, Contract
Pay: $60.00 - $70.00 per hour
Work Location: Remote",$65.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"PrizePicks
4.8",4.8,"Atlanta, GA",Data Engineer,"At PrizePicks, we are the fastest growing sports company in North America, as recognized by Inc. 5000. As the leading platform for Daily Fantasy Sports, we cover a diverse range of sports leagues, including the NFL, NBA, and Esports titles like League of Legends and CS : GO. Our team of over 200 employees thrives in an inclusive culture that values individuals from diverse backgrounds, regardless of their level of sports fandom. Ready to reimagine the DFS industry together?
Our Analytics Team is responsible for building and maintaining analytics tools and workflows to support the PrizePicks business. By developing, maintaining, and testing data generation infrastructures, you will enable the PrizePicks business to make smarter, better, and faster data-driven decisions.
What you'll do:
Create, maintain and orchestrate optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Improve and streamline data systems to drive innovation within Prizepicks
What you have:
3+ years of experience building frameworks for data ingestion pipelines but real time and batch using data modeling, ETL/ELT processes
Stellar SQL skills, experience building DBT pipelines in production and know your way around structured, semi-structured and unstructured data.
Have built and optimized 'big data' data pipelines, architectures and data sets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Experience with Python and other object oriented scripting languages
Cloud experience: AWS, EC2, EMR, RDS, Redshift, GCP, etc..
Not required but would be awesome if you have experience with any of the following data pipeline and workflow tools: Azkaban, Luigi, Airflow, Prefect, etc.
Where you'll live:
Anywhere in the US is fine (we are based in Atlanta, GA)
Benefits you'll receive:
In addition to your great compensation package, company subsidized medical/dental/vision coverage plans and matching 401(k), we'll shower you with perks including:
Break room with ping pong, endless snacks and in-office lunch once a week
Unlimited PTO to encourage a healthy work/life balance (2 week min required!)
Modern work schedule focused on getting the job done, not hours clocked
Workplace flexibility
Company and team outings, we encourage a tight-knit workplace
Generous Maternity AND Paternity leave (16 weeks!)
Annual bonus & stock options
Wellness program
Company equipment provided (Windows & Mac options)
Annual performance reviews with opportunity for growth and career development
#LI-REMOTE

You must be authorized to work for any employer in the U.S. We are unable to sponsor or take over sponsorship or an employment Visa at this time.
PrizePicks is an Equal Opportunity Employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.","$97,814 /yr (est.)",1 to 50 Employees,Company - Private,"Arts, Entertainment & Recreation",Sports & Recreation,2015,Unknown / Non-Applicable
"HCA Healthcare
3.3",3.3,"Nashville, TN",Data Engineer,"Introduction
Last year our HCA Healthcare colleagues invested over 156,000 hours volunteering in our communities. As a Staff Data Engineer with HCA Healthcare you can be a part of an organization that is devoted to giving back!
Benefits
HCA Healthcare, offers a total rewards package that supports the health, life, career and retirement of our colleagues. The available plans and programs include:
Comprehensive medical coverage that covers many common services at no cost or for a low copay. Plans include prescription drug and behavioral health coverage as well as free telemedicine services and free AirMed medical transportation.
Additional options for dental and vision benefits, life and disability coverage, flexible spending accounts, supplemental health protection plans (accident, critical illness, hospital indemnity), auto and home insurance, identity theft protection, legal counseling, long-term care coverage, moving assistance, pet insurance and more.
Free counseling services and resources for emotional, physical and financial wellbeing
401(k) Plan with a 100% match on 3% to 9% of pay (based on years of service)
Employee Stock Purchase Plan with 10% off HCA Healthcare stock
Family support through fertility and family building benefits with Progyny and adoption assistance.
Referral services for child, elder and pet care, home and auto repair, event planning and more
Consumer discounts through Abenity and Consumer Discounts
Retirement readiness, rollover assistance services and preferred banking partnerships
Education assistance (tuition, student loan, certification support, dependent scholarships)
Colleague recognition program
Time Away From Work Program (paid time off, paid family leave, long- and short-term disability coverage and leaves of absence)
Employee Health Assistance Fund that offers free employee-only coverage to full-time and part-time colleagues based on income.
Learn more about Employee Benefits
Note: Eligibility for benefits may vary by location.

Would you like to unlock your potential with a leading healthcare provider dedicated to the growth and development of our colleagues? Join the HCA Healthcare family! We will give you the tools and resources you need to succeed in our organization. We are looking for an enthusiastic Staff Data Engineer to help us reach our goals. Unlock your potential!
Job Summary and Qualifications
Data Engineers within HCA’s Information and Analytics organization are responsible for defining and implementing data management practices across the enterprise. This full-time position will focus primarily on enterprise data management and migrating of data to the cloud. Data Engineers are expected to source and incorporate new data sources into the Enterprise Data Ecosystem. The responsibilities will include writing, testing, and reviewing ETL pipelines for defining and implementing data management practices across the enterprise. Due to the emerging and fast-evolving nature of Cloud technology and practice, the position requires that one stay well-informed of technological advancements and be proficient at putting new innovations into effective practice.
As a Data Engineer, you will work closely with all team members to create a modular, scalable solution that addresses current needs, but will also serve as a foundation for future success. The position will be critical in building the team’s engineering practices in test driven development, continuous integration, and automated deployment and is a hands-on team member who actively coaches the team to solve complex problems. This is a leadership position that assumes the responsibility for project success and the upward development of team members. They are the development team's point of contact that must interface with business partners of varying roles ranging from technical staff to executive leadership.
As a Staff Data Engineer level, the role requires 'self-starters' who are proficient in problem solving and capable of bringing clarity to complex situations. It requires contributing to strategic technical direction and system architecture approaches for individual projects and platform migrations. It also requires working closely with others, frequently in a matrixed environment, and with little supervision. This candidate will have a history of increasing responsibility in a small multi-role team. This position requires a candidate who can analyze business requirements, perform design tasks, construct, test, and implement solutions with minimal supervision.
Our Purpose
Applied to this position, your skills will help transform healthcare through technology and solutions that dramatically improve patient care and business operations.
Core Competencies
At HCA ITG, your deliverables will influence patient care. Every process, technology, and decision matters. This role will provide leadership and deep technical expertise in all aspects of solution design and application development for specific business environments. It will focus on setting technical direction on groups of applications and similar technologies as well as taking responsibility for technically robust solutions encompassing all business, architecture, and technology constraints.
Responsible for building and supporting a Cloud based ecosystem designed for enterprise-wide analysis of structured, semi-structured, and unstructured data. Direct the transformation from HCA Healthcare’s current on premise Teradata platform to Google Cloud Platform to enable analytics and machine learning at scale.
Design the cloud environment from a comprehensive perspective, ensuring that it satisfies all the company’s needs.
Develop, manage, and own full data lifecycle from raw data acquisition through transformation to end user consumption
Share knowledge and experience to contribute to growth of overall team capabilities
Perform activities such as deployment, maintenance, monitoring, and management inside the cloud framework that has been created
Provide guidance on technology choices and design considerations for migrating data to the Cloud
Maintain a holistic view of information assets by creating and maintaining artifacts that illustrate how information is stored, processed, and accessed
Demonstrate deep understanding and act as a leader in the team’s continuous integration and continuous delivery automation pipeline
Collaborate with business analysts, project lead, management, and customers on requirements
Design fit-for-purpose products to ensure products align to the customer's strategic plans and technology road maps
Actively participate in technical group discussions and adopt any new technologies to improve the development and operations.
Assist team members with production issues and offer support, guidance, and assist in communicating issues with appropriate stakeholders when necessary.
Provide leadership on key technology choices for Enterprise Data Ecosystem including data warehouse, analytical and big data platforms.
Ensure architectural, quality, and governance adherence through design reviews.
Education & Experience
Bachelor's degree in computer science or related field - Required
Master's degree in computer science or related field - Preferred
3+ years of experience in Data Engineer/Architect- Required
1+ year(s) of experience in Healthcare - Preferred
8+ years of experience in Information Technology - Required
Knowledge, Skills, Abilities, Behaviors
A successful candidate will have:
Experience developing and supporting data pipelines from various source types (on-prem rdbms, AWS, GCS bucket, flat file) to Big Query utilizing Google Cloud Platform native technologies
Knowledge and experience using the following technologies
o Big Query
o Dataflow, Data Proc, Data Fusion, Cloud Composer
o GSUTIL, GCS, Kafka, Pub/Sub
o Data Catalog/Dataplex
o Python, Unix, Linux
Strong understanding of best practices and standards for cloud application design and implementation.
Extensive experience with relational database management systems; Teradata, Oracle or SQL Server:
o Advanced SQL skills
o Write, tune, and interpret SQL queries
o BTEQs
o Stored procedures
Experience with Unstructured Data
Ability to troubleshoot, maintain, reverse engineer, and optimize existing ETL pipelines.
Requires strong practical experience in agile application development and DevOps discipline, including deployment of CI/CD pipelines in Git
Ability to multitask and to balance competing priorities.
Expertise in planning, implementing, supporting, and tuning Cloud ecosystem environments using a variety of tools and techniques.
Ability to define and utilize best practice techniques and to impose order in a fast-changing environment. Must have strong problem-solving skills.
Strong verbal, written, and interpersonal skills, including a desire to work within a highly-matrixed, team-oriented environment.
A successful candidate may have:
o Experience in Healthcare Domain
o Experience in Patient Data
Certifications (a plus, but not required)
GCP Cloud Professional Data Engineer
HCA Healthcare has been recognized as one of the World’s Most Ethical Companies® by the Ethisphere Institute more than ten times. In recent years, HCA Healthcare spent an estimated $3.7 billion in cost for the delivery of charitable care, uninsured discounts, and other uncompensated expenses.
""There is so much good to do in the world and so many different ways to do it.""- Dr. Thomas Frist, Sr.
HCA Healthcare Co-Founder
Be a part of an organization that invests in you! We are reviewing applications for our Staff Data Engineer opening. Qualified candidates will be contacted for interviews. Submit your application and help us raise the bar in patient care!
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.","$93,734 /yr (est.)",10000+ Employees,Company - Public,Healthcare,Health Care Services & Hospitals,1968,$10+ billion (USD)
"Jane Technologies
4.8",4.8,"Santa Cruz, CA",Data Migration Engineer,"Company Overview:
Jane is an MIT-founded, high-growth, and quickly-growing technology company in the cannabis industry. We believe in the cannabis industry's ability to bring well-being, health, and love into this world, and it is our mission to bring confidence to the online cannabis shopping experience. As the cannabis industry's first complete real-time marketplace, we aim to provide consumers with a confident, safe, and simple shopping experience.
To learn more about who we are, our culture, and whether this is the right place for you, read our Key Values profile: https://www.keyvalues.com/jane. Check out our product at: https://www.iheartjane.com/.

What You'll do:
Jane is looking for a Data Migration Engineer to help with the development, documentation and implementation of data migration services for Jane Point of Sale. As a Data Migration Engineer you will work within the Operations Department and collaborate with Engineering, Partner Success, Content and Tech Support. The Data Migration Engineer will collaborate with cross-functional teams and directly with our partners during the onboarding process to extract, transform and load (ETL) data from diverse sources into Jane POS. You will help ensure our partners experience a smooth transition from their legacy system to our industry-leading point of sale. Culture is the single most important part of Jane's success to date. You will report to our Program Manager.
Responsibilities
Take lead responsibility for researching, implementing, testing, validating, and documenting data migration services for Jane POS
Take lead responsibility for executing data migration services for each Jane POS onboarding, which may include personal health information (PHI) of dispensary customers
Validate and cleanse data to ensure accuracy and integrity during the migration process
Develop data mapping rules and perform data transformations to align source data with target system requirements
Conduct data profiling and quality assessments to resolve any issues or discrepancies
Help create test plans to validate the accuracy of migrated data
Collaborate with cross-functional teams, including developers, project managers, and data owners, to ensure seamless data migration
Follow data protection and privacy regulations, ensuring the secure handling of sensitive customer information
Document migration processes, procedures, and any encountered issues for future reference
Provide ad-hoc support to Deal Desk team migrating internal data within existing Jane systems
Work with Engineering to identify challenges in the Jane POS data model and import tool
Work with Partner Success and Technical Support to identify and remedy issues that occur during or because of the data migration process
Stay updated on industry best practices and latest trends in data migration techniques and technologies
Qualifications
Familiar with Python, software engineering concepts, and current technologies
Familiarity with working with relational databases and API endpoints
Communicate challenges and progress with team members through presentations, group calls.
Ability to leverage and create technical documentation
Bachelor's degree in software engineering or similar field or 2+ years of experience working in a similar role
Our Benefits:
Great compensation package and equity
Remote friendly work environment with employees throughout the US and Canada
Health, Dental, Vision, 401k, Unlimited PTO, and home office stipend

Jane Technologies is proud to be an equal opportunity employer and committed to providing employment opportunities regardless of race, religious creed, color, national origin, ancestry, physical disability, mental disability, medical condition, genetic information, marital status, sex, gender, gender identity, gender expression, pregnancy, childbirth and breastfeeding, age, sexual orientation, military or veteran status, or any other protected classification, in accordance with applicable federal, state, and local laws. EOE, including disability/vets.","$90,292 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Software Development,2017,Unknown / Non-Applicable
"Maven Workforce
4.1",4.1,"McLean, VA",Data Engineer,"MUST HAVES: Hands-on experience with Informatica PowerCenter and/or Informatica Intelligent Cloud Services (IICS), Cloud Data Integration (CDI) , Cloud AWS,UNIX scripting, and any other ETL tools.
Job responsibilities: –
The candidates will be having the IICS module of Informatica more than any other platform modules.
Strong hands-on experience building ETL pipelines and Data Replication
3+ yrs of experience with IICS is required.
Must be hand Ons with AWS services like ECS,S3,EKS etc.
Cloud Data Integration is also required.
Data Engineering Background experience with Spark, Hive is also required.
Preferred:
SQL is preferred, but basic knowledge is required
Job Type: Contract
Salary: $60.00 per hour
Ability to commute/relocate:
Mclean, VA 22101: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica PowerCenter: 7 years (Required)
Informatica Intelligent Cloud Services (IICS): 7 years (Required)
Cloud Data Integration (CDI): 7 years (Required)
UNIX scripting: 7 years (Required)
ETL tools: 7 years (Required)
Work Location: One location
Speak with the employer
+91 7328456015",$60.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2008,Unknown / Non-Applicable
Zenotis Technologies inc,#N/A,"Addison, TX",Big Data Engineer,"Role: Big Data Engineer
Location – Addison, TX/ Charlotte, NC / Chandler, AZ (Hybrid – 3 days onsite 2 days remote)
Position Summary:
Looking for a big data engineer who loves solving complex problems across a full spectrum of technologies. The ideal candidate is excited by experimentation and looking for a new challenge that stretches their talents. The big data engineer will help ensure that our technological infrastructure operates seamlessly in support of business objectives.
The engineer must be able to:
· Translate complex technical and functional requirements into detailed designs
· Guide the development and testing teams in developing and implement pipelines that extract, transform, and load data into an information product that helps the organization reach its strategic goals
· Focus on ingesting, storing, processing, and analyzing large datasets
· Deep understanding on how to create scalable, high-performance web services for tracking data
· Investigate alternatives for data storing and processing to ensure the implementation of the most streamlined solutions
Primary Skill
Hadoop
Secondary Skill
Kafka
Tertiary Skill
Oracle Exadata
Required Skills
· Translate complex technical and functional requirements into detailed designs
· Design data pipelines using design tools and provide solution intent in provided templates
· Work closely with data science team to implement data analytics pipelines
· Analyze vast number of data stores to uncover insights
· Define data governance policies and support data-versioning processes
· Maintain security and data privacy, working closely with data protection guidelines.
· Desired Skills
· Experience with Python, Spark, and Hive
· Understanding of data-warehousing and data-modeling techniques
· Knowledge of industry-wide visualization and analytics tools (ex: Tableau, R)
· Strong data engineering skills with Hadoop platform
· Experience with streaming frameworks such as Kafka
· Knowledge of Core Java, Linux, SQL, and any scripting language
· Good interpersonal skills and positive attitude
Job Type: Full-time
Salary: Up to $70.00 per hour
Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
8 hour shift
Work Location: On the road",$70.00 /hr (est.),201 to 500 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Ripple Effect Consulting
5.0",5.0,"Denver, CO",Data Engineer - 004/005,"Data Engineer
Hybrid-remote based in Denver, CO | $80-$160k
Job Overview:
We are looking for a savvy Data Engineer to join our growing team of analytics experts. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
Responsibilities for Data Engineer
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Qualifications for Data Engineer
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:
1) Experience with big data tools
2) Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
3) Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
4) Experience with cloud services
5) Experience with stream-processing systems: Storm, Spark-Streaming, etc.
6) Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Job Type: Full-time
Pay: $80,000.00 - $160,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Denver, CO: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person","$120,000 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Breadboard,#N/A,"New York, NY",Data Engineer,"About Us
We’re on a mission to revolutionize the global electronics supply chain.
During the pandemic we saw how the inefficiencies in electronics manufacturing can have massive global impact. From over $200B lost in car manufacturing to delays on laptops, gaming systems and much more.
At Breadboard we are rethinking the electronics supply chain down to the finest detail. We’re designing the new operating system in which this entire system will run. We’ve already secured seed funding and are now looking to expand our engineering team to help us set the foundation for the future.
Why work with us?
As a part of our team, you will have the unique opportunity to design and implement the data platform for Breadboard, a critical component in our business.
As one of the first data engineers to join the Breadboard team you will be partnering with the CTO, our founding engineers, and will set the foundation for the design of the operating system for the electronics supply chain.
You will be involved in all aspects of the software development process, architectural decisions, recruiting, and building the organization in general.
We’re a tight-knit, intense team and you will have a ton of end to end ownership.
Work with seasoned founders who have had successful exits.
This is not only a unique opportunity for someone who wants to get their hands dirty, but also take that step back and collaborate with the executive team on setting a strategic future.
You’ll be responsible for
Data Management: Develop and automate robust data acquisition, processing, and normalization pipelines from various sources to ensure data quality and consistency.
Database and ETL Architecture: Design, implement, and optimize a database system for efficient data storage, retrieval, and analysis.
Collaboration on Machine Learning: The prospects of working with data scientists to leverage extracted data features, contributing to the development and refinement of a machine learning model for component matching.
API and UI Integration: Collaborate with the software development team to facilitate seamless integration between the database, machine learning models, and user interface.
Continuous Improvement and Compliance: Monitor system performance and user feedback for continuous improvements, while ensuring adherence to data privacy and security standards.

Requirements
Experience: A minimum of 3-5 years of experience in a data engineering role, preferably in the electronics, manufacturing industry or supply chain industry.
Technical Skills: Proficiency in SQL and experience with database design, data modeling, and performance optimization. Strong coding skills in a major programming language, such as Python or TypeScript. Experience with data processing tools and libraries (e.g., pandas, Spark). Experience with pipeline and workflow orchestration tools, such Airflow. Experience with Snowflake or similar cloud-based data warehousing solutions.
Data Acquisition and Processing: Experience with data acquisition methods, such as APIs and web scraping. Familiarity with data cleaning, normalization, and feature extraction techniques.
Machine Learning: Familiarity with machine learning concepts and algorithms, and experience collaborating with data scientists or machine learning engineers.
API Development: Experience with API development and integration, preferably in a microservices architecture.
Data Governance: Knowledge of data privacy and security regulations, and experience implementing data governance measures.
Bonus Points
Entrepreneurial mindset (we encourage all employees to be future founders and this can be a great stepping stone towards that goal).
Customer-centric and passionate about helping small businesses grow.
Previous experience at a high-growth, fast-paced startup.
Experience with our stack (and some others that may be included shortly):
React, Material UI, Typescript, CSS
NestJS, GraphQL, TypeORM, Knex.js, Objection.js, PostgreSQL, gRPC, Kafka
AWS, Docker, Kubernetes, Helm, Github Actions, CI/CD pipelines.
Compensation + Benefits
Competitive cash compensation
Stock ownership at a fast growing company
Health/dental/vision insurance—100% coverage option
The ability to push for other benefits you think are important
Our Engineering Culture
Openness to diverse opinions and backgrounds - Testing our assumptions
Supported career growth with a high emphasis on learning and mentorship
Lean development
End to end ownership and accountability. We’re all going to make mistakes. It’s about how you own up to them and learn from them.

Equal Opportunity Employer
Breadboard Software does not discriminate in employment on the basis of race, color, religion, sex (including pregnancy and gender identity), national origin, political affiliation, sexual orientation, marital status, disability, genetic information, age, membership in an employee organization, retaliation, parental status, military service, or other non-merit factor.",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Synovize
5.0",5.0,Remote,Big Data Engineer,"Synovize is a cutting-edge technology company that specializes in providing advanced data analytics and intelligence solutions. Our mission is to empower businesses with the tools and insights they need to make data-driven decisions and drive innovation. We are committed to delivering exceptional results by combining our expertise in big data technologies with our passion for creating impactful solutions.
As a Big Data Engineer at Synovize, you will have the opportunity to work on exciting projects that involve designing and implementing scalable data solutions. You will collaborate with a talented team of professionals, including software developers, data scientists, and domain experts, to tackle complex data engineering challenges and deliver high-quality solutions to our clients.
Job Title:
Big Data Engineer
Company: Synovize
Number of Positions: 2
Location: Remote
Length: 12 Months
Tax Terms: 1099/C2C or W-2
Rate/Range: $70-$90/hr
Work Authorization: US Citizen
Security Clearance: None
Skills Needed:
Proficiency in Java and Kotlin programming languages.
Strong knowledge of AWS (Amazon Web Services) and its big data offerings.
Experience with BigData technologies, frameworks, and tools.
Familiarity with Kafka for streaming data processing.
Proficiency in Python programming language.
Responsibilities:
Design and develop scalable data pipelines to process, integrate, and analyze large volumes of structured and unstructured data.
Implement data ingestion, transformation, and storage mechanisms using big data technologies and frameworks.
Collaborate with data scientists and analysts to understand data requirements and optimize data models for analytics and machine learning.
Utilize AWS services and tools to build and maintain data infrastructure, ensuring data quality, integrity, and security.
Troubleshoot and resolve data-related issues, optimizing performance and efficiency.
Stay up-to-date with emerging trends and advancements in big data technologies, recommending and implementing improvements to existing systems and processes.
Requirements:
Bachelor's or master's degree in computer science, engineering, or a related field.
Proven experience as a Big Data Engineer or similar role, with a strong understanding of big data technologies and architectures.
Proficiency in Java and Kotlin programming languages.
Strong knowledge of AWS and its big data offerings.
Experience with BigData technologies, frameworks, and tools.
Familiarity with Kafka for streaming data processing.
Proficiency in Python programming language.
Strong problem-solving skills and ability to work in a fast-paced, collaborative environment.
Excellent communication and interpersonal skills.
Join Synovize and be part of a dynamic team that is at the forefront of data innovation. Together, we will drive meaningful change and unlock the full potential of data-driven decision-making.
Job Types: Full-time, Contract
Pay: $70.00 - $90.00 per hour
Benefits:
401(k)
Dental insurance
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Vision insurance
Schedule:
10 hour shift
4 hour shift
8 hour shift
Choose your own hours
Application Question(s):
Are you a US Citizen?
Years of experience in Java?
Years of experience in Kotlin?
Years of experience in big data?
Years of experience in AWS?
Security Clearance
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$80.00 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Codinix Technologies
5.0",5.0,"Boston, MA",Cloud Data Engineer - AWS,"Only US Citizen
Strong in warehouse technologies (Hadoop, Spark, AWS, etc.)
Bachelor's degree in Computer Science, Information Systems, or another related field
5+ years of data engineering, and cloud engineering experience
Experience in Agile project methodologies and DevOps
Experience with AWS (certification preferred; including Athena, Redshift, Glue), Cloudera
Experience building a CI/CD pipeline to deploy applications on AWS (GitLab, Terraform)
Advanced experience programming and scripting on Unix/Linux (i.e. Python, Bash)
Experience with CTRL-M, Cron (other) scheduling of batch jobs
Experience migrating legacy systems to cloud-ready architectures
Job Type: Contract
Pay: Up to $75.00 per hour
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Boston, MA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",$75.00 /hr (est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Provista Software Corporation,#N/A,Remote,Data Engineer,"As a successful candidate, you’ll play a pivotal role in the data conversion and business intelligence solution efforts during project implementation for our state and local clients. We are looking for you to join our software implementation team as a Senior Data Engineer.
Qualification:
5+ years of experience working independently as ETL developer on data migration projects that covered diverse database platforms and complex data conversions.
Must have experience across leading Database Platforms especially Oracle, Azure SQL, Snowflake.
Expert level experience with Extraction Transform Load (ETL) plan, protocols, and tools.
Expert level experience with programming languages SQL, Phyton, Java, C#.
Good at communication with clients and internally.
Expert at problem solving and troubleshooting skills.
Bachelor's degree, preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience
Must be a U.S. Citizen or Green Card Holder.
Functional Responsibility:
Data Conversion -
In collaboration with Data Stewards, discover inventory of legacy data sources and all data the client expects to be migrated to destination Modified Off the Shelf (MOTS) solution.
Complete gap assessment between legacy data and MOTS data requirements, and options to address the gaps.
Develop data migration map to minimally include data dictionary, detailed data map of all elements of the current database, data transformation rules, relationship rules, validation rules, process of migrating images, and process for recovering missing or erroneous data.
Identify issues, risks, and/or barriers that may interfere with data migration and recommendations to resolve or mitigate.
Conduct testing and quality assurance to ensure the data is migrated accurately.
Migrate data from legacy to destination MOTS solution progressively using a sample set of data and the entirety of the data prior to deployment into relevant go-live environments (e.g., staging, production, etc.)
Prepare data migration reports that includes – record counts of source vs migrated vs modified; field level validation of data at row and column; discovered exceptions.
Business Intelligence -
Develop ETL pipelines in and out of Snowflake data warehouse using combination of Python, SnowSQL, and SQL.
In collaboration with Data Stewards, create Snowflake data mart as curated subset of data to support the analytics and business intelligence needs of subgroup of users.
Translate requirements for business intelligence and reporting to database design and reporting design.
Conduct discovery sessions with users to capture business intelligence reports.
Develop Tableau self-service ad hoc reports to support operational users.
Develop Tableau Dashboards to support executive, management, and operational users.
About us:
Provista Software Corporation is fast growing Software Solution provider for US State and Local Government Clients.
Job Type: Full-time
Pay: $67,107.98 - $134,285.66 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Life insurance
Paid time off
Parental leave
Professional development assistance
Vision insurance
Compensation package:
Yearly pay
Experience level:
5 years
Schedule:
8 hour shift
Application Question(s):
What is your most preferred work time zone ?
Experience:
ETL: 5 years (Required)
Data warehouse: 1 year (Preferred)
Tableau: 1 year (Preferred)
Talend: 1 year (Preferred)
Microsoft SQL Server: 5 years (Required)
Work Location: Remote","$100,697 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"MetroStar
3.7",3.7,United States,Data Engineer (Mid),"As a Data Engineer (Mid), you'll bring creative architect solutions to end customers with the goal to make an impact across the federal government.
We know that you can't have great technology services without amazing people. At MetroStar, we are obsessed with our people and have led a two-decade legacy of building the best and brightest teams. Because we know our future relies on our deep understanding and relentless focus on our people, we live by our mission: A passion for our people. Value for our customers.
If you think you can see yourself delivering our mission and pursuing our goals with us, then check out the job description below!
What you'll do:
Work with AI team members to operationalize data pipelines and ML tasks.
Provide day-to-day support of deploying Python-native ML pipelines and perform data engineering tasks to enable AI/ML capabilities.
Present results to a diverse audience in presentation or report form.
Support architectural leadership, technical support, and advisement services to ensure identity management system technologies are integrated and meeting the appropriate security requirements.
Support leadership who engage with senior level executives at a public facing Federal agency and provide subject matter expertise in security architecture and other key domain areas.
What you'll need to succeed:
5+ years of experience in Data/ML engineering (if school experience is used, at most that would contribute to 2 years of actual experience).
Experience with ETL, Data Labeling and Data Prep.
Experience designing, implementing, and maintaining data architecture and services to be used for AI/ML. Additionally, operationalizing and maintaining AI/ML models in production.
The ability to perform data analytics on program related or system related activities. This will include assessing performance and manual processes implementing methods/algorithms to automate/optimize
A bachelor's degree in Computer Science, Information Technology Management or Engineering, or other comparable degree or experience and the ability to obtain and maintain DHS Suitability.

Like we said, we are obsessed with our people. That's why we offer a generous benefits package, professional growth, and valuable time to recharge. Learn more about our company culture code and benefits. Plus, check out our accolades.
Don't meet every single requirement?
Studies have shown that women, people of color and the LGBTQ+ community are less likely to apply to jobs unless they meet every single qualification. At MetroStar we are dedicated to building a diverse, inclusive, and authentic culture, so, if you're excited about this role, but your previous experience doesn't align perfectly with every qualification in the job description, we encourage you to go ahead and apply. We pride ourselves on making great matches, and you may be the perfect match for this role or another one we have. Best of luck! – The MetroStar People & Culture Team
What we want you to know:
In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification form upon hire.
MetroStar Systems is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. The statements herein are intended to describe the general nature and level of work being performed by employees and are not to be construed as an exhaustive list of responsibilities, duties, and skills required of personnel so classified. Furthermore, they do not establish a contract for employment and are subject to change at the discretion of MetroStar Systems.
Not ready to apply now?
Sign up to join our newsletter here.",#N/A,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1999,$25 to $100 million (USD)
Go Intellects Inc,#N/A,"Washington, DC",Data Engineer,"Work Location: REMOTE (1 – 2 Days On-site/Week may require)
Required Skills:
· Collect, manage, and convert raw data accurately and reliably
· Organize data systems for subgroup access and analyses
· Configure and sustain data cloud structures
· Must have expertise in Data Visualization Tools (Tableau)
· Data Modeling/Science as Python/SAS
· Should have AWS cloud native services, security, data pipeline
· Able to work with structured and unstructured data.
· Validate outputs of data pipelines
· Degree in Data Engineering preferred.
Job Type: Full-time
Pay: Up to $150,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Application Question(s):
Do you have, ""Active Secret (or) Top Secret Security Clearance""?
Work Location: In person","$150,000 /yr (est.)",Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Purple Drive Technologies
4.2",4.2,"Cupertino, CA",Data Engineer,"Terraform along with synapse Azure cloud
Deep expertise in Data Engineering and Data Warehousing (minimum 4+ years)
Azure synapse , Azure Data Factory , Spark Pool , SQL , SQL Pool (minimum 4+ yrs)
CI/CD and Python/Java programming experience (minimum 3+ years)
Ideal to have a 24 x 7 development i.e offshore presence or different time zones
Job Type: Full-time
Salary: $115,871.32 - $207,675.38 per year
Ability to commute/relocate:
Cupertino, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$161,773 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Wallero
4.4",4.4,"Seattle, WA",Data Engineer,"Hi,
Pretty much interested in your resume, we have an urgent opening for Data Engineer role with one of our clients based in Seattle WA.
Position: Data Engineer
Location: Seattle WA - Remote (100%)
Description:
Good Experience in reporting space including PowerBI, ADF, and possibly standing up a data warehouse/data mart.
The main scope centering is around reporting.
Experience with SQL, and agile framework including requirement gathering and story/feature writing skills.
Need someone strong who is comfortable leading and guiding teams in that capacity.
Migrating data from third party to Azure Cloud
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Seattle, WA 98101: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",$75.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2015,$1 to $5 million (USD)
Arcon Group Inc,#N/A,"Mooresville, NC",Data Engineer,"Job Title: Dot NET Full Stack
Location: Charlotte, NC (Day 1 Onsite)
Duration: 12+ Months
Client: Wells Fargo
only on W2
Interview Mode: Phone & TEAMS
Note: We are looking only at OPT & H1-B
Minimum 3-5 years of experience with C#, .NET
Familiarity with the ASP.NET framework, SQL Server, and design/architectural patterns (e.g. Model-View-Controller (MVC))Experienced in implementing niche solutions with C# and .NET
Abundant experience in designing and writing reusable code with C# and .NET
Experienced with SQL/Oracle/Linux/Windows Servers
Work experience with Oracle, SQL, MySQL Database
Good to Have
Familiarity with Any Cloud Functions
C++/Java/Perl
Power Shell script
SAFE Agile Development
Job Type: Full-time
Salary: $45.00 - $55.00 per hour
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Mooresville, NC 28115: Reliably commute or planning to relocate before starting work (Required)
Education:
Master's (Preferred)
Experience:
Informatica: 1 year (Preferred)
SQL: 2 years (Preferred)
ASP.NET: 1 year (Preferred)
License/Certification:
Driver's License (Preferred)
Work Location: One location",$50.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Rite Pros
3.4",3.4,"Portland, ME",Big Data Engineer,"Big Data Engineer with Bachelor’s degree in Computer Science, Computer Information Systems, Information Technology, or a combination of education and experience equating to the U.S. equivalent of a Bachelor’s degree in one of the aforementioned subjects.
Job Duties:
Define and manage the data architecture framework, standards, and principles, including modeling, metadata, security, reference, and master data.
Define reference architecture, and models to provide a common vocabulary, reusable components, and industry best practices.
Create solution frameworks integrating large or complex data sets.
Lead all data modeling efforts within DataBricks, including the design of data structures and the identification of business transformation logic.
Analyzing and translating business requirements into conceptual and fully detailed logical data models.
Creating logical data models based on existing applications and databases.
Working with business architects, and data stewards to capture business requirements in a Logical Data Model.
Transform the logical representation of the model into a physical representation and work with data engineering team to instantiate and manage the data.
Contribute to assessment of appropriate data platform(s) for solutioning efforts.
Collaborate with the BI and Analytics teams on creating the optimized, reusable semantic model, complete with metadata and lineage information.
Define and govern data modeling and design standards, tools, best practices, and related development for enterprise data models
Maintain the metadata - the ""data about the data"" describing the data model, its structure, and semantics
Recommend data and design patterns for common business functionality
Assist in multi-phase roadmaps within and across functional teams
Provide insight and technology recommendations that result in enterprise solutions and architectural risk mitigation.
Contribute to innovation strategies by exploring, investigating, recommending, benchmarking and implementing new data centric technologies for the platform.
Identify, design, and implement internal process improvements such as automation of manual processes, optimization of data delivery and re-design of existing infrastructure for greater scalability.
Build the infrastructure and processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and/or other data technologies.
Create and maintain data-related documentation (e.g. data dictionaries, data maps, data models etc.)
Work with data engineering and analytics experts to strive for greater functionality in our data systems and incorporation of industry best practices
Work experience / Technologies required for the position :
Experience building and optimizing Big Data, Data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, meta data, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected data sets.
Working knowledge of message queuing, stream processing, and highly scalable Big Data, data stores.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience in a Data Engineer or similar roles, who has attained a Bachelor’s degree in Bachelor’s degree in Engineering, Computer Science, Information Technology, related field or equivalent work experience.
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Work location is Portland, ME with required travel to client locations throughout USA.

Rite Pros is an equal opportunity employer (EOE).

Please Mail Resumes to:
Rite Pros, Inc.
565 Congress St, Suite # 305
Portland, ME 04101.
Email: resumes@ritepros.com","$97,894 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Less than $1 million (USD)
Kanini,#N/A,Remote,Data Engineer,"About Kanini
Kanini provides Agile Software Development, Cloud Computing, Data Science, and Location Intelligence services to public and private organizations. We have successfully served our clients in government, finance, transportation, utility, and software industries since 2003.
Why you should join
Working at Kanini is flexible and personal. We are a highly motivated, collaborative team experimenting with the latest technologies. We are committed to everyone having a healthy work/life balance, and we provide extensive mentorship and training resources to help you succeed.
Kanini is looking for a Big Data Engineer who has a deep experience in Data Engineering, AWS, Python, Data Lakes.
Required Skills
At least 10 years software development experience
At least 5 years leading at least one Scrum team of data engineers building data-intensive products with a modern tech stack.
Significant experience with big data ETL pipeline development with Spark, Hive, and related technologies
Significant experience with a general-purpose programming language such as Python, Scala, or Java
Experience with Spark framework and related tools (PySpark, Scala, SparkR, Spark SQL, Spark UI)
Experience with Hadoop ecosystem using HDFS, ADLS Gen2, or AWS S3
Experience with data visualization development using Python, Tableau, or PowerBI
Experience with Azure, AWS or GCP
Solid understanding of performance tuning concepts for relational and distributed database systems
Familiarity with distributed programming, big data concepts, and cloud computing
Education Qualifications
Bachelor’s degree in computer science/Engineering or Technology related field or possess equivalent work experience.
Preferred Qualifications
Cloud certifications from Azure, AWS or GCP
Big data, data engineering or data science certifications from recognized vendors such as Databricks & Cloudera
Kanini Software Solutions, Inc. does not discriminate in employment matters based on race, gender, religion, age, national origin, citizenship, veteran status, family status, disability status, or any other protected class. We support workplace diversity. If you have a disability, please let us know if there is anything we can do to improve the interview process for you; we’re happy to accommodate.
Kanini Software Solutions, Inc., 25 Century Blvd., Ste. 602, Nashville, TN 37214
Job Types: Full-time, Contract
Pay: $60.00 - $70.00 per hour
Work Location: Remote",$65.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"WorkCog
4.3",4.3,"Atlanta, GA",Sr. AWS Data Engineer,"Position: AWS Data Engineer
Location: Atlanta ,GA
Duration: 12+ Months
Interview: Video
JD
Responsibilities
Bachelor’s degree in Computer Science, Software Engineering, or other related field, or equivalent work experience.
Extensive experience in data modeling, writing & performance tuning complex SQL queries & user-defined functions
Hands-on experience in design, implementation and orchestration of data pipelines and ETL batch Jobs.
Hands-on experience on all aspects of data handling: cleansing and standardization, normalization, merging, change data capture, data integration, reconciliation, and transformations
Strong experience in working with Big Data technologies such as Hive/Spark, Data lake and Data warehouse using GCP and/or AWS Services
Ability to write robust code in scripting languages like Python, Scala, PySpark or similar.
Good to Have
Associate Architect or equivalent cloud certification
Experience with processing large datasets using AWS and GCP UDFs (User defined functions)
Experience in ML probability & statistics algorithms, frameworks like Keras or PyTorch and tools like Jupyter notebooks.
Job Type: Contract
Pay: $45.83 - $52.42 per hour
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Atlanta, GA: Reliably commute or planning to relocate before starting work (Preferred)
Experience:
SQL (Preferred)
Informatica (Preferred)
Data warehouse (Preferred)
Work Location: In person",$49.13 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable
"Acrisure Technology Group
3.9",3.9,"Austin, TX",Data Engineer,"Data Engineer
Hybrid Position (3 days per week average in Downtown Austin, TX or Grand Rapids, MI office)
Note: This is a full-time, in-house position. We do not offer C2C or C2H employment and are not able to sponsor visas for this position.
Acrisure Technology Group (ATG) is a fast-paced, AI-driven team building innovative software to disrupt the $6T+ insurance industry. Our mission is to help the world share its risk more intelligently to power a more vibrant economy. To do this, we are transforming insurance distribution and underwriting into a science.
At the core of our operating model is our technology: we're building the premier AI Factory in the world for risk and applying it at the center of Acrisure, a privately held company recognized as one of the world's top 10 insurance brokerages and the fastest growing insurance brokerage globally. By using the latest technology and advances in AI to push the boundaries of understanding risk, we are systematically converting data into predictions, insights, and choices, and we believe we can remove the constraints associated with scale, scope, and learning that have existed in the insurance industry for centuries.
We are a small team of extremely high-caliber engineers, technologists, and successful startup founders, with diverse backgrounds across industries and technologies. Our engineers have worked at large companies such as Google and Amazon, hedge funds such as Two Sigma and Jump Trading, and a variety of smaller startups that quickly grew such as Indeed, Bazaarvoice, RetailMeNot, and Vrbo.
The Role
The Business Intelligence team's mission is to unify data across the enterprise to optimize business decisions made at the strategic, tactical, and operational levels of the organization. We accomplish this by providing an enterprise data warehouse, data lake, reporting platform, and business processes that provide quality data, in a timely fashion, from any channel of the company and present them in such a manner as to maximize the value of that data for both internal and external customers.
The Data Engineer is responsible for designing and developing moderate to complex ETL processes required to populate a data lake and structured data warehouse which supply data for the machine learning, AI & BI teams. Responsibility includes working with a team of contracted developers as well as coaching and mentoring junior and mid-level developers. Ensuring high quality and best practices are maintained through the development cycle is key to this position.
You will interact with some of the top technologists on the planet. Our technology runs on Google Cloud and is configured with Kubernetes, leveraging various services in that environment. Our data storage layer includes BigQuery, BigTable, and Postgres. We code primarily in Kotlin, Python, Java, and JavaScript and make use of many frameworks, including Dataflow, Cloud AI Platform, KubeFlow, Spring, and React.
Here are some of the ways in which you'll achieve impact
Leverage established guidelines and custom designs to create complex ETL processes to meet the needs of the business
Develop from strategic and non-strategic data sources including data preparation/ETL and modeling for data visualizations in a self-service platform
Contribute to the definition and development of the overall reporting roadmap
Translate reporting requirements into reporting models, visualizations and reports by having a strong understanding of the enterprise architecture
Standardize reporting that helps generate efficiencies, optimization, and end user standards
Integrate dashboards and reports from a variety of sources, ensuring that they adhere to data quality, usability, and business rule standards
Independently determine methods and procedures for new or existing requirements and functionality
Work closely with analysts and data engineers to identify opportunities and assess improvements of our products and services
Contribute to workshops with the business user community to further their knowledge and use of the data ecosystem
Produce and maintain accurate project documentation
Collaborate with various data providers to resolve dashboard, reporting and data related issues
Perform Data Services reporting benchmarking, enhancements, optimizations, and platform analytics
Participate in the research, development, and adoption of trends in reporting and analytics
Mentor BI Developers and BI Analysts
Other projects as assigned in order to support necessary business goals across teams
You may be fit for this role if you have
Minimum 5 years required, particularly in an Azure environment with Azure Data Bricks, Azure Data Factory, Azure Data Lake
Minimum 5 years designing data warehouses, data modeling, and end-to-end ETL processes in a MS-SQL environment
Minimum 2 years developing machine learning models with Azure ML, ML Flow, BQML
Expert working knowledge of SQL, Python and Spark (and ideally PySpark) with a demonstrated ability to create ad-hoc SQL queries to analyze data, create prototypes, etc required.
Successfully delivered 2+ end to end projects – from Inception to Execution - in Data Engineering / Data Science / Data Integration as a Tech Senior/Principal
Ability to Analyze, summarize, and characterize large or small data sets with varying degrees of fidelity or quality, and identify and explain any insights or patterns within them.
Experience with multi-source data warehouses
Strong skills in in data analytics and reporting, particularly with Power BI
Experience with other cloud environments (GCS, AWS) a definite plus
Strong experience creating reports, dashboards, and/or summarizing large amounts of data into actionable intelligence to drive business decisions required
Strong understanding of core principles of data science and machine learning; experience developing solutions using related tools and libraries
Hands on experience building logical data models and physical data models and using tools like ER/Studio/Idera
Write SQL fluently, recognize and correct inefficient or error-prone SQL, and perform test-driven validation of SQL queries and their results
Proficient in writing Spark sql using complex syntax and logic like analytic functions etc.
Well versed in Data Lake & Delta Lake Concepts
Well versed in Databricks usage in dealing with Delta tables (external \ managed)
Well versed with Key Vault \ create & maintenance and usage of secrets in both Databricks & ADF
Should be knowledgeable in Stored procedures \ functions and be able to use them by ADF & Databricks as this is a widely used Practice internally
Familiar with DevOps process for Azure artifacts and database artifacts
Well versed with ADF concepts like chaining pipelines, passing parameters, using APIs for ADF & Databricks to perform various activities.
Experience creating and sharing standards, best practices, documentation, and reference examples for data warehouse, integration/ETL systems, and end user reporting
Apply disciplined approach to testing software and data, identifying data anomalies, and correcting both data errors and their root causes
Academics: Undergraduate degree preferred or equivalent experience along with a demonstrated desire for continuing education and improvement
Location: Austin, TX or Grand Rapids, MI
We are interested in every qualified candidate who is eligible to work in the United States. We are not able to sponsor visas for this position.","$96,659 /yr (est.)",5001 to 10000 Employees,Company - Private,Insurance,Insurance Agencies & Brokerages,2005,$1 to $5 billion (USD)
"PRIMUS Global Services, Inc
4.1",4.1,"Minneapolis, MN","Data Engineer – Hadoop, Python – REMOTE WORK 43198","We have an immediate long-term opportunity with one of our key clients for a position of Data Engineer, to work on a remote basis.

The ideal applicant must have extensive experience with Python, Hadoop, Hive and SQL.

**ALL successful candidates for this position are required to work directly for PRIMUS. No agencies please**

For immediate consideration, please contact:

Tejaswini
PRIMUS Global Services
Direct: 972-798-2662
Desk: 972-753-6500 Ext: 204
Email: jobs@primusglobal.com","$91,633 /yr (est.)",501 to 1000 Employees,Private Practice / Firm,Information Technology,Information Technology Support Services,2002,$5 to $25 million (USD)
"AbleTo
3.7",3.7,"New York, NY",Data Engineer,"About AbleTo
Join our mission-driven organization, where your work matters and a diversity of ideas and backgrounds are welcomed. AbleTo is the leading provider of high quality, technology-enabled behavioral health care. We believe that everyone deserves access to high-quality care and offer a suite of technology-enabled services to empower people to lead better lives through better mental health. A proprietary platform connects individuals with AbleTo highly trained licensed providers who deliver weekly sessions by phone or video supported by an integrated digital experience. Members also have access to mental health coaches, and digital support programs. Our outcomes-focused approach is proven to improve both behavioral and physical health and lower medical costs.

Overview
Data is integral to nearly everything that we do at AbleTo, and as a result the data team plays a key role at our company. As a Data Engineer at AbleTo, you will grow and develop your skills as a data engineer to build a world class data platform to support our mental health care mission.

What You Will Do
Build and maintain batch and real-time ETL pipelines in a Google Cloud Platform architecture (BigQuery, Dataproc, Firestore, Informatica, Python, etc.)
Identify code quality issues and implement tests to improve future processes.
Document orchestration work-flows and create run books
Implement data integrity tests to ensure we are ingesting accurate data.
Implement high-quality test-driven code.
Participate in daily team stand ups and other Agile ceremonies

Key Initiatives over the next 12 months
Data Mart client migration
Data Mart enhancements
Automation of manual day to day operational tasks
Decommission legacy pipelines

Who You Are:
1+ years of experience coding in Python.
Experience working with SQL (structured query language).
Familiarity with structuring and writing ETLs.
Experience working with Orchestration tools (Airflow) and Cloud Databases (BigQuery) is a plus.
Experience working in an Agile Development environment is a plus.
BS in Information Systems, Computer Science or related field is preferred.
You are excited to work with data!
Curiosity to dive deeply into issues and feel empowered to make a meaningful impact at a mission-driven company.
Committed to agile development and value delivery and solid engineering principles, as well as continuously improving our craft.
You are a collaborator. You build and maintain strong, productive working relationships with your customers and stakeholders.
You are empathetic and seek to understand each individual's diversity of background and experience contributes to making up a great team.
You drive to improve yourself and others.
You are excited about learning new languages, tools, techniques and technologies.

Why You Should Join Our Team:
We're proud to be a Great Place to Work-Certified™ company. We want you to show up and feel your best at work, and that means respecting your time outside of work. Our inclusive, flexible workspace prioritizes a work/life balance. We offer competitive salaries, comprehensive health benefits (for full-time employees), and professional perks such as 401K matching, fully funded HRA, and generous time off, including mental health days because your well-being is important to us.
At AbleTo, we're empowering people to get better and stay better. Want to join us? Take the next step in your career by applying for this role today.
The salary range is $40,000 to $90,000. Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, AbleTo offers benefits such as a comprehensive benefits package, incentive and recognition programs, and 401k contribution (all benefits are subject to eligibility requirements).

#LI-Remote
Follow AbleTo on LinkedIn, Twitter, and Instagram!
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.
AbleTo is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or veteran status. AbleTo is an E-Verify company.","$65,000 /yr (est.)",501 to 1000 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2008,$25 to $100 million (USD)
"HCA Healthcare
3.3",3.3,"Nashville, TN",Data Engineer,"Introduction
Last year our HCA Healthcare colleagues invested over 156,000 hours volunteering in our communities. As a Staff Data Engineer with HCA Healthcare you can be a part of an organization that is devoted to giving back!
Benefits
HCA Healthcare, offers a total rewards package that supports the health, life, career and retirement of our colleagues. The available plans and programs include:
Comprehensive medical coverage that covers many common services at no cost or for a low copay. Plans include prescription drug and behavioral health coverage as well as free telemedicine services and free AirMed medical transportation.
Additional options for dental and vision benefits, life and disability coverage, flexible spending accounts, supplemental health protection plans (accident, critical illness, hospital indemnity), auto and home insurance, identity theft protection, legal counseling, long-term care coverage, moving assistance, pet insurance and more.
Free counseling services and resources for emotional, physical and financial wellbeing
401(k) Plan with a 100% match on 3% to 9% of pay (based on years of service)
Employee Stock Purchase Plan with 10% off HCA Healthcare stock
Family support through fertility and family building benefits with Progyny and adoption assistance.
Referral services for child, elder and pet care, home and auto repair, event planning and more
Consumer discounts through Abenity and Consumer Discounts
Retirement readiness, rollover assistance services and preferred banking partnerships
Education assistance (tuition, student loan, certification support, dependent scholarships)
Colleague recognition program
Time Away From Work Program (paid time off, paid family leave, long- and short-term disability coverage and leaves of absence)
Employee Health Assistance Fund that offers free employee-only coverage to full-time and part-time colleagues based on income.
Learn more about Employee Benefits
Note: Eligibility for benefits may vary by location.

Would you like to unlock your potential with a leading healthcare provider dedicated to the growth and development of our colleagues? Join the HCA Healthcare family! We will give you the tools and resources you need to succeed in our organization. We are looking for an enthusiastic Staff Data Engineer to help us reach our goals. Unlock your potential!
Job Summary and Qualifications
Data Engineers within HCA’s Information and Analytics organization are responsible for defining and implementing data management practices across the enterprise. This full-time position will focus primarily on enterprise data management and migrating of data to the cloud. Data Engineers are expected to source and incorporate new data sources into the Enterprise Data Ecosystem. The responsibilities will include writing, testing, and reviewing ETL pipelines for defining and implementing data management practices across the enterprise. Due to the emerging and fast-evolving nature of Cloud technology and practice, the position requires that one stay well-informed of technological advancements and be proficient at putting new innovations into effective practice.
As a Data Engineer, you will work closely with all team members to create a modular, scalable solution that addresses current needs, but will also serve as a foundation for future success. The position will be critical in building the team’s engineering practices in test driven development, continuous integration, and automated deployment and is a hands-on team member who actively coaches the team to solve complex problems. This is a leadership position that assumes the responsibility for project success and the upward development of team members. They are the development team's point of contact that must interface with business partners of varying roles ranging from technical staff to executive leadership.
As a Staff Data Engineer level, the role requires 'self-starters' who are proficient in problem solving and capable of bringing clarity to complex situations. It requires contributing to strategic technical direction and system architecture approaches for individual projects and platform migrations. It also requires working closely with others, frequently in a matrixed environment, and with little supervision. This candidate will have a history of increasing responsibility in a small multi-role team. This position requires a candidate who can analyze business requirements, perform design tasks, construct, test, and implement solutions with minimal supervision.
Our Purpose
Applied to this position, your skills will help transform healthcare through technology and solutions that dramatically improve patient care and business operations.
Core Competencies
At HCA ITG, your deliverables will influence patient care. Every process, technology, and decision matters. This role will provide leadership and deep technical expertise in all aspects of solution design and application development for specific business environments. It will focus on setting technical direction on groups of applications and similar technologies as well as taking responsibility for technically robust solutions encompassing all business, architecture, and technology constraints.
Responsible for building and supporting a Cloud based ecosystem designed for enterprise-wide analysis of structured, semi-structured, and unstructured data. Direct the transformation from HCA Healthcare’s current on premise Teradata platform to Google Cloud Platform to enable analytics and machine learning at scale.
Design the cloud environment from a comprehensive perspective, ensuring that it satisfies all the company’s needs.
Develop, manage, and own full data lifecycle from raw data acquisition through transformation to end user consumption
Share knowledge and experience to contribute to growth of overall team capabilities
Perform activities such as deployment, maintenance, monitoring, and management inside the cloud framework that has been created
Provide guidance on technology choices and design considerations for migrating data to the Cloud
Maintain a holistic view of information assets by creating and maintaining artifacts that illustrate how information is stored, processed, and accessed
Demonstrate deep understanding and act as a leader in the team’s continuous integration and continuous delivery automation pipeline
Collaborate with business analysts, project lead, management, and customers on requirements
Design fit-for-purpose products to ensure products align to the customer's strategic plans and technology road maps
Actively participate in technical group discussions and adopt any new technologies to improve the development and operations.
Assist team members with production issues and offer support, guidance, and assist in communicating issues with appropriate stakeholders when necessary.
Provide leadership on key technology choices for Enterprise Data Ecosystem including data warehouse, analytical and big data platforms.
Ensure architectural, quality, and governance adherence through design reviews.
Education & Experience
Bachelor's degree in computer science or related field - Required
Master's degree in computer science or related field - Preferred
3+ years of experience in Data Engineer/Architect- Required
1+ year(s) of experience in Healthcare - Preferred
8+ years of experience in Information Technology - Required
Knowledge, Skills, Abilities, Behaviors
A successful candidate will have:
Experience developing and supporting data pipelines from various source types (on-prem rdbms, AWS, GCS bucket, flat file) to Big Query utilizing Google Cloud Platform native technologies
Knowledge and experience using the following technologies
o Big Query
o Dataflow, Data Proc, Data Fusion, Cloud Composer
o GSUTIL, GCS, Kafka, Pub/Sub
o Data Catalog/Dataplex
o Python, Unix, Linux
Strong understanding of best practices and standards for cloud application design and implementation.
Extensive experience with relational database management systems; Teradata, Oracle or SQL Server:
o Advanced SQL skills
o Write, tune, and interpret SQL queries
o BTEQs
o Stored procedures
Experience with Unstructured Data
Ability to troubleshoot, maintain, reverse engineer, and optimize existing ETL pipelines.
Requires strong practical experience in agile application development and DevOps discipline, including deployment of CI/CD pipelines in Git
Ability to multitask and to balance competing priorities.
Expertise in planning, implementing, supporting, and tuning Cloud ecosystem environments using a variety of tools and techniques.
Ability to define and utilize best practice techniques and to impose order in a fast-changing environment. Must have strong problem-solving skills.
Strong verbal, written, and interpersonal skills, including a desire to work within a highly-matrixed, team-oriented environment.
A successful candidate may have:
o Experience in Healthcare Domain
o Experience in Patient Data
Certifications (a plus, but not required)
GCP Cloud Professional Data Engineer
HCA Healthcare has been recognized as one of the World’s Most Ethical Companies® by the Ethisphere Institute more than ten times. In recent years, HCA Healthcare spent an estimated $3.7 billion in cost for the delivery of charitable care, uninsured discounts, and other uncompensated expenses.
""There is so much good to do in the world and so many different ways to do it.""- Dr. Thomas Frist, Sr.
HCA Healthcare Co-Founder
Be a part of an organization that invests in you! We are reviewing applications for our Staff Data Engineer opening. Qualified candidates will be contacted for interviews. Submit your application and help us raise the bar in patient care!
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.","$93,734 /yr (est.)",10000+ Employees,Company - Public,Healthcare,Health Care Services & Hospitals,1968,$10+ billion (USD)
"Southern Glazer’s Wine and Spirits
3.6",3.6,"Dallas, TX",Data Engineer,"What You Need To Know
Open the door to a groundbreaking tech career with an industry leader. Southern Glazer’s Wine & Spirits is North America’s preeminent wine and spirits distributor, as well as a family-owned, privately held company with a 50+ year legacy of success. To create a new era in alcohol beverage sales and service, we’re heavily invested in the most transformative new technologies – and the most brilliant tech professionals. Southern Glazer’s was named by Newsweek as a Most Loved Workplace and is included on the Forbes lists for Largest Private Companies and Best Employers for Diversity.
As a full-time employee, you can choose from a full menu of our Top Shelf Benefits, including comprehensive medical and prescription drug coverage, dental and vision plans, tax-saving Flexible Spending Accounts, disability coverage, life insurance plans, and a 401(k) plan. We also offer tuition reimbursement, a wellness program, parental leave, vacation accrual, paid sick leave, and more.
We offer continuous learning and career growth in a fast-paced environment where you are respected, your voice is heard, and technology is part of our strategy for success. If you’re looking to fill your glass with opportunity, come join our FAMILY.
Overview
The Data Engineer's role is to design, develop, maintain and enhance interfaces and connectivity to the Data Warehouse ecosystem by coding with a technical language to meet business requirements and business objectives. This can include taking technical specifications and developing an application or integration of data between applications, testing, as well as, completing the appropriate technical documentation. The Data Engineer will use best practices in software development and adhere to SGWS development standards, as well as, focus on quality and innovation. The Data Engineer may also be responsible for delivering support to end users in the organization for specific code, including troubleshooting code.
Specialized Skills and Technologies
Strong PL/SQL skills
Experience in ETL Tools (Preferrable Informatica)
Data Warehouse techniques will be a plus
Experience in cloud platforms like Azure or AWS will be a plus
Knowledge of UNIX/Linux, shell scripting, Python will be a plus
Experience developing Application Programming Interfaces (API's) will be a plus
Experience in Hadoop will be a plus
Primary Responsibilities
Design, develop, implement, and support software applications
Drive technical validity of solution.
Develop user documentation as well as in-code documentation to explain designs and participate/support user training
Structure requirements to facilitate automation of acceptance tests
In conjunction with Data Management Group, develop routines and procedures that provide data quality checks and balances on data delivery/ingestion
Collaborate across the BI / Analytics, Data Management Group, Enterprise Insights and Analytics teams to establish standards, reusable data models and best practices for delivery/ingestion of data from/to Data Warehouse - This includes Publish/Subscription and API options
Obtain any certifications needed to effectively support applications in scope
Support the development of business and technical process documentation and training materials
Structure requirements to facilitate automation of acceptance tests
Provide support for software applications under area of responsibility
Drive Behavior-Driven-Design (BDD) process
Perform other job-related duties as assigned
Minimum Qualifications
Bachelor’s Degree or a combination of work experience and education
Knowledge in application and software development
Knowledge of software design and programming principles
Proficient oral and written communication skills, ability to influence outcomes, and strong attention to detail
Strong analytical, mathematic, and problem-solving skills
Strong team player with ability to demonstrate Agile delivery values working both within a team and working independently
Strategic thinker – can develop a plan to meet a long-term objective
Agile Delivery Values
Openness – Team and stakeholders agree to be open about all work and challenges
Commitment – Personally commit to achieving the goals of the team
Respect – Respect your team members to be capable and independent
Courage – You have courage to do the right thing and work on tough problems
Focus – Everyone focus on the work in the sprint and the goal of the scrum team. Rise and fall as a team
Physical Demands
Physical demands include a considerable amount of time sitting and typing/keyboarding, using a computer (e.g., keyboard, mouse, and monitor), or mobile device
Physical demands with activity or condition may occasionally include walking, bending, reaching, standing, squatting, and stooping
May require occasional lifting/lowering, pushing, carrying, or pulling up to 20lbs
EEO Statement
Southern Glazer's Wine and Spirits, an Affirmative Action/EEO employer, prohibits discrimination and harassment of any type and provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. Southern Glazer's Wine and Spirits provides competitive compensation based on estimated performance level consistent with the past relevant experience, knowledge, skills, abilities and education of employees. Unless otherwise expressly stated, any pay ranges posted here are estimates from outside of Southern Glazer's Wine and Spirits and do not reflect Southern Glazer's pay bands or ranges.","$97,837 /yr (est.)",10000+ Employees,Company - Private,Retail & Wholesale,Wholesale,1968,$10+ billion (USD)
IT Customer Service - POST,#N/A,"Newark, NJ",Data Center Plant Engineer,"Job Title: ISP-OSP Data Center Plant Engineer
Location: Newark NJ, (M-F)
Travel Required: As Needed
Overview
UC-ISP/OSP engineer is responsible for cable management including structured cabling, carrier data and voice circuit cables/equipment, rack-and-stack, and oversight of power, backup power, network equipment, and environmental needs to cabinets. Leveraging Clients Data Center Infrastructure Management System, (DCIM) tool to maintain, report and monitor.
Conducting capacity assessments of existing infrastructure to ensure that it can support future growth to ensure compliance with federal regulations of the Data Center infrastructure Standards. Projects include design and build outs of the physical plant within the typical WAN/LAN space within data centers, telco rooms, MDFs and IDFs as needed.
Functions:
Perform initial engineering surveys based on project needs.
Document engineering surveys to determine feasibility and the estimated cost.
Obtain authorization to proceed and confirm funding.
Create detailed engineering drawings of build, floor plans, equipment elevation drawings, structured cabling diagrams, power, and environmental needs.
Designs must conform to state, local, national, and Client standards and codes, and the NJ historical and or environmental commissions approvals if needed.
Based on the engineering documents create a SOW (Scope of Work) and BOM (Bill of Materials)
Submit to Client manager to approve and proceed when funding is validated.
Create a construction package and submit to contractors or in house for RFQ.
Review quotes and choose both low bid and qualified contractor.
Work with all stakeholders to date and time work schedules.
Confirm workers are safety trained and conform to Client, FRA, and OSHA requirements.
Supervise and manage installations and conduct inspections.
Review work to ensure contractor met obligations under terms of contract.
Inform Client managers of status to approve invoices.
Coordinate with Client network engineering to connect, configure, and test equipment.
Review and confirm a successful installation were completed and working as designed prior to cutting over to production.
Assist with Break fix of IT infrastructure troubles or repairs.
Daily travel to location depending on scheduled or unscheduled work
Job Types: Full-time, Contract
Experience level:
5 years
Schedule:
8 hour shift
Work setting:
In-person
Experience:
Data Center: 5 years (Required)
WAN/LAN: 3 years (Required)
cabling: 3 years (Required)
Work Location: One location",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Green Worldwide Shipping LLC
3.7",3.7,"Aurora, CO",GEM Data Engineer,"About Green
“Global Reach with a personal touch”, that’s the foundation on which four partners (and long-time friends) agreed to when they started Green Worldwide Shipping back in 2008. This ethos has allowed Green to Grow to a 200+ strong team with 15 offices across the US and growing.At green we are a family of innovative problem-solvers who communicate freely to present creative solutions in an environment of trust, respect, and integrity. We are dedicated to customer service, passionate about logistics, and committed to sustainable growth while inspiring others.
Why this role is special..
GEM (Green Engineered Management) is a technology solutions department that supports various commercial technology products, data management, and development for Green Worldwide Shipping.We are seeking a skilled and experienced Data Engineer to join our dynamic team. As a Data Engineer, you will be responsible for designing, developing, and maintaining our data infrastructure and various technology systems to support efficient data processing and analysis. Your expertise in SQL, CargoWise, database management, Altova Flowforce Server, Altova Mapforce ETL software, Python and C#, along with experience in EDI/API connection protocols integration, will be instrumental in achieving success in this role.
How you will be successful…
Design, develop, and maintain data pipelines and ETL processes to extract, transform, and load data from various sources into our databases using Altova Mapforce ETL software.
Collaborate with cross-functional teams to understand data requirements and provide optimal data solutions.
Develop and optimize SQL queries to retrieve and manipulate data from databases, ensuring efficient data retrieval and storage.
Implement and integrate EDI/API connection protocols to enable seamless data exchange between internal and external systems.
Monitor and maintain the performance, availability, and integrity of the databases and data infrastructure.
Support for reporting, document modification, workflow and database support for internal ERP.
Troubleshoot and resolve data-related issues, including data quality and data integration problems.
Implement data security measures and ensure compliance with data privacy regulations.
Stay updated with industry best practices and emerging technologies in data engineering and incorporate them into the data infrastructure.
Provide technical guidance and support to other team members on data-related projects.
Collaborate with data analysts and data scientists to understand data requirements and support their data exploration and analysis needs.
Create and maintain documentation related to data pipelines, database schemas, and data transformation processes.
What you will bring….
Bachelor's degree in Computer Science, Information Systems, or a related field.
Preferred experience in CargoWise (or related ERP) with reporting, document modification, workflow and database support
Proven experience working with SQL and databases, preferably MySQL.
Prefer strong knowledge and hands-on experience with Altova Flowforce Server and Altova Mapforce ETL software, or related applications.
Experience in integrating and working with EDI/API connection protocols.
Proficiency in Python and/or C# (optional but preferred).
Familiarity with data modeling concepts and techniques.
Experience with data warehousing concepts and technologies.
Excellent problem-solving and analytical skills.
Strong attention to detail and the ability to work in a fast-paced environment.
Effective communication skills to collaborate with cross-functional teams.
Ability to work independently and prioritize tasks effectively.
Preferred Skills:
*
Experience with cloud-based data platforms (e.g., AWS, Azure, Google Cloud Platform).
Knowledge of big data technologies (e.g., Hadoop, Spark) and distributed computing.
Familiarity with data visualization tools (e.g., Tableau, Power BI).
Experience with version control systems (e.g., Git).
Understanding of Agile development methodologies.
What we offer:
Employee High Deductible Health Plan (HDHP) currently paid for 100% by Green.
$1,800 contributed to employee HSA Account, if on HDHP.
Employee Dental provided by Green.
Option to purchase disability insurance.
Paid life insurance and option to buy-up.
401(K) and company match.
Parental Leave Program.
Monthly recognition points to give/receive, which can be converted to gift cards.
Tuition Reimbursement Program.
Weekly fruit baskets in office locations
A tree planted for every new employee to Trees.org
Hybrid work schedule.
Wellbeing Program.
Training and Development.
Access to Mentorship Program.
Green Worldwide Shipping, LLC (Green) is committed to the full inclusion of all qualified individuals. As part of this commitment, Green will ensure that persons with disabilities are provided reasonable accommodations. If reasonable accommodation is needed to participate in the job application or interview process and/or to perform essential job functions, please contact Faye Hughes, Head of People and Culture at faye.hughes@greenworldwide.com.Green is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.
Requirements
Bachelor's degree in Computer Science, Information Systems, or a related field.
Preferred experience in CargoWise (or related ERP) with reporting, document modification, workflow and database support
Proven experience working with SQL and databases, preferably MySQL.
Strong knowledge and hands-on experience with Altova Flowforce Server and Altova Mapforce ETL software, or related applications.
Experience in integrating and working with EDI/API connection protocols.
Proficiency in Python and/or C# (optional but preferred).
Familiarity with data modeling concepts and techniques.
Experience with data warehousing concepts and technologies.
Excellent problem-solving and analytical skills.
Strong attention to detail and the ability to work in a fast-paced environment.
Effective communication skills to collaborate with cross-functional teams.
Ability to work independently and prioritize tasks effectively.
Preferred Skills:
Experience with cloud-based data platforms (e.g., AWS, Azure, Google Cloud Platform).
Knowledge of big data technologies (e.g., Hadoop, Spark) and distributed computing.
Familiarity with data visualization tools (e.g., Tableau, Power BI).
Experience with version control systems (e.g., Git).
Understanding of Agile development methodologies.
Summary
GEM (Green Engineered Management) is a technology solutions department that supports various commercial technology products, data management, and development for Green Worldwide Shipping. We are seeking a skilled and experienced Data Engineer to join our dynamic team. As a Data Engineer, you will be responsible for designing, developing, and maintaining our data infrastructure and various technology systems to support efficient data processing and analysis. Your expertise in SQL, CargoWise, database management, Altova Flowforce Server, Altova Mapforce ETL software, Python and C#, along with experience in EDI/API connection protocols integration, will be instrumental in achieving success in this role.
Job Type: Full-time
Pay: $60,000.00 - $75,000.00 per year
Benefits:
401(k) matching
Dental insurance
Employee discount
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Tuition reimbursement
Vision insurance
Experience level:
4 years
Schedule:
Monday to Friday
Ability to commute/relocate:
Aurora, CO 80014: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL/MySQL (Required)
ETL Software (Preferred)
Data warehouse (Preferred)
Python/C# (Preferred)
Work Location: Hybrid remote in Aurora, CO 80014","$67,500 /yr (est.)",51 to 200 Employees,Company - Private,Transportation & Logistics,Shipping & Trucking,2008,$25 to $100 million (USD)
BIGCLFY,#N/A,"Dallas, TX","W2/ 1099, No C2C - SSIS ETL Data Engineer (Minneapolis, MN/ Dallas-Irving)","No C2C. It's W2/1099 Requirement
Only taking USC, GC or H4 EAD, H1 Transfer, L2 Visa or TN Visa - they will not take OPT EAD or CPT
Need to have confirmation in writing that you will be onsite by Day 1
ALL ROLES ARE HYBRID IN THE OFFICE (2-3 DAYS/WEEK) UNLESS REMOTE IS NOTED
Please submit qualified candidates and include FULL LEGAL NAME as it appears on the passport
If your candidate is relocating, need to have confirmation in writing that they will be onsite by Day 1
Senior SSIS/ETL Data Engineer
12 months – 3 days in office
Minneapolis, MN or Dallas-Irving, TX
5+ years of experience with SQL Server 2016 (or newer) and SSIS
7+ years of data engineering
Must be capable of leading teams
2+ years of experience working with large scale SQL implementations
Experience designing, developing, and maintaining complex stored procedures, functions, and views using T-SQL
Experience designing, developing, and maintaining ETL processes using SSIS
Should understand how to unit test their product
Hands on experience with Visual Studio to document
Experience in financial or regulatory environments highly preferred
Ideal candidates will have experience creating conceptual, logical, and physical database designs
Preference given to candidates with extensive knowledge and experience with performance monitoring, database and SQL tuning
Bachelor’s or Master's in computer science or related field or equivalent work experience
Job Type: Contract
Pay: From $73.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Chandler, AZ 85225: Reliably commute or planning to relocate before starting work (Preferred)
Application Question(s):
Expected Payrate on W2/1099 Tax Term? Please mention. Thanks.
Experience:
Risk Analytics: 5 years (Preferred)
Work Location: In person",$73.00 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"DiamondPick
4.5",4.5,"Edison, NJ",Senior Data Engineer,"Hi ,
Greetings from Diamond pick inc.
We are currently looking for the below position for one of our clients... Please let me know your interest along with your updated resume ASAP..
Role:Data engineer
Location: Berkley Heights, NJ(Onsite)(locals only)
9+ years of experience is must
Description
Skills: strong Java,Azure,Spark & sql
Company Description:
Apexon is a digital-first technology services firm backed by Goldman Sachs Asset Management and Everstone Capital. We specialize in accelerating business transformation and delivering human-centric digital experiences. For over 17 years, Apexon has been meeting customers wherever they are in the digital lifecycle and helping them outperform their competition through speed and innovation.
RESPONSIBILITIES
Basic Qualifications for consideration:
5+ Overall industry experience
3+ years' experience with building large scale big data applications development
Bachelors in Computer Science or related field
Provide technical leadership in developing data solutions and building frameworks
Experience building Data Lake, data applications and micro-services using Azure, AWS and Hands-on experience in cloud Data stack (preference is Azure)
Hands-on experience with Springboot framework, MVC architecture, Hibernate, API creation, Restful services, SOAP using Java
Java experience with OOPS concepts, multithreading
Experience deploying code on containers
Hands on experience with leveraging CI/CD to rapidly build & test application code
Conduct code reviews and strive for improvement in software engineering quality
Review data architecture and develop detailed implementation design
Hands-on experience in production rollout and infrastructure configuration
Demonstrable experience of successfully delivering big data projects using Kafka, Spark
Experience working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search
Experience working with PCI Data and working with data scientists is a plus
In depth knowledge of design principles and patterns
Able to tune big data solutions to improve performance
QualificationsBachelor's Degree in Computer Science or Computer Engineering is required
Job Type: Contract
Salary: $43.96 - $70.65 per hour
Ability to commute/relocate:
Edison, NJ 08817: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL (Required)
Java (Required)
Azure (Required)
Work Location: One location",$57.30 /hr (est.),1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,#N/A,Unknown / Non-Applicable
"Spartan Technologies
3.4",3.4,"Kansas City, MO",Data Engineer I,"Spartan Technologies, Inc. - Kansas City, MO
Applicants Need to Know
100% remote in East Coast or Central US only
No Sponsorship
Contract to Hire
Hourly Rate: $45 on W2
Data Engineer I
The Data Engineer is responsible for processing structured and unstructured data, validating data quality, and developing and supporting data products for analytics. Brings together data from different sources into a common dataset with common KPIs and dimensions for stakeholders. Understands the analytics challenges that the marketing organization faces in their day-to-day work and partners with them to design viable data solutions.
Your Job
Gathers and processes raw, structured, semi-structured, and unstructured data using batch and real-time data processing frameworks.
Understand the existing source system data models and perform end to end data validation to maintain accuracy.
Implements and optimizes data solutions in enterprise data warehouses and big data repositories.
Ensures data quality and implements tools and frameworks for automating the identification of data quality issues.
Develop re-useable processes that can be leveraged and standardized for multiple instances
Works with internal and external data providers on data validation providing feedback and making customized changes to data feeds and data mappings.
Recommends improvements to processes, technology, and interfaces that improve the effectiveness of the team and reduce technical debt.
Ensure performance by monitoring existing workflows to ensure they are working correctly and that they reconcile with the source.
Design and develop ETL workflows and datasets to be used in extracts by our BI reporting tool Tableau.
Will write complex SQL queries with multiple joins to automate/manipulate these reporting extracts
Works in partnership with marketing stakeholders concerning the accuracy of data and efficiency of processes
Qualifications
Working experience with data modelling, data access, schemas, and data storage techniques within Snowflake
Working experience with batch and real-time data processing frameworks
Working experience working with relational databases such as SQL
Working experience with business intelligence tools and platforms
Working experience with data quality tools
Experience with ETL processes and tools
Desirable:
Working experience with Snowflake
Working experience with Google Analytics BigQuery
Working Experience with Tableau Server
Working experience with SQL Server Integration Services",$45.00 /hr (est.),1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$5 to $25 million (USD)
"PrizePicks
4.8",4.8,"Atlanta, GA",Data Engineer,"At PrizePicks, we are the fastest growing sports company in North America, as recognized by Inc. 5000. As the leading platform for Daily Fantasy Sports, we cover a diverse range of sports leagues, including the NFL, NBA, and Esports titles like League of Legends and CS : GO. Our team of over 200 employees thrives in an inclusive culture that values individuals from diverse backgrounds, regardless of their level of sports fandom. Ready to reimagine the DFS industry together?
Our Analytics Team is responsible for building and maintaining analytics tools and workflows to support the PrizePicks business. By developing, maintaining, and testing data generation infrastructures, you will enable the PrizePicks business to make smarter, better, and faster data-driven decisions.
What you'll do:
Create, maintain and orchestrate optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Improve and streamline data systems to drive innovation within Prizepicks
What you have:
3+ years of experience building frameworks for data ingestion pipelines but real time and batch using data modeling, ETL/ELT processes
Stellar SQL skills, experience building DBT pipelines in production and know your way around structured, semi-structured and unstructured data.
Have built and optimized 'big data' data pipelines, architectures and data sets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Experience with Python and other object oriented scripting languages
Cloud experience: AWS, EC2, EMR, RDS, Redshift, GCP, etc..
Not required but would be awesome if you have experience with any of the following data pipeline and workflow tools: Azkaban, Luigi, Airflow, Prefect, etc.
Where you'll live:
Anywhere in the US is fine (we are based in Atlanta, GA)
Benefits you'll receive:
In addition to your great compensation package, company subsidized medical/dental/vision coverage plans and matching 401(k), we'll shower you with perks including:
Break room with ping pong, endless snacks and in-office lunch once a week
Unlimited PTO to encourage a healthy work/life balance (2 week min required!)
Modern work schedule focused on getting the job done, not hours clocked
Workplace flexibility
Company and team outings, we encourage a tight-knit workplace
Generous Maternity AND Paternity leave (16 weeks!)
Annual bonus & stock options
Wellness program
Company equipment provided (Windows & Mac options)
Annual performance reviews with opportunity for growth and career development
#LI-REMOTE

You must be authorized to work for any employer in the U.S. We are unable to sponsor or take over sponsorship or an employment Visa at this time.
PrizePicks is an Equal Opportunity Employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.","$97,814 /yr (est.)",1 to 50 Employees,Company - Private,"Arts, Entertainment & Recreation",Sports & Recreation,2015,Unknown / Non-Applicable
"INTELETECH GLOBAL INC
3.7",3.7,"Altamonte Springs, FL",Data Engineer,"Role: Data Engineer
Location: Florida
Type: Contract ( Only w2 )
Visa: Any
Experience level: Mid-level

The Role: · You'll be developing, deploying, and maintaining our production data pipeline which produces risk scores and patient reports vital to the workflows of doctors and care coordinators.· You'll be ensuring product deliverables are executed reliably and accurately on a regular basis.· You'll be managing and monitoring client interfaces to ensure timely delivery of data.

Qualifications :· Bachelor's Degree in computer science, physics, math, or a related field· Minimum 3+ years experience in data engineering in industrial or clinical settings· Experience in deploying data pipelines to production, including large-scale cloud deployments· Adaptability within a dynamic and collaborative environment· Commitment to improve processes and reduce inefficiencies· Deep curiosity to dive into the details of human research studies

We also appreciate if you have familiarity with the following:

Python
SQL
Cloud platforms (e.g. AWS)
Relational and no-SQL database systems
Multi-modal and sensor data","$83,235 /yr (est.)",1 to 50 Employees,Company - Private,Financial Services,Accounting & Tax,#N/A,Unknown / Non-Applicable
"ProIT Inc.
5.0",5.0,"Bellevue, WA",Azure Data Engineer,"Data Engineering experience primarily on Spark. Someone who has worked on Azure cloud with knowledge on Azure DEVOPS (CI/CD & Infrastructure as a code), ADF, ADW & Power BI. Apache Nifi will be good to have.Power BI: Understand business requirements to set functional specifications for reporting applications Build automated reports and dashboards with the help of Power BI reporting tool Be experienced in tools and systems on MS SQL Server BI Stack, including SSRS and TSQL, Power Query, MDX, PowerBI, and DAX Be able to quickly shape data into reporting and analytics solutions Have knowledge of database fundamentals such as multidimensional database design, relational database design, and more Create functional reporting Study, analyze and understand business requirements in context to business intelligence. Design and map data models to shift raw data into meaningful insights. Utilize Power BI to build interactive and visually appealing dashboards and reports. Spot key performance indicators with apt objectives Run DAX queries and functions in Power BI Should have an edge over making DAX queries in Power BI desktop. Developing visual reports, KPI scorecards, and dashboards using Power BI desktop. Connecting data sources, importing data, and transforming data for Business intelligence.
Spark ,Azure DEVOPS (CI/CD & Infrastructure as a code), ADF, ADW & Power
Job Type: Full-time
Pay: $100,154.64 - $104,132.47 per year
Ability to commute/relocate:
Bellevue, WA 98004: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Hybrid remote in Bellevue, WA 98004","$102,144 /yr (est.)",51 to 200 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Wisetek Providers, Inc
4.2",4.2,Remote,Principle Data Engineer,"** We do transfer / Sponsor visa
Role: Principle Data Engineer Duration: 6 months.
Looking for Senior candidates with 10+ years of experience. .
Required 3-5 years of experience with the following technologies:
o Hadoop, Python, Hive, SQL, Shell scripting.
o Apache Spark.
o NoSQL and relational databases.
Required 1-3 years of experience with the following technologies:
o Scheduling tools like Airflow / Tivoli Work scheduler.
o Working in Agile/Scrum environment.
o Jenkins or similar CICD tool, GitHub.
Preferred 2-5 years of experience with the following technologies:
o Apache Kafka.
o API’s.
o Kubernetes.
Job Types: Full-time, Contract
Pay: $80,000.00 - $105,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Monday to Friday
COVID-19 considerations:
Yes
Experience:
Hadoop / Python: 5 years (Required)
Jenkins / CICD: 1 year (Required)
SQL: 5 years (Required)
Apache/ Spark: 5 years (Required)
Work Location: Remote","$92,500 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$1 to $5 million (USD)
"Bright Health
2.8",2.8,"Austin, TX",Senior Data Software Engineer,"Back to Career Site

Our Mission is to Make Healthcare Right. Together. Built upon the belief that by connecting and aligning the best local resources in healthcare delivery with the financing of care, we can deliver a superior consumer experience, lower costs, and optimized clinical outcomes.
What drives our mission? The company values we live and breathe every day. We keep it simple: Be Brave. Be Brilliant. Be Accountable. Be Inclusive. Be Collaborative.
If you share our passion for changing healthcare so all people can live healthy, brighter lives – apply to join our team.

SCOPE OF ROLE
Senior Data Software Engineers spend most of their time hands-on in the creation, feature development, and subsequent maintenance of Bright Health properties. The more senior their level, the more that individual will assist in the continuous improvement of the team's productivity and overall design of our systems. Additionally, senior talent plays a key role in building and maintaining a culture that focuses on making sure every engineer progresses in their career in line with personal goals/expectations.
ROLE RESPONSIBILITIES
The Senior Data Software Engineer description is intended to point out major responsibilities within the role, but it is not limited to these items:
Collaborate directly with teammates to solve business needs and drive solutions to solve key business priorities
Contribute across the complete development lifecycle, including technical design, work estimation, implementation, testing, and addressing feedback or bugs
Lead and contribute to continually improve the team's productivity, culture, and code quality
Peer review team member's code and provide constructive feedback to improve quality
Create documentation and processes as needed to improve team productivity and transparency
EDUCATION, TRAINING, AND PROFESSIONAL EXPERIENCE
Bachelor's degree in Computer Science, Computer Engineering, Information Systems, or equivalent experience required
Five (5) or more years of software development experience and/or a reviewable portfolio of work
Deep experience building/deploying/maintaining scalable and always available consumer-facing products
Working knowledge of Agile methodologies (SCRUM)
PROFESSIONAL COMPETENCIES
Experience with Databricks, Spark, and Scala
Experience data modeling for scalability, and efficiency while meeting business needs
Seasoned estimation skills — can reasonably judge the complexity of a feature and appropriately break it down (if needed) and size it
Strong work ethic plus follow-through on commitments (works smartly to get the job done right and on time)
Proven track record of high standards for self, team, company, and ultimately clients
Proactive approach to problem-solving (acting without being told, driving for measurable results, etc.)
WORK ENVIRONMENT
This is a remote position
We're Making Healthcare Right. Together.
We are realizing a completely different healthcare experience where payors, providers, doctors, and patients can all feel connected, aligned and unified on the same team. By eradicating the frictions of competing needs, we are making it possible to give everyone more of what they want and deserve. We do this by:
Focusing on Consumers
We understand patient pain points, eliminating complexity while increasing transparency, for greater access and easier navigation.
Building on Alignment
We integrate and align individual incentives at all levels, from financing to optimization to delivery of care.
Powered by Technology
We employ our purpose built, integrated data platform to connect clinical, financial, and social data, to deliver exceptional outcomes.

As an Equal Opportunity Employer, we welcome and employ a diverse employee group committed to meeting the needs of Bright Health, our consumers, and the communities we serve. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.","$117,199 /yr (est.)",1001 to 5000 Employees,Company - Public,Healthcare,Health Care Services & Hospitals,2015,$1 to $5 billion (USD)
"Maven Workforce
4.1",4.1,"McLean, VA",Data Engineer,"MUST HAVES: Hands-on experience with Informatica PowerCenter and/or Informatica Intelligent Cloud Services (IICS), Cloud Data Integration (CDI) , Cloud AWS,UNIX scripting, and any other ETL tools.
Job responsibilities: –
The candidates will be having the IICS module of Informatica more than any other platform modules.
Strong hands-on experience building ETL pipelines and Data Replication
3+ yrs of experience with IICS is required.
Must be hand Ons with AWS services like ECS,S3,EKS etc.
Cloud Data Integration is also required.
Data Engineering Background experience with Spark, Hive is also required.
Preferred:
SQL is preferred, but basic knowledge is required
Job Type: Contract
Salary: $60.00 per hour
Ability to commute/relocate:
Mclean, VA 22101: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica PowerCenter: 7 years (Required)
Informatica Intelligent Cloud Services (IICS): 7 years (Required)
Cloud Data Integration (CDI): 7 years (Required)
UNIX scripting: 7 years (Required)
ETL tools: 7 years (Required)
Work Location: One location
Speak with the employer
+91 7328456015",$60.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2008,Unknown / Non-Applicable
"Ripple Effect Consulting
5.0",5.0,"Denver, CO",Data Engineer - 004/005,"Data Engineer
Hybrid-remote based in Denver, CO | $80-$160k
Job Overview:
We are looking for a savvy Data Engineer to join our growing team of analytics experts. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
Responsibilities for Data Engineer
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Qualifications for Data Engineer
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:
1) Experience with big data tools
2) Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
3) Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
4) Experience with cloud services
5) Experience with stream-processing systems: Storm, Spark-Streaming, etc.
6) Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Job Type: Full-time
Pay: $80,000.00 - $160,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Denver, CO: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person","$120,000 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Codinix Technologies
5.0",5.0,"Boston, MA",Cloud Data Engineer - AWS,"Only US Citizen
Strong in warehouse technologies (Hadoop, Spark, AWS, etc.)
Bachelor's degree in Computer Science, Information Systems, or another related field
5+ years of data engineering, and cloud engineering experience
Experience in Agile project methodologies and DevOps
Experience with AWS (certification preferred; including Athena, Redshift, Glue), Cloudera
Experience building a CI/CD pipeline to deploy applications on AWS (GitLab, Terraform)
Advanced experience programming and scripting on Unix/Linux (i.e. Python, Bash)
Experience with CTRL-M, Cron (other) scheduling of batch jobs
Experience migrating legacy systems to cloud-ready architectures
Job Type: Contract
Pay: Up to $75.00 per hour
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Boston, MA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",$75.00 /hr (est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Breadboard,#N/A,"New York, NY",Data Engineer,"About Us
We’re on a mission to revolutionize the global electronics supply chain.
During the pandemic we saw how the inefficiencies in electronics manufacturing can have massive global impact. From over $200B lost in car manufacturing to delays on laptops, gaming systems and much more.
At Breadboard we are rethinking the electronics supply chain down to the finest detail. We’re designing the new operating system in which this entire system will run. We’ve already secured seed funding and are now looking to expand our engineering team to help us set the foundation for the future.
Why work with us?
As a part of our team, you will have the unique opportunity to design and implement the data platform for Breadboard, a critical component in our business.
As one of the first data engineers to join the Breadboard team you will be partnering with the CTO, our founding engineers, and will set the foundation for the design of the operating system for the electronics supply chain.
You will be involved in all aspects of the software development process, architectural decisions, recruiting, and building the organization in general.
We’re a tight-knit, intense team and you will have a ton of end to end ownership.
Work with seasoned founders who have had successful exits.
This is not only a unique opportunity for someone who wants to get their hands dirty, but also take that step back and collaborate with the executive team on setting a strategic future.
You’ll be responsible for
Data Management: Develop and automate robust data acquisition, processing, and normalization pipelines from various sources to ensure data quality and consistency.
Database and ETL Architecture: Design, implement, and optimize a database system for efficient data storage, retrieval, and analysis.
Collaboration on Machine Learning: The prospects of working with data scientists to leverage extracted data features, contributing to the development and refinement of a machine learning model for component matching.
API and UI Integration: Collaborate with the software development team to facilitate seamless integration between the database, machine learning models, and user interface.
Continuous Improvement and Compliance: Monitor system performance and user feedback for continuous improvements, while ensuring adherence to data privacy and security standards.

Requirements
Experience: A minimum of 3-5 years of experience in a data engineering role, preferably in the electronics, manufacturing industry or supply chain industry.
Technical Skills: Proficiency in SQL and experience with database design, data modeling, and performance optimization. Strong coding skills in a major programming language, such as Python or TypeScript. Experience with data processing tools and libraries (e.g., pandas, Spark). Experience with pipeline and workflow orchestration tools, such Airflow. Experience with Snowflake or similar cloud-based data warehousing solutions.
Data Acquisition and Processing: Experience with data acquisition methods, such as APIs and web scraping. Familiarity with data cleaning, normalization, and feature extraction techniques.
Machine Learning: Familiarity with machine learning concepts and algorithms, and experience collaborating with data scientists or machine learning engineers.
API Development: Experience with API development and integration, preferably in a microservices architecture.
Data Governance: Knowledge of data privacy and security regulations, and experience implementing data governance measures.
Bonus Points
Entrepreneurial mindset (we encourage all employees to be future founders and this can be a great stepping stone towards that goal).
Customer-centric and passionate about helping small businesses grow.
Previous experience at a high-growth, fast-paced startup.
Experience with our stack (and some others that may be included shortly):
React, Material UI, Typescript, CSS
NestJS, GraphQL, TypeORM, Knex.js, Objection.js, PostgreSQL, gRPC, Kafka
AWS, Docker, Kubernetes, Helm, Github Actions, CI/CD pipelines.
Compensation + Benefits
Competitive cash compensation
Stock ownership at a fast growing company
Health/dental/vision insurance—100% coverage option
The ability to push for other benefits you think are important
Our Engineering Culture
Openness to diverse opinions and backgrounds - Testing our assumptions
Supported career growth with a high emphasis on learning and mentorship
Lean development
End to end ownership and accountability. We’re all going to make mistakes. It’s about how you own up to them and learn from them.

Equal Opportunity Employer
Breadboard Software does not discriminate in employment on the basis of race, color, religion, sex (including pregnancy and gender identity), national origin, political affiliation, sexual orientation, marital status, disability, genetic information, age, membership in an employee organization, retaliation, parental status, military service, or other non-merit factor.",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Rite Pros
3.4",3.4,"Portland, ME",Big Data Engineer,"Big Data Engineer with Bachelor’s degree in Computer Science, Computer Information Systems, Information Technology, or a combination of education and experience equating to the U.S. equivalent of a Bachelor’s degree in one of the aforementioned subjects.
Job Duties:
Define and manage the data architecture framework, standards, and principles, including modeling, metadata, security, reference, and master data.
Define reference architecture, and models to provide a common vocabulary, reusable components, and industry best practices.
Create solution frameworks integrating large or complex data sets.
Lead all data modeling efforts within DataBricks, including the design of data structures and the identification of business transformation logic.
Analyzing and translating business requirements into conceptual and fully detailed logical data models.
Creating logical data models based on existing applications and databases.
Working with business architects, and data stewards to capture business requirements in a Logical Data Model.
Transform the logical representation of the model into a physical representation and work with data engineering team to instantiate and manage the data.
Contribute to assessment of appropriate data platform(s) for solutioning efforts.
Collaborate with the BI and Analytics teams on creating the optimized, reusable semantic model, complete with metadata and lineage information.
Define and govern data modeling and design standards, tools, best practices, and related development for enterprise data models
Maintain the metadata - the ""data about the data"" describing the data model, its structure, and semantics
Recommend data and design patterns for common business functionality
Assist in multi-phase roadmaps within and across functional teams
Provide insight and technology recommendations that result in enterprise solutions and architectural risk mitigation.
Contribute to innovation strategies by exploring, investigating, recommending, benchmarking and implementing new data centric technologies for the platform.
Identify, design, and implement internal process improvements such as automation of manual processes, optimization of data delivery and re-design of existing infrastructure for greater scalability.
Build the infrastructure and processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and/or other data technologies.
Create and maintain data-related documentation (e.g. data dictionaries, data maps, data models etc.)
Work with data engineering and analytics experts to strive for greater functionality in our data systems and incorporation of industry best practices
Work experience / Technologies required for the position :
Experience building and optimizing Big Data, Data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, meta data, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected data sets.
Working knowledge of message queuing, stream processing, and highly scalable Big Data, data stores.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience in a Data Engineer or similar roles, who has attained a Bachelor’s degree in Bachelor’s degree in Engineering, Computer Science, Information Technology, related field or equivalent work experience.
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Work location is Portland, ME with required travel to client locations throughout USA.

Rite Pros is an equal opportunity employer (EOE).

Please Mail Resumes to:
Rite Pros, Inc.
565 Congress St, Suite # 305
Portland, ME 04101.
Email: resumes@ritepros.com","$97,894 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Less than $1 million (USD)
"Synovize
5.0",5.0,Remote,Big Data Engineer,"Synovize is a cutting-edge technology company that specializes in providing advanced data analytics and intelligence solutions. Our mission is to empower businesses with the tools and insights they need to make data-driven decisions and drive innovation. We are committed to delivering exceptional results by combining our expertise in big data technologies with our passion for creating impactful solutions.
As a Big Data Engineer at Synovize, you will have the opportunity to work on exciting projects that involve designing and implementing scalable data solutions. You will collaborate with a talented team of professionals, including software developers, data scientists, and domain experts, to tackle complex data engineering challenges and deliver high-quality solutions to our clients.
Job Title:
Big Data Engineer
Company: Synovize
Number of Positions: 2
Location: Remote
Length: 12 Months
Tax Terms: 1099/C2C or W-2
Rate/Range: $70-$90/hr
Work Authorization: US Citizen
Security Clearance: None
Skills Needed:
Proficiency in Java and Kotlin programming languages.
Strong knowledge of AWS (Amazon Web Services) and its big data offerings.
Experience with BigData technologies, frameworks, and tools.
Familiarity with Kafka for streaming data processing.
Proficiency in Python programming language.
Responsibilities:
Design and develop scalable data pipelines to process, integrate, and analyze large volumes of structured and unstructured data.
Implement data ingestion, transformation, and storage mechanisms using big data technologies and frameworks.
Collaborate with data scientists and analysts to understand data requirements and optimize data models for analytics and machine learning.
Utilize AWS services and tools to build and maintain data infrastructure, ensuring data quality, integrity, and security.
Troubleshoot and resolve data-related issues, optimizing performance and efficiency.
Stay up-to-date with emerging trends and advancements in big data technologies, recommending and implementing improvements to existing systems and processes.
Requirements:
Bachelor's or master's degree in computer science, engineering, or a related field.
Proven experience as a Big Data Engineer or similar role, with a strong understanding of big data technologies and architectures.
Proficiency in Java and Kotlin programming languages.
Strong knowledge of AWS and its big data offerings.
Experience with BigData technologies, frameworks, and tools.
Familiarity with Kafka for streaming data processing.
Proficiency in Python programming language.
Strong problem-solving skills and ability to work in a fast-paced, collaborative environment.
Excellent communication and interpersonal skills.
Join Synovize and be part of a dynamic team that is at the forefront of data innovation. Together, we will drive meaningful change and unlock the full potential of data-driven decision-making.
Job Types: Full-time, Contract
Pay: $70.00 - $90.00 per hour
Benefits:
401(k)
Dental insurance
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Vision insurance
Schedule:
10 hour shift
4 hour shift
8 hour shift
Choose your own hours
Application Question(s):
Are you a US Citizen?
Years of experience in Java?
Years of experience in Kotlin?
Years of experience in big data?
Years of experience in AWS?
Security Clearance
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$80.00 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Green Worldwide Shipping LLC
3.7",3.7,"Aurora, CO",GEM Data Engineer,"About Green
“Global Reach with a personal touch”, that’s the foundation on which four partners (and long-time friends) agreed to when they started Green Worldwide Shipping back in 2008. This ethos has allowed Green to Grow to a 200+ strong team with 15 offices across the US and growing.At green we are a family of innovative problem-solvers who communicate freely to present creative solutions in an environment of trust, respect, and integrity. We are dedicated to customer service, passionate about logistics, and committed to sustainable growth while inspiring others.
Why this role is special..
GEM (Green Engineered Management) is a technology solutions department that supports various commercial technology products, data management, and development for Green Worldwide Shipping.We are seeking a skilled and experienced Data Engineer to join our dynamic team. As a Data Engineer, you will be responsible for designing, developing, and maintaining our data infrastructure and various technology systems to support efficient data processing and analysis. Your expertise in SQL, CargoWise, database management, Altova Flowforce Server, Altova Mapforce ETL software, Python and C#, along with experience in EDI/API connection protocols integration, will be instrumental in achieving success in this role.
How you will be successful…
Design, develop, and maintain data pipelines and ETL processes to extract, transform, and load data from various sources into our databases using Altova Mapforce ETL software.
Collaborate with cross-functional teams to understand data requirements and provide optimal data solutions.
Develop and optimize SQL queries to retrieve and manipulate data from databases, ensuring efficient data retrieval and storage.
Implement and integrate EDI/API connection protocols to enable seamless data exchange between internal and external systems.
Monitor and maintain the performance, availability, and integrity of the databases and data infrastructure.
Support for reporting, document modification, workflow and database support for internal ERP.
Troubleshoot and resolve data-related issues, including data quality and data integration problems.
Implement data security measures and ensure compliance with data privacy regulations.
Stay updated with industry best practices and emerging technologies in data engineering and incorporate them into the data infrastructure.
Provide technical guidance and support to other team members on data-related projects.
Collaborate with data analysts and data scientists to understand data requirements and support their data exploration and analysis needs.
Create and maintain documentation related to data pipelines, database schemas, and data transformation processes.
What you will bring….
Bachelor's degree in Computer Science, Information Systems, or a related field.
Preferred experience in CargoWise (or related ERP) with reporting, document modification, workflow and database support
Proven experience working with SQL and databases, preferably MySQL.
Prefer strong knowledge and hands-on experience with Altova Flowforce Server and Altova Mapforce ETL software, or related applications.
Experience in integrating and working with EDI/API connection protocols.
Proficiency in Python and/or C# (optional but preferred).
Familiarity with data modeling concepts and techniques.
Experience with data warehousing concepts and technologies.
Excellent problem-solving and analytical skills.
Strong attention to detail and the ability to work in a fast-paced environment.
Effective communication skills to collaborate with cross-functional teams.
Ability to work independently and prioritize tasks effectively.
Preferred Skills:
*
Experience with cloud-based data platforms (e.g., AWS, Azure, Google Cloud Platform).
Knowledge of big data technologies (e.g., Hadoop, Spark) and distributed computing.
Familiarity with data visualization tools (e.g., Tableau, Power BI).
Experience with version control systems (e.g., Git).
Understanding of Agile development methodologies.
What we offer:
Employee High Deductible Health Plan (HDHP) currently paid for 100% by Green.
$1,800 contributed to employee HSA Account, if on HDHP.
Employee Dental provided by Green.
Option to purchase disability insurance.
Paid life insurance and option to buy-up.
401(K) and company match.
Parental Leave Program.
Monthly recognition points to give/receive, which can be converted to gift cards.
Tuition Reimbursement Program.
Weekly fruit baskets in office locations
A tree planted for every new employee to Trees.org
Hybrid work schedule.
Wellbeing Program.
Training and Development.
Access to Mentorship Program.
Green Worldwide Shipping, LLC (Green) is committed to the full inclusion of all qualified individuals. As part of this commitment, Green will ensure that persons with disabilities are provided reasonable accommodations. If reasonable accommodation is needed to participate in the job application or interview process and/or to perform essential job functions, please contact Faye Hughes, Head of People and Culture at faye.hughes@greenworldwide.com.Green is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.
Requirements
Bachelor's degree in Computer Science, Information Systems, or a related field.
Preferred experience in CargoWise (or related ERP) with reporting, document modification, workflow and database support
Proven experience working with SQL and databases, preferably MySQL.
Strong knowledge and hands-on experience with Altova Flowforce Server and Altova Mapforce ETL software, or related applications.
Experience in integrating and working with EDI/API connection protocols.
Proficiency in Python and/or C# (optional but preferred).
Familiarity with data modeling concepts and techniques.
Experience with data warehousing concepts and technologies.
Excellent problem-solving and analytical skills.
Strong attention to detail and the ability to work in a fast-paced environment.
Effective communication skills to collaborate with cross-functional teams.
Ability to work independently and prioritize tasks effectively.
Preferred Skills:
Experience with cloud-based data platforms (e.g., AWS, Azure, Google Cloud Platform).
Knowledge of big data technologies (e.g., Hadoop, Spark) and distributed computing.
Familiarity with data visualization tools (e.g., Tableau, Power BI).
Experience with version control systems (e.g., Git).
Understanding of Agile development methodologies.
Summary
GEM (Green Engineered Management) is a technology solutions department that supports various commercial technology products, data management, and development for Green Worldwide Shipping. We are seeking a skilled and experienced Data Engineer to join our dynamic team. As a Data Engineer, you will be responsible for designing, developing, and maintaining our data infrastructure and various technology systems to support efficient data processing and analysis. Your expertise in SQL, CargoWise, database management, Altova Flowforce Server, Altova Mapforce ETL software, Python and C#, along with experience in EDI/API connection protocols integration, will be instrumental in achieving success in this role.
Job Type: Full-time
Pay: $60,000.00 - $75,000.00 per year
Benefits:
401(k) matching
Dental insurance
Employee discount
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Tuition reimbursement
Vision insurance
Experience level:
4 years
Schedule:
Monday to Friday
Ability to commute/relocate:
Aurora, CO 80014: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL/MySQL (Required)
ETL Software (Preferred)
Data warehouse (Preferred)
Python/C# (Preferred)
Work Location: Hybrid remote in Aurora, CO 80014","$67,500 /yr (est.)",51 to 200 Employees,Company - Private,Transportation & Logistics,Shipping & Trucking,2008,$25 to $100 million (USD)
"DiamondPick
4.5",4.5,"Edison, NJ",Senior Data Engineer,"Hi ,
Greetings from Diamond pick inc.
We are currently looking for the below position for one of our clients... Please let me know your interest along with your updated resume ASAP..
Role:Data engineer
Location: Berkley Heights, NJ(Onsite)(locals only)
9+ years of experience is must
Description
Skills: strong Java,Azure,Spark & sql
Company Description:
Apexon is a digital-first technology services firm backed by Goldman Sachs Asset Management and Everstone Capital. We specialize in accelerating business transformation and delivering human-centric digital experiences. For over 17 years, Apexon has been meeting customers wherever they are in the digital lifecycle and helping them outperform their competition through speed and innovation.
RESPONSIBILITIES
Basic Qualifications for consideration:
5+ Overall industry experience
3+ years' experience with building large scale big data applications development
Bachelors in Computer Science or related field
Provide technical leadership in developing data solutions and building frameworks
Experience building Data Lake, data applications and micro-services using Azure, AWS and Hands-on experience in cloud Data stack (preference is Azure)
Hands-on experience with Springboot framework, MVC architecture, Hibernate, API creation, Restful services, SOAP using Java
Java experience with OOPS concepts, multithreading
Experience deploying code on containers
Hands on experience with leveraging CI/CD to rapidly build & test application code
Conduct code reviews and strive for improvement in software engineering quality
Review data architecture and develop detailed implementation design
Hands-on experience in production rollout and infrastructure configuration
Demonstrable experience of successfully delivering big data projects using Kafka, Spark
Experience working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search
Experience working with PCI Data and working with data scientists is a plus
In depth knowledge of design principles and patterns
Able to tune big data solutions to improve performance
QualificationsBachelor's Degree in Computer Science or Computer Engineering is required
Job Type: Contract
Salary: $43.96 - $70.65 per hour
Ability to commute/relocate:
Edison, NJ 08817: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL (Required)
Java (Required)
Azure (Required)
Work Location: One location",$57.30 /hr (est.),1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,#N/A,Unknown / Non-Applicable
"Global Enterprise Services, LLC
4.0",4.0,"Atlanta, GA",Data Harmonization Engineer,"Data Harmonization Engineer

Collects data models for disparate datasets within a domain. Collaborates with CDC & STLT stakeholders to develop a conceptual model that defines the atomic concepts for that domain.
Constructs semantic mappings from the data elements in the source datasets to atomic or compound concepts in the conceptual model.
Develops sample transformation code to translate among representations.
Configures and manages vocabulary services to allow data modernization users to improve understanding of the relationships among data representations.

Data Harmonization Engineer - Junior [YoE, Edu, Certs: 0-3 yrs & BA/BS] [Salary Range: $81k - $91k]
Data Harmonization Engineer - Journeyman [YoE, Edu, Certs: 4-9 yrs & BA/BS] [Salary Range: $95k - $105k]
Data Harmonization Engineer - Senior [YoE, Edu, Certs: 10+ yrs & MA/MS] [Salary Range: $108k - $108k]","$100,000 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Provista Software Corporation,#N/A,Remote,Data Engineer,"As a successful candidate, you’ll play a pivotal role in the data conversion and business intelligence solution efforts during project implementation for our state and local clients. We are looking for you to join our software implementation team as a Senior Data Engineer.
Qualification:
5+ years of experience working independently as ETL developer on data migration projects that covered diverse database platforms and complex data conversions.
Must have experience across leading Database Platforms especially Oracle, Azure SQL, Snowflake.
Expert level experience with Extraction Transform Load (ETL) plan, protocols, and tools.
Expert level experience with programming languages SQL, Phyton, Java, C#.
Good at communication with clients and internally.
Expert at problem solving and troubleshooting skills.
Bachelor's degree, preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience
Must be a U.S. Citizen or Green Card Holder.
Functional Responsibility:
Data Conversion -
In collaboration with Data Stewards, discover inventory of legacy data sources and all data the client expects to be migrated to destination Modified Off the Shelf (MOTS) solution.
Complete gap assessment between legacy data and MOTS data requirements, and options to address the gaps.
Develop data migration map to minimally include data dictionary, detailed data map of all elements of the current database, data transformation rules, relationship rules, validation rules, process of migrating images, and process for recovering missing or erroneous data.
Identify issues, risks, and/or barriers that may interfere with data migration and recommendations to resolve or mitigate.
Conduct testing and quality assurance to ensure the data is migrated accurately.
Migrate data from legacy to destination MOTS solution progressively using a sample set of data and the entirety of the data prior to deployment into relevant go-live environments (e.g., staging, production, etc.)
Prepare data migration reports that includes – record counts of source vs migrated vs modified; field level validation of data at row and column; discovered exceptions.
Business Intelligence -
Develop ETL pipelines in and out of Snowflake data warehouse using combination of Python, SnowSQL, and SQL.
In collaboration with Data Stewards, create Snowflake data mart as curated subset of data to support the analytics and business intelligence needs of subgroup of users.
Translate requirements for business intelligence and reporting to database design and reporting design.
Conduct discovery sessions with users to capture business intelligence reports.
Develop Tableau self-service ad hoc reports to support operational users.
Develop Tableau Dashboards to support executive, management, and operational users.
About us:
Provista Software Corporation is fast growing Software Solution provider for US State and Local Government Clients.
Job Type: Full-time
Pay: $67,107.98 - $134,285.66 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Life insurance
Paid time off
Parental leave
Professional development assistance
Vision insurance
Compensation package:
Yearly pay
Experience level:
5 years
Schedule:
8 hour shift
Application Question(s):
What is your most preferred work time zone ?
Experience:
ETL: 5 years (Required)
Data warehouse: 1 year (Preferred)
Tableau: 1 year (Preferred)
Talend: 1 year (Preferred)
Microsoft SQL Server: 5 years (Required)
Work Location: Remote","$100,697 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Go Intellects Inc,#N/A,"Washington, DC",Data Engineer,"Work Location: REMOTE (1 – 2 Days On-site/Week may require)
Required Skills:
· Collect, manage, and convert raw data accurately and reliably
· Organize data systems for subgroup access and analyses
· Configure and sustain data cloud structures
· Must have expertise in Data Visualization Tools (Tableau)
· Data Modeling/Science as Python/SAS
· Should have AWS cloud native services, security, data pipeline
· Able to work with structured and unstructured data.
· Validate outputs of data pipelines
· Degree in Data Engineering preferred.
Job Type: Full-time
Pay: Up to $150,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Application Question(s):
Do you have, ""Active Secret (or) Top Secret Security Clearance""?
Work Location: In person","$150,000 /yr (est.)",Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Zenotis Technologies inc,#N/A,"Addison, TX",Big Data Engineer,"Role: Big Data Engineer
Location – Addison, TX/ Charlotte, NC / Chandler, AZ (Hybrid – 3 days onsite 2 days remote)
Position Summary:
Looking for a big data engineer who loves solving complex problems across a full spectrum of technologies. The ideal candidate is excited by experimentation and looking for a new challenge that stretches their talents. The big data engineer will help ensure that our technological infrastructure operates seamlessly in support of business objectives.
The engineer must be able to:
· Translate complex technical and functional requirements into detailed designs
· Guide the development and testing teams in developing and implement pipelines that extract, transform, and load data into an information product that helps the organization reach its strategic goals
· Focus on ingesting, storing, processing, and analyzing large datasets
· Deep understanding on how to create scalable, high-performance web services for tracking data
· Investigate alternatives for data storing and processing to ensure the implementation of the most streamlined solutions
Primary Skill
Hadoop
Secondary Skill
Kafka
Tertiary Skill
Oracle Exadata
Required Skills
· Translate complex technical and functional requirements into detailed designs
· Design data pipelines using design tools and provide solution intent in provided templates
· Work closely with data science team to implement data analytics pipelines
· Analyze vast number of data stores to uncover insights
· Define data governance policies and support data-versioning processes
· Maintain security and data privacy, working closely with data protection guidelines.
· Desired Skills
· Experience with Python, Spark, and Hive
· Understanding of data-warehousing and data-modeling techniques
· Knowledge of industry-wide visualization and analytics tools (ex: Tableau, R)
· Strong data engineering skills with Hadoop platform
· Experience with streaming frameworks such as Kafka
· Knowledge of Core Java, Linux, SQL, and any scripting language
· Good interpersonal skills and positive attitude
Job Type: Full-time
Salary: Up to $70.00 per hour
Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
8 hour shift
Work Location: On the road",$70.00 /hr (est.),201 to 500 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
IT Customer Service - POST,#N/A,"Newark, NJ",Data Center Plant Engineer,"Job Title: ISP-OSP Data Center Plant Engineer
Location: Newark NJ, (M-F)
Travel Required: As Needed
Overview
UC-ISP/OSP engineer is responsible for cable management including structured cabling, carrier data and voice circuit cables/equipment, rack-and-stack, and oversight of power, backup power, network equipment, and environmental needs to cabinets. Leveraging Clients Data Center Infrastructure Management System, (DCIM) tool to maintain, report and monitor.
Conducting capacity assessments of existing infrastructure to ensure that it can support future growth to ensure compliance with federal regulations of the Data Center infrastructure Standards. Projects include design and build outs of the physical plant within the typical WAN/LAN space within data centers, telco rooms, MDFs and IDFs as needed.
Functions:
Perform initial engineering surveys based on project needs.
Document engineering surveys to determine feasibility and the estimated cost.
Obtain authorization to proceed and confirm funding.
Create detailed engineering drawings of build, floor plans, equipment elevation drawings, structured cabling diagrams, power, and environmental needs.
Designs must conform to state, local, national, and Client standards and codes, and the NJ historical and or environmental commissions approvals if needed.
Based on the engineering documents create a SOW (Scope of Work) and BOM (Bill of Materials)
Submit to Client manager to approve and proceed when funding is validated.
Create a construction package and submit to contractors or in house for RFQ.
Review quotes and choose both low bid and qualified contractor.
Work with all stakeholders to date and time work schedules.
Confirm workers are safety trained and conform to Client, FRA, and OSHA requirements.
Supervise and manage installations and conduct inspections.
Review work to ensure contractor met obligations under terms of contract.
Inform Client managers of status to approve invoices.
Coordinate with Client network engineering to connect, configure, and test equipment.
Review and confirm a successful installation were completed and working as designed prior to cutting over to production.
Assist with Break fix of IT infrastructure troubles or repairs.
Daily travel to location depending on scheduled or unscheduled work
Job Types: Full-time, Contract
Experience level:
5 years
Schedule:
8 hour shift
Work setting:
In-person
Experience:
Data Center: 5 years (Required)
WAN/LAN: 3 years (Required)
cabling: 3 years (Required)
Work Location: One location",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Purple Drive Technologies
4.2",4.2,"Cupertino, CA",Data Engineer,"Terraform along with synapse Azure cloud
Deep expertise in Data Engineering and Data Warehousing (minimum 4+ years)
Azure synapse , Azure Data Factory , Spark Pool , SQL , SQL Pool (minimum 4+ yrs)
CI/CD and Python/Java programming experience (minimum 3+ years)
Ideal to have a 24 x 7 development i.e offshore presence or different time zones
Job Type: Full-time
Salary: $115,871.32 - $207,675.38 per year
Ability to commute/relocate:
Cupertino, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$161,773 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"MetroStar
3.7",3.7,United States,Data Engineer (Mid),"As a Data Engineer (Mid), you'll bring creative architect solutions to end customers with the goal to make an impact across the federal government.
We know that you can't have great technology services without amazing people. At MetroStar, we are obsessed with our people and have led a two-decade legacy of building the best and brightest teams. Because we know our future relies on our deep understanding and relentless focus on our people, we live by our mission: A passion for our people. Value for our customers.
If you think you can see yourself delivering our mission and pursuing our goals with us, then check out the job description below!
What you'll do:
Work with AI team members to operationalize data pipelines and ML tasks.
Provide day-to-day support of deploying Python-native ML pipelines and perform data engineering tasks to enable AI/ML capabilities.
Present results to a diverse audience in presentation or report form.
Support architectural leadership, technical support, and advisement services to ensure identity management system technologies are integrated and meeting the appropriate security requirements.
Support leadership who engage with senior level executives at a public facing Federal agency and provide subject matter expertise in security architecture and other key domain areas.
What you'll need to succeed:
5+ years of experience in Data/ML engineering (if school experience is used, at most that would contribute to 2 years of actual experience).
Experience with ETL, Data Labeling and Data Prep.
Experience designing, implementing, and maintaining data architecture and services to be used for AI/ML. Additionally, operationalizing and maintaining AI/ML models in production.
The ability to perform data analytics on program related or system related activities. This will include assessing performance and manual processes implementing methods/algorithms to automate/optimize
A bachelor's degree in Computer Science, Information Technology Management or Engineering, or other comparable degree or experience and the ability to obtain and maintain DHS Suitability.

Like we said, we are obsessed with our people. That's why we offer a generous benefits package, professional growth, and valuable time to recharge. Learn more about our company culture code and benefits. Plus, check out our accolades.
Don't meet every single requirement?
Studies have shown that women, people of color and the LGBTQ+ community are less likely to apply to jobs unless they meet every single qualification. At MetroStar we are dedicated to building a diverse, inclusive, and authentic culture, so, if you're excited about this role, but your previous experience doesn't align perfectly with every qualification in the job description, we encourage you to go ahead and apply. We pride ourselves on making great matches, and you may be the perfect match for this role or another one we have. Best of luck! – The MetroStar People & Culture Team
What we want you to know:
In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification form upon hire.
MetroStar Systems is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. The statements herein are intended to describe the general nature and level of work being performed by employees and are not to be construed as an exhaustive list of responsibilities, duties, and skills required of personnel so classified. Furthermore, they do not establish a contract for employment and are subject to change at the discretion of MetroStar Systems.
Not ready to apply now?
Sign up to join our newsletter here.",#N/A,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1999,$25 to $100 million (USD)
Arcon Group Inc,#N/A,"Mooresville, NC",Data Engineer,"Job Title: Dot NET Full Stack
Location: Charlotte, NC (Day 1 Onsite)
Duration: 12+ Months
Client: Wells Fargo
only on W2
Interview Mode: Phone & TEAMS
Note: We are looking only at OPT & H1-B
Minimum 3-5 years of experience with C#, .NET
Familiarity with the ASP.NET framework, SQL Server, and design/architectural patterns (e.g. Model-View-Controller (MVC))Experienced in implementing niche solutions with C# and .NET
Abundant experience in designing and writing reusable code with C# and .NET
Experienced with SQL/Oracle/Linux/Windows Servers
Work experience with Oracle, SQL, MySQL Database
Good to Have
Familiarity with Any Cloud Functions
C++/Java/Perl
Power Shell script
SAFE Agile Development
Job Type: Full-time
Salary: $45.00 - $55.00 per hour
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Mooresville, NC 28115: Reliably commute or planning to relocate before starting work (Required)
Education:
Master's (Preferred)
Experience:
Informatica: 1 year (Preferred)
SQL: 2 years (Preferred)
ASP.NET: 1 year (Preferred)
License/Certification:
Driver's License (Preferred)
Work Location: One location",$50.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Schober Consulting LLC,#N/A,"Minneapolis, MN",Data Engineer (Junior),"We are seeking a talented and experienced Data Engineer to join our team. The ideal candidate will have 2-3 years of professional experience in data engineering, with a bonus if they have worked in the real estate industry. Additionally, an interest in machine learning is highly desirable.
Responsibilities:
Design, develop, and maintain scalable and efficient data pipelines and ETL processes
Build and optimize data models and data storage solutions for large-scale datasets
Collaborate with cross-functional teams, including data scientists and analysts, to gather requirements and design data-driven solutions
Develop and maintain data infrastructure to support advanced analytics and machine learning initiatives
Ensure data quality and integrity by implementing data validation and cleansing processes
Continuously monitor and optimize data pipelines and systems for performance and reliability
Stay updated with the latest industry trends and technologies in data engineering and machine learning
Qualifications:
Bachelor's or Master's degree in Computer Science, Data Engineering, or a related field
2-3 years of professional experience as a Data Engineer, preferably with exposure to the real estate industry
Strong understanding of data engineering principles, ETL processes, and data warehousing concepts
Proficiency in SQL and experience with relational databases
Solid programming skills in Python, Java, or other relevant languages
Familiarity with cloud computing platforms, preferably AWS or Azure
Experience with data modeling and designing efficient database schemas
Knowledge of machine learning concepts and a strong interest in applying machine learning to data engineering tasks
Excellent problem-solving and analytical abilities
Strong communication and collaboration skills
If you are a self-motivated individual with a passion for data engineering and an interest in the real estate industry and machine learning, we encourage you to apply. We offer a competitive salary, comprehensive benefits package, and a collaborative work environment that fosters professional growth and development.
Job Type: Full-time
Pay: $70,000.00 - $100,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Compensation package:
Employee stock ownership plan
Experience level:
2 years
Schedule:
Monday to Friday
Ability to commute/relocate:
Minneapolis, MN 55402: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
SQL (Preferred)
Data warehouse (Preferred)
Python (Required)
Work Location: Hybrid remote in Minneapolis, MN 55402","$85,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"WorkCog
4.3",4.3,"Atlanta, GA",Sr. AWS Data Engineer,"Position: AWS Data Engineer
Location: Atlanta ,GA
Duration: 12+ Months
Interview: Video
JD
Responsibilities
Bachelor’s degree in Computer Science, Software Engineering, or other related field, or equivalent work experience.
Extensive experience in data modeling, writing & performance tuning complex SQL queries & user-defined functions
Hands-on experience in design, implementation and orchestration of data pipelines and ETL batch Jobs.
Hands-on experience on all aspects of data handling: cleansing and standardization, normalization, merging, change data capture, data integration, reconciliation, and transformations
Strong experience in working with Big Data technologies such as Hive/Spark, Data lake and Data warehouse using GCP and/or AWS Services
Ability to write robust code in scripting languages like Python, Scala, PySpark or similar.
Good to Have
Associate Architect or equivalent cloud certification
Experience with processing large datasets using AWS and GCP UDFs (User defined functions)
Experience in ML probability & statistics algorithms, frameworks like Keras or PyTorch and tools like Jupyter notebooks.
Job Type: Contract
Pay: $45.83 - $52.42 per hour
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Atlanta, GA: Reliably commute or planning to relocate before starting work (Preferred)
Experience:
SQL (Preferred)
Informatica (Preferred)
Data warehouse (Preferred)
Work Location: In person",$49.13 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable
"Wallero
4.4",4.4,"Seattle, WA",Data Engineer,"Hi,
Pretty much interested in your resume, we have an urgent opening for Data Engineer role with one of our clients based in Seattle WA.
Position: Data Engineer
Location: Seattle WA - Remote (100%)
Description:
Good Experience in reporting space including PowerBI, ADF, and possibly standing up a data warehouse/data mart.
The main scope centering is around reporting.
Experience with SQL, and agile framework including requirement gathering and story/feature writing skills.
Need someone strong who is comfortable leading and guiding teams in that capacity.
Migrating data from third party to Azure Cloud
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Seattle, WA 98101: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",$75.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2015,$1 to $5 million (USD)
"INTELETECH GLOBAL INC
3.7",3.7,"Altamonte Springs, FL",Data Engineer,"Role: Data Engineer
Location: Florida
Type: Contract ( Only w2 )
Visa: Any
Experience level: Mid-level

The Role: · You'll be developing, deploying, and maintaining our production data pipeline which produces risk scores and patient reports vital to the workflows of doctors and care coordinators.· You'll be ensuring product deliverables are executed reliably and accurately on a regular basis.· You'll be managing and monitoring client interfaces to ensure timely delivery of data.

Qualifications :· Bachelor's Degree in computer science, physics, math, or a related field· Minimum 3+ years experience in data engineering in industrial or clinical settings· Experience in deploying data pipelines to production, including large-scale cloud deployments· Adaptability within a dynamic and collaborative environment· Commitment to improve processes and reduce inefficiencies· Deep curiosity to dive into the details of human research studies

We also appreciate if you have familiarity with the following:

Python
SQL
Cloud platforms (e.g. AWS)
Relational and no-SQL database systems
Multi-modal and sensor data","$83,235 /yr (est.)",1 to 50 Employees,Company - Private,Financial Services,Accounting & Tax,#N/A,Unknown / Non-Applicable
"Lcp Tracker Inc
4.4",4.4,"New Braunfels, TX",Data Engineer,"Company Summary
LCPtracker, Inc. is a leading software service provider specializing in construction site compliance related software, headquartered in Orange, CA. Our main solution, LCPtracker Pro, is a powerful web-based SaaS solution for collecting, verifying, and managing certified payrolls and other labor compliance related documents. Over 200 government agencies and 100,000 contractors have used LCPtracker for their certified payroll reporting.
In 2023, our growth continues at a rapid pace, making LCPtracker one of the fastest growing small companies in Orange County, California, recognized by the Orange County Business Journal. In 2017, 2018, 2019, 2020, 2021 and 2022, LCPtracker was recognized as an Orange County ""Best Places to Work"" by the Orange County Register.

Position Summary
As a Data Engineer at LCPtracker, you will leverage your experience with SQL, ETL, and reporting to drive the design and development of data-driven solutions. The position is responsible for performing advanced technical and analytical work in the development and support of standardized and customized reports, as well as the testing and maintaining of data integrity.
Other Duties: Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.

Core Competencies
Confidentiality: This role may be privy to confidential and/or sensitive information. Must demonstrate integrity in maintaining confidential and sensitive information and demonstrate strict adherence to organizational policies and procedures.
Communication Proficiency: uses friendly and proficient communication to interact with a wide range of people, frequently exchanging information about office operations.
Time Management: Must manage their own time. They use an electronic calendar in an email program to set meetings, to request others to attend and to coordinate their responses. They respond to requests for attendance at various meetings.
Initiative and Proactivity: Correctly anticipates a need, volunteers readily, and acts without being told to do so. Brings new ideas to the company. Undertakes self-development activities; seeks increased responsibilities; takes calculated risks; looks for and takes advantage of opportunities; asks for and offers help when needed.
Drive for Results: Is goal-oriented; maintains focus on the objective.
Problem Solving, Personal Judgment: Identifies and resolves problems in a timely manner; gathers and analyzes information skillfully; develops alternative solutions; works well in group problem-solving situations; uses reason even when dealing with emotional topics. Solicits and applies feedback.
Quality Management: Looks for ways to improve and promote quality; demonstrates accuracy and thoroughness. Does not cut corners; monitors work to ensure quality; applies feedback to improve performance.
Primary Duties and Responsibilities
Develop, modify, maintain, and support custom reports (MS SQL, SSRS) for both ad-hoc and ongoing business needs.
Develop MS SQL objects (tables, stored procedures, functions, views, etc.) as applicable.
Create and customize weekly, monthly, quarterly, and annual reports using Microsoft Excel or other reporting tools as applicable.
Ensure high data quality through regular quality checks.
Extract, filter, and aggregate data through logical queries and programming.
Maintain a high level of confidentiality and use discretion when needed.
Perform other work including specific tasks or special projects as required.
Promote and maintain positive morale through teamwork.
Other Duties: Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.

Salary Range
Data Engineer rate $100,000 to $140,000 annual salary
Pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, and experience. The total compensation package for this position may also include other elements dependent on the position offered. Details of participation in these benefit plans will be provided if an employee receives an offer of employment.

Benefits
Paid Time Off
9 Paid Holidays
Phantom Stock
401k Plan with up to 4% company match
Medical Benefits (Health, Vision and Dental)
Life Insurance
LTD & STD

Work Environment
This position performs its duties from our New Braunfels, TX office. This position operates in a professional office environment and role routinely uses standard office equipment such as computers, phones, mobile devices, photocopiers, filing cabinets and fax machines.

Physical Requirements
While performing the functions of this job, the employee is regularly required to sit; frequently required to talk and hear, use hands and fingers to type, scroll and use computer equipment. The employee is required to have close visual acuity to perform an activity such as: preparing and analyzing data and figures; transcribing; viewing a computer terminal; extensive reading; visual inspection of text/data in both print and electronic forms.
Ability to lift and move up to 25 pounds.
Position Type and Expected Hours of Work
This is a full-time exempt position reporting to our New Braunfels, TX office M-F 8am – 5pm. Days/hours worked are dependent on the workload at the time. General availability and presence in the office is expected during regular business hours Monday-Friday. However, some flexibility is allowed. Occasional evening and weekend work may be required as job duties demand.

Travel
There is no major travel requirement for this position. However, infrequent travel may be necessary to visit remote office(s), attend conferences/industry events, etc. Attendance at our corporate Staff Retreat is required. This event is a 2-3-day retreat. Attendance at our annual User Conference as assigned.

LCPtracker, Inc. is an equal opportunity employer of all qualified individuals. All applicants will be afforded equal opportunity without discrimination because of race, color, religion, sex, sexual orientation, marital status, order of protection status, national origin or ancestry, citizenship status, age, physical or mental disability unrelated to ability, military status or an unfavorable discharge from military service. LCPtracker, Inc. will consider for employment qualified applicants with criminal histories in a manner consistent with all federal, state, and local ordinances.
LCPtracker is committed to the full inclusion of all qualified individuals. In keeping with our commitment, LCPtracker will take steps to assure that people with disabilities are provided reasonable accommodations. Accordingly, if reasonable accommodation is required to fully participate in the job application or interview process, to perform the essential functions of the position, and/or to receive all other benefits and privileges of employment, please contact the LCPtracker Human Resources Department at HR@lcptracker.com.
Education and Experience
MUST HAVE:
Bachelor’s degree in software engineering disciplines, computer science or other related field and/or the equivalent combination of education and experience.
7+ years of experience with SSRS reporting tools and MS SQL server.
7+ years of experience using ETL tools such as SSIS and/or ADF.
7+ years of experience in Data Warehousing with SSAS or AAS.
Experience in applying security to SSAS or AAS models using authentication frameworks such as AAD or Active Directory.
Adept at queries, report writing and presenting findings.
Ability to comprehend, analyze, and systematically compile technical, statistical, and information into comprehensive reports or other formats.
Effective business writing and composition skills with good command of the English language.
Ability to independently plan, organize, and complete a variety of projects within established standards, objectives and time frames.
Ability to work in fast-paced, multi-tasking environment with shifting priorities and demanding deadlines.
Ability to work independently in finding solutions
Ability to work in an agile work environment
Ability to work in a team environment
Must be detailed-oriented and able to effectively prioritize and organize workload, with efficient time management.
Minimum 1-year experience working on Scrum teams.
Basic understanding of the Agile methodology.
Scrum certifications are a plus.
Strong interpersonal communication skills.","$120,000 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,1992,$1 to $5 million (USD)
Dobbs Defense Solutions,#N/A,"Fort Meade, MD",Data/NiFi Engineer,"Our Mission
At Dobbs Defense, we deliver mission-centric IT, Cyber, and data analytics solutions for our government and commercial clients through the convergence of automation, innovation, training, and education. Delivering high-quality IT, cybersecurity, and data analytics solutions through proven and innovative methods is our vision.

Job Description
Dobbs Defense Solutions is seeking a Senior Data/NiFi Engineer to design, develop, implement, test, secure, and support data transport and brokering for multiple classified networks and between networks.

Duties
Design, develop, implement, test, secure, and support data transport and brokering.
Data flow engineering for one of DISA’s premier boundary defense systems.
Perform system administration of Niagara Files (NiFi) on Linux VMs.
Support backup/failover/load balancing of data brokering to improve availability.
Monitor availability & amp, usage.
Respond to outages, take corrective action, and issue outage reports.
Familiarity with DISA STIGs and SCAP.
Familiarity with Java programing.

Qualifications
Required education and experience:
5+ years of Data Engineering experience.
Significant experience with Niagara Files (NiFi) and MiNiFi or similar data engineering tools.
Excellent written and verbal communication skills.
IAM or IAT Level II Certification (e.g., GSEC, SEC+, SCNP, SSCP, etc.).
Active Secret clearance.

Required Clearance
Secret

Working Environment
Onsite

Our Equal Employment Opportunity Policy
Dobbs Defense Solutions is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, creed, sex (including pregnancy, childbirth, breastfeeding, or medical conditions related to pregnancy, childbirth or breastfeeding), sexual orientation, gender or gender identity (except where gender is a bona fide occupational qualification), gender expression and transgender status, national origin, ancestry, age, disability, military or veteran status, marital or domestic partner status, genetic information, citizenship, low-income status or any other status or characteristic protected by applicable law. We are committed to equal employment opportunity in all decisions related to employment, promotion, wages, benefits, and all other privileges, terms, and conditions of employment.","$83,925 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"AbleTo
3.7",3.7,"New York, NY",Data Engineer,"About AbleTo
Join our mission-driven organization, where your work matters and a diversity of ideas and backgrounds are welcomed. AbleTo is the leading provider of high quality, technology-enabled behavioral health care. We believe that everyone deserves access to high-quality care and offer a suite of technology-enabled services to empower people to lead better lives through better mental health. A proprietary platform connects individuals with AbleTo highly trained licensed providers who deliver weekly sessions by phone or video supported by an integrated digital experience. Members also have access to mental health coaches, and digital support programs. Our outcomes-focused approach is proven to improve both behavioral and physical health and lower medical costs.

Overview
Data is integral to nearly everything that we do at AbleTo, and as a result the data team plays a key role at our company. As a Data Engineer at AbleTo, you will grow and develop your skills as a data engineer to build a world class data platform to support our mental health care mission.

What You Will Do
Build and maintain batch and real-time ETL pipelines in a Google Cloud Platform architecture (BigQuery, Dataproc, Firestore, Informatica, Python, etc.)
Identify code quality issues and implement tests to improve future processes.
Document orchestration work-flows and create run books
Implement data integrity tests to ensure we are ingesting accurate data.
Implement high-quality test-driven code.
Participate in daily team stand ups and other Agile ceremonies

Key Initiatives over the next 12 months
Data Mart client migration
Data Mart enhancements
Automation of manual day to day operational tasks
Decommission legacy pipelines

Who You Are:
1+ years of experience coding in Python.
Experience working with SQL (structured query language).
Familiarity with structuring and writing ETLs.
Experience working with Orchestration tools (Airflow) and Cloud Databases (BigQuery) is a plus.
Experience working in an Agile Development environment is a plus.
BS in Information Systems, Computer Science or related field is preferred.
You are excited to work with data!
Curiosity to dive deeply into issues and feel empowered to make a meaningful impact at a mission-driven company.
Committed to agile development and value delivery and solid engineering principles, as well as continuously improving our craft.
You are a collaborator. You build and maintain strong, productive working relationships with your customers and stakeholders.
You are empathetic and seek to understand each individual's diversity of background and experience contributes to making up a great team.
You drive to improve yourself and others.
You are excited about learning new languages, tools, techniques and technologies.

Why You Should Join Our Team:
We're proud to be a Great Place to Work-Certified™ company. We want you to show up and feel your best at work, and that means respecting your time outside of work. Our inclusive, flexible workspace prioritizes a work/life balance. We offer competitive salaries, comprehensive health benefits (for full-time employees), and professional perks such as 401K matching, fully funded HRA, and generous time off, including mental health days because your well-being is important to us.
At AbleTo, we're empowering people to get better and stay better. Want to join us? Take the next step in your career by applying for this role today.
The salary range is $40,000 to $90,000. Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, AbleTo offers benefits such as a comprehensive benefits package, incentive and recognition programs, and 401k contribution (all benefits are subject to eligibility requirements).

#LI-Remote
Follow AbleTo on LinkedIn, Twitter, and Instagram!
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.
AbleTo is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or veteran status. AbleTo is an E-Verify company.","$65,000 /yr (est.)",501 to 1000 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2008,$25 to $100 million (USD)
"The Bouqs Company
3.1",3.1,"Marina del Rey, CA",Data Analytics Engineer,"The role contributes to The Bouq’s mission of revolutionizing the way we commemorate life’s moments by connecting people to beautifully designed floral experiences and the responsible partners who create them by being a key member of the data team. As an Analytics Engineer, you will work closely with the Product, Engineering, and Data teams to build and maintain the data infrastructure needed to support our data and business needs. You will also be responsible for developing, optimizing, and maintaining the best-in-class data pipelines, data models, and ETL processes to ensure that data is accurate, reliable, and available to stakeholders in a timely manner. The Analytics Engineer will also serve as the liaison between Engineering and Analytics and will serve as an active member in both teams.
Responsibilities:
Lead the transfer of data modeling from legacy systems to DBT
Contribute to building the data modeling layer, which exposes clean, transformed data to the whole company for analytics
Build datasets in DBT (cloud) for data analysts to improve speed and accuracy for the team
Improve current processes, whether that includes modularizing and standardizing a piece of commonly used code
Design and develop new data pipelines and streaming processes that are highly available, scalable, and reliable
Develop review processes for new data models and take charge on implementing SQL standards for the team
Actively strive towards writing performant SQL rather than just SQL that works, while also ensuring the same SQL is easy to understand when new eyes look at it
Optimize data processing and flow within our Snowflake Data Warehouse
Document new datasets and pipelines and the reasoning/story behind their structure
Work closely with engineering to keep track of schema changes in the production database and adjust pipelines, as needed
Support existing data pipelines and systems in production
Apply software engineering best practices to analytics code such as version control and testing
Develop and communicate strong opinions about best practices in analytics
Help explore and evaluate new technologies
Qualifications:
4+ years of experience working within a data team, preferably as an Analyst/Data Engineer
Bachelor's degree in a quantitative field such as statistics, mathematics, economics, or computer science preferred
Strong SQL fluency in both DDL/DML and analytics (Snowflake experience is a plus)
Experience working with JSON, DBT or other data transformation tools
Experience working with an ETL tool such as Fivetran or Stitch
Knowledge of data structures and how to write performant SQL
Experience with ensuring data quality through testing, deltas, lineage, etc
Strong communication and critical thinking skills to deliver solutions that not only solve problems but also serve as tools we didn’t know we needed
Ability to transform raw data into intuitive datasets that serve as building blocks for analytics
Comfortable leading the growth of a data warehouse and maintaining it
Capable of working through uninformative assumptions and built-biases in datasets and are not stalled when data is not perfect/sparse
Compensation & Perks:
Competitive Base Salary Range of $120,000.00 - $180,000.00 USD + Equity Package
Health, Dental & Vision with 100% employee coverage
401k Matching
Three Weeks Paid Vacation
Discounts on The World’s Best Flowers (obviously!)
Work on cutting edge new technologies
About The Bouqs:
Our mission here at The Bouqs is to revolutionize the way we commemorate life’s moments by connecting people to beautifully designed flowers and the responsible partners who create them. Grounded in transparency, responsibility, and simplicity, we create genuine moments of emotional connection for our customers, build meaningful relationships with like-minded farmers and florists while empowering them to thrive, and eliminate unnecessary waste along the way.

Founded in 2012, The Bouqs is a venture-backed online floral retailer that delivers flowers fresh from eco-friendly, sustainable farms to doorsteps nationwide. Headquartered in Marina Del Rey, CA, The Bouqs connects farms and a curated network of artisan florists directly to consumers and disrupts the traditional supply chain by eliminating overhead costs like warehouses, importers, distributors, auctioneers and more. In turn, this model enables a superior product and redefines the experience and economics for both consumers and producers alike.

For more information, visit www.bouqs.com and follow the #BouqLove on Facebook, Instagram and Twitter.
The Bouqs is an Equal Opportunity Employer!","$150,000 /yr (est.)",51 to 200 Employees,Company - Private,Retail & Wholesale,Other Retail Stores,2012,$25 to $100 million (USD)
Kanini,#N/A,Remote,Data Engineer,"About Kanini
Kanini provides Agile Software Development, Cloud Computing, Data Science, and Location Intelligence services to public and private organizations. We have successfully served our clients in government, finance, transportation, utility, and software industries since 2003.
Why you should join
Working at Kanini is flexible and personal. We are a highly motivated, collaborative team experimenting with the latest technologies. We are committed to everyone having a healthy work/life balance, and we provide extensive mentorship and training resources to help you succeed.
Kanini is looking for a Big Data Engineer who has a deep experience in Data Engineering, AWS, Python, Data Lakes.
Required Skills
At least 10 years software development experience
At least 5 years leading at least one Scrum team of data engineers building data-intensive products with a modern tech stack.
Significant experience with big data ETL pipeline development with Spark, Hive, and related technologies
Significant experience with a general-purpose programming language such as Python, Scala, or Java
Experience with Spark framework and related tools (PySpark, Scala, SparkR, Spark SQL, Spark UI)
Experience with Hadoop ecosystem using HDFS, ADLS Gen2, or AWS S3
Experience with data visualization development using Python, Tableau, or PowerBI
Experience with Azure, AWS or GCP
Solid understanding of performance tuning concepts for relational and distributed database systems
Familiarity with distributed programming, big data concepts, and cloud computing
Education Qualifications
Bachelor’s degree in computer science/Engineering or Technology related field or possess equivalent work experience.
Preferred Qualifications
Cloud certifications from Azure, AWS or GCP
Big data, data engineering or data science certifications from recognized vendors such as Databricks & Cloudera
Kanini Software Solutions, Inc. does not discriminate in employment matters based on race, gender, religion, age, national origin, citizenship, veteran status, family status, disability status, or any other protected class. We support workplace diversity. If you have a disability, please let us know if there is anything we can do to improve the interview process for you; we’re happy to accommodate.
Kanini Software Solutions, Inc., 25 Century Blvd., Ste. 602, Nashville, TN 37214
Job Types: Full-time, Contract
Pay: $60.00 - $70.00 per hour
Work Location: Remote",$65.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Premier Consulting Group,#N/A,"Boca Raton, FL",Senior Data Engineer,"* The right person could be located in Florida or Massachusetts as company has office locations in Boca Roton and Boston.
* Must be a US Citizen
Overview Summary of the Work:
ETL Azure Data Factory work: A Data Engineer who is to be able to import data from a variety of different sources (SFTP, API, Fileshare) and file types (CSV, Excel, JSON). Company also captures files via email now, but they have a pattern set up to be able to do that, so the person would just repeat how they’ve done that. Check existing Azure Data Factory pipelines for failures and troubleshoot. Familiar with dynamic expression and syntax in Azure Data Factory pipelines.
Backlog of stored procedures: Be skilled in stored procedure development and SQL skills.
Data quality and data cleansing: Be able to evaluate data (incoming and existing). Look for issues and be able to resolve them. In general, someone who can look at the data and mentally make the leap of “hey, this looks wrong.” Look at data and determine how it might be better utilized (examples: 2.5% as a string or 0.025 as a number, identify values as empty strings and save as nulls instead, etc).
Technical Experience Required/Preferred:
SQL, Azure SQL, Azure Data Factory, Data warehousing environments (dims and facts), Snowflake (highly preferred), Python (highly preferred).
Job Types: Full-time, Permanent
Pay: $140,000.00 - $160,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Bonus pay
Yearly bonus
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Weekend availability
Work Location: Hybrid remote in Boca Raton, FL 33431","$150,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"HCA Healthcare
3.3",3.3,"Nashville, TN",Data Engineer,"Introduction
Last year our HCA Healthcare colleagues invested over 156,000 hours volunteering in our communities. As a Staff Data Engineer with HCA Healthcare you can be a part of an organization that is devoted to giving back!
Benefits
HCA Healthcare, offers a total rewards package that supports the health, life, career and retirement of our colleagues. The available plans and programs include:
Comprehensive medical coverage that covers many common services at no cost or for a low copay. Plans include prescription drug and behavioral health coverage as well as free telemedicine services and free AirMed medical transportation.
Additional options for dental and vision benefits, life and disability coverage, flexible spending accounts, supplemental health protection plans (accident, critical illness, hospital indemnity), auto and home insurance, identity theft protection, legal counseling, long-term care coverage, moving assistance, pet insurance and more.
Free counseling services and resources for emotional, physical and financial wellbeing
401(k) Plan with a 100% match on 3% to 9% of pay (based on years of service)
Employee Stock Purchase Plan with 10% off HCA Healthcare stock
Family support through fertility and family building benefits with Progyny and adoption assistance.
Referral services for child, elder and pet care, home and auto repair, event planning and more
Consumer discounts through Abenity and Consumer Discounts
Retirement readiness, rollover assistance services and preferred banking partnerships
Education assistance (tuition, student loan, certification support, dependent scholarships)
Colleague recognition program
Time Away From Work Program (paid time off, paid family leave, long- and short-term disability coverage and leaves of absence)
Employee Health Assistance Fund that offers free employee-only coverage to full-time and part-time colleagues based on income.
Learn more about Employee Benefits
Note: Eligibility for benefits may vary by location.

Would you like to unlock your potential with a leading healthcare provider dedicated to the growth and development of our colleagues? Join the HCA Healthcare family! We will give you the tools and resources you need to succeed in our organization. We are looking for an enthusiastic Staff Data Engineer to help us reach our goals. Unlock your potential!
Job Summary and Qualifications
Data Engineers within HCA’s Information and Analytics organization are responsible for defining and implementing data management practices across the enterprise. This full-time position will focus primarily on enterprise data management and migrating of data to the cloud. Data Engineers are expected to source and incorporate new data sources into the Enterprise Data Ecosystem. The responsibilities will include writing, testing, and reviewing ETL pipelines for defining and implementing data management practices across the enterprise. Due to the emerging and fast-evolving nature of Cloud technology and practice, the position requires that one stay well-informed of technological advancements and be proficient at putting new innovations into effective practice.
As a Data Engineer, you will work closely with all team members to create a modular, scalable solution that addresses current needs, but will also serve as a foundation for future success. The position will be critical in building the team’s engineering practices in test driven development, continuous integration, and automated deployment and is a hands-on team member who actively coaches the team to solve complex problems. This is a leadership position that assumes the responsibility for project success and the upward development of team members. They are the development team's point of contact that must interface with business partners of varying roles ranging from technical staff to executive leadership.
As a Staff Data Engineer level, the role requires 'self-starters' who are proficient in problem solving and capable of bringing clarity to complex situations. It requires contributing to strategic technical direction and system architecture approaches for individual projects and platform migrations. It also requires working closely with others, frequently in a matrixed environment, and with little supervision. This candidate will have a history of increasing responsibility in a small multi-role team. This position requires a candidate who can analyze business requirements, perform design tasks, construct, test, and implement solutions with minimal supervision.
Our Purpose
Applied to this position, your skills will help transform healthcare through technology and solutions that dramatically improve patient care and business operations.
Core Competencies
At HCA ITG, your deliverables will influence patient care. Every process, technology, and decision matters. This role will provide leadership and deep technical expertise in all aspects of solution design and application development for specific business environments. It will focus on setting technical direction on groups of applications and similar technologies as well as taking responsibility for technically robust solutions encompassing all business, architecture, and technology constraints.
Responsible for building and supporting a Cloud based ecosystem designed for enterprise-wide analysis of structured, semi-structured, and unstructured data. Direct the transformation from HCA Healthcare’s current on premise Teradata platform to Google Cloud Platform to enable analytics and machine learning at scale.
Design the cloud environment from a comprehensive perspective, ensuring that it satisfies all the company’s needs.
Develop, manage, and own full data lifecycle from raw data acquisition through transformation to end user consumption
Share knowledge and experience to contribute to growth of overall team capabilities
Perform activities such as deployment, maintenance, monitoring, and management inside the cloud framework that has been created
Provide guidance on technology choices and design considerations for migrating data to the Cloud
Maintain a holistic view of information assets by creating and maintaining artifacts that illustrate how information is stored, processed, and accessed
Demonstrate deep understanding and act as a leader in the team’s continuous integration and continuous delivery automation pipeline
Collaborate with business analysts, project lead, management, and customers on requirements
Design fit-for-purpose products to ensure products align to the customer's strategic plans and technology road maps
Actively participate in technical group discussions and adopt any new technologies to improve the development and operations.
Assist team members with production issues and offer support, guidance, and assist in communicating issues with appropriate stakeholders when necessary.
Provide leadership on key technology choices for Enterprise Data Ecosystem including data warehouse, analytical and big data platforms.
Ensure architectural, quality, and governance adherence through design reviews.
Education & Experience
Bachelor's degree in computer science or related field - Required
Master's degree in computer science or related field - Preferred
3+ years of experience in Data Engineer/Architect- Required
1+ year(s) of experience in Healthcare - Preferred
8+ years of experience in Information Technology - Required
Knowledge, Skills, Abilities, Behaviors
A successful candidate will have:
Experience developing and supporting data pipelines from various source types (on-prem rdbms, AWS, GCS bucket, flat file) to Big Query utilizing Google Cloud Platform native technologies
Knowledge and experience using the following technologies
o Big Query
o Dataflow, Data Proc, Data Fusion, Cloud Composer
o GSUTIL, GCS, Kafka, Pub/Sub
o Data Catalog/Dataplex
o Python, Unix, Linux
Strong understanding of best practices and standards for cloud application design and implementation.
Extensive experience with relational database management systems; Teradata, Oracle or SQL Server:
o Advanced SQL skills
o Write, tune, and interpret SQL queries
o BTEQs
o Stored procedures
Experience with Unstructured Data
Ability to troubleshoot, maintain, reverse engineer, and optimize existing ETL pipelines.
Requires strong practical experience in agile application development and DevOps discipline, including deployment of CI/CD pipelines in Git
Ability to multitask and to balance competing priorities.
Expertise in planning, implementing, supporting, and tuning Cloud ecosystem environments using a variety of tools and techniques.
Ability to define and utilize best practice techniques and to impose order in a fast-changing environment. Must have strong problem-solving skills.
Strong verbal, written, and interpersonal skills, including a desire to work within a highly-matrixed, team-oriented environment.
A successful candidate may have:
o Experience in Healthcare Domain
o Experience in Patient Data
Certifications (a plus, but not required)
GCP Cloud Professional Data Engineer
HCA Healthcare has been recognized as one of the World’s Most Ethical Companies® by the Ethisphere Institute more than ten times. In recent years, HCA Healthcare spent an estimated $3.7 billion in cost for the delivery of charitable care, uninsured discounts, and other uncompensated expenses.
""There is so much good to do in the world and so many different ways to do it.""- Dr. Thomas Frist, Sr.
HCA Healthcare Co-Founder
Be a part of an organization that invests in you! We are reviewing applications for our Staff Data Engineer opening. Qualified candidates will be contacted for interviews. Submit your application and help us raise the bar in patient care!
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.","$93,734 /yr (est.)",10000+ Employees,Company - Public,Healthcare,Health Care Services & Hospitals,1968,$10+ billion (USD)
"Acrisure Technology Group
3.9",3.9,"Austin, TX",Data Engineer,"Data Engineer
Hybrid Position (3 days per week average in Downtown Austin, TX or Grand Rapids, MI office)
Note: This is a full-time, in-house position. We do not offer C2C or C2H employment and are not able to sponsor visas for this position.
Acrisure Technology Group (ATG) is a fast-paced, AI-driven team building innovative software to disrupt the $6T+ insurance industry. Our mission is to help the world share its risk more intelligently to power a more vibrant economy. To do this, we are transforming insurance distribution and underwriting into a science.
At the core of our operating model is our technology: we're building the premier AI Factory in the world for risk and applying it at the center of Acrisure, a privately held company recognized as one of the world's top 10 insurance brokerages and the fastest growing insurance brokerage globally. By using the latest technology and advances in AI to push the boundaries of understanding risk, we are systematically converting data into predictions, insights, and choices, and we believe we can remove the constraints associated with scale, scope, and learning that have existed in the insurance industry for centuries.
We are a small team of extremely high-caliber engineers, technologists, and successful startup founders, with diverse backgrounds across industries and technologies. Our engineers have worked at large companies such as Google and Amazon, hedge funds such as Two Sigma and Jump Trading, and a variety of smaller startups that quickly grew such as Indeed, Bazaarvoice, RetailMeNot, and Vrbo.
The Role
The Business Intelligence team's mission is to unify data across the enterprise to optimize business decisions made at the strategic, tactical, and operational levels of the organization. We accomplish this by providing an enterprise data warehouse, data lake, reporting platform, and business processes that provide quality data, in a timely fashion, from any channel of the company and present them in such a manner as to maximize the value of that data for both internal and external customers.
The Data Engineer is responsible for designing and developing moderate to complex ETL processes required to populate a data lake and structured data warehouse which supply data for the machine learning, AI & BI teams. Responsibility includes working with a team of contracted developers as well as coaching and mentoring junior and mid-level developers. Ensuring high quality and best practices are maintained through the development cycle is key to this position.
You will interact with some of the top technologists on the planet. Our technology runs on Google Cloud and is configured with Kubernetes, leveraging various services in that environment. Our data storage layer includes BigQuery, BigTable, and Postgres. We code primarily in Kotlin, Python, Java, and JavaScript and make use of many frameworks, including Dataflow, Cloud AI Platform, KubeFlow, Spring, and React.
Here are some of the ways in which you'll achieve impact
Leverage established guidelines and custom designs to create complex ETL processes to meet the needs of the business
Develop from strategic and non-strategic data sources including data preparation/ETL and modeling for data visualizations in a self-service platform
Contribute to the definition and development of the overall reporting roadmap
Translate reporting requirements into reporting models, visualizations and reports by having a strong understanding of the enterprise architecture
Standardize reporting that helps generate efficiencies, optimization, and end user standards
Integrate dashboards and reports from a variety of sources, ensuring that they adhere to data quality, usability, and business rule standards
Independently determine methods and procedures for new or existing requirements and functionality
Work closely with analysts and data engineers to identify opportunities and assess improvements of our products and services
Contribute to workshops with the business user community to further their knowledge and use of the data ecosystem
Produce and maintain accurate project documentation
Collaborate with various data providers to resolve dashboard, reporting and data related issues
Perform Data Services reporting benchmarking, enhancements, optimizations, and platform analytics
Participate in the research, development, and adoption of trends in reporting and analytics
Mentor BI Developers and BI Analysts
Other projects as assigned in order to support necessary business goals across teams
You may be fit for this role if you have
Minimum 5 years required, particularly in an Azure environment with Azure Data Bricks, Azure Data Factory, Azure Data Lake
Minimum 5 years designing data warehouses, data modeling, and end-to-end ETL processes in a MS-SQL environment
Minimum 2 years developing machine learning models with Azure ML, ML Flow, BQML
Expert working knowledge of SQL, Python and Spark (and ideally PySpark) with a demonstrated ability to create ad-hoc SQL queries to analyze data, create prototypes, etc required.
Successfully delivered 2+ end to end projects – from Inception to Execution - in Data Engineering / Data Science / Data Integration as a Tech Senior/Principal
Ability to Analyze, summarize, and characterize large or small data sets with varying degrees of fidelity or quality, and identify and explain any insights or patterns within them.
Experience with multi-source data warehouses
Strong skills in in data analytics and reporting, particularly with Power BI
Experience with other cloud environments (GCS, AWS) a definite plus
Strong experience creating reports, dashboards, and/or summarizing large amounts of data into actionable intelligence to drive business decisions required
Strong understanding of core principles of data science and machine learning; experience developing solutions using related tools and libraries
Hands on experience building logical data models and physical data models and using tools like ER/Studio/Idera
Write SQL fluently, recognize and correct inefficient or error-prone SQL, and perform test-driven validation of SQL queries and their results
Proficient in writing Spark sql using complex syntax and logic like analytic functions etc.
Well versed in Data Lake & Delta Lake Concepts
Well versed in Databricks usage in dealing with Delta tables (external \ managed)
Well versed with Key Vault \ create & maintenance and usage of secrets in both Databricks & ADF
Should be knowledgeable in Stored procedures \ functions and be able to use them by ADF & Databricks as this is a widely used Practice internally
Familiar with DevOps process for Azure artifacts and database artifacts
Well versed with ADF concepts like chaining pipelines, passing parameters, using APIs for ADF & Databricks to perform various activities.
Experience creating and sharing standards, best practices, documentation, and reference examples for data warehouse, integration/ETL systems, and end user reporting
Apply disciplined approach to testing software and data, identifying data anomalies, and correcting both data errors and their root causes
Academics: Undergraduate degree preferred or equivalent experience along with a demonstrated desire for continuing education and improvement
Location: Austin, TX or Grand Rapids, MI
We are interested in every qualified candidate who is eligible to work in the United States. We are not able to sponsor visas for this position.","$96,659 /yr (est.)",5001 to 10000 Employees,Company - Private,Insurance,Insurance Agencies & Brokerages,2005,$1 to $5 billion (USD)
"Colorado Rockies Baseball Club
3.8",3.8,"Denver, CO",Senior Data Engineer,"Senior Data Engineer

The Role
The Colorado Rockies Baseball Club is building a platform to house all our baseball data, from scouting reports to baseball statistics and rosters, into an all-encompassing application that will help us more effectively and efficiently make baseball decisions. This role will be responsible for building and maintaining the ingestion, transformation, and cloud storage of baseball data that will drive analysis and decision-making in all facets of Baseball Operations. The Senior Engineer will be a technical leader on the team and help grow less-experienced engineers.

Essential Duties and Responsibilities
Implement ETL pipelines from various external vendors for the use of internal statistical analysis.
Facilitate the aggregation of data for consumption by technical and non-technical users.
Maintain existing pipelines and debug problems that arise.
Work closely with a web development team to deliver the data they need for an internal information application.
Assist less-experienced engineers with their projects to increase their skill sets.

Job Requirements
Bachelor’s degree or completion of an immersive technical program in Computer Science, Information Systems, Computer Engineering, Web Development, or a related field preferred.
At least five years of experience working in data engineering.
Experience and strong understanding of AWS cloud services including S3, Cloudwatch, Glue, and CDK.
Expert-level understanding of relational databases, including SQL Server, MySQL, and Postgres.
In depth experience using a scripting language for data management or analysis, such as Python or R.
Strong understanding of handling and parsing various data formats, including XML, JSON, and CSV.
Relocation and on-site work are required for this position.

Preferred Skills
Strongly prefer experience with commonly used baseball data sources, such as StatCast, EBIS, Trackman and CollegeSplits.
Understanding of modern baseball analysis.
Experience working with Apache Airflow and additional ingestion management tools not mentioned above.

SALARY RANGE:
$115,000 - $135,000 a year. This is a regular status, full-time position eligible for all company benefits including but not limited to Health Insurance (medical, dental, vision), Retirement, and accrued time off (Vacation/ Sick/ Holiday).

EQUAL OPPORTUNITY EMPLOYER:
Rockies baseball is for everyone! We pride ourselves on hiring, developing, and promoting talent as an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, national origin, gender, age, religion, disability, veteran status, or any other category protected by law. In addition, we will endeavor to provide reasonable accommodation to otherwise qualified job applicants and employees with known physical or mental disabilities in compliance with the ADA. All employment and promotion decisions will be decided on the basis of qualifications, merit, and business needs","$125,000 /yr (est.)",501 to 1000 Employees,Company - Private,"Arts, Entertainment & Recreation",Sports & Recreation,1991,Less than $1 million (USD)
BIGCLFY,#N/A,"Dallas, TX","W2/ 1099, No C2C - SSIS ETL Data Engineer (Minneapolis, MN/ Dallas-Irving)","No C2C. It's W2/1099 Requirement
Only taking USC, GC or H4 EAD, H1 Transfer, L2 Visa or TN Visa - they will not take OPT EAD or CPT
Need to have confirmation in writing that you will be onsite by Day 1
ALL ROLES ARE HYBRID IN THE OFFICE (2-3 DAYS/WEEK) UNLESS REMOTE IS NOTED
Please submit qualified candidates and include FULL LEGAL NAME as it appears on the passport
If your candidate is relocating, need to have confirmation in writing that they will be onsite by Day 1
Senior SSIS/ETL Data Engineer
12 months – 3 days in office
Minneapolis, MN or Dallas-Irving, TX
5+ years of experience with SQL Server 2016 (or newer) and SSIS
7+ years of data engineering
Must be capable of leading teams
2+ years of experience working with large scale SQL implementations
Experience designing, developing, and maintaining complex stored procedures, functions, and views using T-SQL
Experience designing, developing, and maintaining ETL processes using SSIS
Should understand how to unit test their product
Hands on experience with Visual Studio to document
Experience in financial or regulatory environments highly preferred
Ideal candidates will have experience creating conceptual, logical, and physical database designs
Preference given to candidates with extensive knowledge and experience with performance monitoring, database and SQL tuning
Bachelor’s or Master's in computer science or related field or equivalent work experience
Job Type: Contract
Pay: From $73.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Chandler, AZ 85225: Reliably commute or planning to relocate before starting work (Preferred)
Application Question(s):
Expected Payrate on W2/1099 Tax Term? Please mention. Thanks.
Experience:
Risk Analytics: 5 years (Preferred)
Work Location: In person",$73.00 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
