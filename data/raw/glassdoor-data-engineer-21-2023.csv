company,company_rating,location,job_title,job_description,salary_estimate,company_size,company_type,company_sector,company_industry,company_founded,company_revenue
"VedaInfo Inc
4.1",4.1,"Phoenix, AZ",Big Data Engineer,"Hi,
I hope this note finds you well
I am reaching out to you regarding your career opportunity with us for “Big Data Engineer” for a “6+ Months Contract” @Phoenix, AZ
Please find the below requirement details.
Job Title: Big Data Engineer
Location: Phoenix, AZ (Onsite)
Duration: 6+ months
Rate: $60/hr C2C
Responsibilities:
Design, implement, and maintain big data systems handling large volumes of data.
Utilize Hadoop, Hive, and Spark for efficient data processing and analysis.
Collaborate with cross-functional teams to understand data requirements.
Develop and optimize data ingestion, storage, and transformation processes.
Build scalable data pipelines for seamless data processing and analysis.
Monitor and troubleshoot data processing and performance issues.
Stay updated with emerging big data technologies.
Requirements:
Bachelor's/Master's degree in CS, Engineering, or related field
7+ years of experience as a Big Data Engineer
Strong proficiency in Hadoop, Hive, and Spark
Extensive experience with AWS or Azure
Solid understanding of data ingestion, storage, and transformation
Excellent problem-solving and communication skills
Proactive and self-motivated
This is a 6+ month onsite contract position in Phoenix, AZ, with the possibility of extension. Compensation is $60/hr on C2C basis.
Please submit your updated resume highlighting relevant experience.
Thank you for your interest!
Thanks & Regards
Mohammed ZAIN
Job Type: Contract
Pay: $50.00 - $60.00 per hour
Experience level:
7 years
Ability to commute/relocate:
Phoenix, AZ: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
To improve your chances of getting Shortlisted, make sure to tailor your Profile to the Job Requirements, Highlight your Relevant Skills and Experience, and then upload your Resume.
Experience:
AWS (Amazon Web Services): 7 years (Required)
Azure: 7 years (Required)
Big data: 7 years (Required)
Hadoop: 7 years (Required)
Apache Hive: 7 years (Required)
Spark: 7 years (Required)
Work Location: One location",$55.00 /hr (est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$1 to $5 million (USD)
Sconcept,#N/A,Remote,Azure Data Engineer,"** W2 LONG TERM CONTRACT BASED....**(NO C2C)
JOB TYPE:- Long Term Contract(6 months)Extendable
JOB LOCATION:-USA
About Sconcept LLC:-Software Concepts Consulting was established in 2008. With our “hands-on” approach we are very successful in delivering high performance, secure IT solutions and IT services. With headquarters in Carrollton, Texas we are specialized in providing business solutions to the Fortune 500 clients. Our clients include Software, Banking, Financial, Healthcare, Retail, Consumer goods and Insurance sectors. We deliver candidates based on client requirements.
Required Skills:-
Must have worked on several (5+) projects building solutions for Data Pipelines, Data warehousing, Data Modelling on Azure Data Platform (AWS, Google Cloud Platform - good to have)
Expertise in design and development of Data pipelines and ETL using ADF, Databricks to move data from relational/structured/unstructured data from source to data lake to data warehouse, on Azure
Creating Pipelines and integrations of multiple data sources
Data processing and transformation using Databricks (Scala/Python)
Expertise in optimizing cost and performance of data pipelines
Experience working with Data Lake, Synapse on Azure
Designing Data Lake storage layer (Landing/staging, raw, trusted/curated zones)
Designing data storage DB schema, data models and processing on Azure Synapse DW
Experience with Synapse Spark Pool
Experience in Data Modelling
Building Data Models from structured and semi-structured data (Data Model, Common Data Model etc.)
Performance optimization of data model for high performance analytical and reporting workloads
Expertise with SQL Server and extensive experience with SQL programming
Programming and optimizing DB objects Views, Stored Procs, Functions
Good experience in defining data access policy and security for azure data services
Experience with Oracle EDW
Knowledge/Experience migrating from Oracle EDW to any cloud DW
Experience integrating AI skills/ML models with Data and reporting solutions
Experience in building API layer for downstream consumption
Job Type: Contract
Pay: $45.00 - $50.00 per hour
Benefits:
Health insurance
Compensation package:
Bonus pay
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$47.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
META FORCE IT LLC,#N/A,"Cincinnati, OH",Data Engineer,"Job Description
The data engineer designs and builds platforms tools and solutions that help the bank manage secure and generate value from its data. The person in this role creates scalable and reusable solutions for gathering collecting storing processing and serving data on both small and very large i.e. Big Data scales. These solutions can include on-premise and cloud-based data platforms and solutions in any of the following domains ETL business intelligence analytics persistence relational NoSQL data lakes search messaging data warehousing stream processing and machine learning.
Responsible and accountable for risk by openly exchanging ideas and opinions elevating concerns and personally following policies and procedures as defined. Accountable for always doing the right thing for customers and colleagues and ensures that actions and behaviors drive a positive customer experience. While operating within the Bank's risk appetite achieves results by consistently identifying assessing managing monitoring and reporting risks of all types.
ESSENTIAL DUTIES AND RESPONSIBILITIES:
Responsible for design Development and Support of data solutions APIs tools and processes to enable rapid delivery of business capabilities.
Work closely with IT application teams Enterprise architecture infrastructure information security and LOB stakeholders to translate business and technical strategies into data-driven solutions for the Bank.
Act as a technical Expert addressing problems related to system and application design performance integration security etc.
Conduct research and Development based on current trends and technologies related to the banking industry data engineering and architecture data security and related topics.
Work with developers to Build CI/CD pipelines Self-service Build tools and automated deployment processes.
Evaluate software products and Provide documented recommendations as needed.
Provide Support and troubleshooting for data platforms. Must be willing to Provide escalated on-call Support for complicated and/or critical incidents.
Participate in the planning process for hardware and software.
Plan and work on internal projects as needed including legacy system replacement Monitoring and analytics improvements tool Development and technical documentation.
Provide technical guidance and mentoring for other team members.
Manage and prioritize multiple assignments.
MINIMUM KNOWLEDGE SKILLS AND ABILITIES REQUIRED:
Bachelor's degree in Computer Science/Information Systems or equivalent combination of education and experience.
Must be able to communicate ideas both verbally and in writing to management business and IT sponsors and technical resources in language that is appropriate for each group.
Fundamental understanding of distributed computing principles
Knowledge of application and data security concepts best practices and common vulnerabilities.
Conceptual understanding of one or more of the following disciplines preferred big data technologies and distributions metadata management products commercial ETL tools Bi and reporting tools messaging systems data warehousing Java language and run time environment major version control systems continuous integration/delivery tools infrastructure automation and virtualization tools major cloud or rest API design and development.
Job Type: Contract
Pay: $45.96 - $50.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Cincinnati, OH 45215: Reliably commute or planning to relocate before starting work (Required)
Experience:
data engineer: 10 years (Preferred)
Big data: 3 years (Preferred)
Work Location: Hybrid remote in Cincinnati, OH 45215",$47.98 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"ProGrad
4.2",4.2,Remote,Data Engineer,"We are looking for data engineers that are excited about building and maintaining data pipelines for a variety of interests. The pipelines will ingest data from a variety of sources ranging from internal change data streams to external integrations with various SaaS platforms through webhooks and events. The ideal candidate is self driven and looking to grow as a data engineer while having an outsized impact on the future of data at the company. A love for database architecture and optimisation is a must as is the ability to work across diverse teams.
Requirements/Qualifications:
Bachelor's or Master's degree in Data Science (specialization)
Software development experience coding in a general-purpose programming language such as C, C++, Java, JavaScript, Golang, or Python
Experience working with data structures or algorithms (i. e., data structures /algorithms class, coursework/projects, research, internships, or other practical experience in/outside of school or work (including open source hobby coding)
Proficiency in SQL, and NoSQL Databases
Experience working with large datasets
Experience writing and debugging complex SQL queries
A strong understanding of cloud database best practices and performance optimisation
Experience building and maintaining data pipelines, coupled with knowledge of good programming practices such as unit and integration testing
. A strong understanding of cloud database best practices and performance optimisation
Good to have skills:
. Experience building and maintaining data pipelines, coupled with knowledge of good programming practices such as unit and integration testing
Job Types: Full-time, Permanent
Salary: Up to $1,200,000.00 per year
Experience level:
1 year
Under 1 year
Schedule:
Day shift
Work Location: Remote
Speak with the employer
+91 9057202434",#N/A,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Enterprise Minds,#N/A,Remote,Data Engineer,"Job Role: SQL Data Engineer
Location: Remote Opportunity - Need to work in PST hours
Visa: US Citizen Only
Duration: Long term
Roles and Responsibilities:
3+ years of experience with a complex build system in AWS using Starburst.
5+ years of experience with installation, maintenance, and administration of Oracle databases- PL/SQL and ANSI SQL
Oracle DBA + Oracle SQL to ANSI SQL conversion, performance/query tuning
3+ years of experience configuring, integrating, and securing multiple AWS database offerings (ex. RDS, Aurora, Casandra, and Dynamo)
2+ years of experience with supporting Linux systems engineering efforts in system design and evaluation, solution engineering, software development, or system administration.
Experience with Database Schema as Code tools
Ability to design and develop complex SQL queries and reports to retrieve required data.
Experience with database partitioning and microservices
Knowledge of Agile methodologies or the software development life cycle (SDLC)
US Citizenship is Mandatory.
Job Type: Full-time
Experience:
Data Engineer: 10 years (Preferred)
AWS using Starburst: 4 years (Preferred)
PL/SQL and ANSI SQL: 2 years (Preferred)
AWS RDS: 1 year (Preferred)
Work Location: Remote",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"MARVEL TECHNOLOGIES INC
3.7",3.7,"Cincinnati, OH",Data Engineer,"Job Description
The data engineer designs and builds platforms tools and solutions that help the bank manage secure and generate value from its data. The person in this role creates scalable and reusable solutions for gathering collecting storing processing and serving data on both small and very large i.e. Big Data scales. These solutions can include on-premise and cloud-based data platforms and solutions in any of the following domains ETL business intelligence analytics persistence relational NoSQL data lakes search messaging data warehousing stream processing and machine learning.
Responsible and accountable for risk by openly exchanging ideas and opinions elevating concerns and personally following policies and procedures as defined. Accountable for always doing the right thing for customers and colleagues and ensures that actions and behaviors drive a positive customer experience. While operating within the Bank's risk appetite achieves results by consistently identifying assessing managing monitoring and reporting risks of all types.
ESSENTIAL DUTIES AND RESPONSIBILITIES:
Responsible for design Development and Support of data solutions APIs tools and processes to enable rapid delivery of business capabilities.
Work closely with IT application teams Enterprise architecture infrastructure information security and LOB stakeholders to translate business and technical strategies into data-driven solutions for the Bank.
Act as a technical Expert addressing problems related to system and application design performance integration security etc.
Conduct research and Development based on current trends and technologies related to the banking industry data engineering and architecture data security and related topics.
Work with developers to Build CI/CD pipelines Self-service Build tools and automated deployment processes.
Evaluate software products and Provide documented recommendations as needed.
Provide Support and troubleshooting for data platforms. Must be willing to Provide escalated on-call Support for complicated and/or critical incidents.
Participate in the planning process for hardware and software.
Plan and work on internal projects as needed including legacy system replacement Monitoring and analytics improvements tool Development and technical documentation.
Provide technical guidance and mentoring for other team members.
Manage and prioritize multiple assignments.
MINIMUM KNOWLEDGE SKILLS AND ABILITIES REQUIRED:
Bachelor's degree in Computer Science/Information Systems or equivalent combination of education and experience.
Must be able to communicate ideas both verbally and in writing to management business and IT sponsors and technical resources in language that is appropriate for each group.
Fundamental understanding of distributed computing principles
Knowledge of application and data security concepts best practices and common vulnerabilities.
Conceptual understanding of one or more of the following disciplines preferred big data technologies and distributions metadata management products commercial ETL tools Bi and reporting tools messaging systems data warehousing Java language and run time environment major version control systems continuous integration/delivery tools infrastructure automation and virtualization tools major cloud or rest API design and development.
Job Types: Full-time, Contract
Pay: $60.00 - $62.00 per hour
Compensation package:
Hourly pay
Yearly pay
Experience level:
9 years
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",$61.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,#N/A,$5 to $25 million (USD)
"DiamondPick
4.5",4.5,Remote,GCP Data Engineer,"Position: GCP Data Engineer (8+years)
Location: Remote
Type: Contract
Job Details:
· Relevant Industry Work Experience (6+ for Developer, preferred 9+ years)
· Experience extracting data from a variety of sources, and a desire to expand those skills (Excellent knowledge in SQL and Spark is mandatory)
· Strong knowledge of Google BigQuery and architecting data pipelines from on-prem to GCP.
· Experience building applications using Google Cloud Platform related frameworks such as DataProc and GCS at the minimum.
· Excellent Communication Skills to Understand and Pass on Requirements.
· Excellent Data Analysis skills. Must be comfortable with querying and analyzing large amount of data on Hadoop HDFS using Hive and Spark.
· Professional experience with a cloud hosting platform (GCP preferred)
· GCP certification is preferred.
Job Type: Contract
Salary: Up to $65.00 per hour
Experience level:
10 years
8 years
9 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
Google BigQuery: 4 years (Preferred)
DataProc: 2 years (Preferred)
Google Cloud Platform: 9 years (Required)
Spark: 8 years (Required)
SQL: 8 years (Required)
License/Certification:
Google Cloud Platform(GCP) Certification (Required)
Work Location: Remote",$65.00 /hr (est.),1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,#N/A,Unknown / Non-Applicable
Zllius Inc.,#N/A,"Chicago, IL",Data Engineer,"Hi,
Greetings from Zllius Inc!
We have an urgent opening for the role Data Engineer. If interested, revert me back with your Updated resume, along with contact details, work authorization.
Job Role: Data Engineer
Duration: Long-Term
Position Type: W2/ 1099
Visa: Any
Job Description:
Technical/Functional Skills
Successful candidates will have:
Hands on experience in Dremio and Scripting
Hands on Scripting experience in Python and Flask
Hands on experience in Integrating data from multiple source systems (PostgreSQL and CSV)
Experience with Apache Superset
Ability to write complex SQL queries
Experience in building data pipelines using PostgreSQL, Oracle database, Dremio, AWS Open Search
Experience with Docker Containerization of Code
Work as a part of a scrum team in Agile methodology
ETL, Data Pipelines, PostgreSQL, SQL Queries, Scripting
Job Types: Full-time, Contract
Salary: $111,076.11 - $133,769.08 per year
Schedule:
8 hour shift
Day shift
Monday to Friday
Work Location: On the road","$122,423 /yr (est.)",Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
shreetek,#N/A,"Charlotte, NC",Senior Data Engineer,"Job Title : Data Engineer
ONLY USC
On W2 only
Job Location : Newjersey OR Charlotte,NC
Collect, manage, and convert raw data accurately and reliably
Organize data systems for subgroup access and analyses
Configure and sustain data cloud structures
Able to work with structured and unstructured data.
Validate outputs of data pipelines
Degree in Data Engineering preferred.
Two Programmers –
Convert existing SAS code to python/pyspark code for model operation in the cloud.
Create and sustain policy analysis models in the cloud.
Troubleshoot user interfaces in the cloud.
Create and sustain intuitive user interfaces in the cloud.
A degree in Computer Science is preferred.
Overall across resources keep following stack as reference
Visualization : Tableau
Data Modeling/Science : Python / SAS
Engg : AWS cloud native services, security, data pipeline
Job Type: Contract
Salary: $50.00 - $60.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Charlotte, NC 28202: Reliably commute or planning to relocate before starting work (Required)
Work Location: One location",$55.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Amazee Global Ventures Inc
5.0",5.0,Remote,Data Engineer,"(W2 ONLY)
Job Title: Data Engineer
Duration : 12 months
Location: Remote
6-7 years of experience
Healthcare experience is must
Design, develop, and maintain Tableau dashboards and reports
Develop and maintain data models and ETL processes
Analyze data to identify trends and patterns
Create and maintain data dictionaries and data models
Develop and maintain data pipelines
Collaborate with stakeholders to understand data requirements
Develop and maintain data quality processes
Monitor data quality and performance
Troubleshoot data issues
Develop and maintain documentation
Requirements:
Bachelor’s degree in Computer Science, Information Systems, or related field
4+ years of experience in data engineering and analysis
Expertise in Tableau
Experience with SQL and data modeling
Knowledge of ETL processes
Knowledge of data warehousing
Excellent problem-solving and communication skills
Ability to work independently and in a team environment\
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Type: Full-time
Salary: $40.33 per hour
Work Location: Remote",$40.33 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,2019,Less than $1 million (USD)
"Dentsu Aegis Network
3.6",3.6,"New York, NY",Data Engineer - BI Developer,"Company Description

Dentsu International is a modern marketing solutions company. Our mission is to help clients navigate, progress and thrive in a world of change. Businesses rely on our integrated network of agencies and specialized practices to champion meaningful progress through creative, media, commerce, data, and technology. Part of Dentsu Group, our global network comprises 66,000 diverse people in 143 countries, who are dedicated to teaming for growth and good. Some of our award-winning agencies include 360i, Carat, dentsumcgarrybowen, DEG, dentsuX, iProspect and Merkle. Follow us on Twitter @DentsuUSA and visit dentsu.com/us.
Part of Dentsu International, Dentsu Creative is a Global Creative Network that transforms brands and businesses through the power of Modern Creativity. Led by Global Chief Creative Officer Fred Levron, 9,000 experts across the globe work seamlessly together to deliver ideas that Create Culture, Shape Society and Invent the Future. Dentsu Creative was launched in June 2022 to address a client need for simplicity and will be Dentsu International’s sole creative network by the end of 2022.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.

Job Description

The Data Engineer is a new key role within the Business Platforms Americas team. We are looking for a well-rounded Senior BI Developer expertise with strong knowledge of MS SQL, PowerBI, and data warehousing techniques. This is a unique opportunity to be involved in delivering leading-edge business analytics using the latest and greatest cutting-edge BI tools, such as cloud-based databases, self-service analytics and leading visualization tools enabling the company’s aim to become a fully digital organization.
Key Responsibilities
Collaborate with the BI Dev team members to evaluate, design, develop BI reports and dashboards according to functional specifications while maintaining data integrity and data quality
Deliver Technical Design Document capturing specific processes and data flows, data definitions and relevant business rules
Apply best practices of data integration for data quality and automation
Work with business analysts to understand business requirements and use cases to write and assign technical stories and tasks
Work independently within guidelines, responsible for initiating, planning, executing, controlling, and implementation of projects using a formal project management and agile methodology
Work collaboratively with key stakeholders to translate business information needs into well-defined data requirements to implement the BI solutions
Work with team to provide support for existing analytics and PowerBI reporting platforms
Coaching, mentoring, and providing technical direction and training to other IT personnel
Working with BI & Analytic teams to develop and establish BI road Map/Vision

Qualifications

Experience:
Excellent communication skills
Over 7-10 years of experience in Data warehousing and Business Intelligence
Over 5 years’ experience in a Business Intelligence Analyst or Developer roles
Over 4 years’ experience using ADF for data warehousing
Experience in designing and performance tuning data warehouses and data lakes.
2+ years experience in developing data models and dashboards using Power BI within an IT department
Being delivery-focused with a can-do attitude in a sometimes-challenging environment is essential.
Experience using Power BI to visualize data held in SQL Server
Experience working with finance data highly desirable
Other key Competencies:
Strong communications skills and ability to turn business requirements into technical solutions
Experience in developing data lakes and data warehouses using Microsoft Azure
Demonstrable experience designing high-quality dashboards using Power BI
Strong database design skills, including an understanding of both normalized form and dimensional form databases.
In-depth knowledge and experience of data-warehousing strategies and techniques e.g., Kimble Data warehousing
Experience in Cloud based data integration tools like Azure Data Factory
Experience in power bi data modelling and DAX is preferred
Experience in Azure Dev Ops or JIRA is a plus
Familiarity with agile development techniques and objectives

Additional Information

The anticipated salary range for this position is $94,000-146K. Salary is based on a wide range of factors that include relevant experience, knowledge, skills, other job-related qualifications, and geography. A range of medical, dental, vision, 401(k) matching, paid time off, and/or other benefits also are available. For more information regarding dentsu benefits, please visit https://dentsubenefitsplus.com/.

Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact recruiting@dentsuaegis.com if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.

#LI-AJ1
About dentsu
Dentsu is the network designed for what’s next, helping clients predict and plan for disruptive future opportunities in the sustainable economy. Taking a people-centered approach to business transformation, dentsu combines Japanese innovation with a diverse, global perspective to drive client growth and to shape society www.dentsu.com.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.
Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact your recruiter if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.","$120,000 /yr (est.)",10000+ Employees,Company - Public,Media & Communication,Advertising & Public Relations,#N/A,Unknown / Non-Applicable
"Wallero
4.4",4.4,"Seattle, WA",Data Engineer,"Hi,
Pretty much interested in your resume, we have an urgent opening for Data Engineer role with one of our clients based in Seattle WA.
Position: Data Engineer
Location: Seattle WA - Remote (100%)
Description:
Good Experience in reporting space including PowerBI, ADF, and possibly standing up a data warehouse/data mart.
The main scope centering is around reporting.
Experience with SQL, and agile framework including requirement gathering and story/feature writing skills.
Need someone strong who is comfortable leading and guiding teams in that capacity.
Migrating data from third party to Azure Cloud
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Seattle, WA 98101: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",$75.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2015,$1 to $5 million (USD)
NAVA TECH LLC,#N/A,Remote,Azure Data Engineer,"Azure Data Engineer
Remote
Long Term
Note: Need 10+ years of experience
Sound skills and hands on experience with Azure Data Lake, Azure Data Factory, SQL Data Warehouse Azure Blob, Azure Storage Explorer
Proficient in creating Data Factory pipelines for on-cloud ETL processing; copy activity, custom Azure development etc.
Knowledge of Azure Data Catalog, Event Grid, Service Bus, SQL and Synapse
Experience using Azure Databricks platform.
Experience with Python programming.
Experience using pandas and numpy for data engineering and data cleansing;
Ability to troubleshoot the job scheduler and compute clusters.
Experience with databricks CLI and secrets module.
Experience with Azure Blob storage access configuration.
p l e a s e s h a r e r e s u m e s t o m o h a m m e d (a t) n a v a t e c h (d o t) u s O R 4 4 3 5 3 7 9 8 3 5
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$62.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Fincons Group
4.0",4.0,Remote,Data Engineer – Databricks / Kafka / SQL,"Fincons Group is an IT business consulting company that has been designing the digital future of leading companies on international markets for 40 years. Fincons Group built its reputation on foreseeing and interpreting new business models and the rapid evolution of IT systems by building a complete range of services: from research and consulting to design and development, from system integration of leading vendor software solutions right up to application management, supporting clients step by step along their digital transformation journey. Fincons is a multinational with over 2,600 people and several offices worldwide (in Italy, Switzerland, Germany, France, the United Kingdom, Belgium and the USA), but above all a Family Company where the founders play a strategic role with commitment and passion, grounding the company in the same principles of a united and caring family.
We firmly believe in the value of cooperation and in the contribution that every idea and intuition can bring. We believe that everyone’s effort can make a difference. Transforming knowledge into a strategic resource is our company mission, and we pursue it with the help of our most strategic asset: our people.

Job Summary
Looking for an experienced data engineer with hands on experience in developing solutions based on Databricks. If you are a curious team-player with passion to learn and working for different clients to develop modern data streaming platforms, please reach out to us!

Work location: any US location might be considered
Employment type: freelance / contractor
Start date: about July 1th
Duration: about 6 months with possible renewal

Responsibilities and Duties
Design, develop and implement real time data ingestion pipelines from multiple sources using Databricks
Develop high-quality, scalable and re-usable frameworks for ingesting high volume and large data sets
Apply best practices to Integrate and ingest various big data formats like Protobuf, Parquet, AVRO and JSON
Work with event based / streaming technologies like Kafka, or Kinesis, to ingest and transform the data using KafkaStreams or KSQL
Working with other members of the project team to support delivery of additional project components (API interfaces, Search)
Work to load test the pipelines for quality and better performance using JMeter, or Gatling etc

Qualifications and skills
Bachelor’s degree and first experience designing, developing, deploying and/or supporting data pipelines using Databricks
Experience in designing and implementing Medallion architecture is preferred
Expertise in designing and deploying data applications on cloud solutions, such as Azure or AWS
Hands on experience in performance tuning and optimizing code running in Databricks environment
Proficient in programming languages like Pyspark and Python
Good understanding of SQL, T-SQL and/or PL/SQL
Demonstrated analytical and problem-solving skills particularly those that apply to a big data environment
Experience with agile development methodologies.
Ability to work effectively both independently and in a group or team environment

Authorization to Work
Applicants for employment in the US must be authorized to work in the US. Fincons Group will not sponsor applicants for work visas.
All qualified applicants will receive consideration for employment without regard to race, colour, religion, sex, sexual orientation, gender identity, national origin, age, disability, or protected veteran status, or any other legally protected basis, in accordance with applicable law.
The data will be processed and stored exclusively for the purposes of this or future selections, guaranteeing the rights referred to in art. 13 Legislative Decree 196/03 and EU regulation 679/2016 (GDPR)
2yrKJdtT13",#N/A,201 to 500 Employees,Company - Private,Financial Services,Accounting & Tax,#N/A,Unknown / Non-Applicable
Cloudburst Technologies,#N/A,Remote,Data Collection Engineer,"About Us
Cloudburst is a seed-funded New York-based remote-first company helping customers with detection, prevention, and investigation into cryptocurrency market manipulation and fraud.
Cloudburst provides regulators, financial institutions, trading platforms, and others with access to a real-time, machine-readable crypto market monitoring tool, enabling advanced levels of diligence and customer/market protection. The Cloudburst team has multiple years of experience working together, plus it has worked directly with international law enforcement agencies and policymakers on cybercrime, terrorism, and high-level financial fraud.
What We’re Looking For
The ideal candidate
is an empathetic teammate who loves to help others, to bring order to chaos, and to document their path for others to follow
focuses primarily on web scraping, API integration, and collection of raw data
is capable of becoming fluent in Layer 7 protocols, blockchains, and emerging protocols
has high moral and ethical standards
is proficient in production-level Python
has worked in a threat intel or security environment
What You Will Do
Your primary responsibility will be to become expert at collecting open source data from the web and from other online protocols.
You will be comfortable building tools to bring in data from random files, varying degrees of quality of web sites, and new communications tools. You will have the opportunity to creatively apply your knowledge of software automation towards scaling data collection.
You will understand popular chat sites and how fraud actors use them. You will understand emerging technologies and will actively evaluate them for potential opportunities to collect data.
Why Cloudburst?
At Cloudburst we want to minimize our software engineers working on pixel-pushing and to maximize time spent understanding our domain and building products to help our customers investigate and prevent fraud.
We care about developing and sponsoring our engineers internally. The department adheres to these principles:
Rapid deployment of innovative new techniques for signal generation and attribution
KISS architecture that doesn’t get in developers’ or R&D’s way; we want code to be easy to understand, less abstract, easy to test, fast to deploy, and reducible to automation or simple manual processes
Building a diverse, unconventional team that cross-trains and grows together without ego or sacrifice to work-life balance
What You Will Be Using
Python
Cloud-based infrastructure
3rd party vendor integrations
Novel techniques for data collection using home-grown or discovered tools and frameworks
Hiring Process
Initial email
Initial phone screen with the hiring manager
Take-home test OR link to previous code
Panel interview
Call with CEO
Offer
Investors
Strategic Cyber Ventures
Coinbase Ventures
Bloccelerate
More Info:
https://www.crunchbase.com/organization/cloudburst-technologies-0a3e
https://www.prnewswire.com/news-releases/cloudburst-technologies-raises-3m-in-seed-funding-led-by-strategic-cyber-ventures-joined-by-coinbase-ventures-and-bloccelerate-301817742.html
https://burst.cloud/
https://open.spotify.com/episode/2cv1Is77s8jGLtHPPYRLPU
Job Type: Full-time
Pay: $115,000.00 - $150,000.00 per year
Benefits:
Dental insurance
Flexible schedule
Health insurance
Paid time off
Vision insurance
Compensation package:
Stock options
Experience level:
3 years
Schedule:
Choose your own hours
No nights
No weekends
Work Location: Remote","$132,500 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
NLB Technology Services,#N/A,"Dallas, TX",CRM data Analytics Engineer,"MUST HAVE:
o Tableau CRMA & Einstein Discovery Certification
o Experience building Dashboards using CRMA (aka Tableau CRM and Einstein Analytics)
o Design, build, and support datasets, data recipes, dataflows, and dashboards using Salesforce and non-Salesforce data sources
o Troubleshoot functional, data, and performance issues with CRMA Dashboards
o Skilled in SAQL, JSON
o Develop and deploy AI/ML solutions using Einstein Discovery and deploy AI/ML Models built outside Salesforce
o Experience working with large data (1 B+ rows) and supporting large user base (10,000+)
o Mentor and coordinate the development efforts of other CRMA DevelopersAT&T
Job Type: Contract
Salary: $65.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Vision insurance
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
CRM ANalytics: 8 years (Required)
einstein: 1 year (Required)
Salesforce: 1 year (Required)
Work Location: One location",$70.00 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"E-Business International INC
3.6",3.6,"Alexandria, VA",AWS Data Engineer,"Role: AWS Data Engineers
Location: Alexandria, VA
Full-Time/Permanent
AWS Engineers
We are seeking TWO (2) AWS Engineers to join our growing team. The qualified applicants will become part of the Enterprise Data Analytics Services (EDAS) program for a large federal agency in Alexandria, VA. Hybrid work options are available.
The AWS Engineer is responsible for the development and support of cloud-based solutions. Solutions include moving assets from on-premise to cloud environment, evaluation, and optimization. AWS Engineer will collaborate with the team effectively to ensure the team's success.
Responsibilities/Duties:
Work on automating the migration process in AWS from development to production.
Create CFT deploying/updating cloud resources.
Advise the engineering and software engineering team as they migrate from on-premise to cloud infrastructure.
Optimize cloud workloads for cost, scalability, availability, governance, compliance, etc.
Guide and/or provide hands-on support to administer production, staging, and deployment environments.
Partner with multi-disciplinary teams to understand requirements and plan architecture and solutions.
Apply insight and expertise across AWS services.
Apply knowledge of scripting and automation using tools like PowerShell, Python, Bash, Ruby, Perl, etc.
Experience and Qualifications:
Good working knowledge of AWS Services, including Compute, Storage Networking, Database, Management tools, Security, Identity, and Compliance.
3 years’ experience in AWS (EC2, Glue, S3, Redshift, Python programming).
Experience implementing and debugging workloads in AWS.
Good knowledge of creating CFTs.
Experience building infrastructure inside AWS via code. Familiarity with tools such as Terraform or CloudFormation.
Ability to proactively communicate, interact, and solve critical customer problems.
Ability to think and act strategically and proactively.
3 years of experience in software development tools and methodologies.
3 years of experience in infrastructure architecture, database architecture, and networking.
3 years of experience architecting/deploying operating solutions built on AWS.
Education/Certifications:
Bachelor's Degree in Computer Science, Information Technology, Engineering, or associated discipline.
AWS certification is a plus!
Clearance Requirements:
Ability to attain and maintain a Public Trust.
Job Type: Full-time
Salary: From $100,000.00 per year
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Alexandria, VA 20598: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Expected Salary Range (Must)?
Visa Status/ Work Authorization (Must)?
Experience:
AWS: 4 years (Required)
Work Location: One location","$100,000 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,1992,Unknown / Non-Applicable
"Dun & Bradstreet
3.9",3.9,Remote,Data Engineer I (R-14521),"Why We Work at Dun & Bradstreet
Dun & Bradstreet unlocks the power of data through analytics, creating a better tomorrow. Each day, we are finding new ways to strengthen our award-winning culture and accelerate creativity, innovation and growth. Our 6,000+ global team members are passionate about what we do. We are dedicated to helping clients turn uncertainty into confidence, risk into opportunity and potential into prosperity. Bold and diverse thinkers are always welcome. Come join us!

The Data Engineer I will work with the senior data engineers to build and maintain applications that are responsible for the ingestion of data into the Dun and Bradstreet Contact Pipeline.

The Data Engineer I will work on analyzing performance/throughput blockers of the applications and will work with other team members to remove the bottlenecks in the applications.

The Data Engineer I will also work with creating metrics from the ingestion applications to help determine the areas that need work in terms of performance and/or throughput.
Responsibilities:
Create and maintain applications in python.
Take ownership of existing applications for further development/improvements
Work closely with related groups to ensure business continuity
Perform analysis on code bases to increase performance.
Work as part of the team to code review and test other members’ code changes.
Work as a member of one or more agile teams, using lean principles and SCRUM methodology

Requirements:
Bachelor’s degree (preferable in computer science, mathematics, data science, or a related field)
Experience with Python for application development (2-5 years)
Experience with SQL for data analysis and querying (2-5 years)
Ability to work independently to deliver critical projects on time
Ability to work closely with others to problem solve
Experience with hosted environments, AWS, Azure, or other cloud service providers preferred
Benefits We Offer
Generous paid time off in your first year, increasing with tenure.
Up to 16 weeks 100% paid parental leave after one year of employment.
Paid sick time to care for yourself or family members.
Education assistance and extensive training resources.
Do Good Program: Paid volunteer days & donation matching.
Competitive 401k & Employee Stock Purchase Plan with company matching.
Health & wellness benefits, including discounted Gympass membership rates.
Medical, dental & vision insurance for you, spouse/partner & dependents.
Learn more about our benefits: http://bit.ly/41Yyc3d.

Pay Transparency
Dun & Bradstreet is an equal employment opportunity employer and believes in honesty and transparency in the employment hiring process, including pay transparency. Accordingly, listed on this posting is a good faith reasonable estimate of the salary range and other compensation in the job posting, as of the date of this posting. Actual compensation decisions for base salary and other compensation will be dependent upon a wide range of factors including but not limited to: an individual’s skill sets, experience, qualification, training, education, location, and any other legally permissible factors. Successful applicants will also be eligible for D&B’s generous benefit package, outlined above.

All Dun & Bradstreet job postings can be found at https://www.dnb.com/about-us/careers-and-people/joblistings.html. Official communication from Dun & Bradstreet will come from an email address ending in @dnb.com.

Equal Employment Opportunity (EEO): Dun & Bradstreet is an Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, creed, sex, age, national origin, citizenship status, disability status, sexual orientation, gender identity or expression, pregnancy, genetic information, protected military and veteran status, ancestry, marital status, medical condition (cancer and genetic characteristics) or any other characteristic protected by law. View the EEO is the Law poster here and its supplement here. View the pay transparency policy here.

Global Recruitment Privacy Notice",#N/A,5001 to 10000 Employees,Company - Public,Information Technology,Information Technology Support Services,1841,$1 to $5 million (USD)
"Vedainfo
4.1",4.1,"Peoria, IL",Data Engineer,"Job Description
· Create and develop optimal data pipeline which fetches data from raw and apply business transformation and load derived.
· Build the AWS Fargate components required for optimal extraction, transformation, and loading of data-to-data lake.
· Develop data pipeline setup between base and derive values based on Caterpillar specification.
· Implement quality validation rules and follow process defined by stakeholder.
· Develop business events as defined by the solution team to generate CDC events for consumption layer (Kinesis)
· Create re-usable and configurable components wherever applicable.
· Create and execute unit, integration, and regression tests
· Ensure the code is developed satisfying all business rules, perform unit testing where applicable.
· Data pipelines development
· Python Development
· AWS cloud services
· Data quality validation rules
· Java (preferred)
· Implement CDC
· Transformation rules creation
· Message broker tools like Kinesis
· Azure DevOps – CI/CD pipelines
· Automated and Manual Testing experience
Job Type: Contract
Pay: Up to $60.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Peoria, IL 61602: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
This position is remote initially and you will be asked to work any of the client location once offer is made. Please confirm your current location.
Experience:
Data Engineer: 1 year (Preferred)
Python Development: 1 year (Preferred)
AWS: 1 year (Preferred)
Message broker tools: 1 year (Preferred)
Work Location: In person",$60.00 /hr (est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$1 to $5 million (USD)
"Deako
4.9",4.9,"Seattle, WA",Data Engineer II,"About Deako
Deako is delivering a revolutionary plug-n-play smart home platform with a laser focus on the untapped new home construction market. We make smart lighting so easy to upgrade that even those who would have never considered smart lighting are making it happen. We've built a company based on trust; where forming personal relationships is key to our success.
Software at Deako
The software team is a tight-knit team of smart, dedicated people. We are customer focused, we ship often, and are constantly asking “why.” We are passionate about the software we write. We work closely with other Deako teams. Developers at Deako are always looking for a better way and understand that no code is perfect.
Data at Deako
We believe that Data and Software go hand in hand. That's why the Data and Software teams work closely together to ensure we're collecting the right data, and getting it to the right people. Data is new at Deako, and we are working to make it a cornerstone of our company. We want every team at Deako to have a data-driven mindset.
Day to Day Expectations:
Participate in the code review process in our DBT repository
Build maintainable, testable SQL queries for various dashboards
Help maintain, contribute to and improve our data infrastructure and existing pipelines
Help maintain and improve our CI/CD pipelines
Responsible for obtaining, cleaning, and munging data and getting it into a form that our data analysts can access and analyze
Participate in meetings with stakeholders around data collection and cleaning

Must Have:
Demonstrable knowledge in SQL + Some Programming Language (Typescript, Python, Ect)
Demonstrates a high level autonomy and willingness to learn
1+ years of experience working as a Data Analyst/Engineer or Comparable field.
Experience with git, dbt
Nice to Have:
Experience in Snowflake, Fivetran, Hightouch
Experience with Gitlab CI
Experience with geospatial data
Stock Options
Hybrid work environment (Office + Remote)
Medical/Dental/Vision/Life/401K
Unlimited PTO
Free Snacks/Coffee/Drinks (Non-Alcoholic and Alcoholic)
Quarterly Company Parties","$115,000 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,2015,$5 to $25 million (USD)
"Myticas Consulting
3.9",3.9,Arizona,BHJOB15656_20058 - Data Engineer,"Myticas's direct client based out of Phoenix, AZ is currently seeking a Data Engineer for a 100% remote contract position.
Pay Rate: Up to $34/hr

Job Description:
A Data Engineer can confidently understand, write, troubleshoot, optimize, and peer review SQL code. They understand business requirements and create code and reporting that exceeds specifications through anticipating underlying needs. They also understand relational database design, table structure, data types, and data models. They are experienced with and confident in development in reporting visualizations in Excel, PowerBI, Tableau, QuickSight, SSRS, or other data products. They can create reporting dashboards and products that inform the business. They easily grasp and retain complex data concepts and are able to explain them to others. Looking for engineers who are driven to innovate, use cutting edge technologies to solve problems and think outside the box.
What You’ll Do:
Digital Transformation into AWS
Data Warehouse related work (SQL, ETL, Replication, Databases)
Understand, write, troubleshoot, optimize, and peer review SQL code.
Understand business requirements and create code and reporting that exceeds specifications through anticipating underlying needs.
Understand relational database design, table structure, data types, and data models. 6.Experienced with and confidence in development in reporting visualizations in Excel, PowerBI, Tableau, QuickSight, SSRS, or other data products.
Data Warehouse related work (SQL, ETL, Replication, Databases
Perform other duties as assigned or apparent.
MINIMUM EDUCATION AND RELATED WORK EXPERIENCE:
Bachelor's degree in a Computer Science field and 2 years' work experience
2 years of experience in most phases of IT systems deployments in one or more of the following areas: design and deployment of cloud services, data migration to AWS cloud, Digital Transformation into AWS, and Data Warehouse related work
INDCHI
INDREM",$34.00 /hr (est.),1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"adroitts
4.5",4.5,Remote,Data Analyst/ Data Visualization Engineer,"adroitts is a fast-growing IT solutions company that helps businesses adapt and grow in a continuously evolving market. Our tailor-made technological solutions are perfectly aligned to our client’s business goals and objectives. we strive to be a long-term trusted and reliable partner for our customers organization to help overcome IT challenges. Our solutions, methodologies and implementations are designed with customer centric focus and customers ROI. We pride ourselves in our employees feeling welcomed, valued, and involved.
We are seeking a Data Analyst/ Data Visualization Engineer with Tableau Experience with 10+ years of experience
Requirements:
· Remote position
· Data analysis and profiling
· Maintenance of existing processes
· Querying and analyzing data with SQL and Python
· Creating visualizations using Tableau
· Executing monthly reports.
Deliverable:
· Querying and analyzing data with SQL and Python
· Creating visualizations using Tableau
We are looking forward to hearing from you!
Best,
adroitts
Job Type: Contract
Pay: Up to $70.00 per hour
Schedule:
8 hour shift
Experience:
Tableau: 10 years (Preferred)
Python: 10 years (Preferred)
SQL: 10 years (Preferred)
Work Location: Remote",$70.00 /hr (est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Locate Software,#N/A,Remote,Cloud Data Engineer-DBA,"Role: Sr :Cloud Data Engineer with DBA Experience.
Location: Bellevue ,WA(Remote)
Experince:8+ (Senior Level)
Duration : 12+ Months
Looking for an experienced senior-level data engineer (DBA)with PROVEN and varied cloud experience who can hit the ground running as both contributor and strategist for our architects and engineers across multiple scrum teams. This is a role that must be able to deliver with results and excellence. Will expect to see well-rounded individuals with a deep understanding and proven diverse experience working with APIs, backend platforms and cloud (AWS,OpenSearch and Elastic search a must).
KEY EXPERIENCE & SKILLS
8+ years demonstrated expertise as data engineer and DBA responsibilities, diverse cloud experience with ability to be strategic and tactical.
Extensive experience working on both relational and SQL databases (Oracle, RDS, etc.) on security aspects like database provisioning configuration, DB identity management using Domain auth or secrets management tools like CyberArk or Vault, enforcing Role-based access controls following least privilege principle, data encryption and archival processes.
Understanding and enforcement of Security best practices for databases in support application development and operations.
Minimum 2-3 years diverse experience working with these Cloud technologies (AWS preferred) like Lambda, Amazon API Gateway, S3, DynamoDB, Cloudwatch.
Hands-on scripting experience using shell, python, Java to create DB automation utilities.
Proven security experience with role-based access control, access management and data replication
Experience working in an onshore, scrum Agile team and DevOps practices.
Experience with data integration principles within heterogeneous distributed database Platforms
Proven experience designing and building reliable, scalable data infrastructure with leading privacy and security techniques to safeguard data using AWS technologies.
Has built frameworks for data ingestion pipelines both real-time and batch using best practices in data modeling, ETL/ELT processes with seamless hand-off to data engineers
Demonstrated ability to work with architects, engineers and stakeholders in a variety of ways – tactical to strategic and consulting
Excellent collaborator and communicator
Experience presenting, communicating and partnering with leadership and executive stakeholders
Ability to drive technology direction and selection by making recommendations based on experience and research
Must Have:
Open-search and elastic search / AWS cloud experience.
Cloud (distributed with data).
NOT Big Data, Data transformation, data migration, SQL focused… this is an engineer / engineering consultant who will be HANDS ON with our engineers, architects, and principal engineers the language in this description chosen carefully to reflect the focus from a DBA/ Data perspective.
We are using data engineer instead of DBA on purpose – this is not a traditional DBA role, more of a data engineer role.
Entry level or lower mid-level will not work. We need someone who can hit the ground running with proven experience.
Able to communicate VERY well and collaborate in a fast-paced environment.
Job Types: Full-time, Contract
Pay: $117,796.00 - $127,676.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Schedule:
Monday to Friday
Experience:
Data Engineer: 8 years (Required)
Database administration: 5 years (Required)
AWS: 6 years (Required)
Elasticsearch: 4 years (Required)
Lambda: 5 years (Required)
DBA: 4 years (Required)
Work Location: Remote","$122,736 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Maven Workforce
4.1",4.1,"Alpharetta, GA",Data Engineer,"Must Have skills:
Building Data Pipeline exp is a must
ETL Tools – SSIS, Alteryx
Data modelling
SQL Server development
Microsoft SQL Stack
Responsibilities:
Data acquisition and ingestion - Identify data sources and build pipelines using various ETL tools such as but not limited to, SSIS, and Alteryx. Data sources including but not limited to SQL Server, SAP, Teradata, Hadoop\Hive, PostgreSQL, Oracle and flat files.
Identifying ways to improve data reliability, efficiency and quality by various data solution techniques.
Expertise in Data project management with JIRA stories. Work as a liaison between business and IT to ensure successful and timely completion of the projects.
Assist in defining the data architecture framework, standards and principles, including modeling, metadata, security and reference data.
Create and optimize data models to support various business applications.
Reviewing modifications of existing data systems for cross-compatibility.
Automate and support workflows to ensure timely delivery.
Must Have:
5+ years of SQL Server development experience.
3+ years ALTERYX Admin /User management experience and advance workflow management.
5+ years data modeling experience.
5+ years of ETL experience.
5+ years of experience in working on more than one database technologies Microsoft SQL server, Teradata.
2+ years big data experience, Hadoop, Hive, Spark
Expert knowledge of data warehousing.
DESIRED SKILLS:
BI Lifecycle management
Working understanding of Microsoft VBA, HTML, Python
Data model development using ERWIN
Job Type: Contract
Salary: $50.00 per hour
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Building Data Pipeline: 8 years (Required)
ETL Tools – SSIS, Alteryx: 7 years (Required)
Data modelling: 6 years (Required)
SQL Server development: 8 years (Required)
Microsoft SQL Stack: 8 years (Required)
Work Location: One location
Speak with the employer
+91 7328456015",$50.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2008,Unknown / Non-Applicable
TekValue IT Solutions,#N/A,"Houston, TX",Data Engineer with Migrating,"Required Skills:
8-10 Required Experince on Data Engineer and Revalent Skills
Experience with NoSQL (MongoDB)
Experience with API'S
Good Knowledge on Data Migration like Oracle to Mongo db
Experience with Python Programming
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Houston, TX 77002: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 8 years (Required)
NoSQL: 6 years (Required)
MongoDB: 6 years (Required)
Migrating: 5 years (Required)
Work Location: One location
Speak with the employer
+91 7328323606",$72.50 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"M247
4.3",4.3,"Saint Petersburg, FL",Data Center Networking Engineer,"About Us:
At M247 www.m247.net, we believe that a well-designed and well-built infrastructure makes for a strong foundation. It is this belief that drives us to continuously research and discover new products and solutions and refine our process and methods so we can continually serve our customers.
Our mission is to empower businesses with seamless, reliable, and secure IT solutions. We strive to create innovative and customized networking services that cater to the unique needs of our customers across healthcare, financial, energy, and retail sectors. Our passionate commitment to excellence and customer satisfaction drives us to shape the future of IT, enabling organizations to thrive and grow in an increasingly connected world.
We are currently accepting resumes from competent people for our team of Data Center Engineers.
Overview:
M247 is seeking a highly skilled Data Center Networking Engineer to join our team on a contract basis. The successful candidate will have extensive experience designing, implementing, and managing data center networks.
Responsibilities:
Designing, building, and maintaining the data center networks
Installing and configuring networking equipment and related systems
Managing and troubleshooting data center network issues
Conduct network assessments and audits
Participate in the planning and implementation of data center upgrades and expansions
Create and maintain network documentation
Collaborate with cross-functional teams to deliver network solutions
Personality Traits:
Extremely performance driven
Ability to work independently
Excellent written and verbal communication skills
Excellent problem-solving and analytical skills
Ability to work in a fast-paced environment with a high degree of accuracy
Ability to work effectively in a team environment
Qualifications:
Strong communication and interpersonal skills
CCNP Data Center certification or higher
Minimum 5 years of experience in the following Data Center technologies:
Network Virtualization and Automation – NFV, SDN, DNA, ISE, Ansible, Puppet, Chef or other automation tools
Storage Networking – SAN, NAS, FC, FCoE, iSCSI, MDS
Security – Cisco ASA and Firepower, ACLs, VACLs, TrustSec, Secure connectivity (IPsec, SSL/TLS VPNs)
Hardware Cisco Nexus and Catalyst switches, Cisco ISR and ASR, and Cisco UCS, VMware NSX, and F5 load balancers
Strong knowledge of networking protocols:
TCP/IP and IPv4/IPv6
BGP, OSPF, EIGRP, and IS-IS (Routing Protocols)
VxLAN, OTV, and LISP (Overlay Networking)
STP, RSTP, and MSTP
EtherChannel and LACP
Requirements:
Must pass a thorough background check and 12-panel drug test
Ability to lift and move heavy equipment
Travel requirement up to 50%
US citizens or green card holders only
1099 or C2C only
No agencies or recruiting firms
Compensation:
Hourly rate range $55 – $95
Travel expenses reimbursed per client travel policy

This is a remote position.",$75.00 /hr (est.),201 to 500 Employees,Company - Private,Telecommunications,"Cable, Internet & Telephone Providers",2000,$100 to $500 million (USD)
"Devcare Solutions
3.4",3.4,"Columbus, OH",Sr Data Engineer/data architect,"Role : Data Architect / Bigdata Architect / Hadoop / senior data engineer/ senior Bigdata
skills: oracle, SQL, Hadoop, bigdata , cloud era, Hive, data migration etc.
8+ years Data analysis/architecture experience in Waterfall and Agile Methodology in various domains (prefer Healthcare) in a data warehouse environment.
Good knowledge of relational database, Hadoop big data platform and tools, data vault and dimensional model design.
Strong SQL experience (prefer Oracle, Hive and Impala) in creating DDL’s and DML’s in Oracle, Hive and Impala (minimum of 8 years’ experience).
Experience in analysis, design, development, support and enhancements in data warehouse environment with Cloudera Bigdata Technologies (with a minimum of 8-9 years’ experience in Hadoop, MapReduce, Sqoop, PySpark, Spark, HDFS, Hive, Impala, Stream Sets, Kudu, Oozie, Hue, Kafka, Yarn, Python, Flume, Zookeeper, Sentry, Cloudera Navigator) along with Informatica.
Experience (minimum of 8 years) in working with Sqoop scripts, PySpark programs, HDFS commands, HDFS file formats (Parquet, Avro, ORC etc.), Stream Sets pipelines, jobs scheduling, hive/impala queries, Unix commands, scripting and shell scripting etc.
Experience in migrating data from relational database (prefer Oracle) to big data – Hadoop platform is a plus.
Experience eliciting, analyzing and documenting functional and non-functional requirements.
Ability to document business, functional and non-functional requirements, meeting minutes, and key decisions/actions.
Experience in identifying data anomalies.
Experience building data sets and familiarity with PHI and PII data.
Ability to establish priorities & follow through on projects, paying close attention to detail with minimal supervision.
Effective communication, presentation, & organizational skills.
Good experience in working with Visio, Excel, PowerPoint, Word, etc.
Effective team player in a fast paced and quick delivery environment.
Required Education: BS/BA degree or combination of education & experience.
DESIRED Skill Sets:
Demonstrate effective leadership, analytical and problem-solving skills
Required excellent written and oral communication skills with technical and business teams.
Ability to work independently, as well as part of a team
Stay abreast of current technologies in area of IT assigned
Establish facts and draw valid conclusions
Recognize patterns and opportunities for improvement throughout the entire organization
Ability to discern critical from minor problems and innovate new solutions
Skill
Data analysis/architecture experience in Waterfall and Agile Methodology in various domains (prefer Healthcare) in a data warehouse environment.
Good knowledge of relational database, Hadoop big data platform and tools, data vault and dimensional model design.
Strong SQL experience (prefer Oracle, Hive and Impala) in creating DDL’s and DML’s in Oracle, Hive and Impala
analysis, design, development, support and enhancements in data warehouse environment with Cloudera Bigdata Technologies, along with Informatica
Experience in migrating data from relational database (prefer Oracle) to big data – Hadoop platform is a plus.
Job Type: Contract
Salary: $70.00 - $90.00 per hour
Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
8 hour shift
Monday to Friday
Experience:
total: 10 years (Required)
oracle: 4 years (Required)
SQL: 3 years (Required)
cloud era: 3 years (Required)
hadoop / Bigdata: 3 years (Required)
data architect: 1 year (Required)
Data warehouse: 1 year (Required)
Willingness to travel:
100% (Required)
Work Location: On the road
Speak with the employer
+91 6148083833",$80.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2005,Unknown / Non-Applicable
"Stefanini, Inc
3.8",3.8,"Dearborn, MI",Data Engineer,"Stefanini Group is hiring!
Stefanini is looking for Data Engineer at Dearborn, MI (Hybrid)
For quick apply, please reach out to Rajat Baloria
Phone: (248) 728-2620
Email: Rajatsingh.Baloria@stefanini.com

Open to W2 candidates only!

Position Description:
Looking for a Software Engineer focused on delivering software leveraging Java and Python based on proven Lean/Agile methods. Knowledge of Big Data technologies like Hadoop and Spark is a plus.
The Software Engineer will work in a small, cross-functional, and co-located team. The Software Engineer will collaborate directly and continuously with business partners, product managers and designers, and will release early and often.
Position Responsibilities:
Work hands-on with the team and other stakeholders to deliver quality software products that meet our customer's requirements and needs.
Grow technical capabilities / expertise and provide guidance to other members on the team



Skills Required:
Exceptional software engineering knowledge; OO Design Principles
Basic understanding of Big Data and potential use cases
Strong desire to learn new skills and apply to solve business problems/opportunities “Spring Boot, Java, Angular, API, Micro Services, PCF, GCP (especially Cloud Run)”

Experience Required:
Overall 6 years of work experience in delivering customer facing products
Minimum 2 years of strong development experience in at least one of the following technologies: Java or Python on a Hadoop/Spark Platform

Education Required:
An associate's degree in Computer Science or similar technical discipline
Education Preferred:
Google Cloud Certification

Listed Salary Range may vary based on experience, qualifications, and local market, Also some positions may Include bonuses and other Incentives

About Stefanini Group
The Stefanini Group is a global provider of offshore, onshore and near shore outsourcing, IT digital consulting, systems integration, application and strategic staffing services to Fortune 1000 enterprises around the world. Our presence is in countries like Americas, Europe, Africa and Asia, and more than 400 clients across a broad spectrum of markets, including financial services, manufacturing, telecommunications, chemical services, technology, public sector, and utilities. Stefanini is a CMM level 5, IT consulting, company with global presence. We are CMM Level 5 company.",$82.00 /hr (est.),10000+ Employees,Subsidiary or Business Segment,Information Technology,Information Technology Support Services,1987,$1 to $5 billion (USD)
"Orange County's Credit Union
4.0",4.0,"Santa Ana, CA",Data Engineer,"Great Opportunity At Orange County's Credit Union
Must reside in the state of CA, AZ, NV or TX.
Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk.
Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture.
Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential.
Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY!
More about Orange County's Credit Union:
Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace.
As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service.
Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference!
Overview:
Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization.
Essential Functions:
Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making.
Translate business requirements to technical solutions by applying technical knowledge and strong business acumen.
Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services.
Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform.
Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc).
Technical Must Haves for this Role:
5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment.
3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases).
3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.).
2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores.
2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS).
Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.

The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.

We perform thorough background checks and credit checks. EOE.","$92,145 /yr (est.)",201 to 500 Employees,Nonprofit Organization,Financial Services,Banking & Lending,1938,$25 to $100 million (USD)
"Augeo Affinity Marketing, Inc.
3.7",3.7,Minnesota,Data Analytics Engineer,"Hello, we're Augeo-architects of enterprise engagement and loyalty platforms, delivering compelling experiences and fostering meaningful connections with employees, customers, members and channel partners across industries. We serve thousands of clients, including more than 70 of the top Fortune 500 companies, representing millions of end users. We are thought leaders and disruptors who think differently and creatively, built by entrepreneurs, operators and innovators.
At Augeo, we're passionate about providing an inclusive workplace that values diversity. Everyone is welcome, and our employees are comfortable bringing their authentic whole selves to work. Be you.
We work hard, we play hard-and most importantly, we care to our CORE about our teams and each other. We over-communicate around everything, especially while we are all connected virtually.
Summary/Objective
A data analytics engineer is responsible for creating and implementing personalized user recommendations. This involves using data and analytics to understand user behavior and preferences, and developing algorithms and models to recommend personalized content, products, and services to each user.
The main responsibilities may include:
Collaborating with designers, product managers, and other stakeholders to understand user needs and develop personalized solutions.
Designing and implementing personalization algorithms and models using machine learning, data mining, and other relevant techniques.
Analyzing user data and behavior to identify patterns and develop insights that can be used to improve the personalized user experience.
Keeping up to date with the latest trends and technologies in personalization and machine learning and applying these insights to enhance the personalization capabilities of the platform.
The ideal candidate for this role should have a strong background in computer science, software engineering, data science, or a related field. They should have experience with the AWS cloud platform, preferably the AWS Personalize service, and be familiar with machine learning and data mining techniques. Additionally, strong analytical and problem-solving skills are essential, as well as excellent communication and collaboration abilities to work in a team.
Requirements
Coding proficiency in at least one modern programming language (Python, R, Java, etc)
Relational database experience in SQL, AWS RDS
Experience with ML Services in AWS (Personalize, SageMaker) or equivalent a plus.
Strong critical thinking, communication, and problem solving skills and a quick learner.
Experience with cloud-based platforms (particularly AWS)
Experience working in multi-developer environment, using version control (i.e. Git)
Write Extract-Transform-Load (ETL) jobs to calculate business metrics
Own the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions
Monitor and troubleshoot operational or data issues in the data pipelines
Strong verbal and written communication skills.
Ability to operate independently and in teams.
Experience working within an Agile environment and JIRA preferred
Education and Experience
Bachelor’s Degree in Computer Science or related area
1-3 years of relevant work experience in analytics, data engineering, business intelligence or related field.
Experience using SQL queries, experience in writing and optimizing SQL queries in a business environment with large-scale, complex datasets
Knowledge of data warehouse technical architecture, infrastructure components, ETL and reporting/analytic tools and environment
Benefits of joining our team
Medical, Dental & Vision Insurance
Employer-sponsored Long-term disability and Life Insurance
Paid Time Off and flexible work schedule
401(k) Plan
Fun and casual work environment
Career growth opportunities
Rewards & Incentives",#N/A,201 to 500 Employees,Company - Private,Media & Communication,Advertising & Public Relations,1999,$100 to $500 million (USD)
"Double Line, Inc.
4.1",4.1,"Austin, TX",Data Engineer,"(This is a remote position open to candidates residing in North Carolina and Texas. We have an office location in Austin, TX for use at our employees' convenience. We have no plans to return to the office on a mandatory basis.)

Feeling underappreciated? Underutilized? Want to be a part of a specialized team with exposure to a wide variety of data puzzles to solve, while using your skills to improve education? Come join a team where you can Fly the Airplane, not just be a passenger in the back. We're a growing company focused on expanding our Operations team with a solutions-focused Data Engineer. Sound interesting?

If so, we're looking for a motivated and driven person like you who has:
Strength in thinking creatively and collaborating with other data experts in figuring out solutions to really tough data loads or transformation problems
Experience leveraging SQL and/or ETL development, data mapping, and data modeling to manage and organize client data
A passion for continuous improvement in refining the approach and doing it better and faster the next time

Bonus points if you're bringing knowledge of or really want to learn the following:
Consultancy experience with a focus on Agile practices
AWS and Azure Cloud
Python or similar scripting languages
AWS Quicksight, Tableau, Power BI, or other visualization tools

In return, we offer:
A mission-driven company with a long-term focus on helping the world by untangling the technical knots that plague state and local governments, particularly in education, healthcare, and similar fields
A home where your voice matters and you can affect real change
An employer who cares about you, makes sure you're engaged with exciting work, and offers robust benefits, 401k with employer match, and a great culture

We do not want you to make the leap without knowing what we need, so here is how we define success for this position:
Soak up knowledge from the existing team of experts in the first 30 days
Bring fresh eyes to our processes and techniques and bring new ideas to the table in the first 2 months
Mentor a new data engineering hire in your first 90 days

We need to know - can you make this happen? If so, we definitely need to talk to you.

Double Line understands the importance of creating a safe and comfortable work environment and encourages individualism and authenticity in every member of our team. We provide equal employment opportunities to all employees and applicants for employment and prohibit discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws. This policy applies to all terms and conditions of employment.

Double Line does not currently offer relocation assistance.

iS4dbmGU5w","$87,121 /yr (est.)",51 to 200 Employees,Company - Private,Management & Consulting,Business Consulting,2009,$5 to $25 million (USD)
"Guardsman Group
3.0",3.0,United States,Data Engineer,"A Little About Us
The Guardsman Group stands on over 40 years of experience, unmatched technical capabilities and the unwavering belief in the right of safety for all. Each and every day, our matchless range of services puts Guardsman in the lives of people in every corner of Jamaica and throughout the Caribbean. We've pioneered technologies and perfected procedures to give our customers the best solutions for their homes and businesses. As we enter another decade, we continue to be the industry leader. Today, Guardsman consists of 13 companies and over seven thousand talented staffers who are proud to call themselves a Guardsman.
The Role
This role is within the Business Performance, Analytics and Intelligence (BPAI) Unit of the Guardsman Group and is directly responsible for the shaping, building and implementation of solutions that satisfy the business intelligence needs across the Group.
What You'll Be Doing
Create and maintain optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build analytictools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics
Collaborate with other members of the BPAI unit to build and improve on the availability, integrity, accuracyand reliability of data pipelines
Work closely with all levels of management, IT department and other members of the BPAI Unit to achieve task objectives
A Little Bit About You
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases
Experience building and optimizing 'big data' data pipelines, architectures and data sets
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Strong analytic skills related to working with unstructured datasets
Build processes supporting data transformation, data structures, metadata, dependency and workload management
Minimum Qualifications
Bachelor's Degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline",#N/A,5001 to 10000 Employees,Company - Private,Management & Consulting,Security & Protective,#N/A,Unknown / Non-Applicable
"GTA (Global Technology Associates)
4.6",4.6,"Plano, TX",Data Logging Engineer,"Looking for a Data Logging Engineer
What you will be doing as a Data Logging Engineer
· Perform system level log captures/analysis of several Network elements in the 4G/5G Network architecture to identify root causes of product and key performance issues.
· Document and maintain the troubleshooting steps including updating technical documentation for primary areas of responsibility
· Identify automation opportunities & implement scripts for log analysis, for discussions with external customers/internal customers
· Strong on customer support as well as data analysis of logs, etc.
· Must be able to drive for issue resolution with other Engineering team
· Initial analysis of Network KPI degradation to quantify and qualify the issue
· Be able to proactively perform Ad-hoc/periodic parameter audit, issue signature audit and SW/FW audit
· Ensure the latest ticket status ticket status is provided to the customers
· Software and patch upgrades on network element to support feature testing and issue resolution verification
· Provide 24X7 support as needed for commercial Network outages and network performance issues. Also provide technical reviews and provide support to customers as needed
What you will bring to the table as a Data Logging Engineer:
· Bachelor’s Degree in Telecommunication or Network Engineering with at least 2 years of experience in Wireless Telecom environment.
· Good knowledge of UNIX/LINUX operating systems
· Good Scripting skills using but not limited to Python, Shell or Perl
· Strong knowledge of MS office tools like Excel, power point and Word.
· Good analytical skills to investigate and evaluate trending data for issue resolution
· Good knowledge of 4G and 5G wireless network architecture concepts, RAN protocols and eNB interfaces S1-MME, S1-U is a plus
· Good knowledge of trouble ticket and knowledge management systems
· Willing to work during night to accommodate debugging involving offshore R&D teams
What you didn’t know about us:
· Competitive salary
· Health, Dental and Vision Benefits
· Short/Long Term Disability and Critical Care/Illness Protection
· Life Insurance and Retirement Plans
· Employee Assistance Program
· With this position, you will get the opportunity to work with our game changing clients and further advance your already valuable experience in the telecom industry!
We are Connectors. We thrive on ‘quality over quantity’ and put in the work building strong relationships. We create connections, discover qualities, uncover skills, and place people with accuracy. We are your true partner!
We are Collaborators. You’ll be working with a wholly-owned subsidiary of Kelly and part of the Kelly Telecom division. It allows us to be as nimble and fiercely competitive as a startup while having the backing of a multibillion dollar publicly traded company which has been in business for 75 years. With direct access to hiring managers, services don’t stop at standard recruiting processes. We use our expertise to improve your application skills and provide ongoing career support.
We give 24/7 Support. We are in this together. We provide around the clock availability, competitive employee benefits, and continuously check-in to make sure things are going smoothly. Check out our Glassdoor page!
Kelly Telecom is an equal opportunity employer and will consider all applications without regard to race, genetic information, sex, age, color, religion, national origin, veteran status, disability, or any other characteristic protected by law. For more information click Equal Employment Opportunity is the law.
You should know: Your safety matters! Vaccination against COVID-19 may be a requirement for this job in compliance with current client and governmental policies. A recruiter will confirm and share more details with you during the interview process.
#JobsAtKellyTelecom
Job Types: Full-time, Contract
Salary: From $50.00 per hour
Schedule:
8 hour shift
Work Location: In person",$50.00 /hr (est.),Unknown,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Javara
3.1",3.1,Remote,Data Engineer II,"Summary:
Javara’s Data Engineer II is responsible for comprehensive technical subject matter expertise to maintain Javara’s enterprise data platform and business glossary driven data availability within Javara’s Technology department. To that end, the Data Engineer II builds and maintains pipelines that ingest, move, store, and prepare data based on conceptualized data frameworks and architecture for downstream analytics or operational use by data architects or business intelligence staff. With guiding principles of driven by integrity and empowered to make an impact, the senior data engineer is responsible for ensuring that the strategic data assets to support internal and external stakeholder success are available, trusted, and secure. Primary business use cases are varied across enterprise use cases— operational, clinical, and clinical research.
The Data Engineer II reports to the Vice President of Data and Analytics.
Essential Duties and Responsibilities:
Designing, developing, managing, and determining how Javara’s data will be stored and utilized, including management of the business data architecture to deliver maximal business value.
Assess internal and external data and design and maintain a blueprint to manage the available data.
Collaborating with business units to develop data solutions and models for their needs, as well as partner with enterprise architects to create data models in line with Javara’s business needs.
Provide technical subject matter expertise to internal and external teams for clinical trial data mapping, ingestion, and transformation best practices.
Create inventory of enterprise data and store data in an easily accessible format.
Design and develop complex database management systems and separate public data from private ones.
Implement a secured AWS-based disaster-recovery plan to cater for the data needs of the company in times of emergency or cyber-attack in close coordination with our Security Officer.
Collaborate with enterprise management needs to create data models in line with the organizations need.
Research to collate new data and update the company’s data warehouse from time to time.
Meld existing data architecture with new ones as new technology emerges.
Learn new techniques for data modeling and management of the data platform.
Design efficient and scalable data processing systems and pipelines on Databricks mounted on AWS, various AWS Services, Snowflake, Azure Active Directory IdP, Microsoft Power BI, and several third-party application technologies such as APIs, SFTPs, and EHRs/EMRs/clinical data warehouses.
Create technical solutions that solve business problems and are well engineered, operable, maintainable, and delivered.
Adhere to AWS based HIPAA data guidelines through all processes of the data lifecycle.
Develop scalable and reusable frameworks for ingestion and transformation of large datasets.
Ensure that data and metadata is accurate, complete, and across all platforms by designing and implementing tools to detect data anomalies.
Develop data models and mappings and build new data assets required by users.
Perform exploratory data analysis on existing products and datasets.
Provide technical guidance to help data users adopt new data pipelines and tools.
Understand trends and latest technologies and apply to evaluation of Javara’s requirements.
Identifying strategic data requirements and design models to make sure such data requirements align with the overall architecture for Javara.
Assessing the enterprise’s internal and external data and design blueprint to manage the available data.
Creating an inventory of Javara’s data and secure storage of the data in an easily accessible manner.
Protecting Javara’s data and ensuring data redundancy in times of emergency or cyber-attack.
Researching and discovering new data management models and techniques.
Meld existing data architecture with new ones as technology emerges.
Continuously improve the quality, consistency, accessibility, and security of Javara’s data activity across company needs.
Contribute to documentation and publications.
Qualifications:
Required
Fluent English.
Evidence of a successful track record of manipulating, processing and extracting value from large, disconnected datasets.
5 + years’ experience in data or systems engineering, ideally in Databricks.
3+ years’ experience in batch and streaming ETL using Spark, Python, Scala, or comparable language on Databricks.
Experience in delta sharing, JDBC connections, or other secure and scalable external data sharing services.
Familiarity with AWS Services not limited to Glue, Athena, Lambda, S3, and DynamoDB.
Demonstrated experience implementing data management life cycle, using data quality functions such as standardization, transformation, rationalization, linking and matching.
Demonstrated experience of data modeling techniques and database performance and cost optimization.
A Bachelor’s degree in information technology or equivalent experience.
Preferred
Experience in healthcare industry and/or clinical research.
Experience with FHIR, SDTM, HIPAA, 21 CFR Part 11.
Experience in predictive modeling.
Working knowledge of Microsoft services not limited to Azure Active Directory and Power BI.
Experience prepping structured and unstructured data for business glossary master data set and data visualization use cases.
Work Environment:
The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job.
This job operates in a professional environment
The noise level in this work environment is usually light to moderate
Travel: This position may involve rare to minimal travel.
Pre-Employment Screening: Drug screen and background check required.
This job description covers the most essential functions of this position and is not designed to contain a comprehensive listing of activities, duties or responsibilities that are required of the employee in this job. Duties, responsibilities and activities may change at any time with or without notice.
Job Type: Full-time
Pay: $57,748.13 - $69,546.13 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
How many years of experience do you have in data or systems engineering?
How many years of experience do you have in Databricks?
How many years of experience experience do you have in batch and streaming ETL using Spark, Python, Scala, or comparable language on Databricks?
How many years of experience do you have in delta sharing, JDBC connections, or other secure and scalable external data sharing services?
How many years of experience do you have in AWS Services (Glue, Athena, Lambda, S3, DynamoDB)?
Do you have experience in the healthcare or life sciences industry?
What are your salary requirements?
Work Location: Remote","$63,647 /yr (est.)",201 to 500 Employees,Unknown,Healthcare,Medical Testing & Clinical Laboratories,2018,Unknown / Non-Applicable
SecurePro,#N/A,"Arlington, VA",Data Engineer,"Collect, manage, and convert raw data accurately and reliably
Organize data systems for subgroup access and analyses
Configure and sustain data cloud structures
Able to work with structured and unstructured data.
Validate outputs of data pipelines
Convert existing SAS code to python/pyspark code for model operation in the cloud
Create and sustain policy analysis models in the cloud
Troubleshoot user interfaces in the cloud
Create and sustain intuitive user interfaces in the cloud
Degree in Data Engineering preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $100.00 per hour
Experience level:
3 years
Schedule:
8 hour shift
Application Question(s):
Are you a US citizen?
Must have DOD secret Clearance?
Work Location: Remote",$90.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"New York Technology Partners
3.9",3.9,"Charlotte, NC",AWS Data engineer,"Job Title: AWS Cloud Engineer
Location: Charlotte NC
Position: Contract
This is a major platform initiative which will set the pace and direction for Analytics in Vanguard.
Work with serverless cloud services to create integrations, APIs endpoints and related components
· Work on features and POCs with industry-leading technologies and tools
· Work on greenfield space to shape the modernization efforts for the analytics ecosystem
· Create automated tests for functional, performance, and security in order to ensure flexibility over time.
· Work alongside multiple lines of business and platform teams
· Create custom integrations between existing, AWS components, and vendor products.
About you:
· You care about software quality and maintainability
· You have an interest in serverless architecture
· You want to shape the Analytics modernization effort enterprise wide
· Qualifications
· Experience and technologies that you will work with:
· Scripting & Programming: Python – 5+ years’ experience
· 5+ years’ experience with Infrastructure as a Service
· 3+ years’ experience with building serverless applications
· AWS Core Services: Cloudformation, Step Functions, DynamoDB, S3, Glue, Athena
· AWS Serverless Infrastructure: Lambda, SQS, SNS, EventBridge, API Gateway, Application Load Balancer
Nice to have:
· AWS Associate or Professional Certification
· EMR knowledge
· Docker & Kubernetes
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Charlotte, NC 28202: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 5 years (Required)
IaaS: 3 years (Required)
serverless application: 3 years (Required)
Work Location: One location",$60.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,1999,$25 to $100 million (USD)
TheRIIM LLC,#N/A,"Houston, TX",Data Engineer,"Company Description

Client is in Banking & financial domain.

Job Description

Job Description Summary
Summary: As data engineer, you will join the Data, Analytics and Reporting team. Our goal is to help the business we support unlock the full potential of their data. In this role, you will design, implement and support data and analytics solution using a broad range of technologies including Hadoop, Spark, AWS and various NoSQL databases.

Description:
We are looking for individuals who are passionate about data and take pride in delivering high quality software.
As data engineer, you will join the Data, Analytics and Reporting team. Our goal is to help the business we support unlock the full potential of their data. In this role, you will design, implement and support data and analytics solution using a broad range of technologies including Hadoop, Spark, AWS and various NoSQL databases.
You’ll bring your in-depth knowledge of big data technologies best practice and a desire to work in a DevOps environment.

Qualifications

Your technical capabilities will include:
To excel in this role, you will be:
highly proficient in Hadoop and related technologies including HDFS, Spark, Impala and Hive
highly proficient in Java, Scala and/or Python
proficient in AWS including S3, Redshift, EKS, IAM and EC2
experienced with Linux
experienced with data modelling
experienced with CI/CD
familiar with AWS including S3, IAM and EC2
proactive and have great communication skills

Additional Information

Selection process includes Code exercise..",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Kinertia
5.0",5.0,"Pittsburgh, PA",Data Engineer,"Do you like working with big data? Looking for a great company to work for with great clients, too? We are seeking a strong data professional with the experience and know-how to take charge of our largest client's needs while keeping things sane with strong organization of assets.
As a Data Engineer with a focus on GoogleSQL, you will be responsible for designing, developing, and maintaining the data infrastructure for the company. This includes building and managing data warehouses, data lakes, and data pipelines using GoogleSQL. You will also be responsible for developing and implementing data security and governance policies.
Responsibilities:
Design, develop, and maintain optimal data pipeline architecture
Build and manage data warehouses, data lakes, and data pipelines using GoogleSQL
Assemble large, complex data sets that meet functional / non-functional business requirements
Develop and implement data security and governance policies
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and other ‘big data’ technologies
Work with business stakeholders to understand their data needs and requirements
Stay up-to-date on the latest data technologies and trends
Qualifications:
Bachelor's degree in computer science, information technology, or a related field
5+ years of experience in data engineering, with a focus on GoogleSQL
Prefer 2-3 years of knowledge and experience in development and tuning of ETL processes
Experience with data warehousing, data lakes, and data pipelines using GoogleSQL
Experience with data security and governance using GoogleSQL
Experience with programming languages such as Python, Java, or Scala
Experience with big data technologies such as Hadoop, Spark, or Hive
Strong problem-solving and analytical skills
Excellent communication and teamwork skills
Results driven, ability to explain projects to both internal and external stakeholders
Benefits:
Very competitive salary and great benefits (medical, dental, vision), matching 401(k), company-paid life insurance and long-term disability
Great work-life balance, flexible work schedule
Opportunity to work on cutting-edge data projects
Collaborative and supportive work environment
Chance to make a real impact on the business
Job Type: Full-time
Pay: $120,000.00 - $160,000.00 per year
Benefits:
401(k) matching
Dental insurance
Employee assistance program
Health insurance
Health savings account
Life insurance
Paid time off
Vision insurance
Compensation package:
Performance bonus
Yearly pay
Experience level:
5 years
Schedule:
Day shift
Monday to Friday
Ability to commute/relocate:
Pittsburgh, PA: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
Big data: 5 years (Required)
SQL: 2 years (Preferred)
Data warehouse: 2 years (Preferred)
Work Location: In person","$140,000 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Cloud Destinations
4.7",4.7,Remote,Cloud Data Protection Engineer,"My name is Muralidharan and I’m working with Cloud Destinations LLC as a Technical Recruiter.
Below are the job details as needed by the hiring manager. Kindly advise if you might be interested and qualified for the below opportunity and if we can discuss further on this
Position: Cloud Data Protection Engineer
Duration: 12-24+ Months Contract
Location: Remote
They are open to having remote resources (would have to travel occasionally), and would prefer candidates based in Charlotte, NC, Islen, NJ, or Dallas, TX.
Cloud Data Protection Engineer is responsible for designing, engineering and implementing a new, cutting edge, cloud platform security for transforming our business applications into scalable, elastic systems that can be instantiated on demand, on cloud.
The role requires for the Engineer to design, develop, configure, test, debug and document all layers of the Cloud stack to satisfy the new big data system & Security requirements.
This is expected to range from Cloud hosting platform to the design and implementation of higher level services such as the IaaS, PaaS and SaaS layers, big data platform, Authentication/Authorization, Data encryption techniques (field level, db level, application layer encryption) masking tokenization techniques, and other security services.
The focus of this role is on the Security of the product and service for cloud big data platform, with understanding of ETL, data consumption and data migration security as well as data loss prevention techniques.
The ideal candidate should be comfortable being directly involved with the design, development, testing, and operation of the solutions that will be composed into the Cloud Services environment.
They will also provide comprehensive security consultation to business unit, IT management and staff at the highest technical level. Be able to conduct detailed threat analysis and identify mitigating security controls and solutions.
They will work closely with Cloud Services management to identify and specify complex business requirements and processes that drive the platform and application security Patterns and roadmap.
They will research and evaluate alternative solutions and make recommendations for changes that would enhance the security of the platform
Job Type: Contract
Salary: $70.00 - $78.00 per hour
Schedule:
8 hour shift
Experience:
Cloud Security: 10 years (Required)
IaaS, PaaS, SaaS: 4 years (Required)
Data Encryption: 5 years (Required)
Data Protection: 4 years (Required)
Data loss prevention: 5 years (Required)
Big data: 1 year (Required)
Work Location: Remote
Speak with the employer
+91 925-887-0055",$74.00 /hr (est.),201 to 500 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2016,$5 to $25 million (USD)
Hire Force Global,#N/A,Remote,Lead Data Engineer,"We are hiring Lead Data Engineer with Min 6+ years experience with AWS Technologies.
Visa Type – GC/USC
Job Type-W2
Job Mode – Remote
Job Description:
Minimum 12+ years’ experience with designing, developing, delivering and maintaining large scalable Cloud systems.
Minimum 6+ year experience with AWS Technologies
Lead Data Engineer will be a key contributor to our Strategic Products Engineering team and will have the below responsibilities:
Work with product owners to understand existing application capabilities and build solutions that deliver the business value.
Apply technical background/understanding, business knowledge, system knowledge in the elicitation of Systems Requirements for projects
Leverage industry standard and design patterns to build technical approaches and to guide the team towards writing better code.
Take on development responsibility for key technical features.
Lead requirements and design for non-functional requirements like security, observability and performance as it pertains to AWS
Adhere to existing processes/standards including the project development lifecycle, business technology architecture, risk and production capacity guidelines and escalate issues as required
Experience working in an Agile environment with business and technical teams
Excellent time management skills with ability to prioritize and coordinate multiple tasks to ensure deadlines are met.
Experience with AWS Technologies like AWS S3, EC2, AWS Glue, Lamba Function, API Gateway and Python will be plus.
Ability to be forward-thinking and be able to analyse and anticipate project, technology, and team solutions to ensure successful project delivery
Ability to own and drive technological and team issues to resolution with minimal guidance.
Self-motivated, curious, eager to learn and able to thrive in a fast-paced, remote, or onsite environment
Strong communication (written, oral), documentation, and interpersonal skills.
Job Type: Contract
Pay: $50.00 - $55.00 per hour
Experience level:
10 years
6 years
Schedule:
8 hour shift
Application Question(s):
How many years of experience do you have in C#?
Experience:
Cloud development: 10 years (Required)
AWS: 6 years (Required)
Work Location: Remote",$52.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"ArchsystemInc
4.1",4.1,Remote,Azure Data Engineer,"Databricks – Experience using Azure Databricks platform; Experience with Python programming; Experience using pandas and numpy for data engineering and data cleansing; Ability to troubleshoot the job scheduler and compute clusters; Experience with databricks CLI and secrets module; Experience with Azure Blob storage access configuration;
Job Type: Full-time
Salary: From $110,000.00 per year
Schedule:
Monday to Friday
Application Question(s):
Are you US Citizen or Green Care?
Do you have experience in Numpy or Pandas
Experience:
Azure Databricks: 2 years (Required)
Work Location: Remote","$110,000 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,$5 to $25 million (USD)
"Braintrust
4.6",4.6,"San Francisco, CA",Data Engineer,"ABOUT US:
Braintrust is a user-owned talent network that connects you with great jobs with no fees or membership costs—so you keep 100% of what you earn.

JOB TYPE: Freelance, Contract Position (no agencies/C2C - see notes below)
LOCATION: Remote - United States only (TimeZone: EST | Partial overlap)
HOURLY RANGE: Our client is looking to pay $100 – $105/hr
ESTIMATED DURATION: 40h/week - Long term

ABOUT THE HIRING PROCESS:
When you join Braintrust, you will be invited to a screening process for Braintrust to learn more about your previous work experiences. Once completed, you will have access to the employer for this role and other top companies that seek high-quality talent. Apply to this job to kick off the process.
THE OPPORTUNITY
Requirements
Summary:
Advanced analytics SQL - minimum 2 years, preferable 5
Python - minimum 1 year, preferable 2
Ability to work in cloud platform
Qualities that will help you thrive in this role:
You understand that being an effective engineer is about communicating with people as much as it is about writing code.
You are willing to work with and improve code you did not originally write, primarily in SQL and Python.
You are generous with your time and experience and can mentor and learn from other engineers.
You are comfortable with best practices for traditional data warehousing.
You love SQL and writing efficient and optimized ETL pipelines.
You are familiar with building and monitoring cloud services and infrastructure.
What you’ll be working on
What’s the role?
As a member of our client's Data Applications, Data Warehouse team, you’ll help us improve the stability, performance, and usability of their BigQuery data warehouse while advising their stakeholders on best practices and optimizations. Your work will enable other developers, data scientists, and analysts to write the high-performing pipelines that power data science, machine learning, and product development.
In addition to BigQuery SQL, our client's toolset includes Looker, Java, Python, and Spark, as well as Airflow, Terraform, and Kubernetes, and GCP services like Dataproc and Dataflow.
About The Team
They build highly-performant systems and data warehouses that are maintainable and cost effective.
They develop robust, highly available, well-monitored data infrastructure.
They stay in close communication with the internal customers and make strategic improvements to ensure those that depend on us have a great experience using data
What does the day-to-day look like?
You should have experience building data warehouses, data marts, and aggregate tables - supporting them at scale, and collaborating with other teams that depend on them.
Experience building applications and managing infrastructure using one of the major cloud providers is preferred but not required. (Our client uses Google Cloud).
Our client values curiosity, passion, responsibility, and generosity of spirit.
Apply Now!
Braintrust Job ID: 6590


C2C Candidates: This role is not available to C2C candidates working with an agency. If you are a professional contractor who has created an LLC/corp around their consulting practice, this is well aligned with Braintrust and we’d welcome your application.

Braintrust values the multitude of talents and perspectives that a diverse workforce brings. All qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status.

This is a remote position.",$102.50 /hr (est.),1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,2018,$100 to $500 million (USD)
Gointellects Inc.,#N/A,Remote,Data Engineer,"Required Skills:
· Collect, manage, and convert raw data accurately and reliably
· Organize data systems for subgroup access and analyses
· Configure and sustain data cloud structures
· Must have expertise in Data Visualization Tools (Tableau)
· Data Modeling/Science as Python/SAS
· Should have AWS cloud native services, security, data pipeline
· Able to work with structured and unstructured data.
· Validate outputs of data pipelines
· Degree in Data Engineering preferred.
Job Type: Full-time
Salary: Up to $190,774.76 per year
Compensation package:
Yearly pay
Schedule:
8 hour shift
Application Question(s):
Must have, ""Active Secret (or) Top Secret Security Clearance.
Must go on-site at least One Day in a week as requested by the client for internal meetings/demos etc at Washington DC.
Work Location: Remote","$190,775 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Veriforce LLC
3.7",3.7,"Spring, TX",Customer Success Data Engineer,"Who We Are
Veriforce® is a recognized leader in delivering supply chain risk management and compliance solutions that help bring workers home safely each day. The company’s SaaS-based contractor management and compliance software solutions, along with its standardized safety training programs and library of over 400 training courses, empower leading organizations to drive safety and compliance down to the worker level and more effectively mitigate supply chain and regulatory risk. With the industry’s largest safety and compliance network – comprised of 3200 hiring clients, 70,000 contractor companies, 9,000 accredited safety trainers and authorized evaluators, and 2.5 million individual workers – Veriforce is relied upon for innovative risk management solutions that help connect safety-conscious companies with a safe and qualified third-party workforce and make job sites safer, more productive, and more efficient.
The Role You’ll Play
The Data Engineer for Customer Success will be responsible for designing, building, and maintaining data processing architectures and solutions enabling the efficient conversion of structured and unstructured data to insights at enterprise scale. The CS Data Engineer work assignments are varied and frequently require interpretation and independent determination of the appropriate courses of action regarding data-driven decisions that lead to improved customer retention, advocacy, communication, and engagement outcomes. The Customer Success Team will depend on this person’s expertise and subject matter background to take initiative by digging and leaning into the data with heightened curiosity of trends and trajectories. A self-starter, this role will effectively translate data into addressable components that our customer-facing teams, leaders, and cross-org executives can act upon. This role will work closely with the entire Customer Success and Professional Services team to understand and then drive the strategic execution of reporting and system needs, then work to configure that vision into reality.
Core Responsibilities Will Be:
Ingesting data from internal and external sources utilizing cloud native platforms and software development best practices and patterns.
Develops software tools that leverage analytical and big-data techniques to cleanse, organize, and transform data into insights and actions that enables Veriforce To better serve our clients.
Understands department, segment, and organizational strategy and operating objectives, including their linkages to related areas.
Follow established guidelines/procedures.
Must be passionate about contributing to an organization focused on continuously improving consumer experiences
Arming the customer success organization with data driven insights by developing key dashboards for use by the CS and CS leadership team
Strategically collaborating with CS Leadership to enable the CS organization with the data they need to drive revenue growth and prevent churn
Developing key dashboards for use by the executive team for reflecting the activities and metrics within customer success
Creating the reporting frameworks and deliverables to support our Customer Success team in the ability to deliver peer benchmarking data, metrics, and other data points for Impact Assessments and Success Planning engagements.
Managing and updating a dashboard of key CS related reports for CSMs, Managers, Senior Leadership and the Chief Customer Officer
Identifying and gathering requirements from users, stakeholders, and CS leadership then proposing and building appropriate reports to support the identified needs.
Required Qualifications
Bachelor's degree or equivalent experience
5 years of technical experience
Experience with BI Tools such as PowerBI
Knowledge of SQL and relational database models
Documenting processes related to database design, configuration, and performance.
Experience designing, developing, and testing of software applications and/or infrastructure
Experience with APIs to expose large datasets to applications and data analytics solutions
Preferred Qualifications
Experience working with Customer Success Teams, reporting on customer churn, net dollar retention, gross dollar retention, etc.
Experience with Cloud-based solutions, Business Intelligence tools, and programming applications
Experience with Customer Success based systems, usage tracking, and CRM solutions such as Totango, Salesforce, and Pendo
Ability to create and manage data feeds to and from systems such as Salesforce, Totango, Pendo, Financial Systems, and others.
Ability to interpret data in order to help relay an accurate understanding of root causes that are primary drivers behind the data
Ability to collaborate with other analysts, engineers and data scientists in order to exploit data to drive the business forward
What Success Looks Like:
A successful candidate in this role will demonstrate:
The ability to quickly take action to mitigate any issues pertaining to data inaccuracies
Ability to quickly learn the company’s internal systems and the manner in which data is gathered.
A high focus on QA/QC efforts to ensure accuracy in the data
Enthusiasm toward continued personal/professional development
A desire to implement best practice technical solutions
Responsiveness to questions and system feedback
Here are just a few of the great reasons you should join our team!
We are mission-focused and mission-driven to help bring worker home safe every day. Our training products and compliance platform help keep workers safe.
Work with a global team! We have colleagues and customers across North America and overseas.
Veriforce is a great place to work! Our leaders and teams cite culture as one the top reasons this is a great place to work.
Veriforce provides
100% paid employee medical and dental insurance
Monthly contributions to Health Savings Accounts
A 401(k) match that is immediately fully vested
Outstanding time off benefits
Paid time off for volunteer activities
The successful candidate will have to undergo a criminal record check as condition of their employment.","$94,232 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Internet & Web Services,2001,Unknown / Non-Applicable
"Clairvoyant
4.1",4.1,"Hartford, CT",GCP Data Engineer,"Job type:C2C/W2
Location:Hartford, Connecticut or Remote with monthly visit to Client location.
Clairvoyant AI is looking for a highly motivated and experienced Data Engineer to join and help build our growing Cloud Engineering practice. The candidate would be responsible for working directly with customers to identify their Data Engineering needs, architecting any required solutions in Google cloud compute, working directly with our Onshore/Offshore team to facilitate task understanding and completion, maintaining the existing Data Pipelines, and also helping to implement solutions.
Must-Have
5+ Years of Experience in Data Engineering and building and maintaining large scale data pipelines
Experience with designing and implementing a large scale DataLake on Cloud Infrastructure
Strong technical expertise in Python and SQL
Extremely well-versed in Google Compute Platform including BigQuery, Cloud Storage, Cloud Composer, DataProc, Dataflow, Pub/Sub.
Experience with Big Data Tools such as Hadoop and Apache Spark (Pyspark)
Experience Developing DAGs in Apache Airflow 1.10.x or 2.x
Good Problem Solving Skills
Detail Oriented
Strong Analytical skills working with a large store of Databases and Tables
Ability to work with geographically diverse teams.
Nice to Have
Certification in GCP services
Experience with Kubernetes
Experience with Docker
Experience with CircleCI for Deployment
Experience with Great Expectations
Job Type: Full-time
Salary: $70.00 - $75.00 per hour
Schedule:
Monday to Friday
Experience:
Python: 5 years (Required)
SQL: 5 years (Required)
Hadoop: 5 years (Required)
Data lake: 5 years (Required)
GCP: 4 years (Required)
Work Location: Remote",$72.50 /hr (est.),501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,2012,Unknown / Non-Applicable
"spar information systems
3.5",3.5,Remote,Data Engineer,"Raja
Sr. Technical Recruiter
SPAR Information Systems
(a E-verify Company)
Phone: 469 – 750 – 0601
Fax : 1-214-291-2507
Email : Raja@sparinfosys.com
www.sparinfosys.com
Hello,
Hope you all doing great.
Kindly find the below JD and let me know if anyone interested for this below role on W2.
Sr. Data Engineer
Location: Remote
Duration: 3 Months Contract to Hire
Client: GEICO Insurance
No of positions: 30
Job Description:
Geico treats Data as Product and our Senior Engineer will be a key member of the engineering staff working across Business Services Engineering, Data Engineering, Platform Engineering, and Infrastructure Engineering to ensure that we provide a fiction-less experience to our customers, maintain the highest standards of protection and availability. Our team thrives and succeeds in supporting Data Driven company and delivering high quality technology products and services in a hyper-growth environment where priorities shift quickly. The ideal candidate has broad and deep technical knowledge in data, typically ranging from front-end UIs through back-end systems and all points in between.
Required:
3+ years of experience in data software development, programming languages and developing with big data technologies
2+ years of experience designing and building on existing and new data applications
2+ years of experience in Cloud DevOps concepts, Cloud Services and Architecture, and Azure/AWS/GCP DevOps Operational Framework
1+ years of experience in open-source data tools and frameworks, or one of the following: .net Core, asp.Net, Angular, or Express.
Additional Skills:
Experience in data software development, using data technologies such as Relational & NoSQL databases, open data formats, and programming languages such as Python, Scala, and/or other frameworks, building data pipelines (ETL and ELT) with batch or streaming ingestion, error handling, loading, and transforming data, and developing with big data technologies such as Spark, Hadoop, and MapReduce. Experience with analytics solutions.
Experience in development using Python or PySpark, Spark, Scala.
Advanced understanding of designing and building for data quality assurance, reliability, availability, and scalability, on existing and new data applications.
Advanced understanding of DevOps Concepts, Cloud Architecture, and Azure DevOps Operational Framework, Pipelines, Kubernetes.
Advanced understanding of designing and building solutions for data quality and observability, metadata management, data lineage, and data discovery.
Advanced understanding of building products of micro-services oriented architecture and extensible REST APIs.
Advanced understanding of open-source frameworks.
Experience with continuous delivery and infrastructure as code.
Experience in existing Monitoring Portals: Splunk or Application Insights.
Advanced understanding of Security Protocols & Products: Understanding of Active Directory, Windows Authentication, SAML, OAuth.
Advanced understanding of Azure Network (Subscription, Security zoning, etc) & tools like Genesis.
Advanced understanding of existing Operational Portals such as Azure Portal.
Knowledge of CS data structures and algorithms.
Knowledge of developer tooling across the software development life cycle (task management, source code, building, deployment, operations, real-time communication).
Practical knowledge of working in an Agile environment (Scrum/Kanban/SAFe).
Strong problem-solving ability.
Ability to excel in a fast-paced, startup-like environment.
Bachelor’s degree in Computer Science, Information Systems, or equivalent education or work experience
Thanks & Regards,
Raja
Sr. Technical Recruiter
SPAR Information Systems
(a E-verify Company)
Phone: 469 – 750 – 0601
Fax : 1-214-291-2507
Email : Raja@sparinfosys.com
Job Types: Full-time, Contract
Pay: $126,956.00 - $137,592.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Life insurance
Paid time off
Parental leave
Tuition reimbursement
Vision insurance
Compensation package:
1099 contract
Bonus pay
RSU
Stock options
Yearly pay
Experience level:
10 years
11+ years
Schedule:
8 hour shift
Application Question(s):
Must work on our W2 this is contract to hire role
Experience:
SQL (Required)
Azure (Required)
DevOps (Required)
10+ Years (Required)
Work Location: Remote","$132,274 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2012,$25 to $100 million (USD)
"Asana
4.5",4.5,"San Francisco, CA",Data Engineer,"Join our team to build the data foundation that powers Asana's metrics and ML models. Our data artifacts are leveraged by Product and Business data science teams to optimize adoption, growth and experience for Asana users. We partner with the infrastructure team to build self-service analytics platform for the company. Our mission is to ensure high quality data to enable data informed decision making across Asana.
What you'll achieve
Design, implement and scale end-to-end data products and solutions that support growing data processing and analytical needs
Build critical data artifacts that drive product strategy, enable deep dive analysis and reporting
Partner with data scientists, business domain experts, and engineering teams to build a roadmap aligned with business goals
Broad ownership to influence and shape the company's data strategy
Implement systems to track and ensure data accuracy and data availability
About you
3+ years of experience in data engineering (or) software engineering
Experienced in data modeling and building scalable data pipelines involving complex transformations
Proficient in data processing and storage technologies like AWS/S3/HDFS, Redshift, Python/Scala, SQL, Spark, Airflow
Ability to identify bottlenecks in existing workflows and build solutions to achieve operational excellence
Motivated to work with cross-functional partners to help evolve our analytical data model
What we'll offer
Our comprehensive compensation package plays a big part in how we recognize you for the impact you have on our path to achieving our mission. We believe that compensation should be reflective of the value you create relative to the market value of your role. To ensure pay is fair and not impacted by biases, we're committed to looking at market value which is why we check ourselves and conduct a yearly pay equity audit.
For this role, the estimated base salary range is between $171,000 - $258,000. The actual base salary will vary based on various factors, including market and individual qualifications objectively assessed during the interview process. The listed range above is a guideline, and the base salary range for this role may be modified.
In addition to base salary, your compensation package may include additional components such as equity, sales incentive pay (for most sales roles), and benefits. If you're interviewing for this role, speak with your Talent Acquisition Partner to learn more about the total compensation and benefits for this role.
About us
Asana helps teams orchestrate their work, from small projects to strategic initiatives. Millions of teams around the world rely on Asana to achieve their most important goals, faster. Asana has been named a Top 10 Best Workplace for 5 years in a row, is Fortune's #1 Best Workplace in the Bay Area, and one of Glassdoor's and Inc.'s Best Places to Work. After spending more than a year physically distanced, Team Asana is safely and mindfully returning to in-person collaboration, incorporating flexibility that adds hybrid elements to our office-centric culture. With 11+ offices all over the world, we are always looking for individuals who care about building technology that drives positive change in the world and a culture where everyone feels that they belong.
We believe in supporting people to do their best work and thrive, and building a diverse, equitable, and inclusive company is core to our mission. Our goal is to ensure that Asana upholds an inclusive environment where all people feel that they are equally respected and valued, whether they are applying for an open position or working at the company. We provide equal employment opportunities to all applicants without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by law.
#LI-ML4","$214,500 /yr (est.)",1001 to 5000 Employees,Company - Public,Information Technology,Enterprise Software & Network Solutions,2008,Unknown / Non-Applicable
"FanDuel
4.0",4.0,"Atlanta, GA",Data Engineer,"ABOUT FANDUEL GROUP
There are more ways to win, here at FanDuel. We're willing to bet on it.
THE ROSTER…
At FanDuel Group, we give fans a new and innovative way to interact with their favorite games, sports and teams. We're dedicated to building a winning team and we pride ourselves on being able to make every moment mean more, especially when it comes to your career. So, what does ""winning"" look like at FanDuel? It's recognition for your hard-earned results, a culture that brings out your best work—and a roster full of talented coworkers. Make no mistake, we are here to win, but we believe in winning right. That means we'll never compromise when it comes to looking out for our teammates. From creatives professionals to cutting edge technology innovators, FanDuel offers a wide range of career opportunities, best in class benefits, and the tools to explore and grow into your best selves. At FanDuel, our principle of ""We Are One Team"" runs through all our offices across the globe, and you can expect to be a part of an exciting company with many opportunities to grow and be successful.
WHO WE ARE…
FanDuel Group is an innovative sports-tech entertainment company that is changing the way consumers engage with their favorite sports, teams, and leagues. The premier gaming destination in the United States, FanDuel Group consists of a portfolio of leading brands across gaming, sports betting, daily fantasy sports, advance-deposit wagering, and TV/media.
FanDuel Group has a presence across all 50 states with approximately 17 million customers and nearly 30 retail locations. The company is based in New York with offices in California, New Jersey, Florida, Oregon, Georgia, Portugal, Romania and Scotland.
Its network FanDuel TV and FanDuel+ are broadly distributed on linear cable television and through its relationships with leading direct-to-consumer OTT platforms.
FanDuel Group is a subsidiary of Flutter Entertainment plc, the world's largest sports betting and gaming operator with a portfolio of globally recognized brands and a constituent of the FTSE 100 index of the London Stock Exchange.
THE POSITION
Our roster has an opening with your name on it
FanDuel Group is looking for an experienced Data Engineer with deep understanding of large-scale data handling and processing best practices in a cloud environment to help us build scalable systems. As our data is a key component of the business used by almost every facet of the company, including product development, marketing, operations, and finance. It is vital that we deliver robust solutions that ensure reliable access to data with a focus on quality and availability.
Our competitive edge comes from making decisions based on accurate and timely data and your work will provide access to that data across the whole company. Looking ahead to the next phase of our data platform we are keen to do more with real time data processing and working with our data scientists to create machine learning pipelines
THE GAME PLAN
Everyone on our team has a part to play
Creating and maintain optimal data pipeline architecture
Designing and implementing data pipelines required in the data warehouse and data lake in batch or real-time using data transformation technologies.
Identifying, designing, and implementing internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability
Designing and deploying data models and views with large datasets that meet functional / non-functional business requirements
Delivering timely after-action reporting to state regulatory groups
Delivering quality production-ready code in an agile environment
Delivering test plans, monitoring, debugging and technical documents as a part of development cycle
Creating data tools for analytics and working with stakeholders across all departments to assist with data-related technical issues and supporting their data infrastructure needs
THE STATS
What we're looking for in our next teammate
Experience writing Python scripts
Working SQL knowledge and experience working with relational databases
Build processes supporting data transformation, data structures, metadata, dependency, and workload management,
Show proficiency understanding complex ETL processes
Demonstrate the ability to optimize processes
Knowledge of data integrity and relational rules
Understanding of AWS and Google Cloud knowledge of DMS tasks and processes are nice to have.
Ability to quickly learn new technologies is critical
Proficiency with agile or lean development practices
Understanding of regulated systems and sensitive data
PLAYER CONTRACT
We treat our team right
From our many opportunities for professional development to our generous insurance and paid leave policies, we're committed to making sure our employees get as much out of FanDuel as we ask them to give. Competitive compensation is just the beginning. As part of our team, you can expect:
An exciting and fun environment committed to driving real growth
Opportunities to build really cool products that fans love
Mentorship and professional development resources to help you refine your game
Flexible vacation allowance to let you refuel
Hall of Fame benefit programs and platforms
FanDuel Group is an equal opportunities employer and we believe, as one of our principal states, ""We Are One Team!"" We are committed to equal employment opportunity regardless of race, color, ethnicity, ancestry, religion, creed, sex, national origin, sexual orientation, age, citizenship status, marital status, disability, gender identity, gender expression, and Veteran status. We believe FanDuel is strongest and best able to compete if all employees feel valued, respected, and included. We want our team to include diverse individuals because diversity of thought, diversity of perspectives, and diversity of experiences leads to better performance. Having a diverse and inclusive workforce is a core value that we believe makes our company stronger and more competitive as One Team!
#LI-Hybrid","$97,160 /yr (est.)",501 to 1000 Employees,Company - Private,"Arts, Entertainment & Recreation",Sports & Recreation,2009,$100 to $500 million (USD)
"Akira Technologies, Inc
3.9",3.9,"Washington, DC",Senior Data Engineer,"Akira Technologies, Inc. is looking for a Senior Data Engineer to add their skills and experience to our team developing, deploying, and maintaining enterprise wide data services supporting the US federal agencies. This is an exciting and dynamic job opportunity where you will help modernize the applications and workloads used by a dynamic operational customer, and advance the government agencies' analytic and operational capabilities.
As a member of the data analytics team, you will work on developing innovative strategies to identify, process, and develop automation and integration methodologies for large datasets used by analysts to inform policy makers and operational planners.

Your Responsibilities:
Drive technology solutions in the data analytics and emerging technologies space
Help clients modernize, build and support reporting, data integration and data science environments in a cloud or hybrid environments
Create data quality dashboards and reporting to improve observability
Design and build well architected and managed data streaming services
Develop, transform, and model data to improve decision making
Create workflows and predictive models for end-users using system tools
Manage the analytics request process by capturing requirements
Provide mentorship and guidance to other data analytics colleagues within the company
You may also be responsible for data extraction and manipulation of data sets, the creation and maintenance of surveys and deciphering survey analytics, and implementation of AI/ML strategies.
The ideal person for this position is detail oriented, has expertise as a data engineer, and brings good working knowledge of the leading data platforms, analysis tools, and relevant cloud technologies. He/she will work closely with product owners across various functional areas to understand the objectives of the agency / program and prioritize any requirements.
Required Qualifications & Experience:
Bachelor's degree in Computer Science, Mathematics, Information Management, Data Science, Data Analytics, or a related field
5 or more years of experience developing data streaming and data management services and solutions
Strong, hands-on experience with one of more of Python/Jupyter, R, SAS, Apache Spark, Kafka, AWS Kinesis.
Strong Azure Cloud experience, particularly migrating R/Python workloads to Azure Cloud.
Strong data pipeline experience in Azure Cloud.
Ability to understand business needs and relay into easy to understand, non-technical language
High-level written and verbal communication skills
Optional: Experience with unified data analytics platforms like Databricks and Snowflake

About Akira Technologies:
Akira strives to meet and exceed the mission and objectives of US federal agencies. As a leading small business cloud modernization and data analytics services provider, we deliver trusted and highly differentiated solutions and technologies that serve the needs of our customers and citizens. Akira serves as a valued partner to essential government agencies across the intelligence, cyber, defense, civilian, and health markets. Every day, our employees deliver transformational outcomes, solving the most daunting challenges facing our customers.
Akira is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.

GcBLU1hIwe","$120,113 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2003,Unknown / Non-Applicable
"Fresh Consulting
3.9",3.9,"Menlo Park, CA",Data Engineer,"Fresh Consulting is a design-led, software development and hardware engineering company, offering end-to-end digital services to help companies innovate. We bring together amazing UX designers, sophisticated developers, digital strategists, and top-notch engineers to help companies create fresh experiences that connect humans, systems, and machines. We’ve been growing fast and need someone to help us continue to manage the delivery of high-quality work in a fast-paced environment.
See more at freshconsulting.com Visit freshconsulting.com/portfolio to see our project work across several industries.
View and apply to all jobs - https://freshconsulting.applytojob.com/apply/ or visit freshconsulting.com/careers
Title: Data Engineer
Duration: 6 months with possible extension
Location: Onsite Menlo Park, CA
Benefits: Employee benefits at 100% including Medical, PTO, Holiday Pay, 401K Plan, and much more!
Hours: Minimum 40 Hours/Week
Role:
Design, construct, install, test, and maintain highly scalable data management systems.
Ensure systems meet business requirements and industry practices.
Design, implement, automate, and maintain large-scale enterprise data ETL processes.
Build high-performance algorithms, prototypes, predictive models, and proof of concepts.
Skills:
5+ years of work experience as a Data Engineer.
Ability to work as part of a team, as well as work independently or with minimal direction.
Excellent written, presentation, and verbal communication skills.
Collaborate with data architects, modelers, and IT team members on project goals.
Strong PC skills including knowledge of Microsoft SharePoint.
Process certification, such as Six Sigma, CBPP, BPM, ISO 20000, ITIL, and CMMI.
Education: BSCSE or related.
FRESH-
Work on engineering and research assignments with F500 companies and startups.
The relationships that we have created with our clients are one of a kind.
We help solve problems in many technologies focusing on R&D, product development, and manufacturing.
We work with the most cutting-edge and latest technologies from AR/VR to Autonomous technologies.
Closely working with our clients, we believe that long-term investments are extremely important to maintain the culture we together have created.
We’re a handpicked team of Engineers, digital strategists, designers, and developers united together in creating a fresh experience. Whether we are strategizing, designing, developing, or analyzing, our integrated team works as an extension of yours to improve your impact, your usability, and your customer conversion. In the process, we collaborate with you to get to know your business, understand your industry, and incorporate your big ideas into memorable experiences that keep your customers coming back for more.
Equal employment opportunity: All qualified persons will be considered for employment without regard to race, color, religion, sex, national origin, age, marital status, familial status, gender identity, sexual orientation, disability for which a reasonable accommodation can be made or any other status protected by law. Assistance will be gladly provided upon request for any applicant with sensory or non-sensory disabilities.
Fresh Consulting is a participating E-Verify company.
freshconsulting.com
Compensation offered will be determined by factors such as location, level, job-related knowledge, skills, and experience. Range $70/hr - $80/hr.
ylM5WIC1Wr",$75.00 /hr (est.),201 to 500 Employees,Company - Private,Management & Consulting,Business Consulting,2007,$25 to $100 million (USD)
diverse team,#N/A,"New York, NY",Data Integration Engineer,"Job Title: Data Integration Engineer
Location: NYC NY and Charlotte NC – Hybrid
Contract
Major Responsibilities:
Experience designing and developing Enterprise Data Warehouse solutions.
Demonstrated proficiency with Data Analytics, Data Insights
Proficient writing SQL queries and programming including stored procedures and reverse engineering existing process
Leverage SQL, programming language (Python or similar) and/or ETL Tools (Azure Data Factory, Data Bricks, Talend and SnowSQL) to develop data pipeline solutions to ingest and exploit new and existing data sources.
Perform code reviews to ensure fit to requirements, optimal execution patterns and adherence to established standards.
Skills:
10+ years of experience in Enterprise Data Management
10+ years of experience in SQL Server based development of large datasets
5+ years’ experience in Data Architecture
2+ years Python coding experience
Good experience in Data Visualization tools
Hands-on experience with Snowflake utilities such as SnowSQL and SnowPipe
Working knowledge of MS Azure configuration items with respect to Snowflake.
Hands-on experience with Tasks, Streams, Time travel, Optimizer, Metadata Manager, data sharing
Experience in Data warehousing - OLTP, OLAP, Dimensions, Facts, and Data modeling.
Previous experience leading an enterprise-wide Cloud Data Platform migration with strong architectural and design skills
Capable of discussing enterprise level services independent of technology stack
Experience with Cloud based data architectures, messaging, analytics
Cloud certification(s) is a big plus
Financial Domain experience is must
Any experience with Regulatory Reporting is a Plus
Job Type: Full-time
Salary: Up to $70.00 per hour
Compensation package:
Overtime pay
Performance bonus
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
Data Integration: 8 years (Required)
SQL: 10 years (Required)
Data warehouse: 7 years (Required)
Work Location: On the road",$70.00 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Central City Concern
3.4",3.4,"Portland, OR",Data Engineer I,"Central City Concern (CCC) is an innovative nonprofit agency providing comprehensive services to single adults and families impacted by homelessness, poverty, and addictions in the Portland metro area. We hire skilled and passionate people to meet our mission to end homelessness through innovative outcome-based strategies that support personal and community transformation.
Data Engineer I will assist with designing, implementing, and supporting a data platform while interfacing with business partners across CCC, including Health Services, Housing, Employment, Finance, HR, and other shared service functions. The Data Engineer I will assist with Extract, Transformation, and Load (ETL) strategies, perform data modeling to meet customers’ data needs, and continually improve ongoing reporting and analysis processes while automating or simplifying self-service support for datasets. Data Engineer I will work closely with the Decision Support team to deliver timely reports. This position will look to the Data Engineer II and Data Engineer Senior for coaching/mentoring and join them on projects and development tasks.
Schedule: Monday - Friday 8.00 am-5.00 pm
RESPONSIBILITIES:
Assist in maintaining/developing the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and cloud technologies
Assist with designing, optimizing, testing, and maintaining architectures for our client databases, data pipelines, and processing systems, as well as optimizing data flow and collection for cross-functional teams.
Assist in serving relevant data to all stakeholders, internally and externally, and maintain the infrastructure so the decision support team can present their work to the organization.
Assist in maintaining/developing a scalable data warehouse by connecting disparate data housed across numerous organizational systems and business lines.
Assist in collecting and documenting user requirements, development of user stories, and time estimates.
Supports key meetings and events (governance user groups, discovery events, etc.).
Partners with other Data Engineers to facilitate technical review meetings.
Generates runbook documentation of supported systems.
Actively audits and monitors system performance.
Supports during system and unit testing events.
Assists during code and system upgrades.
Develops and implements new technology with review by DE2 or DE Senior.
Escalates issues to DE2 and/or DE Senior, as needed.
Adhere to all state and federal privacy regulations, including HIPAA and 42 CFR Part 2, and CCC policies and agreements regarding confidentiality, privacy, and security. Support compliance with all privacy and security requirements pursuant to community partners' and outside providers’ patient confidentiality agreements, including privacy and security requirements for EMR access. This includes immediately reporting any breach of protected health information or personal identification information of any person receiving CCC services by CCC or an outside provider to the CCC Compliance Department, as well as to your supervisor or their designee.
Provide the highest standard of customer service to internal and external stakeholders, including CCC clients, CCC staff, and community members.
Attend all mandatory CCC training promptly.
Other duties as assigned.
QUALIFICATIONS:
Bachelor’s degree in related field required and 1+ years of recent data engineer experience that includes Python, Java and/or other object-oriented script language and experience using SQL and relational databases, including query authoring
OR Associate’s degree or trade school certificate in an IT related field with 2+ years of recent data engineer experience that includes experience with Python, Java and/or other object-oriented script language and experience using SQL and relational databases, including query authoring
OR 3+ years of recent data engineer experience that includes experience with Python, Java, and/or other object-oriented script language experience using SQL and relational databases, including query authoring
Familiarity with a variety of databases
Experience with manipulating, processing, and extracting value from large, disconnected datasets
Familiar with gathering data through multiple sources through API calls and scripting languages
Understanding reporting tools, like Power Bl, SQL Server Analysis Services, and SQL Server Report Services.
Understanding of the Agile principles and methodologies
Must be able to work within an integrated, multidisciplinary setting
Adhering to Central City Concern’s drug-free workplace encourages a safe, healthy, and productive work environment and strictly complies with the Drug-Free Work Place Act of 1988. An employee shall not, in the workplace, unlawfully manufacture, distribute, dispense, possess, or use a controlled substance or alcohol.
Must pass a pre-employment drug screen, TB Test, and background check.
Must adhere to the agency’s non-discrimination policies.
Ability to effectively interact with co-workers and clients with diverse ethnic backgrounds, religious views, cultural backgrounds, lifestyles, and sexual orientations and treat each individual with respect and dignity.
BENEFITS:
Central City Concern offers an incredible benefits package to our employees!
Generous paid time off plan beginning at 4 weeks per year at the time of hire. Accrual increases with longevity.
Amazing 403(b) Retirement Savings plan with an employer match of 4.25% in your 1styear, 6% in the 2nd year, and 8% in your 3rd year!
11 paid Holidays PLUS 2 Personal Holidays to be used at the employee’s discretion.
Comprehensive Medical, Vision, and Dental insurance coverage.
Employer Paid Life, Short-Term Disability, AND Long-Term Disability Insurance!
Sabbatical Program is offers extended time off in years 7, 14, and 21.
This description is intended to provide a snapshot of the work performed. It is not designed to contain a comprehensive inventory of all duties, responsibilities, and qualifications required for the position.
Central City Concern is a second-chance employer and complies with applicable laws regarding considering criminal background for employment purposes. Government regulations, contractual requirements, or the duties of this particular job may require CCC to conduct a background check and take appropriate action to address prior criminal convictions.
Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities
The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information. 41 CFR 60-1.35(c)","$77,601 /yr (est.)",Unknown,Nonprofit Organization,Nonprofit & NGO,Civic & Social Services,1979,Unknown / Non-Applicable
"GTA (Global Technology Associates)
4.6",4.6,"San Francisco, CA",Data Communications Engineer - III,"Looking for a Data Communications Engineer - III
What you will be doing as a Data Communications Engineer – III
This member of technical staff will be responsible for managing the standalone IT network and 4G/5G infrastructure and mobile edge cloud environments for a test and development lab program.
This is a technical position focused on supporting the development of technology solutions testing and incubation of new platforms and the product engineering of greenlit projects.
In this role the candidate will work with internal technology and product groups and external partners to support the development of new solutions.
He/She will be responsible for the architecture design and deployment of new 4G/5G and MEC platforms and features and will work very closely with Device testing Product engineering Business groups and peer TPD teams.
The candidate will be a member of a small team of Sr. Network engineers who manage remote lab access configure and manage virtual environments to support projects monitor and troubleshoot problems and make recommendations on how to streamline development with internal and external partners.
What you will bring to the table as a Data Communications Engineer - III
BA in IT/EE or Computer Engineering
The ideal candidate needs to have a strong technical background with experience in wireless and wireline networks hands on integration experience with MEC/Cloud/IT
Experience working in a customer facing environment Expert understanding of computer technologies and architectures including O/S Database NFV Cloud and SDN.
Passion for working with cutting edge technology Ability to work well in a team environment.
What you didn’t know about us:
Competitive salary
Health, Dental and Vision Benefits
Short/Long Term Disability and Critical Care/Illness Protection
Life Insurance and Retirement Plans
Employee Assistance Program
With this position, you will get the opportunity to work with our game changing clients and further advance your already valuable experience in the telecom industry!
We are Connectors. We thrive on ‘quality over quantity’ and put in the work building strong relationships. We create connections, discover qualities, uncover skills, and place people with accuracy. We are your true partner!
We are Collaborators. You’ll be working with a wholly-owned subsidiary of Kelly and part of the Kelly Telecom division. It allows us to be as nimble and fiercely competitive as a startup while having the backing of a multibillion dollar publicly traded company which has been in business for 75 years. With direct access to hiring managers, services don’t stop at standard recruiting processes. We use our expertise to improve your application skills and provide ongoing career support.
We give 24/7 Support. We are in this together. We provide around the clock availability, competitive employee benefits, and continuously check-in to make sure things are going smoothly. Check out our Glassdoor page!
Kelly Telecom is an equal opportunity employer and will consider all applications without regard to race, genetic information, sex, age, color, religion, national origin, veteran status, disability, or any other characteristic protected by law. For more information click Equal Employment Opportunity is the law.
You should know: Your safety matters! Vaccination against COVID-19 may be a requirement for this job in compliance with current client and governmental policies. A recruiter will confirm and share more details with you during the interview process.
#JobsAtKellyTelecom
Job Types: Full-time, Contract
Salary: $91,144.17 - $180,998.90 per year
Work Location: In person","$136,072 /yr (est.)",Unknown,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"BigLynx Computer Software
4.9",4.9,"Redmond, WA",Databricks Data Engineer,"BigLynx, Inc is an American multinational technology corporation headquartered in Seattle, Washington, with operations in the United States, Canada, and India. The company began in 2016, as a product development company specializing in AI/ML Data Engineering in the Retail vertical space with its products warehouse & fast. Post Pandemic in 2022, BigLynx added a business division of boutique technology consulting, specializing inData Engineering, Full Stack , and Microsoft Dynamics helping clients build the next generation data platform and big data pipelines.
Data pipeline development: Design, develop, and maintain scalable and efficient data pipelines using Databricks to ingest, transform, and load data from various sources. This includes data extraction, data cleansing, data transformation, and data loading processes.
Data modeling and schema design: Design and implement data models, database schemas, and data structures on Databricks. Optimize data models for performance, scalability, and ease of use.
ETL processes: Develop and maintain ETL (Extract, Transform, Load) processes using Databricks to transform and cleanse data. Implement efficient data integration and transformation logic using languages such as Python, SQL, or Scala.
Data integration: Integrate data from multiple systems and sources, ensuring data consistency, accuracy, and quality. Develop and maintain data connectors, APIs, and data ingestion processes.
Performance optimization: Identify and address performance bottlenecks in data pipelines and data models. Optimize query performance, data loading, and data processing capabilities on Databricks.
Data governance and security: Implement data governance practices, data privacy measures, and security controls on Databricks. Ensure compliance with data governance policies and regulations.
Monitoring and troubleshooting: Monitor the health and performance of Databricks data infrastructure, data pipelines, and data processing jobs. Troubleshoot issues and provide timely resolutions.
Collaboration and teamwork: Collaborate with cross-functional teams, including data scientists, data analysts, and business stakeholders, to understand data requirements, provide data engineering expertise, and support their data-related needs.
Qualifications:
Databricks expertise: Strong knowledge and hands-on experience with the Databricks platform, including Databricks notebooks, Databricks runtime, and Databricks clusters.
Data engineering skills: Proficiency in data engineering principles, ETL processes, data modeling, and data integration techniques. Experience with programming languages such as Python, SQL, or Scala.
Big data technologies: Experience with big data technologies, such as Apache Spark, Apache Hadoop, or related frameworks. Familiarity with distributed computing and data processing concepts.
Cloud platforms: Experience working with cloud platforms, preferably Azure Databricks, AWS Databricks, or Google Cloud Databricks. Knowledge of cloud storage, compute, and networking services.
Database and data warehouse concepts: Understanding of relational databases, data warehousing concepts, and SQL. Familiarity with data warehousing best practices and dimensional modeling.
Performance optimization: Strong skills in optimizing Spark jobs and queries on Databricks. Ability to identify and resolve performance bottlenecks.
Problem-solving skills: Strong analytical and problem-solving abilities to tackle complex data engineering challenges and troubleshoot issues.
Collaboration and communication: Excellent collaboration and communication skills to work effectively with cross-functional teams and stakeholders, translating business requirements into technical solutions and providing technical guidance.
Education: A bachelor's or master's degree in computer science, data engineering, or a related field is typically required. Relevant certifications, such as Databricks Certified Developer or similar, are h
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Experience level:
7 years
Schedule:
Monday to Friday
Work Location: In person",$75.00 /hr (est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"PRIMUS Global Services, Inc
4.1",4.1,"Austin, TX","Data Engineer – Snowflake, SQL – REMOTE WORK 43357","We have an immediate long-term opportunity with one of our key clients for a position of Senior Data Engineer, to work on a remote basis.

The ideal applicant must have extensive experience with Snowflake, SQL and Multiple ETL tools.

**ALL successful candidates for this position are required to work directly for PRIMUS. No agencies please**

For immediate consideration, please contact:

Tanya Khatri
PRIMUS Global Services
Direct: 972-200-4514
Phone No: 972-753-6500 Ext: 258
Email: jobs@primusglobal.com","$89,729 /yr (est.)",501 to 1000 Employees,Private Practice / Firm,Information Technology,Information Technology Support Services,2002,$5 to $25 million (USD)
"Smartbridge
3.3",3.3,"Houston, TX",Data Engineer / Consultant,"Hybrid in Houston Texas
Salary range 105K to 150K Annual Salary (see note below)
All the benefits and perks you need:
100% Paid Healthcare Insurance, including First Primary Care Wellness Program
Start accruing Paid Days Off from Day One
Career Development
Fitness Reimbursement
Matching 401K Retirement Plan
Mentoring Program
Team Events
And more….
Smartbridge
We simplify business transformation, applying thought leadership and innovation to create digitally connected enterprises. From Strategy to Implementation, we bring our clients’ digital agenda to life. One of the keys to our success as a company is finding and hiring exemplary employees. We believe that each member of our team contributes directly to Smartbridge’s growth and success, and we take pride and celebrate with our employees as they continue to grow and succeed with their career paths as well.
We are seeking an experienced Senior Data Engineer who will work with clients and members of the consulting team on the architecture, design, and development of highly scalable data integration and data engineering processes. The Senior Consultant must have a strong understanding and experience with data & analytics solution architecture, including data warehousing, data lakes, ETL/ELT workload patterns, and related BI & analytics systems.
Additional responsibilities of this role will include the following:
Deliver consulting projects/work on-time, on-budget, and in a way that accomplishes client goals
Develop and implement technical best practices for data ingestion, data quality, data cleansing, and other data integration/ETL/Engineering-related activities
Understand and experience maintaining a multi-terabyte enterprise data warehouse with accompanying incremental data pipelines
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and modern cloud technologies.
Conduct or participate in meetings with owners of key system components to fully understand current data and systems environments
Resolve source data issues and refine transformation rules
Analyze source system data to assess transformation logic and data quality through data profiling
Leverage data quality processes to assist with data cleansing requirements
Work with technical and business representatives to determine strategies for handling data anomalies that are identified
Design ETL processes and develop source-to-target data mappings, integration workflows, and load processes
Develop, test, integrate, and deploy data pipelines using a variety of tools and external programming/scripting languages as necessary
Provide technical documentation and other artifacts for data pipelines, ingestion, integration or other data solutions
Identify problems, develop ideas and propose solutions within differing situations requiring analytical, evaluative or constructive thinking in daily work
Apply creative thinking to identify possible reporting solution alternatives
Other duties assigned as needed
Requirements and Qualifications
3+ years hands-on experience with one or more of these data integration/ETL tools:
Azure Data Factory
Databricks/Spark
Experience building on-prem data warehousing solutions
Experience with designing and developing ETL's, Data Marts, Star Schema's
Experience with building data warehousing solutions in Azure
Moving data from on-prem to cloud
Designing a data warehouse solution using Synapse or Azure SQL DB
Experience building pipelines using Synapse or Azure Data Factory to ingest data from various sources
Understanding of integration run times available in Azure
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases
Knowledge of scripting languages like Python, Scala.
Microsoft Azure Cloud platform certifications (nice to have)
Must be able to travel to client locations based on project needs
Please note that the compensation information that follows is a good faith estimate for this position only and is provided pursuant to the Equal Pay Transparency Laws in numerous states we operate. It takes into consideration a candidate’s education, training, and experience, as well as the position’s work location, expected quality and quantity of work, required travel (if any), external market and internal value, including seniority and merit systems, and internal pay alignment when determining the salary level for potential new employees.
7ttzIjxalD","$127,500 /yr (est.)",51 to 200 Employees,Company - Private,Management & Consulting,Business Consulting,2003,$5 to $25 million (USD)
Koantek,#N/A,Remote,Adobe CDP Data engineer,"As an Adobe Real-Time CDP Data Engineer, you will be responsible for performing
data integration and configuration of schemas, datasets, and profiles against a
variety of sources and microservices. You will be working closely with clients and
stakeholders to understand their data needs, and design and implement the
necessary data solutions to meet those needs. This role requires expertise in Adobe
Real-Time CDP, as well as experience in data integration and configuration.
Responsibilities:
Design and implement data solutions using Adobe Real-Time CDP
Perform data integration and configuration of schemas, datasets, and profiles
against a variety of sources and microservices
Work with clients and stakeholders to understand their data needs and design
appropriate data solutions
Develop, test, and maintain data pipelines and workflows
Troubleshoot data issues and provide solutions
Work with data analysts to ensure data quality and accuracy
Stay up-to-date with the latest Adobe Real-Time CDP features and technologies
Collaborate with cross-functional teams including product, engineering, and data
science to ensure successful project delivery
Provide guidance and mentorship to junior data engineers
Participate in code reviews and ensure compliance with coding standards and
best practices
Requirements:
Bachelor's degree in computer science, data science, or a related field
Strong experience in Adobe Real-Time CDP, including data integration and
configuration of schemas, datasets, and profiles against a variety of sources and
microservices
Experience with data integration tools and platforms
Strong understanding of data modeling and database design principles
Experience with SQL, NoSQL, and other data storage technologies
Experience with data warehousing, ETL, and data transformation
Knowledge of data governance and data security best practices
Strong problem-solving and analytical skills
Excellent communication and collaboration skills
Ability to work independently and as part of a team
Job Type: Contract
Contract length: 3 months
Salary: Up to $36,000.00 per month
Experience:
total work: 1 year (Preferred)
Work Location: Remote
Speak with the employer
+91 9411943957","$36,000 /mo (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Expression Networks
4.4",4.4,"Washington, DC",Junior Data Engineer,"We are looking to hire a Jr. Data Engineer to add to the continued growth we are seeing with our Data Science division. This position will work in a team led by a Sr. Data Engineer on tasks related to designing and delivering high-impact data architecture and engineering solutions to our customers across a breadth of domains and use cases. You will be working on developing, testing, and documenting software code for data extraction, ingestion, transformation, cleaning, correlation, and analytics. You will be using various databases and tools. You will have the opportunity to strengthen your skill set while learning new technologies utilized across the Expression Networks' breadth and depth of projects.
About Expression Networks
Founded in 1997 and headquartered in Washington DC, Expression Networks provides data fusion, data analytics, software engineering, information technology, and electromagnetic spectrum management solutions to the U.S. Department of Defense, Department of State, and national security community. Expression's “Perpetual Innovation” culture focuses on creating immediate and sustainable value for our clients via agile delivery of tailored solutions built through constant engagement with our clients. Expression Networks was ranked #1 on the Washington Technology 2018's Fast 50 list of fastest-growing small business Government contractors and a Top 20 Big Data Solutions Provider by CIO Review.
We make sure to provide everyone with the tools and opportunities to grow while working on some of the newest technologies in the industry. With Covid-19 being a major theme over the last two years having a growing collaborative culture has been one of the key focuses of our C-suite and upper management since our humble beginnings. We get excited about celebrating our professionals' milestones, accomplishments, promotions, overcoming challenges, and many other aspects that make an engaging collaborative environment.
Expression Networks HQ is conveniently located northeast of Union Station. We are a 5-minute walk from the Gallaudet - Noma DC Metro station. The U-line building has a parking garage attached to the building. Whether you are commuting via train or car we have a commuter assistance program that covers VRE, MARC, Metro, and parking expenses!
Security Clearance:
US Citizenship is required.
Ability to obtain and maintain a Secret or higher clearance.
Location:
Hybrid in the DMV area, with the ability to attend periodic events and deployments at our client site (Annapolis and Northern Virginia)
Required Skills:
Bachelor's degree or higher in engineering, Computer Science, Computer Engineering, Machine Learning, Mathematics, Physics, or related field and 1+ years of industry experience working on projects such as real-time SLAM and 3D reconstruction, sensor fusion, and active depth sensing, object and body tracking and pose estimation, and/or image processing
Proficient with state-of-the-art object detection algorithms
Deployment of vision algorithms in AWS
Experience with programming languages (e.g., JavaScript, TypeScript, React) and strong knowledge of programming techniques, especially for parallel architectures
Familiarity with neural network frameworks such as Theano, Torch, or Caffe
Occasional travel to conferences and customer visits may be required
Experience building or maintaining databases
Experience with Caffe, TensorFlow, or other deep learning frameworks
Programming experience with computer vision and 3D geometry libraries
Must be hands-on and work well within a team of algorithm, software, and hardware engineers
Preferred Skills
Master's or Advance Degree in Deep Learning, Computer Vision, Robotics, or Artificial Intelligence
Up-to-date knowledge and understanding of recent advances in machine learning, particularly deep learning
Certification in AWS, SQS, SNS, S3, SDL, SWLC, ISO26262, MISRA, RDS, CDK
Benefits:
Expression Networks offers competitive salaries and benefits, such as:
401k matching
PPO and HDHP medical/dental/vision insurance
Education reimbursement
Complimentary life insurance
Generous PTO and holiday leave
Onsite office gym access
Commuter Benefits Plan
Equal Opportunity Employer/Veterans/Disabled","$89,499 /yr (est.)",51 to 200 Employees,Contract,Information Technology,Enterprise Software & Network Solutions,1997,Unknown / Non-Applicable
"Bluesky
2.0",2.0,"San Francisco, CA",Founding Data Engineer,"Company website: http://getbluesky.io/
Company/Founders’ Location: Menlo Park, California
Local to SF/Bayarea, California preferred
We are a stealth mode early-stage startup with the mission to build a new generation of data infra on the cloud. Today, users suffer from unexpected incidents, slowness, and huge bills. We are big data domain experts with 15+ years of experience solving similar problems across Google, Hadapt, Vertica, Uber, Dropbox, Facebook, and Yahoo, and thought leaders in the big data industry. We are privileged to have received funding from top-tier VCs, angels and big data thought leaders such as founders of Cloudera and Qubole. We are building a world-class team to create systems that will have a profound impact on how people use big data.

We are searching for a Founding Data Engineer with strong interests in big data to help design and build a next-generation data cloud. As a key early team member, you will be responsible for critical architectural decisions that will shape the technology stack for years to come. There is also the opportunity to make an immediate impact to the big data stack at multiple well-known silicon valley startups (our design partners) simultaneously. Last but not least, you will receive generous equity compensation with huge growth potentials.
What You’ll Do
Development - Design and build highly-efficient and scalable ETL pipelines with Snowflake, Airbyte, dbt, and Prefect.
Management - Configure, monitor, and manage Snowflake, Airbyte, dbt, and Prefect software in production.
Modeling - Apply your expertise to help model structured data with emphasis on Snowflake’s ACCOUNT_USAGE schema. Own these models at a high level, and provide consulting help for others to model applications on top of your data models.
Performance Tuning - Analyze and improve the performance and efficiency of the ETL pipelines by identifying bottlenecks, finding out root causes, applying best practices like incremental computation and parallel computation, and dogfooding Bluesky product.
Stewardship - Own or support the data definitions and lineage across our entire data warehouse.
Mentoring - Help teach other team members about data architecture, and also be a consultant for developers who need help with data.
Learning - Stay current on technical knowledge and tooling to help the team utilize new technologies.
What We Require
BS level technical degree or equivalent
4+ years of professional experience in data engineering or data scientists
1+ year of experience with Snowflake
1+ year of experience with dbt
Preferred Qualifications
Experience with more than one big data computation framework: Spark, Tez, MapReduce, Hive, Pig, Presto
Experience with cloud deployments and containers
Why Join Bluesky
We raised $8.8M to solve the biggest challenges of big data cloud adoption: the cost management and workload optimization. We are backed by Greylock venture capitalists. Our founder, Mingsheng Hong has decades of experience in the big data industry, and is behind the names of well-known big data technology like Apache Hive and Vertica. Bluesky has also officially joined the Snowflake Partner Network. We achieved Select Tier status less than a month after coming out of stealth. Please see techcrunch and data engineering podcast for details.
View all job listings here https://jobs.ashbyhq.com/Bluesky","$114,866 /yr (est.)",Unknown,Company - Public,"Energy, Mining & Utilities",Energy & Utilities,#N/A,Unknown / Non-Applicable
"Contact Government Services, LLC
4.7",4.7,"Washington, DC",Data Engineer,"CGS is seeking a passionate and driven Data Engineer to support a rapidly growing Data Analytics and Business Intelligence platform focused on providing solutions that empower our Federal customers with the tools and capabilities needed to turn data into actionable insights. The ideal candidate is a critical thinker and perpetual learner; excited to gain exposure and build skillsets across a range of technologies while solving some of our clients’ toughest challenges. As a Data Engineer, you will be integral to data operations for the development and integration of multiple data types across a range of data sets and sources.

You will be responsible for the day-to-day operations of systems that depend on data, ensuring data is properly processed and securely transferred to its appropriate location, in a timely manner. Processing data will include managing, manipulating, storing and parsing data in a data pipeline for variety of target sources. You will also support maintenance of applications and tools that reside on these systems such as upgrades, patches, configuration changes, etc. The work is performed in a multidisciplinary team environment using agile methodologies. The candidate we seek must be highly motivated and enthusiastic about implementing new technologies and learning about new data in a small team environment where deadlines are important.
Responsibilities:
Complete development efforts across data pipeline to store, manage, store, and provision to data consumers
Being an active and collaborating member of an Agile/Scrum team and following all Agile/Scrum best practices
Write code to ensure the performance and reliability of data extraction and processing
Support continuous process automation for data ingest
Achieve technical excellence by advocating for and adhering to lean-agile engineering principles and practices such as API-first design, simple design, continuous integration, version control, and automated testing
Work with program management and engineers to implement and document complex and evolving requirements
Help cultivate an environment that promotes customer service excellence, innovation, collaboration, and teamwork
Collaborate with others as part of a cross-functional team that includes user experience researchers and designers, product managers, engineers, and other functional specialists
Required Skills:
Must be a US Citizen
Must be able to obtain a Public Trust Clearance
7+ years of IT experience including experience in design, management, and solutioning of large, complex data sets and models.
Experience with developing data pipelines from many sources from structured and unstructured data sets in a variety of formats
Proficiency developing ETL processes, and performing test and validation steps
Proficiency to manipulate data (Python, R, SQL, SAS)
Strong knowledge of big data analysis and storage tools and technologies
Strong understanding of the agile principles and ability to apply them
Strong understanding of the CI/CD pipelines and ability to apply them
Experience with relational database, such as, PostgreSQL
Work comfortably in version control systems, such as, Git Repositories
Contact Government Services strives to simplify and enhance government bureaucracy through the optimization of human, technical, and financial resources. We combine cutting edge technology with world class personnel to deliver customized solutions that fit specific needs and solve the most challenging, dynamic problems. Our team emphasizes technical expertise and an empathy to public service to provide unique top tier support for our government engagements.
SOME BENEFITS OF THIS POSITION INCLUDE:
Health, Dental, & Vision insurance
Life Insurance
401k
Flexible Spending Account (Health, Dependent Care, and Commuter)
Paid Time Off & Observance of State/Federal Holidays
For more information about CGS please visit: https://www.cgsfederal.com
1YGuQTHTUE","$103,515 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Oran Inc
3.3",3.3,Remote,Big Data Engineer,"Job Title: Senior Big Data Technical Lead
Company: Oran Inc.
Location: [Location]
Job Type: Full-Time, Permanent
Job Summary: Oran Inc. is seeking a highly skilled and experienced Senior Big Data Technical Lead to join our team. In this role, you will be responsible for the development, integration, operations, and sustainment of various operational and in-development systems. You will play a crucial role in designing and developing integration solutions using AWS platforms, collaborating with solution architects, and ensuring alignment with the strategic vision. Your expertise in data ingestion, processing, and cloud technologies will be essential in delivering advanced production and dissemination capabilities.
Responsibilities:
Drive hands-on development of solutions in an Agile/Scrum environment, ensuring timely delivery and high-quality code.
Collaborate with solution architects to understand business and technical requirements and deliver solutions accordingly.
Analyze requirements and guide the implementation from the conceptual phase to the implementation phase.
Design and build data ingestion pipelines using Kafka, Glue ETL, Lambda, and NIFI frameworks.
Develop and guide developers in coding solutions using Spark framework in Scala, Java, or Python.
Design and develop solutions in AWS cloud services, including AWS MSK, AWS Elastic, AWS EMR, AWS SageMaker, S3 Archival, and AWS Kinesis.
Have a thorough understanding of container-based application architectures, including ECS and EKS on AWS.
Implement and ensure compliance with cyber security best practices, including code security using tools like HP Fortify and SonarQube.
Implement AWS security controls such as SAML, Kerberos authentication, RBAC, TLS, and data encryption controls.
Collaborate closely with Scrum team members to deliver high-quality products.
Provide daily support to internal clients with a sense of urgency.
Stay updated with new technologies and concepts and apply them to improve existing systems.
Contribute positively in a team environment and make technical contributions to drive business impact.
Think creatively, challenge existing thinking, and stimulate new ideas.
Qualifications:
Bachelor's degree in a relevant field with 6 years of prior experience, or Master's degree with 4 years of prior experience. Relevant experience may be considered in lieu of a degree.
U.S. citizenship is required.
Minimum of 5 years of strong server-side knowledge in back-end programming.
Minimum of 3 years of experience in creating Rest API Micro-services using ReactJS or AngularJS.
Minimum of 1 year of experience working in an Agile delivery environment.
Minimum of 3 years of experience working on a Cloud platform (AWS, Azure, GCP, or similar).
Strong experience in developing data ingestion pipelines using Kafka, Glue ETL, Lambda, and NIFI frameworks.
Proficiency in coding solutions using Spark framework in Scala, Java, or Python.
In-depth knowledge of AWS cloud services such as AWS MSK, AWS Elastic, AWS EMR, AWS SageMaker, S3 Archival, and AWS Kinesis.
Familiarity with container-based application architectures, including ECS and EKS on AWS.
Understanding of cyber security best practices, including code security using tools like HP Fortify and SonarQube.
Experience implementing AWS security controls such as SAML, Kerberos authentication, RBAC, TLS, and data encryption controls.
Strong problem-solving skills and ability to deliver high-quality code within deadlines.
Excellent communication and collaboration skills, with a positive and can-do attitude.","$150,000 /yr (est.)",1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,2004,$1 to $5 million (USD)
"Serenity Healthcare
2.6",2.6,"Lehi, UT",Junior Data Engineer,"Junior Data Engineer
Serenity Healthcare is hiring a Junior Data Engineer for our Lehi, UT headquarters. (Remote availability for residents of Utah or Colorado) While previous ETL experience is preferred, we are open to exceptional entry-level talent for this role.
We intend to provide on the job training in data-skills: SQL, BI (PowerBI), ETL (SSIS), Warehousing (SQL Stored Procedures), Exploratory Data Analysis, etc. It’s our intention to train you in Microsoft’s new tool: PowerApps.
Desired skill sets:
Must be a quick learner
SSIS experience strongly preferred
Skills used in the role:
SQL 20%
SSIS 40%
PowerApps 40%
Day-to-day work description:
The Junior Data Engineer will be responsible for keeping the data flowing, building new data pipelines, and creating business applications using MS-PowerApps. You’ll need to be comfortable with SQL, SQL Server, and SSIS. You’ll be reading API documentation to establish new ETL flows, as well as automating report delivery.
Job Fit:
Capable of “Deep Work”
Problem Solver
Reliable and consistent
Attention to detail
What We Offer to You:
Competitive pay (DOE), including additional target compensation
Opportunity to work and grow your career in a fast-paced environment
Medical, Dental, Vision Insurance (90% coverage for you and codependents)
Life Insurance
Flexible spending account
Paid time off
Vision insurance
401k
Open and friendly, professional office environment
Who We Are:
We have helped thousands of patients take back their lives from mental illness with specialized clinical expertise and the foremost cutting-edge technology available in mental health today. Serenity’s approach to treating mental illnesses is to offer holistic options and treat the whole person by providing an atmosphere of positivity, support, and healing in an outpatient setting.
We believe people should live their best lives, and mental health is a substantial segment of total well-being. We bring the same passion we have for improving our patient’s lives to providing a work experience that will help you do your best work, enjoy the time you invest at work, and succeed in life outside of work. We take our people and culture seriously and make it a priority to invest in both.
Serenity Mental Health Centers is an equal opportunity employer. This position is contingent on successfully completing a criminal background check and drug screen upon hire.","$85,462 /yr (est.)",201 to 500 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2017,Unknown / Non-Applicable
"Lightcast
4.4",4.4,"Moscow, ID",Senior Data Engineer,"A Senior Data Engineer will architect big data analytical frameworks, and translate complex functional and technical requirements into detailed architecture design and high-performing software. A Senior Data Engineer will select data solution software and define hardware requirements. The position will be responsible for developing and implementing standards and processes for integration projects and initiatives.

Major Responsibilities:
Architect software applications, and test and build automated tools.
Select data solution software and define hardware requirements
Develop standards and processes for integration projects and initiatives.
Lead the design/development of software applications, testing, and building tools
Ensure database changes are reviewed and approved to standards
Lead and communicate to leadership on solution design
Provide technical assistance to junior members and to colleagues across the company
Skills & Abilities:
Analytical mind, personality and aptitude for working with data
Ability to look at the numbers, trends, and data to derive conclusions based on findings
Work closely with management to prioritize business and information needs
Proven ability to work effectively to meet goals and deadlines with minimal supervision
Highly creative problem-solving skills and an ability to tailor efforts based on the importance of the issue being addressed.
A hands-on, detail-oriented mindset with the ability to look beyond the box
Expert knowledge of object-oriented design, data structures, and algorithms
Demonstrated experience with agile or other rapid application development methods, object-oriented design, coding, testing patterns with a variety of languages
Significant knowledge of data structures, algorithms, data modeling and disaster recovery of data systems
Education & Experience:
Requires 5 years of data/software engineering/science experience
Bachelor's degree in computer science or other technical disciplines (masters preferred)
About Lightcast:
Lightcast is a global leader in labor market insights with headquarters in Moscow (ID) and Boston (MA) and offices in the United Kingdom, Europe, and India. We work with partners across six continents to help drive economic prosperity and mobility by providing the insights needed to build and develop our people, our institutions and companies, and our communities.

Lightcast is proud to be an equal opportunity workplace and is committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. Lightcast has always been, and always will be, committed to diversity, equity and inclusion. We seek dynamic professionals from all backgrounds to join our teams, and we encourage our employees to bring their authentic, original, and best selves to work.

#LI-SS1","$118,643 /yr (est.)",1 to 50 Employees,Company - Private,Government & Public Administration,State & Regional Agencies,2011,Unknown / Non-Applicable
"Geo Owl
4.6",4.6,"Fort Gordon, GA",Senior Data Engineer,"Geo Owl is currently looking for a motivated and qualified Senior Data Engineer to support our Department of Defense contract opportunity. To be qualified, you need knowledge of Army structure and defense level intelligence, intelligence collection, fusion, analysis, production, and dissemination for intelligence databases and products, and meet the requirements listed below. If interested, apply now, or contact one of our recruiters.
Location: Fort Gordon, GA
Clearance: TS/SCI
Requirements: Must meet all the requirements listed below.
Excellent written & oral communication, research, and analytic skills
Expert ability to manage personnel, requirements, and coordination of projects
Expert capabilities to research, create, develop, and deliver professional briefings, multimedia presentations, and written reports
Experience utilizing programming languages such as SAS, R, Java, C, MATLAB, ScaLa, or Python; experience accelerating large data transactions across industry-leading GPU architectures to answer analytic questions
Experience with assessments, enterprise data integration, governance, and metrics, including the application of metadata management techniques and ability to interrogate databases efficiently using SQL
Experience with tradecraft and publication; ability to coordinate and support cross-community meetings and working groups; assimilate large volumes of information, and independently produce reports using data science focused libraries such as Pandas, Scikit, TensorFlow and Gensim to answer analytical questions
Desired Requirements:
Knowledge of Army structure and defense level intelligence operations: intelligence collection, fusion,
analysis, production, and dissemination for intelligence databases and products
Knowledge and experience with intelligence operations and in assisting with drafting expert assessments
across operations priorities on behalf of the stakeholder
Specialized training from any intelligence collection and analysis school or certification to include GEOINT Professional Certification (GPC-F, GPC_IA-II, GPC_GA-II, GPC_IS-II, etc)
Knowledge and understanding of the National System for GEOINT (NSG) and Intelligence Community;
knowledge of private sector data science/analytics, machine learning, and data visualization communities
Education Requirements:
MA or MS in Data Science, Data Analytics, Informatics, Statistics, or related field AND 2 years CURRENT
Intelligence Analysis experience; OR
BA or BS in Data Science, Data Analytics, Informatics, Statistics, or related field AND 5 years CURRENT
Intelligence Analysis experience; OR
Undergraduate degree with graduate/professional certificate in Data Science, Data Analytics, Informatics,
Statistics, or related field AND at least 10 years of Intelligence Analysis experience

Benefits:
Health Insurance (Geo Owl pays 80%+ of the premium).
401k matching.
Dental, Vision, and other supplemental insurance plans available.
Company-paid short-term and long-term disability and life insurance.
Peer-to-Peer spot bonuses.
120 hours of PTO per year plus federal holidays.
Joining the Geo Owl Team | What to Expect
At Geo Owl, we highly value our team members. We offer challenging but rewarding opportunities for those who want to work hard to provide a great experience for the customer and strive to reach their professional goals. As a member of the Geo Owl family, you will be working alongside people who share this work ethic and are aiming to be the best partner for our customer. We are all proud to be a part of this company and we want you to be too.
Our Mission
Provide high quality solutions to our mission partners in the United States through our expert analysts.
Be recognized as the best at what we do by our customers.
Be a team our team members are proud and excited to be a part of.
Continually strive for excellence and seek to tackle the most difficult challenges our industry has to offer.
About Us
Geo Owl is a premiere provider of Full-Motion Video (FMV), Geospatial, ISR, Intelligence and IT services to the Department of Defense and Intelligence Community. We are vitalized by our engaged team of professionals that truly value each other and the important missions we support.
Equal Opportunities
Geo Owl is an equal opportunity employer and does not discriminate on the basis of race, color, religion, creed, sex, age, sexual orientation, national origin, disability, marital status, military status, genetic predisposition, or any other basis protected by law.
To stay up to date about new career opportunities:
Follow us on Twitter
Follow us on Instagram
Follow us on LinkedIn

I8pR3c9K0u","$126,295 /yr (est.)",51 to 200 Employees,Company - Private,Aerospace & Defense,Aerospace & Defense,2013,Unknown / Non-Applicable
"Titan America, LLC.
3.9",3.9,"Roanoke, VA",Data Engineer,"Data Engineer
We are seeking a motivated and detail-oriented Data Engineer to join our team based in Roanoke, Virginia. As a Data Engineer, you will play a crucial role in designing, developing and maintaining scalable data pipelines and transformations for business-critical use cases. In addition, you may take on various Software Development and Data Science responsibilities as we frequently share responsibilities and knowledge across our growing team.
Responsibilities:
Design, develop and maintain scalable data pipelines for extracting, transforming, and loading data from various sources
Collaborate with cross-functional teams to identify data needs and determine the best data solutions
Develop and implement data models to support business requirements and ensure data quality
Ensure the security and privacy of sensitive data by implementing appropriate access controls
Monitor and optimize data pipeline performance to ensure timely and accurate data delivery
Document data pipeline processes, data dictionaries, and data storage solutions
Requirements:
Bachelor's degree in Computer Science, Computer Engineering, or a related technical field
Minimum of five years of professional experience working as a Data Engineer or Software Developer
Strong hands-on experience with data warehouse and transformation solutions, i.e. Domo, Snowflake or similar
Proficient in at least one scripting language such as Python, JavaScript, or R
Understanding of data modeling, data integration and data quality processes
Familiarity with cloud platforms such as AWS, Azure, or Google Cloud Platform
Strong analytical and problem solving skills
Full Stack Software Development experience in a professional setting is highly desired, but not required
This is an excellent opportunity for a driven and collaborative individual to make a significant impact in a dynamic and growing team. If you have a passion for data and a desire to work in a fast-paced and dynamic environment, we want to hear from you!
Job Type: Full-time
Pay: $75,000.00 - $87,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Health insurance
Life insurance
Paid time off
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
On call
Ability to commute/relocate:
Roanoke, VA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 5 years (Preferred)
SQL: 5 years (Preferred)
Data warehouse: 5 years (Preferred)
Work Location: In person","$81,000 /yr (est.)",1001 to 5000 Employees,Company - Public,"Energy, Mining & Utilities",Mining & Metals,1902,Unknown / Non-Applicable
"JPMorgan Chase Bank, N.A.
3.8",3.8,"Wilmington, DE",Data Engineer III,"As a Data Engineer III under Consumer and Community Banking within the ADE Data Governance team at JPMorgan Chase, you serve as a seasoned member of an agile team to design and deliver trusted data collection, storage, access, and analytics solutions in a secure, stable, and scalable way. You are responsible for developing, testing, and maintaining critical data pipelines and architectures across multiple technical areas within various business functions in support of the firm's business objectives.

Job responsibilities
Supports review of controls to ensure sufficient protection of enterprise data
Responsible for advising and making custom configuration changes in one to two tools to generate a product at the business or customer request
Updates logical or physical data models based on new use cases
Frequently uses SQL and understands NoSQL databases and their niche in the marketplace
Adds to team culture of diversity, equity, inclusion, and respect

Required qualifications, capabilities, and skills
Experience across the data lifecycle
Advanced at SQL (e.g., joins and aggregations)
Working understanding of NoSQL databases
Proficiency with JSON/Java, Erwin, Oracle, Snowflake, AWS
Significant experience with statistical data analysis and ability to determine appropriate tools and data patterns to perform analysis, including data modeling
Experience customizing changes in a tool to generate product
Executes data architecture solutions and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions and break down problems
Produces data architecture and design artifacts for complex applications while being accountable for ensuring design constraints are met by software code development
Gathers, analyzes, synthesizes, and develops visualizations and reporting from large, diverse data sets in service of continuous improvement of data frameworks, applications, and systems that proactively identifies hidden problems and patterns in data and uses these insights to drive improvements to coding hygiene and system architecture

Preferred qualifications, capabilities, and skills
Familiarity with modern technologies
Exposure to cloud technologies
Chase is a leading financial services firm, helping nearly half of America's households and small businesses achieve their financial goals through a broad range of financial products. Our mission is to create engaged, lifelong relationships and put our customers at the heart of everything we do. We also help small businesses, nonprofits and cities grow, delivering solutions to solve all their financial needs.

We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.
The health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the ""WELL Health-Safety Rating"" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.
As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm's current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm's vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.
We offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, discretionary incentive compensation which may be awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.
Equal Opportunity Employer/Disability/Veterans
Base Pay/Salary
Jersey City,NJ $97,850.00 - $150,000.00 / year","$123,925 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Banking & Lending,1799,$10+ billion (USD)
"SEDAA
3.4",3.4,"Oakland, CA",Sr. Data Engineer,"Description:
FTE/ Client's DIRECT HIRE
Job title: Data Engineer
Job Category: Information Technology

Work Type: Hybrid

Job Location: Oakland
Team Overview
The Decision Products team strives to utilize best in class modeling techniques and industry leading data science to drive client’s transition to the sustainable energy network of the future through data driven decision making. This work moves beyond descriptive reporting and is focused on pushing the business forward through applied statistics, predictive and prescriptive analytics, and insightful tool design. The cornerstone of these high value analytics is one of the largest smart meter usage databases in the industry, that when combined with billing, program engagement, customer demographic, grid, and other data sources has unprecedented potential.
Current and past projects include:
Deployment of computer vision algorithms in tools that accelerate and automate asset inspections processes
Predicting electric distribution equipment failure before it occurs allowing for proactive maintenance
Optimizing renewable resource portfolios, including location and resource adequacy considerations
Supporting asset strategy decision making including, where should PG&E underground electrical assets
Supervised and unsupervised machine learning models using Python and Spark, trained on AWS, deployed on Palantir Foundry
Position Summary

We are looking for a savvy and driven Data Engineer to join our growing team of analytics experts. In this role you will work as part of cross functional teams, including data scientists, other data engineers, technology experts, and subject matter experts to develop data driven solutions. Successful candidates will be responsible for building, expanding, and optimizing our data, data storage, and data pipeline. This individual will support team members (data scientists, software developers, etc.) and decision products to ensure that data delivery is reliable and optimized. They will be supporting the data needs of multiple teams, systems, and products. This role will help the team continue its history of success. Qualified candidates will have a unique opportunity to be at the forefront of the utility industry and gain a comprehensive view of the nation’s most advanced smart grid. It is the perfect role for someone who would like to continue to build upon their professional experience and help advance
client’s sustainability goals.
client is providing the salary range that the company in good faith believes it might pay for this position at the time of the job posting. This compensation range is specific to the locality of the job. The actual salary paid to an individual will be based on multiple factors, including, but not limited to, specific skills, education, licenses or certifications, experience, market value, geographic location, and internal equity. We would not anticipate that the individual hired into this role would land at or near the top half of the range described below, but the decision will be dependent on the facts and circumstances of each case.

A reasonable salary range is:
ACTUAL BAY AREA SALARY RANGE: $98,000.00 to $122,000.00

ACTUAL CALIFORNIA SALARY RANGE: $93,000.00 to $116,000.00
This position is hybrid, working from your remote office and your assigned work location based on business need.

Responsibilities
Enhance and maintain our current data pipelines and associated infrastructure

Assemble large, moderately complex data sets that meet functional / non-functional business requirements.
Engage with different stakeholder teams to troubleshoot various database systems
Build and maintain tools that monitor data and system health
Identify, design, and implement internal process improvements to optimize production of results and enable cost savings.
Performance tune and optimize data pipeline on Spark
Create and maintain documentation describing data catalog and data objects

Minimum Requirements

Bachelor’s degree in computer science, an engineering field, or equivalent work experience in an engineering field
3 years of experience with data engineering/ETL ecosystem, such as Palantir Foundry, Spark, Informatica, SAP BODS, OBIEE
Required Skills

Experience with data engineering/ETL ecosystem, such as Palantir Foundry, Spark, Informatica, SAP BODS, OBIEE
Database design fundamentals
Experience with Python, Pandas and APIs
Knowledge of Time Series data set development.
Demonstrated commitment to teamwork and enabling others
Proven ability to translate business desires into technical requirements
Ability to communicate with various stakeholders and leadership
Ability to breakdown an ambiguous problems
Desired Skills

Experience with Scikit Learn, PySpark or equivalent big data processing framework, CI/CD tool
Experience with an infrastructure as code tool, writing production-level code, writing health checks, unit tests, integration tests, schema validations
Familiarity with cloud computing security fundamentals
Experience with the Palantir Foundry platform
Experience working with data scientists and machine learning engineers
Familiarity with model deployment
Front end tools: PowerBi, Tableau","$110,000 /yr (est.)",51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2000,Unknown / Non-Applicable
"Health Plan One
3.2",3.2,"Trumbull, CT",Data Engineer,"The Data Engineer will be responsible for designing, developing, and maintaining our data infrastructure and pipelines, as well as ensuring the quality and reliability of our data. The successful candidate will work closely with our data scientists, analysts, and other stakeholders to ensure that our data is accurate, accessible, and secure.
Duties/Responsibilities:
Design and implement data pipelines and infrastructure to ensure efficient data processing and storage.
Ensure the quality and reliability of data, including identifying and resolving data inconsistencies, errors, and anomalies.
Develop and maintain ETL processes and data integration workflows.
Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Collaborate with data scientists, analysts, and other stakeholders to understand data needs and requirements.
Monitor and optimize data performance, including troubleshooting and resolving issues as they arise.
Keep up to date with new technologies and approaches to data engineering and recommend improvements to our data infrastructure.
Performs other related duties as required.
Required Skills/Abilities:
Bachelor's degree in Computer Science, Data Science, or a related field.
Minimum of 3 years of experience in data engineering or a related field.
2+ years in data modeling, data governance, and or data architecture
Strong experience with ETL processes, data integration, and data pipelines.
Proficiency with SQL, Python or similar.
Experience with data warehousing and data modeling concepts.
Knowledge of cloud computing and storage solutions, such as Azure or AWS.
Strong analytical and problem-solving skills.
Excellent communication and collaboration skills.
Preferred Skills/Abilities:
Master's degree in Computer Science, Data Science, or a related field.
Experience with data visualization tools such as Domo, Tableau or Power BI.
Experience with data integration through APIs, web services, SOAP, and/or REST services
Physical Requirements:
Prolonged periods of sitting at a desk and working on a computer, typically in an office or cubicle environment (constant noise, fluorescent overhead lighting)
Our centers are consistent with CDC guidelines and align with local government orders pertaining to all Company physical locations in relation to COVID-19.
Equal Employment Opportunity (EEO) is a fundamental principle at HPOne, where employment is based upon personal capabilities and qualifications. HPOne does not discriminate because of actual or perceived sex, sexual orientation or preference, gender identity, gender, transgender, race, color, religion, national origin, creed, citizenship status, ancestry, age, marital status, pregnancy, childbirth or related medical conditions, medical conditions including genetic characteristics, mental or physical disability, military and veteran status, or any other protected characteristic as established by law. HPOne requires the necessary drug testing and background checks as part of our pre-employment practices.
Job Type: Full-time
Benefits:
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
Day shift
Monday to Friday
Work Location: Hybrid remote in Trumbull, CT 06611","$94,155 /yr (est.)",1001 to 5000 Employees,Company - Private,Insurance,Insurance Carriers,2006,$25 to $100 million (USD)
"Northeastern University
4.3",4.3,"Boston, MA",Data Engineer,"About the Opportunity
Do you love learning the shapes of datasets, and molding them into something new? The Digital Scholarship Group (DSG) in the Northeastern University Library is excited to open a search for a Data Engineer. Working within a warm and collaborative environment dedicated to social justice, the Data Engineer gathers, organizes, manipulates, transforms, and documents a variety of humanities research data. The Data Engineer works with colleagues across the university to create sustainable platforms and data for community-led digital scholarship.
The Data Engineer position is situated within the DSG, which is part of the Northeastern University Library. The Library is a vital partner in learning, teaching, and community-engaged research for a diverse R1 university. Northeastern is committed to intensive research and experiential learning for students at all levels.
The Digital Scholarship Group is based on Northeastern’s Boston campus. This position is eligible for a hybrid work arrangement. Specific arrangements can be negotiated at the time of hire.
Responsibilities
The Data Engineer has responsibility for helping DSG work with data in a wide range of formats, across multiple projects and often in unforeseen contexts. The Data Engineer develops data dictionaries, mappings between data standards, transformation routines, and other curatorial systems. This position also manages projects and engages in high-level needs analysis and project planning. The DSG is committed to digital approaches that consider the pedagogical, research, social, and ethical implications of data and its design and use.
Working closely with other DSG and library staff, faculty collaborators, and students, the Data Engineer contributes to grant-funded and internal projects including the Boston Research Center; the Civil Rights and Restorative Justice project; the Digital Archive of Indigenous Language Persistence; the TEI Archiving, Publishing, and Access Service; Digital Humanities Quarterly; and the Women Writers Project. They will also take the lead on building DSG’s policies and practices in working with external data platforms such as Wikidata and partner project APIs. To support this work, expertise with tools like regular expressions and OpenRefine, and facility with data including RDF, JSON, XML, various API responses, and other formats will be important.
We warmly invite people with various skills and levels of expertise to apply to this position. Candidates who meet some, but not all, of the qualifications listed below are strongly encouraged to apply. We seek colleagues who are committed to building an inclusive and diverse working environment and who have been and remain underrepresented or marginalized in the field of librarianship – including but not limited to people of color, LGBTQ+ people, individuals with disabilities and applicants from lower-income and first-generation library or academic backgrounds. We expect this position to be an ongoing learning experience and are committed to supporting professional development.
Qualifications
We realize that this is a lengthy list of activities and qualifications. There are multiple paths toward success in this position, and each may look somewhat different depending on the successful candidate’s interests and experience.
Bachelor’s degree required; Master’s degree or similar training in data science, information science, information design, or other relevant discipline preferred
Minimum of 2 years of experience working or studying in a data-intensive environment, preferably in an academic or non-profit research setting
Experience working with quantitative and qualitative datasets, especially with historical and cultural heritage data
Experience working with structured data formats (for instance, XML, RDF, JSON, CSV, relational databases) and with data conversion, data enhancement, and data analysis
Ability to write code to assist in carrying out these kinds of data-related work (for instance, using R, Python, SQL, SPARQL, XSLT, Perl, and/or regular expressions)
Ability to work on multiple concurrent projects and adapt to the evolving landscape of digital humanities
Collaborative problem-solving skills, and the ability to research and recommend solutions as part of a participatory design process
Commitment to thoughtful, adaptive engagement with the needs of community collaborators
Strong oral and written skills, ability to communicate across expertise levels and prepare project documentation
Desire and aptitude to grow skills (especially in technical areas) and learn new things
The following skills are desirable but are not all essential for applicants to possess at the outset; we can provide training:
Knowledge of metadata standards relevant to research data, such as the Data Documentation Initiative
Experience creating, manipulating, and querying linked open data
Experience in open-source development practices and workflows, preferably within an academic or non-profit environment
Experience working with databases, data management systems, and APIs
Experience with developing and leading workshops
Experience communicating complex ideas about data and how it is used to many audiences
Salary Range:
$82,725 - $93,000
About the Digital Scholarship Group
A recognized leader in the field, the Digital Scholarship Group supports digital modes of research, publication, and collaboration through applied research, systems and tools development, and consultative services. The DSG offers a friendly and closely collaborative work environment, and actively fosters the professional and intellectual development of all of our colleagues and collaborators, including training opportunities and mentorship.
Our team engages with faculty in the digital humanities and quantitative social sciences from across the university to develop digital research and teaching projects, organize events, plan grant-funded initiatives and provide training and mentorship. We also work in close partnership with Northeastern’s Archives and Special Collections, the NULab for Maps, Texts, and Networks, and with cultural heritage partners in Boston including the Massachusetts Historical Society and the Boston Public Library.
We develop tools and platforms for working with digital artifacts and data, for querying and publishing them. We also provide workshops, mentorship opportunities, and pedagogical frameworks to the Northeastern community. Some of our major projects include the Boston Research Center, the Civil Rights and Restorative Justice Project, and the Digital Archive of Indigenous Language Persistence, as well as a number of digital archiving projects from the Library’s Archives and Special Collections. In all of our projects, we are attentive to inclusive and anti-racist approaches to data modeling, platform development, and collaborative working processes.
About the Library
The Northeastern University Library supports the mission of the University by working in partnership with the University community to develop and disseminate new scholarship. The Library fosters intellectual and professional growth, enriches the research, teaching, and learning environment, and promotes the effective use of knowledge by managing and delivering information resources and services to library users.
Northeastern University is an equal opportunity employer, seeking to recruit and support a broadly diverse community of faculty and staff. Northeastern values and celebrates diversity in all its forms and strives to foster an inclusive culture built on respect that affirms inter-group relations and builds cohesion.
All qualified applicants are encouraged to apply and will receive consideration for employment without regard to race, religion, color, national origin, age, sex, sexual orientation, disability status, or any other characteristic protected by applicable law.
To learn more about Northeastern University’s commitment and support of diversity and inclusion, please see www.northeastern.edu/diversity.
About Northeastern
Founded in 1898, Northeastern is a global research university and the recognized leader in experience-driven lifelong learning. Our world-renowned experiential approach empowers our students, faculty, alumni, and partners to create impact far beyond the confines of discipline, degree, and campus.
Our locations—in Boston; Charlotte, North Carolina; London; Portland, Maine; San Francisco Bay area; Seattle; Silicon Valley; Toronto; Vancouver; and the Massachusetts communities of Burlington and Nahant—are nodes in our growing global university system. Through this network, we expand opportunities for flexible, student-centered learning and collaborative, solutions-focused research.
Northeastern’s comprehensive array of undergraduate and graduate programs— in a variety of on-campus and online formats—lead to degrees through the doctorate in nine colleges and schools. Among these, we offer more than 195 multi-discipline majors and degrees designed to prepare students for purposeful lives and careers.
The position will remain open until filled but application review will begin after June 16.
Position Type
Information Technology
Additional Information
Northeastern University considers factors such as candidate work experience, education and skills when extending an offer.
Northeastern has a comprehensive benefits package for benefit eligible employees. This includes medical, vision, dental, paid time off, tuition assistance, wellness & life, retirement- as well as commuting & transportation. Visit
https://hr.northeastern.edu/benefits/
for more information.
Northeastern University is an equal opportunity employer, seeking to recruit and support a broadly diverse community of faculty and staff. Northeastern values and celebrates diversity in all its forms and strives to foster an inclusive culture built on respect that affirms inter-group relations and builds cohesion.
All qualified applicants are encouraged to apply and will receive consideration for employment without regard to race, religion, color, national origin, age, sex, sexual orientation, disability status, or any other characteristic protected by applicable law.
To learn more about Northeastern University’s commitment and support of diversity and inclusion, please see
www.northeastern.edu/diversity
.","$87,863 /yr (est.)",1001 to 5000 Employees,College / University,Education,Colleges & Universities,1898,$100 to $500 million (USD)
"Fuge Technologies Inc
4.7",4.7,"Palmyra, NJ",Senior Data Engineer,"Job Description:
RESPONSIBILITIES
Basic Qualifications for consideration:
5+ Overall industry experience
3+ years' experience with building large scale big data applications development
Bachelor’s in computer science or related field
Provide technical leadership in developing data solutions and building frameworks
Experience building Data Lake, data applications and micro-services using Azure, AWS and Hands-on experience in cloud Data stack (preference is Azure)
Hands-on experience with Springboot framework, MVC architecture, Hibernate, API creation, Restful services, SOAP using Java
Java experience with OOPS concepts, multithreading
Experience deploying code on containers
Hands on experience with leveraging CI/CD to rapidly build & test application code
Conduct code reviews and strive for improvement in software engineering quality
Review data architecture and develop detailed implementation design
Hands-on experience in production rollout and infrastructure configuration
Demonstrable experience of successfully delivering big data projects using Kafka, Spark
Experience working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search
Experience working with PCI Data and working with data scientists is a plus
In depth knowledge of design principles and patterns
Able to tune big data solutions to improve performance
Qualifications
Bachelor’s degree in computer science or computer Engineering is required
Skills Set Mandate :
Java, Spark, and Azure cloud
Job Type: Contract
Salary: $55.00 - $60.00 per hour
Ability to commute/relocate:
Palmyra, NJ 08065: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",$57.50 /hr (est.),1 to 50 Employees,Contract,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"EnergyHub
3.6",3.6,"Brooklyn, NY",Staff Data Engineer,"EnergyHub empowers utilities and their customers to create a clean, distributed energy future. We help consumers turn their smart thermostats, EVs, batteries, and other products into virtual power plants that keep the grid stable and enable higher penetration of solar and wind power.
Data is core to the function and operation of EnergyHub. We spend most of our time on customer facing systems, which means that your work on this team directly improves a product that impacts climate change while improving grid stability at the same time. If you'd like to make a difference for current and future generations using your data skills, you are in the right place.
We operate on a fivetran/DMS/snowflake/airflow stack, and we are in the middle of transitioning our transformation tooling to DBT from a mix of custom python batch processing in aws batch and snowpark wrappers.
Main Responsibilities
Design data models in partnership with product managers, data analysts, data scientists, and occasionally a backend engineer on another team.
Port old code into DBT, possibly spotting uncaught bugs along the way
Write new pipelines, in DBT, optimizing first for correctness, second for maintainability, and third for performance
Set the bar for engineering best practices in an analytics engineering context
Occasionally write or patch a data integration (written in python)
find, evaluate, and adopt great tools and vendors
give and accept feedback readily and with an open mind
Participate in and contribute to architecture reviews in data engineering and engineering more broadly
Educate, mentor and guide members of the Data team to improve their skill sets
Key Skills and Experience
We recognize that years of experience is an imperfect proxy for ability. If you have deep or intense experiences with any skill listed which do not meet the years listed and you achieved success with those skills, be sure to mention those experiences in your resume!
6 years of cumulative experience in roles for which the primary responsibility is programmatic data processing using at least one of the following languages/frameworks or something substantially similar: python, R, SQL, Snowflake, Redshift, BigQuery, Vertica, ClickHouse, Julia, Scala, Spark, Flink, Kafka
4+ years of experience using DBT
5+ years of experience using SQL
4+ years of experience with python/R/julia or a similar scripting language, of which at least one year is with pandas
2+ years of experience writing api integration
Preferred Skills and Experience
Mastery of SQL: you have not found a data transformation task that you cannot solve relatively cleanly in SQL (or at least SQL + Jinja/DBT). You can read, explain, and, refactor a large and complex sql statement into a collection of smaller ones, without access to someone who can answer questions about it.
DBT expertise: You are comfortable implementing custom materialization types. You have implemented multiple project structures and can neutrally articulate pros and cons of different approaches. You are fluent in model selection syntax.
Mastery of Snowflake: You can articulate the query profile that you are expecting before running a query, given the clustering keys of tables involved. You might have used MATCH_RECOGNIZE for event analysis. You have 1+ years of experience navigating database permissions, or a willingness to read a lot of documentation. You have a working knowledge of future grants and RBAC.
The salary range for this position is $150,000-$190,000. Base pay offered may vary based on location, job-related knowledge, skills and experience.
Why work for EnergyHub?
Collaborate with outstanding people: Our employees work hard, do great work, and enjoy collaborating and learning from each other.
Make an immediate impact: New employees can expect to be given real responsibility for bringing new technologies to the marketplace. You are empowered to perform as soon as you join the team!
Gain well rounded experience: EnergyHub offers a diverse and dynamic environment where you will get the chance to work directly with executives and develop expertise across multiple areas of the business.
Work with the latest technologies: You'll gain exposure to a broad spectrum of IoT, SaaS and machine learning obstacles, including distributed fault-tolerance, device control optimization, and process modeling to support scalable interaction with disparate downstream APIs.
Be part of something important: Help create the future of how energy is produced and consumed. Make a positive impact on our climate.
Focus on fun: EnergyHub places high value on our team culture. Happy hours and holiday parties are important to us, but what's also important is how our employees feel every single day.
Company Information
EnergyHub is a growing enterprise software company that works with the most forward-thinking companies in smart energy. Our platform lets consumers turn their smart thermostats, electric cars, water heaters, and other products into virtual power plants that keep the grid stable and enable higher penetration of solar and wind power. We work on technology that already provides energy and cost savings to millions of people through partnerships with the most innovative companies in the Internet of Things.
Company Benefits
EnergyHub offers a generous benefits package including 100% paid medical for employees and a 401(k) with employer match. We offer a casual environment, the flexibility to set your own schedule, a fully stocked fridge and pantry, free Citi Bike membership, secure bike rack, gym subsidy, paid parental leave, and an education assistance program.
EnergyHub is an Equal Opportunity Employer
In connection with your application, we collect information that identifies, reasonably relates to or describes you (""Personal Information""). The categories of Personal Information that we may collect include your name, government-issued identification number(s), email address, mailing address, other contact information, emergency contact information, employment history, educational history, and demographic information. We collect and use those categories of Personal Information about you for human resources and other business management purposes, including identifying and evaluating you as a candidate for potential or future employment or future positions, recordkeeping in relation to recruiting and hiring, conducting analytics, and ensuring compliance with applicable legal requirements and Company policies.","$170,000 /yr (est.)",51 to 200 Employees,Subsidiary or Business Segment,Information Technology,Computer Hardware Development,2007,$5 to $25 million (USD)
Dataworks,#N/A,Remote,Senior Data Engineer,"TECH STACK: SQL, Python, BigQuery, Redshift, Airflow, Snowflake, DBT, PowerBI
ABOUT THE COMPANY
This is an exciting opportunity to join one of the world’s leading technology platforms, with billions of clicks and multi-million active users per month. This product is used by businesses to capture insights and track user journeys on their platforms. This organization is a truly data-driven business who are growing exponentially over the next two years in both headcount and revenue.
ABOUT THE ROLE
You will work in the data team and will be responsible for delivering data to customers and transferring data for actionable insights. You will also be responsible for maintaining the internal pipelines and mentoring, leading and supporting your team mates.
Key things you will be responsible for are:
Creating and maintaining pipelines.
Writing Python code.
Designing APIs.
Working with a variety of data tools.
SKILLS & EXPERIENCE:
The chosen candidate MUST have:
· 5+ Years of experience in engineering
· Excellent hands-on analytics experience (Python, SQL) in data manipulation and solving issues.
· Excellent communication and teamwork skills
· Successful ability to communicate effectively with a broad range of audiences and stakeholders.
· Proactive in tackling concerns, innovative problem-solving skills
· Expert with version control, GitHub preferred.
· Experienced in using Apache Airflow and Beam
· Fluent with BI and Visualization tools
· Experienced with Golang
· Significantly strong background in analyzing data from different platforms.
· Highly efficient in prioritizing, multitasking, and managing multiple projects with high attention to detail.
· Consistent passion for learning new techniques/tools.
BENEFITS
The chosen candidate will have the opportunity to work in a growth-orientated organization and a market leader. They are offering market-leading employee benefits, and this is an opportunity for fully remote working.
***This position is open to fully remote workers; however, you must be currently living in, OR happy to relocate to certain states in the US (more details upon application)
UNFORTUNATELY, THIS ORGANISATION ARE UNABLE TO TRANSFER OR SPONSOR ANY WORK VISA's AND THIS ROLE IS NOT OPEN TO CONTRACTORS
Job Types: Full-time, Permanent
Salary: Up to $180,000.00 per year
Benefits:
401(k)
Health insurance
Paid time off
Experience level:
5 years
Schedule:
Monday to Friday
Application Question(s):
Will you require sponsorship or a visa transfer in the future?
Experience:
RedShift/BigQuery: 2 years (Required)
Python: 5 years (Preferred)
SQL: 5 years (Preferred)
Work Location: Remote","$180,000 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2021,$1 to $5 million (USD)
"HOLT CAT
4.1",4.1,"San Antonio, TX",Data Engineer 2,"As a Data Engineer 2 on our Enterprise Solution Delivery team, you will have a chance to design intelligent, automated data systems using advanced data engineering knowledge in the data warehousing space to redefine best practices with a cloud-based approach to scalability and automation. In partnership with other developers and business analysts, you will work backwards from our business questions to build reliable and scalable data solutions to meet the business needs. You will maintain and support new and existing data pipelines. By scaling up our data ecosystem around cloud-based CRM, ERP, and data platforms, we will improve speed, lower the total cost of ownership, and provide a unified view of entities.
You will be an integral part of the development team, sometimes investigating new requirements and design and at times refactoring existing functionality for performance and maintainability, but always working on ways to make us more efficient and provide better solutions (data pipelines) to our end customers. The candidate will perform hands-on activities including design, documentation, development, and test of new functionality. Candidate must be flexible and willing to switch tasks based on team’s needs.
The incumbent in this position is expected to model the following practices daily: 1) Demonstrate alignment with the company's mission and core business values; 2) Collaborate with key internal/external resources; 3) Participate in ongoing self- development.

Essential Functions:
Develop, evaluate, and influence effective and consistent productivity and teamwork to ensure the delivery of Legendary Customer Service (LCS)
Model, promote, reinforce, and reward the consistent use of HOLT’s Values Based Leadership (VBL) tools, models, and processes to ensure alignment with our Vision, Values, and Mission
Design, develop, implement, test, document, and operate mid-scale, high- volume, high-performance data structures for business intelligence analytics
Create and propose technical design documentation, which includes current and future ETL functionality, database objects affected, specifications, and flows and diagrams to detail the proposed implementation.
Implement data structures using best practices in data modeling to support on- line reporting, analysis, business intelligence and building a logical abstraction layer against large, multi-dimensional datasets and multiple sources.
Understand existing databases and warehouse structures to best determine how to consolidate and aggregate data in an efficient and scalable way.
Design and code all aspects of data solutions using cloud-based tools to build out a data warehouse.
Design ETL/ELT processes and data pipelines to bring data from various sources into a central data repository.
Work closely with Integration developers, ETL developers, application teams, and vendors to develop optimal solutions.
Analyze new/disparate data sources for integration with existing datasets to tell a comprehensive data story.
Improve business process agility and outcomes, drive innovation, and reduce time to market for our innovative IT solutions.
Works safely always and adheres to all applicable safety policies; complies with all company policies, procedures, and standards.
Performs other duties as assigned.

Knowledge, Skills, and Abilities:
Capable of speaking articulately to a breadth of topics such as RDBMS, NoSQL, Azure data store technologies, ETL, data warehousing, data modeling, role- based access, etc.
Background in supporting business intelligence teams by providing subject matter expertise and guidance in modern data engineering.
Advanced SQL and query performance tuning skills
Experience with MPP (massively parallel processing) data
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.
Ability to work with ambiguous requirements and drive clarity by collaborating with business groups.
Drive and desire to learn and grow both technical and functional skill sets, High energy, stamina, enthusiasm, organization, and curiosity.
Innovative thinker who is positive, proactive, and readily embraces change.
Detail-oriented individual with the ability to rapidly learn and take advantage of new concepts, tools, and technologies; Ability to quickly ramp up new projects, understanding the business needs and support engagements.
Ability to manage workload, multiple priorities, excellent problem solving, and troubleshooting skills.
Mentoring Associate developers as needed.
Experience working in a matrix environment and foster motivation within the project team to meet tight deadlines.

Education and Experience:
High School diploma or equivalent is required, Bachelor's degree in information technology, or related field preferred.
5+ years of experience designing, developing, and deploying data solutions using ETL/ELT/DWH/BI technologies required.
2+ years of coding experience with modern programming or scripting languages (Python, C# etc.) required.
Experience in Informatica IICS and Snowflake required.
Experience in developing/operating large-scale ETL/ELT processes with on-prem and cloud platforms; database technologies; data modeling required
Experience developing/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets from multiple systems required.
Experience operating a very large data environment that may include data lake, and/or data warehouse etc. required.
Experience developing and implementing data models for data warehouse or related data processes for heavy equipment rental and dealership operations required.
Basic programming skills such as JavaScript and PowerShell scripting. Experience with source control and build technologies (e.g., Azure DevOps, GIT) required.
Data Engineering related certifications preferred.
Experience working in an Agile environment required.

Travel:
Up to 20% with occasionally overnight stay may be expected.
Valid driver’s license.

Work Environment:
Works primarily in a professional office environment.
This role constantly uses standard office equipment such as computers, phones, photocopiers, filing cabinets, and fax machines.
Frequently works at fast pace with unscheduled interruptions.

Physical Requirements:
This position involves extended periods in a stationary position; additionally, occasional movement inside the office to access office machinery, and file cabinets.

Disclaimer:
Please note that the above statements are intended to describe the general nature and level of work being performed by employees assigned to this classification. They are not to be interpreted as an exhaustive list of all responsibilities, duties, and skills required of the incumbents so classified. All incumbents may be required to perform duties outside of their normal responsibilities, as needed.
Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities
The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information. 41 CFR 60-1.35(c)","$101,333 /yr (est.)",1001 to 5000 Employees,Company - Private,"Construction, Repair & Maintenance Services",Construction,1933,$1 to $5 billion (USD)
"Second Wave Delivery Systems, LLC",#N/A,Remote,Data Engineer (Looker/LookML),"Second Wave Delivery Systems, LLC (SWDS) is determined to improve clinical outcomes in the United States while addressing the rising costs of healthcare. We are developing a technology-enabled, integrated system tailored to optimize value-based arrangements for payors and providers serving Medicare and Medicaid enrollees. Through an increased focus on primary and preventive care, we strive to provide patients with significantly better outcomes and satisfaction in each community we serve.

The Technology organization at SWDS is responsible for building the next generation of value-based healthcare management and optimization through a family of systems and the foundational technologies required to support SWDS’ ambitious mission. Predicated on the latest technologies and built upon a mantra of Engineering and Operational Excellence, we develop mission-critical applications and infrastructure quickly, reliably, and while having fun. We strive to grow, foster, and maintain an entrepreneurial, diverse, collaborative, and flexible team and structure. As a fully remote, fast-paced, and quickly growing organization, we are always looking for talented, passionate, and ambitious individuals who seek to solve exciting and challenging technology problems while having a transformative impact on US healthcare.

Core values and ways of working
We are a tight group of engineers focusing on engineering and operational excellence while delivering high quality solutions to our clients and business partners
We take great pride in our team and the products we deliver, and treat both with utmost respect and care
We are firm believers in engineering high-quality, fault-tolerant systems that scale with business needs with minimal unforeseen disruption
We value great work and great ideas, not ego. We're looking for talented individuals that are accustomed to working within a fast-paced environment and want to ship often to make an impact

Responsibilities
Build Looker LookML for new datasets
Maintain and improve Looker LookML data models
Create Looker looks and dashboards, including both standalone and embedded dashboards
Optimize existing dashboards as well as Looker instance to improve response time and user experience
Promote best practices with development of Looker dashboards and embedded visualizations
Work closely with the Business Intelligence and Client management teams to implement and track reporting deliverables for clients
Implement operational processes in BigQuery and Looker
Maintain tools used in data analysis and reporting
Train teams on using and creating reports on an ad-hoc basis

Job Benefits
Health insurance
Vision insurance
Dental insurance
Life insurance
Simple IRA matching
Paid time off

Minimum Qualifications
Experience with data modeling using LookML
Experience with big data analysis tools such as BigQuery
Experience utilizing SQL and other tools to manipulate and analyze complex data sets
Strong knowledge and understanding of data integrity and data quality
Experience implementing best practices for data governance related to standard naming conventions and data definitions
Practical knowledge of Information Security concepts and Data Loss Prevention
Experience communicating to various stakeholders utilizing visualization tools
Excellent verbal and written communication skills

Preferred Qualifications
Experience extracting and correlating healthcare data analytics",#N/A,51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2020,Unknown / Non-Applicable
"Stark Dev, LLC",#N/A,"Plano, TX",Data Engineer/Analyst / W2 /USC or GC (GC EAD) or H4 holders,"This position is open for United States Citizens or Green Card holders (GC EAD)/H4 EAD ONLY.
This is an on-site position in Plano/Dallas
CONTRACT W2
Top Skills Details
1) Experience working on a data migration project as a Data Analyst.
- This person will be working on their Permitting, Planning, and Inspection System Project. They are moving from a legacy permitting system, TRAKIT, to a completely new Salesforce application, Clariti. (TRAKIT and Clariti experience not required)
2) Experience doing data discovery, classification, verifying data, mapping rules
- On this project this team will be moving all of the historical data from the old application, TRAKIT, to the new application Clariti.
3) Proficient with SQL and writing SQL queries
\* Interpret data, analyze results using statistical techniques and provide ongoing reports
\* Develop and implement databases, data collection systems, data analytics and other strategies that optimize statistical efficiency and quality
\* Acquire data from primary or secondary data sources and maintain databases/data systems
\* Identify, analyze, and interpret trends or patterns in complex data sets
\* Filter and “clean” data by reviewing computer reports, printouts, and performance indicators to locate and correct code problems
\* Work with management to prioritize business and information needs
\* Locate and define new process improvement opportunities
Drug Test Required
false
Go To Work
false
Workplace Type
On-site
Experience Level
Expert Level
Job Type: Contract
Pay: $55.00 - $60.00 per hour
Schedule:
Monday to Friday
Application Question(s):
What is your visa status?
Work Location: In person",$57.50 /hr (est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Geopaq Logic
4.7",4.7,"San Jose, CA",Network Engineer/Data Center,"Job Title: IP Telephony Engineer
Work location: San Jose, CA (on-site)
Duration: 10+Months
Korean/English Bilingual Required
IP Telephony Engineer
This position is responsible for technical support for Communication Practice-related business products and services supported by Business Operations.
This operational position requires the Engineer to speak directly to internal and external customers about a wide variety of technical issues/requests.
Responsibilities
Responsibilities will include providing the necessary support in order to resolve the customer's issues and/or fulfill requests in a manner that meets or exceeds agreed upon Service Level Agreements (OLAs/SLAs) with internal/external customers.
Provide technical support to internal/external customers for VoIP-related incidents, requests, and inquiries relating to a variety of business products and services
Document work updates in a company-provided ticketing system until a resolution is complete and the ticket is closed with the customer
Collaborate with other analysts to determine resolutions for customer incidents and requests
Western IPT Network & System management (hardware/software)
Other duties as assigned
Knowledge/Skills
Excellent written and oral communication skills
Excellent customer service and conflict resolution skills
Working understanding of Information Technology and computing systems
Working understanding of IP networking fundamentals
Familiar with Microsoft applications such as Windows operating system, Office applications, Outlook, and SharePoint
Familiar with common ticketing systems
Demonstrated desire for self-directed education regarding IP networking and VoIP technologies
Job Types: Full-time, Contract
Ability to commute/relocate:
San Jose, CA 95110: Reliably commute or planning to relocate before starting work (Required)
Experience:
Computer networking: 1 year (Preferred)
LAN: 1 year (Preferred)
Language:
Korean (Required)
Work Location: In person","$102,582 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,2014,$1 to $5 million (USD)
"2ULaundry
3.8",3.8,"Charlotte, NC",Data Engineer,"2ULaundry and LaundroLab are on a mission to create time for the world to focus on the things that matter most - and that probably isn't laundry! We're a rapidly growing startup that is rewriting the rules around this time-consuming task by building two complementary brands. With our convenient pick-up and drop-off service and modern, inviting laundromats, we provide people with an effortless and convenient solution to laundry and dry cleaning to give them back time because as we all know, time is valuable.
The Data Engineer Role:
We are looking for a Data Engineer who will be responsible for managing and processing large amounts of data. This includes integrating data from various sources, such as internal systems and external APIs, and ensuring its quality and integrity. You would design and maintain data warehousing solutions for efficient storage and retrieval, while also optimizing data processing workflows for analysis and reporting purposes. Database management tasks, including performance tuning and security measures, would be part of your responsibilities. Additionally, you would contribute to data governance practices, documentation, and compliance. Collaboration with cross-functional teams and effective communication with stakeholders are crucial aspects of the role. Overall, your focus would be on leveraging data to drive operational efficiency and enhance customer services in the logistics and consumer services domain.
This role directly reports to our Director of Engineering.
What are the day-to-day responsibilities of the Data Engineer role?
Partner with company leaders, product managers, engineering and other data analysts to support our service delivery with diverse data and analytics initiatives.
Develop, test, and maintain robust, scalable data structures and pipelines.
Ensure systems meet business requirements and industry best practices, with an emphasis on platform scalability to support our rapid growth.
Design and implement high-performance algorithms, predictive models, and proof of concepts to improve service delivery and customer experience.
Identify opportunities for data acquisition and leverage existing data for additional business value.
Foster a data-driven culture by educating the team on data analytics best practices and data literacy.
Integrate emerging data management technologies and software engineering tools into existing structures to enhance system performance.
Build and extend our BI tools to include robust models that enable better self-service capabilities for non-technical stakeholders.
What are the requirements for the Data Engineer role?
Minimum 3 years of experience in data engineering, database architecture, and data warehousing
Proficiency in SQL and other programming languages
Experience with data processing tools and cloud platforms (Google Cloud preferred.
Strong understanding of databases, data processing, data storage, and data privacy, particularly in the context of service delivery.
Excellent analytical, problem-solving, and communication skills.
What are the perks of being the Data Engineer
Ability to get in on the ground floor and have a significant impact at one of Charlotte's fastest growing franchise companies
Opportunities to grow in the marketing field
Direct and regular access to thought leaders in the startup, franchising, and laundry industries
Benefits package that includes medical, dental, and vision insurance, a 401k, and an Employee Assistance Program
Unlimited PTO policy that our employees actually use!
Working with a supportive, driven team working to build our startup into a nationally recognized brand
Free laundry!
2ULaundry and LaundroLab are equal opportunity employers. We value diversity and strive to create an inclusive environment representative of a variety of backgrounds and experiences. Employment is decided solely on the basis of qualifications, merit, and business need.","$90,174 /yr (est.)",51 to 200 Employees,Company - Private,Personal Consumer Services,Laundry & Dry Cleaning,2015,Unknown / Non-Applicable
"Octaura LL TradingCo LLC
5.0",5.0,"New York, NY",Data Engineer,"We’re on a mission
At Octaura, we continually evolve markets to unleash value for clients. It’s in our DNA to make a difference and do things differently.

Existing workflows within our markets are painful for clients: they are outdated, overcomplicated, and time-consuming. We want to change that. Octaura fundamentally rebuilds and redefines the markets by streamlining workflows, digitizing platforms, and bringing transactions, data and analytics together for the first time.

Join our inclusive culture
At Octaura, everyone belongs.

It’s so important to us that all Octaurians are confident in knowing they have the space to use their voice and talents. We love the diversity we see in the world and we actively want our team to reflect this. We’re a values-driven company and by engaging, solving and evolving together, we create a culture that is collaborative, switched-on, and fun to be part of.

The role in a nutshell
We are looking for a Data Engineer who can steer the organization's data architecture and management strategy, playing a crucial role in consolidating disparate data sources into a single, easily accessible data pool. The right candidate can navigate the complexity of data systems, advocate for data governance, and have an inherent knack for translating raw data into actionable business intelligence.
This is an exciting time for Octaura as we are rapidly growing and are looking for energetic, collaborative and driven thinkers to join us! Please note this role is based out of our NYC office and our current structure is 4 days in the office, 1 day from home.
Core responsibilities
Design, build, and maintain scalable data pipelines, leveraging and integrating structured and unstructured data from diverse sources.
Adopt, implement, and maintain a Cloud Data Infrastructure working with services such as Snowflake, Databricks, or similar.
Normalize data and ensure that it is secure, reliable, and easily accessible for consumption and sale.
Implement strategies to manage a centralized data pool, facilitating its use in BI and other reporting tools.
Uphold data integrity and enforce data security policies in alignment with compliance and privacy standards.
Collaborate with all Product, Engineering, Sales, Marketing and Executive teams to deliver customized data reports for internal and external stakeholders.
Feed processed data back into the trading platform to enhance our product and provide valuable insights for our customers' investment decisions.
Stay informed about emerging trends and technologies in the field of Data Engineering, suggesting and implementing new tools and methodologies as appropriate.
Desired qualifications
Bachelor's or Master's degree in Computer Science, Information Systems, or a related field.
Minimum of 5 years of experience as a Data Engineer or in a similar role, preferably in financial services or FinTech.
Proficiency in NoSQL databases, MongoDB, and other data warehousing technologies.
Strong programming skills in Python, with demonstrated ability to manipulate data and build data solutions.
Extensive experience with ETL tools and processes, data modelling, and data architecture.
Advanced proficiency with BI tools like Tableau, with the ability to generate complex, large-scale data reports.
Knowledge of cloud-based data solutions (AWS) and big data technologies (Hadoop, Spark) is a plus.
Strong understanding of data privacy and security principles, along with experience in relevant data protection regulations.
Exceptional problem-solving skills and the ability to work both independently and collaboratively.
Excellent communication skills, with the ability to translate complex data insights into understandable and actionable information.
Experience building and adopting best practices in a start-up environment
The base pay range for this position in New York is $140,000 - $170,000 annually. Pay may vary depending on job-related knowledge, skills, and experience. Equity and year-end bonus may be provided as part of the compensation package, in addition to a full range of medical, financial, and other benefits, dependent on the position offered. Applicants should apply via Octaura's internal or external careers site.

Octaura Work Perks
At Octaura, our people are our most valuable asset, and we are pleased to offer the following benefits to all full-time employees:
Competitive compensation and equity
Unlimited Paid Time Off
Competitive Parental Leave
Daily breakfast, coffee and snacks in the office
90% company-paid healthcare
Onsite gym & discounted membership

We’re committed to equal opportunity employment
Octaura is committed to a diverse and inclusive workplace. We are an equal opportunity employer and do not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status.","$155,000 /yr (est.)",1 to 50 Employees,Company - Private,Financial Services,Financial Transaction Processing,2022,Unknown / Non-Applicable
"Insight Enterprises, Inc.
3.9",3.9,United States,Azure Data Factory Engineer,"Microsoft Azure Data Factory Engineer
Our client is looking for a Microsoft Azure Data Engineer to add to their team in a 6 month contract to hire opportunity.

Please note: W2 only for this role due to the nature of the role converting into a perminant postion. ($80 - 85/hr) Fully remote!

Title: Microsoft Azure Data Engineer
Job Location: Remote

Responsibilities:
Design, develop, and maintain data pipelines using Azure Data Factory to ingest, transform, and load large-scale data from various sources into data lakes or data warehouses.

Qualifications:
Candidate must have:
Strong experience in data engineering, including designing, developing, and maintaining data pipelines using Azure Data Factory
Proficient in SQL and experience with data modeling concepts.
Good understanding of data warehousing concepts including Azure Synapse Analytics, Azure SQL Data Warehouse
Knowledge in data modeling and database management
Experience preparing data for Data Science and Machine Learning.
Experience preparing data for use in Azure Machine Learning / Azure Databricks
Good communication skills, both oral & written.

Nice to Have:
Advanced working knowledge or Python and PySpark
Relevant certifications in Azure Data Factory, Azure Databricks or other relevant data engineering technologies are a plus

Insight helps Fortune 500 and mid-tier enterprises transform their data centers to meet tomorrow’s IT challenges today. From recommending ways to bridge the gap between IT and user expectations, to implementing advanced technologies like ITaaS and cloud, to providing robust managed services and technical support, we make IT relevant – standardizing, optimizing, and managing how business gets done.
]]>",$82.50 /hr (est.),10000+ Employees,Company - Public,Information Technology,Enterprise Software & Network Solutions,1988,$25 to $100 million (USD)
"Zillion Technologies
3.8",3.8,"Saint Louis, MO",Data Engineer III – Hybrid,"Job Location: Data Engineer III – Hybrid
Location: St. Louis, MO
Position Description:
· Create and maintain optimal data pipeline architecture,
· Assemble large, complex data sets that meet functional / non-functional business requirements.
· Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
· Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies. Knowledge on Teradata and/or Databricks must.
· Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
· Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
· Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
· Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
· Work with data and analytics experts to strive for greater functionality in our data systems.
Key skill set:
· Teradata SQL, Fast Export and other Teradata native tools. Teradata Vantage knowledge is plus.
· Informatica Power Center and Informatica Cloud Data Integration
· Databricks
· AWS cloud basic knowledge, S3, Lamba etc.
· Linux
· Jenkins (CI/CD)
· GitHub
QUALIFICATIONS (education, experience special skills):
· Bachelor of Science degree in a business or technical field from an accredited college/university required.
· 5+ years of direct experience working with Data Modeling and Data Solution Development.
· Experience developing processes to manage data model development, principles, and standards.
· Experience working on Enterprise Data Warehouse initiatives.
· Proficient in the use of collaboration tools such as Jira, Confluence, Excel, and SharePoint.
· Experience in utility/energy Corporation preferred.
· Successful candidate will demonstrate: good analytical, communication, leadership and human relations skills required.
Job Types: Full-time, Contract
Pay: $70.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
St. Louis, MO 63103: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Work Authorization?
Experience:
Data Modeling and Data Solution Development: 5 years (Required)
Teradata, Informatica, and AWS: 5 years (Required)
Enterprise Data warehouse: 5 years (Required)
Work Location: In person",$72.50 /hr (est.),201 to 500 Employees,Company - Private,Management & Consulting,Business Consulting,#N/A,$5 to $25 million (USD)
GrowthLoop,#N/A,"New York, NY",Senior Data Engineer,"GrowthLoop
GrowthLoop is a high-growth technology startup offering a customer segmentation platform on Snowflake and BigQuery that is changing the way businesses acquire, retain, and win back their customers. We are a completely bootstrapped and profitable startup with clients like Indeed, Google, and Uber.
Our Mission
Unlock the value of data in the cloud data warehouse to make it drive massive, real-world impact.
How we work
A collaborative team that strongly believes in taking the learner's mindset
We encourage exploration of new technologies
We believe in empowering every person on our team to do their best work
Dedication to building a product people love
We build slowly but surely for the long term. We are transparent about the challenges of building a great company. We are humble in facing those challenges. But, we know that if we keep improving every day the GrowthLoop spins
About You
We are looking for an outstanding Senior Data Engineer. This is a unique role to work directly with our customers, along with GrowthLoop Platform Engineering, Data Science, and DevOps teams to help us architect and build both new product features along with tools to enable more efficient data modeling and engineering workflows. You have experience with various open source and third-party tools (DBT, Matillion, SQLAlchemy) and have a deep understanding of tool-independent principles for good data modeling. Our customers are some of the most advanced companies in the world, their first-party data lives in Google BigQuery, Snowflake, and Amazon Redshift. You will help build new tools and services to support the data modeling needs of our customers and the data needs of new product features. High-energy self-starter with experience and passion for data and big data scale problems.
Responsibilities
Architect and design scalable data platforms that work hand in hand with the product
Support and Develop SQL data pipelines using tools like Airflow, DBT, and SQLAlchemy
Snowflake, BigQuery, Redshift experience
Optimize queries and incremental SQL pipelines for cost efficiency and speed
Work hand in hand with customers to understand their needs, architect and design thoughtful and scalable data models, ready for BI and machine learning use cases
Work on data services like Identity Resolution and Campaign performance Evaluation
Support a cross-functional team, including analytics, engineering, and product in developing all of the above in complex hybrid cloud environments
Experience with 3rd party and open-source data import tools like Singer, Matillion, Fivetran
Qualifications
3+ years work experience in Data Modeling, Data Engineering, or equivalent
Expertise in SQL, scalable data pipeline development, column-oriented databases (e.g. BigQuery and Redshift), and analytical data modeling
Hands-on experience with all aspects of design, development, and maintenance of analytical data warehouses/lakes using both distributed file systems and cloud-scale data processing systems
Experience building data integration frameworks to accelerate time to value on common integration challenges
Strong analytical and problem-solving skills
Insatiable intellectual curiosity to learn new (cloud) technologies with an ability to self-manage (we don't micromanage here)
We are a collaborative team that strongly believes in taking the learner's mindset to everything we do. This role will have the opportunity to learn how to apply machine learning / artificial intelligence models to some of the most important problems businesses face.
By joining our team, we hope you will change the trajectory of your professional career and that of our business.
The estimated base salary for this role is $110,000-$130,000, with the option for additional variable based compensation. Final base salary decisions will be based on a variety of non-discriminatory factors, such as the individual's performance, experience and qualifications.
Benefits
Meritocracy
Spot bonuses for major milestones
Grow into leadership roles as we scale this rocket-ship
Startup equity for star performers
Generous Time-off
Take as much vacation as you like!
Flexible remote work policies
Platinum Benefits ‍
Free Platinum Health Insurance with Aetna
401(k) Program with Generous Company Match
Flexible PTO and WFH Policies
Learn and Grow ✍️
Education Stipend towards your professional development
Work directly with Founders (ex-Googlers)
Learners' mindset culture of 'Friendly Geniuses'","$120,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Infinity Quest
3.9",3.9,"Cleveland, OH",Data Center Engineer,"Required Skillsets:
1. Datacenter Management
2. Hardware Rack & Stack Cabling
3. Knowledge and operational of Diesel Generators, Cooling & Chillers
4. Inventory Asset management
5. DC Monitoring DC availability
6. Handling of CISCO UCS HW and VX RAIL, POWER FLEX
Activities:
1. Operate and Manage both routine and emergency service on a variety of state-of-the-art critical systems such as:
a. Medium voltage switchgear,
b. diesel generators,
c. UPS systems
d. Power distribution equipment, chillers, cooling towers, computer room air handlers,
e. Fire detection / suppression; building monitoring systems (BMS), building Automated Systems (BAS) ; etc
2. Supervise the on-site management of sub-contractors and vendors, ensuring that all work is performed according to established practices and procedures.
3. Manage local client relationship and act as the point of contact for the company at this site.
4. Establish performance benchmarks, conduct analyses and prepare reports on all aspects of the critical facility operations and maintenance.
5. Work with IT managers and other business leaders to coordinate projects, manage capacity and optimize plant safety, performance, reliability and efficiency.
6. Create, utilize and administer MOPs , SOPs, and Preventative Maintenance Procedures for all work on critical data center facility equipment.
7. Schedule work activities, within specified change control / management protocol.
8. Maintain a constant state of readiness in support of the mission goal of 99.999% uptime
9. Racking, Stacking of Infrastructure
a. Cabling Management
b. Patching, Network port swapping
c. Hardware reboots
d. Vendor coordination
e. Project Coordination
· Inventory asset management, Physical access management
Monitoring of Datacenter availability & update respective teams accordingly
Job Type: Contract
Salary: From $80.00 per year
Schedule:
8 hour shift
Ability to commute/relocate:
Cleveland, OH: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data center: 10 years (Preferred)
CISCO UCS: 10 years (Preferred)
MOPS: 10 years (Preferred)
SOP: 10 years (Preferred)
Cabling Management: 10 years (Preferred)
Network protocols: 10 years (Preferred)
Hardware reb: 10 years (Preferred)
VENDOR COORDINATION: 10 years (Preferred)
Project coordination: 10 years (Preferred)
Security clearance:
Confidential (Preferred)
Speak with the employer
+91 8838059965",#N/A,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,Unknown / Non-Applicable
"Evergreen Technologies, LLC.
5.0",5.0,"Madison, NJ",Data Engineer,"Position: Data Engineer
Location: Madison, NJ
Duration: 9 Months (possible extension)
Responsibilities:
The Data Platform team in our Companys Animal Health IT (MAHI-IT) designs and implements end to end data solutions to support customer facing applications in animal traceability, monitoring, well-being, and more.
We seek a data engineer to help the team setting up, maintaining, optimizing, and scaling data pipelines from multiple sources and across different functional teams in a cloud environment.
Assist in developing best practices for deploying, monitoring, and scaling data pipelines in the cloud Identify requirements for ingestion, transformation, and storage of data Design and implement optimal and scalable data pipelines
Use cloud tools to integrate data from multiple data sources into the data lake and design and implement ways to expose it
Identify opportunities for automation and optimization of data pipelines and re-design of data architecture and infrastructure for great scalability and optimal delivery
Implement cloud/ data infrastructure required to extract, transform, and load data from multiple sources
Identify required security and governance procedures to keep the data safe in a cloud environment
Assist in developing and executing testing plans to help with QA efforts.
Qualifications Requirements/ Qualification:
Bachelors degree in Data Engineering, Computer Science, or related field.
Experience designing and implementing data engineering pipelines.
Advanced knowledge in Python and PySpark.
Working knowledge of one or more SQL languages.
3+ years of hands-on experience with developing data warehouse solutions and data products.
1+ year of hands-on experience developing a distributed data processing platform with Hadoop, Hive, Spark, Airflow, Kafka, etc.
3+ years of hands-on experience in modeling and designing data schemas.
Advanced experience with programming languages: Python, Pyspark, Scala, etc. Knowledge of scripting languages: Perl, Shell, etc.
Practice working with, processing, and leading large data sets.
Experience with cloud tools for ingesting and processing data.
Preferred Experience And Skills:
Experience with AWS tools big data platforms S3, EMR, EKS, Lambda, etc.
Experience with data ingestion and transformation tools like Streamsets and Databricks
Experience working with DevOps teams
Experience with container technologies such as docker and Kubernetes
Experience with data warehousing tools like Snowflake and Redshift","$96,690 /yr (est.)",201 to 500 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
i-Link Solutions Inc,#N/A,"Boston, MA",Data Engineer,"i-Link Solutions is seeking a Data Engineer to join our team in Hanscom, MA. The Data Engineer will be responsible for designing, developing, and maintaining the data infrastructure that supports the Kessel Run program. The ideal candidate will have experience with AWS, open-source tools, and data engineering best practices
Responsibilities
Design, develop, and maintain the data infrastructure that supports the Kessel Run program
Work with stakeholders to understand business requirements and translate them into technical solutions
Build and maintain data pipelines that ingest, transform, and load data into data warehouses and data lakes
Develop and deploy data models and algorithms to support data-driven decision making
Work with data scientists and analysts to develop and deploy data products
Ensure the security and compliance of data assets
Qualifications
Bachelor's degree in Computer Science, Information Technology, or a related field
10+ years of experience in data engineering
Experience with AWS, open-source tools, and data engineering best practices
Strong programming skills in Python, Java, Scala, or SQL
Experience with data modeling and data warehousing
Experience with data visualization and reporting
Strong analytical and problem-solving skills
Excellent communication and teamwork skills
US Citizenship
Secret Clearance
Kessel Run Experience Desirable
Job Types: Full-time, Permanent
Pay: $89,571.07 - $140,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Referral program
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Boston, MA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineering: 10 years (Required)
AWS: 3 years (Required)
Data warehouse: 4 years (Required)
Work Location: In person","$114,786 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Jetty
4.2",4.2,United States,Data & Analytics Engineer,"Welcome to Jetty, the financial services platform on a mission to make renting a home more affordable and flexible. We've built multiple financial products that benefit both renters and property managers - and we're just getting started.

We are growing our data organization and looking to hire a Data and Analytics Engineer. As a Data and Analytics Engineer, your goal is to cultivate a data-informed culture and create insights that will be leveraged across the entire organization. You have experience executing at a high level, solving complex problems, and delivering solutions with real business impact - and you're excited by the opportunity to apply those principles to a new, best in class function.

Role & Responsibilities
Build / Support our modern data stack (Snowflake / Fivetran / DBT / Tableau)
Be an enthusiastic evangelist of our modern data stack (Fivetran / DBT / Snowflake / Tableau)
Develop analytics data models using SQL
Document our data models in a user friendly way for our business stakeholders
Implement the Five Pillars of Data Observability
Write ELT code using modern software engineering practices (Git, automated testing and deployments)
Build and maintain data pipelines to support various business processes and reporting (Fivetran / AWS Lambdas)
Partner with the Product Engineering team to ensure we are capturing the data we need from our applications for analytics and to iterate on our development practices for the data analytics team.
Be the resident resource on building standard reports and BI dashboards
Experience & Qualifications
3-5 years of experience working in a data / analytics engineering role
High proficiency in Snowflake / Fivetran / dbt / Tableau
High proficiency in SQL and Python
Ability to collect, interpret, and synthesize inputs from various parts of the business into data model requirements
Ability to simplify without being simplistic - ability to communicate complex topics and actionable insights in a compelling way that can be understood by a variety of audiences
Inherent curiosity and analytical follow-through — you can't help but ask ""why?"" and love using data and logic to explore potential solutions
Ability to balance ""Rigor"" and ""Scrappiness"" — you know the difference between 80/20 and giving something 110%; as well as when each is appropriate.
Deep understanding of the first and second order effects of reporting — you know the power of presenting the right data to the right people at the right time
Experience in a data/analytics function at a high-growth startup managing multiple stakeholders and delivering actionable insights
About Jetty
At Jetty, we know renting a home can be a financial challenge. That's why we're on a mission to make renting accessible to everyone. Jetty offers four financial products designed to help our members every step of the renting process: Jetty Deposit, a low-cost security deposit product that dramatically reduces move-in costs; Jetty Rent, a flexible rent payment program to eliminate pricey late rent fees; Jetty Credit, a credit building service that helps renters build credit just by paying rent; and Jetty Protect, an affordable renters insurance product that provides comprehensive coverage in just a few clicks.

Jetty has raised multiple rounds of venture capital from investors including Khosla Ventures, Ribbit Capital, Citi, Valar, and strategic investors. We've built a highly collaborative team working remotely around the country, and we believe in finding the best talent—regardless of where they live. To learn more about life at Jetty, visit jetty.com/careers.

Jetty is firmly committed to building a team as diverse as our Members. We are proud to provide equal employment opportunities for all candidates regardless of race, ancestry, citizenship, sex, gender identity or expression, religion, sexual orientation, marital status, age, disability, or veteran status.

Benefits & Perks
Health (with HSA and FSA options), dental, and vision insurance through Aetna & MetLife
401(k) retirement savings program
Optional life and disability coverage
20 days of PTO + 12 holidays, ""Jetty Winter Break,"" and flexible sick days
Generous parental leave policy
Flexible remote work in any US location (keeping east coast hours)
Stipends to cover WFH set-up, childcare, phone/internet bill, and optional co-working space",#N/A,51 to 200 Employees,Company - Private,Insurance,Insurance Agencies & Brokerages,2015,Unknown / Non-Applicable
"Division of Information Technology - NYC Department of Health and Mental Hygiene
3.2",3.2,"Long Island City, NY",Data Engineer,"The New York City Department of Health and Mental Hygiene (DOHMH) is the nation's leading public health agency protecting and promoting health of all New Yorkers. Our 7,000-plus team members bring an extraordinary array of languages, cultures, and experiences to bear on the work of public health. Our diversity fuels creativity because all perspectives are heard and valued. DOHMH aims to improve the health outcomes of all New Yorkers by centering persistent racial inequities and promotion of social justice at the core of its work.
The Division of Information Technology aims to align technology solutions with the DOHMH mission by prioritizing resource use and deploying innovations that facilitate the agency’s day-to-day activities and enhance staff productivity and efficiency. Our goal is to provide users with a reliable, stable, and safe computing environment, through the collaboration of the Bureau of Technology Strategy & Project Management provides business analysis and IT project management services to define and deliver IT solutions that meet all program needs.
**This a grant-funded W-2 position with full employment benefits that expires 6/30/2025 (possibility for extension) hired through Public Health Solutions and will be assigned to NYC DOHMH. Only those with authorization to work in the U.S. without sponsorship should apply. Remote or hybrid option available. Professional references are required. **
The agency is seeking a Data Engineer (Azure, SQL, Python) to help in the infrastructure, process, and procedures to create “data lakes” in the cloud for large amounts of public health data including but not limited to syndromic surveillance, electronic laboratory and case reporting, notifiable diseases, vital records, mental health, maternal mortality, chronic diseases, as well as agency operational data such as finance, human resources, emergency operations, and information technology data. This data will be made available to analysts across DOHMH to consume and process in analytic platforms such as R, Python, SAS and visualize them in tools like Tableau, Power BI, Data Wrapper. In addition, the data can be processed with machine learning algorithms to help make more informed policy and operational decisions.
Job Duties:
Engineer data pipelines using a variety of technologies including Azure Synapse, SQL, SSIS, Python to extract, transform and load data.
Load and transform data from external sources and partners in a variety of different structured and unstructured file formats as well as internal on-premises data and applications primarily in SQL databases and CSV file formats.
Process public health data requests following applicable procedures and document changes in an electronic repository.
Apply technical knowledge to architect solutions that meet business needs, create Data Platform, AA/AI roadmaps, and ensure long term technical viability of new deployments, infusing key analytics and AI technologies where appropriate (e.g., Azure ML, ML Server, BOT framework, Cognitive Services, Big Data, Data Lake, Azure Databricks, etc.).
Ensure that solution exhibits high levels of performance, security, scalability, maintainability, appropriate reusability, and reliability upon deployment
Develop deep relationships with key customer IT decision makers, who drive long-term cloud adoption within their company to enable them to be cloud advocates.
Assess the Customers' knowledge of Azure platform and overall cloud readiness to support customers through a structured learning plan and ensure its delivery through partners.
For senior data engineers, supervisor or mentor junior data engineer and analysts.
Qualifications and Requirements:
5+years of experience with Azure data platform including Azure SQL, Azure Data Factory, Azure Databricks, Azure SQL Data Warehouse (Synapse Analytics), Azure Data Lake Storage, Azure Cosmos DB, SQL Server, SSIS, SSRS, etc.
Strong Object relational mapping experience with UML modeling and OO modeling.
Experience with T-SQL, SSIS, Power BI and Azure Analysis Services
Experience with developing data pipelines using python.
Strong problem solving and analytical skills.
Passion for public health and solving problems and creating new insights through data.
Experience working with Healthcare data and proven track of implementation experience in EDI data formats.
Experience in job roles involving metadata management, relational dimensional modeling and big data solution approaches with native Azure Data Platform tools or 1st party services.
Excellent communication, presentation skills with the right attitude towards problem solving in diverse teams.
Strong aptitude and technical skills with conceptual strength in logical solutions driven towards balanced optimal considerations.
Solid knowledge in SQL, security standards, BI tools, ETL tools and Microsoft Azure specific technologies.
Certifications in Azure, data modeling and metadata management.
Ability to use BI tools like Power BI to represent insights and other presentation techniques.
Job Type: Full-time
Pay: $100,000.00 - $130,000.00 per year
Benefits:
Health insurance
Schedule:
8 hour shift
Work Location: In person","$115,000 /yr (est.)",Unknown,Government,Government & Public Administration,State & Regional Agencies,#N/A,Unknown / Non-Applicable
"Dupaco Community Credit Union
4.6",4.6,"Dubuque, IA",Data Engineer,"Are you looking to use your technical expertise to uncover the value in data? Shift your career over to the Data Engineer role at Dupaco! As a Data Engineer, you'll collaborate with various departments to accomplish strategic goals by creating automated processes and pipelines by extracting data from source systems and migrating it to our enterprise data warehouse or other end points!
As an employee at Dupaco, you’ll be part of an inclusive team that believes that by working together we allow our members to build a life worth loving. Pair that with the training opportunities, career growth potential, attractive compensation, amazing benefit package, and the ability to have a voice in the success of the organization, you’ll have a career worth loving!
You’ll be:
Extracting data from source systems, translating the raw data into analysis-ready datasets then migrating it to our enterprise data warehouse or other end points
Developing automated data pipelines that take raw data from disparate sources and model it into formats that are useful for operations and reporting
Utilizing a variety of programing languages and tools (i.e. Java, SQL, Talend, Python) to marry systems together
Researching opportunities for data acquisition and new uses for existing data
Implement data processing flows that adhere to data security requirements
Recommending ways to improve data reliability, efficiency and quality
Constructing, installing, testing and maintaining highly scalable data management systems
Collaborating with Data Team to accomplish various data sprint projects
Explaining complex technical concepts to other stakeholders in a way that is easy for them to understand
Providing quality service to members, potential members and coworkers
Demonstrating teamwork and professionalism in all interactions with coworkers
Performing other duties as assigned
You’ll need:
An excitement for working with large volumes of data
Bachelor’s degree in Computer Science or Computer Information Systems or equivalent experience
Proficient experience with programming languages (i.e. Java, Python, C#, etc.)
Familiarity with ETL (Extract, Transform, Load) tools (i.e. Talend, Informatica, SSIS, or DataStage), strongly preferred
Experience working with SQL queries
Ability to demonstrate complex problem solving and strong decision-making skills
Strong interpersonal and communication (verbal & written) skills
To be self-motivated, resourceful and well organized with the ability to prioritize work assignments efficiently
Ability to accurately handle large volumes of details and effectively manage multiple projects simultaneously
Ability to promote a professional image of the credit union at all times
This role does not allow for remote work outside of Dupaco's geographic branch network (Iowa, Wisconsin, and Illinois) and the selected candidate must reside or relocate to this area.
Qualifications
Skills
Behaviors
:
Motivations
Required
Ability to Make an Impact: Inspired to perform well by the ability to contribute to the success of a project or the organization
:
Education
Required
Bachelors or better in Computer Science or related field.
Experience
Required
Experience working with SQL queries
Proficient experience working with programming languages (i.e. Java, Python, C#, etc.)
Preferred
Familiarity with ETL (Extract, Transform, Load) tools (i.e. Talend, Informatica, SSIS, or DataStage), strongly preferred
Licenses & Certifications
Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities
The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information. 41 CFR 60-1.35(c)","$87,014 /yr (est.)",501 to 1000 Employees,Nonprofit Organization,Financial Services,Banking & Lending,1948,$1 to $5 billion (USD)
"Intelliswift Software, Inc.
4.2",4.2,"Menlo Park, CA",Data Engineer III,"Job Title: Data Engineer III
Duration: Longterm Contract
Location: Menlo Park, CA
Pay Range: $80-$90/hr
Intelliswift Software Inc. conceptualizes, builds, and supports the world's most amazing technology products and solutions. Our team of rich experts from diverse backgrounds contributes to making Intelliswift one of the most reliable partners in IT and Talent solutions. We specialize in delivering world-class Digital Product Engineering, Data Management and Analytics, and Staffing Solutions services to Fortune companies, SMBs, ISVs, and fast-growing startups.

Job Description: Onsite at Menlo Park location only.
Summary:
The main function of the Data Engineer is to develop, evaluate, test and maintain architectures and data solutions within our organization. The typical Data Engineer executes plans, policies, and practices that control, protect, deliver, and enhance the value of the organization’s data assets.

Job Responsibilities:
Design, construct, install, test and maintain highly scalable data management systems.
Ensure systems meet business requirements and industry practices.
Design, implement, automate and maintain large scale enterprise data ETL processes.
Build high-performance algorithms, prototypes, predictive models and proof of concepts.

Skills:
Ability to work as part of a team, as well as work independently or with minimal direction.
Excellent written, presentation, and verbal communication skills.
Collaborate with data architects, modelers and IT team members on project goals.
Strong PC skills including knowledge of Microsoft SharePoint.

Education/Experience:
Bachelor's degree in a technical field such as computer science, computer engineering or related field required.

Process certification, such as, Six Sigma, CBPP, BPM, ISO 20000, ITIL, CMMI.",$85.00 /hr (est.),1001 to 5000 Employees,Company - Private,Information Technology,Software Development,2001,$100 to $500 million (USD)
"LTIMindtree
3.8",3.8,"Cincinnati, OH",GCP Data Engineer - Contractor,"Job Details:

GCP Data Architecture and designing streaming & Batch pipelines.
Implementing Big Data solutions leveraging GCP cloud data technologies with extensive experience in ingestion, processing, and transformation.
Proficient in Python, PySpark, relational and NoSQL databases and GCP data technologies such BigQuery, Dataproc, Dataflow, Data Fusion, Cloud Composer, PubSub, DataPrep, Dataplex
Cloud databases: Spanner, Cloud SQL, Memory store, BigQuery.
Looker studio & Operations suite (Cloud monitoring and Logging).
Excellent relationship management skills and leadership skills.

Good to have:
Experience on open-source or commercial Modern Data Stack tools such as Airbyte, Fivetran, dbt, Monte Carlo, CDAP, etc. is an ad added advantage
Flask FastAPI Development
Google certified Data Engineer
Role Description:
Hands-on experience architecting, designing and implementing Big Data solutions leveraging GCP cloud data technologies; extensive experience in the area of ingestion, processing and transformation
Proficient in Python, PySpark, relational and NoSQL databases and GCP data technologies such BigQuery, Dataproc, Dataflow, Data Fusion, Cloud Composer, PubSub, DataPrep, Dataplex
Experience on open-source or commercial Modern Data Stack tools such as Airbyte, Fivetran, dbt, Monte Carlo, CDAP, etc. is an ad added advantage.
Good Analytics skill is needed on issue identification and resolution.
Experience in distributed data processing, performance tuning
Experience in maintenance & enhancement projects.
Complete ownership of the tasks and deliverable
Good communication skills.
Flexibility to support customer across the Geos and time zones.
Ensures consistency of process and usage, and champions best practices in data management Oversees data accuracy processes, goals and assessment.
Ensures resolution of data conflicts between systems and within systems’ data universe.
Works with internal stakeholders to develop strategies for leveraging data to gain a deeper insight into IIE’s business, impact on international education, and IIE’s story.
Ensures rigorous adherence to IIE policies regarding PII protection and data security. Oversees data expiry practices and establishes and implements processes for data archival and expiry.
Manages and develops the staffing of the Data & Reporting unit; champions team collaboration; monitors and develops career paths within the unit. Make recommendations concerning employment, termination, performance evaluations, salary actions, and other personnel actions.
Responsible for user experience with data and reporting tools.
Member of the Knowledge Management Cabinet.
Excellent relationship management skills
Familiarity working and collaborating with teams from various geographies culture and time zones","$92,426 /yr (est.)",10000+ Employees,Company - Private,Information Technology,Information Technology Support Services,1997,Unknown / Non-Applicable
"Mashvisor Inc.
3.7",3.7,Remote,Data Science Engineer,"Build the solution that transforms the real estate industry!
We are looking for a Data Science Engineer superhero with a sixth sense for data who can ignite our day-to-day activities with their creativity.
Want to infuse a $30B+ sector of the insurance and real estate industry with predictive analytics and a tech-forward customer experience? Looking for a fully remote startup culture supported by a profitable business model? Join Mashvisor and help us build an entirely new type of real estate model.

Our Values

Customer Obsessed – We always put our customers first.
Solution Driven – We solve problems that other people are afraid to.
Product led: We are always one step ahead of our customer's needs and create / add features they love every time
One Team – We believe inclusion and teamwork produce the best results.
Open and Direct – We communicate with honesty and respect to our colleagues, customers, and partners.

What You’ll Do
Designing, developing, and researching Machine Learning systems, models, and schemes
Studying, transforming, and converting data science prototypes
Searching and selecting appropriate data sets
Performing statistical analysis and using results to improve models
Training and retraining ML systems and models as needed
Identifying differences in data distribution that could affect model performance in real-world situations
Visualizing data for deeper insights
Analyzing the use cases of ML algorithms and ranking them by their success probability
Understanding when your findings can be applied to business decisions
Enriching existing ML frameworks and libraries
Verifying data quality and/or ensuring it via data cleaning

What You’ll Need
BS or Masters degree in Mathematics, Statistics, Economics, Data Science or another quantitative field
3+ years of hands-on experience utilizing data science to manage, enhance and develop models and deploy solutions to solve complex business problems
Expertise in SQL and programming in SQL, R, Python, C++, Java, and beneficial to know Lisp and Prolog
Strong organizational, interpersonal, and communication skills (both written and verbal)
A bias towards solving problems from a customer-centric lens and an intuitive sense for how the work aligns closely with business objectives
Solid experience with managing databases and datasets and structuring and optimizing the framework
A thorough understanding of SQL databases
Bonus: background in US real estate data, insurance or financial markets analysis
The ideal candidate will be a creative problem solver with an excellent work history on data analytics projects.

We want the work you do here to be the best work of your life.
Compensation: We offer a great salary with a yearly bonus based on performance.
Attitude: Work with a Can-Do team across the world.
Freedom: Work anywhere, anytime.
Time Off: Yearly vacations and sick leaves.
Responsibility: Ability to excel in a fully remote work environment.
Are you the one?",#N/A,1 to 50 Employees,Company - Private,#N/A,#N/A,2015,$1 to $5 million (USD)
Zllius Inc.,#N/A,"North Chicago, IL",Data Engineer,"Hi,
Greetings from Zllius Inc!
We have an urgent opening for the role Data Engineer. If interested, revert me back with your Updated resume, along with contact details, work authorization.
Job Role: Data Engineer
Duration: Long-Term
Position Type: W2/ 1099
Visa: Any
Job Description:
Technical/Functional Skills
Successful candidates will have:
Hands on experience in Dremio and Scripting
Hands on Scripting experience in Python and Flask
Hands on experience in Integrating data from multiple source systems (PostgreSQL and CSV)
Experience with Apache Superset
Ability to write complex SQL queries
Experience in building data pipelines using PostgreSQL, Oracle database, Dremio, AWS Open Search
Experience with Docker Containerization of Code
Work as a part of a scrum team in Agile methodology
ETL, Data Pipelines, PostgreSQL, SQL Queries, Scripting
Thanks & Regards
Zllius Inc.
Job Types: Full-time, Contract
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
North Chicago, IL 60064: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person","$100,218 /yr (est.)",Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Metropolitan Washington Airports Authority
2.9",2.9,United States,Network Engineer Data I,"Compensation Grade:
S22
Salary Range:
$109,695.00-$159,058.00
Opening Date:
June 2, 2023
Closing Date:
June 17, 2023
Please Note: All job announcements close at 11:59 p.m. of the day before the posted closing date.
As a Network Engineer Data I, you will assist with all functions related to networked systems. You will perform maintenance testing, operational integrity assessments, and management of networked systems. Your work will directly impact the design, implementation, and maintenance of network and telecommunications infrastructure to effectively support the Airports Authority’s activities.

Network Engineer Data I

Serves in the Technology Operations and Support Department of the Office of Technology. Position will be located at either Ronald Reagan Washington National Airport (DCA) or Washington Dulles International Airport (IAD).

Assists with the planning, designing, developing, configuring, analyzing, implementing, installing, and integrating of networked systems transmitting information, with an emphasis on data and switch networks. Assists with testing, maintaining operational integrity, performing quality assurance, and managing networked systems. Performs related functions.
GENERAL RESPONSIBILITIES
Oversees assigned networked systems, including the planning, analysis, design, development, modification, configuration, installation, integration, maintenance testing, operational integrity assessment, and backup for the networked systems.
Installs, configures, and troubleshoots network equipment, such as routers, switches, firewalls, load balancers, cabling systems, modems, multiplexers, and concentrators.
Identifies, analyzes, and corrects complex network related problems, to include security measures.
Develops and implements configuration management plans.
Develops and maintains local procedures for network, infrastructure, system operations, and product assembly and installation.
Prepares recommendations, justifications, and specifications for Local Area, Wide Area, and Wireless Local Area Networks (LANs, WANs, and WLANs); Virtual Private Networks (VPNs) and Virtual Machines (VMs) or Networks; and associated Voice over Internet Protocol (VoIP) equipment.
Creates and maintains documentation relating to network configuration, network mapping, processes, and service records.
Performs other duties as assigned.
QUALIFICATIONS
Seven years of progressively responsible experience in implementing and managing hardware and software and creating operating procedures for enterprise-wide network systems.
Knowledge of and skill in working on enterprise-wide LANs, WANs, WLANs, VPNs, VMs, and virtual networks.
Knowledge of and skill in working on Cisco devices.
Knowledge of and skill in working on current network architect protocols and standards.
Knowledge of and skill in working on current network technologies.
Knowledge of and ability to compare wired and wireless systems to determine the best network solution.
Knowledge of and skill in setting up, troubleshooting, and maintaining wireless networks.
Ability to make detailed analyses of data and information and make recommendations.
Ability to speak and write effectively, with emphasis on communicating technical issues to nontechnical audiences.
PREFERRED QUALIFICATIONS
Certified as a Cisco Certified Network Associate (CCNA), Cisco Certified Internetwork Expert (CCIE), Cisco Certified Design Professional (CCDP), or Customer Voice Portal (CVP) Technician by Cisco.
Experience with Data Center Consolidation, penetration testing, ethical hacking, and performing security assessments.
EDUCATION
A Bachelor’s Degree in Computer Science, Information Technology, or related field.
CERTIFICATIONS AND LICENSES REQUIRED
A state driver’s license in good standing.
NECESSARY SPECIAL FACTORS
Work is typically reviewed in progress and upon completion for quantity, quality, timeliness, teamwork, customer service, and other factors.
May climb or crawl, or work in cramped or awkward positions.
May be subject to night/weekend work, as well as standard on-call rotation.
May be subject to hold over and recall on a 24-hour basis for information technology (IT) emergencies.
A background security investigation will be required for all new hires.
Metropolitan Washington Airports Authority is an Equal Opportunity Employer.| Follow us on Twitter @MWAAcareers.","$134,377 /yr (est.)",1001 to 5000 Employees,Government,Government & Public Administration,State & Regional Agencies,1987,$500 million to $1 billion (USD)
"Optimal Inc.
3.6",3.6,"Dearborn, MI",Data Engineer - Google cloud platform,"Position Description:
As a Data Engineer you will be responsible for providing data support for enterprise data management tasks, standardization, enrichment, mastering and assembly of data products for downstream applications in Google Cloud Platform. Provide visibility to Data Quality issues and work with the business owners to fix the issues. Implement an Enterprise Data Governance model and actively promote the concept of data standardization, integration, fusion, and quality. Support the data requirements of the different functional teams like MS&S, PD, Quality, etc. and all the regional KPI / Metrics initiatives. Continuously increase Data Coverage by working closely with stakeholders and Data Scientists, understanding, and evaluating their data requirements to create meaningful, organized, and structured ""information""
Skills Required:
Candidates should have experience with using data analysis tools such as: Hive, PySpark, SQL, Python and ETL tools.
Experience of working within a complex business environment, including at least a year in a single function, with deep understanding of the information constructs of that business.
Demonstrated experience and expertise in conceptual thinking of how to apply information solutions to a business challenge.
Experience of applying problem solving capabilities. Proven capability to robustly examine large data sets and highlight patterns, anomalies, relationships, and trends.
Self-starter, demonstrating high levels of data integrity. Ability to manage deliverables according to a robust project plan.
Experience in Google Cloud Platform or other cloud platform is a plus.
Experience Required:
Minimum of a year experience in Google Cloud Platform Minimum of a year of experience in a Data Engineering role creating data products, writing codes/queries/scripts, and building data visualizations.
Minimum of a year of experience in data design, data architecture and data modeling (both transactional and analytic).
Experience in Google Cloud Platform or other cloud platform is a plus.
Education Required:
Bachelor's degree in computer science or related field from an accredited college or university
Prefer Master's degree in computer science or related field from an accredited college or university","$103,365 /yr (est.)",1 to 50 Employees,Nonprofit Organization,Education,Education & Training Services,2004,Unknown / Non-Applicable
"grow.com
4.1",4.1,"Albany, NY",Azure Data Engineer,"General information
Office (s)
, Albany, NY, Atlanta, GA, Austin, TX, Cincinnati, OH, Cleveland / Akron, OH, Minneapolis, MN, Philadelphia, PA, Salt Lake City, UT, San Diego, CA, San Francisco Bay Area, CA
Date Published
Friday, June 2, 2023
Country
United States
Job ID
22303
Function
Finance
Salary Range
70,000 - 180,000
Description & Requirements
Azure Data Engineer
Job Description
As a member of the Epicor Cloud Data team, you are joining a team that invests in your success by providing comprehensive learning and mentorship programs. You will be the Principal engineer driving innovation and success of our next-generation Microsoft Azure-based Data Lake/DataMart and Reporting platforms. You will be innovating and mentoring the rest of the engineering org as we grow our teams.
What You'll Do
Create and maintain data pipeline architecture on the Azure platform. Ensure that system designs adhere to solution architecture design and are traceable to functional and non-functional requirements.
Leverage Azure Data Factory and Databricks to assemble large, complex data sets. Design new solutions and services to improve overall user experience.
Identify, design, and implement internal process improvements such as automating manual processes and optimizing data delivery. Define system design standards to improve and sustain standardization.
Build the infrastructure required for optimal data extraction, transformation, and loading from a wide variety of data sources.
Design relational and non-relational data stores on Azure.
Assist Manager-Data Administration in leading coordination with other staff to ensure data handling meets organizational objectives for data quality, business process management, and risk management.
Provide technical mentoring to team members
Perform all duties and maintain all standards in accordance with company policies, procedures, and internal controls such as SOX.
What You Bring:
4 years of experience working as part of an IT Data team using Azure-based technologies to solve business problems and build solutions.
Azure PaaS, Data analytics, Data warehousing, and Data science.
Azure Synapse Analytics, SQL Pool
Azure Spark, ADF, T-SQL Scripting, Stored Procs, Data bricks, Python
Microsoft related certifications
Experience with visualization tools such as Tableau (preferred)
The Team:
We put a high value on work-life balance. It isn’t about how many hours you spend at home or at work; it’s about the flow you establish that brings energy to both parts of your life. Most of our engineers have been with us for 5+ years growing in their respective roles. We believe striking the right balance between your personal and professional life is critical to life-long happiness and fulfillment while offering flexibility in working hours and encouraging you to find your own balance.
Why Epicor?

At Epicor, we know that success comes from working together. Everyone has a role to play, and it’s the essential partnerships across our company that are crucial to our customer’s success and our growth as a business.

We’re truly a team. Working in close partnership, we bring wide-ranging talents together in powerful collaborations. We think innovatively, share our knowledge generously, and constantly learn from our colleagues. We’re proud of the success we achieve every day, but we never stop challenging ourselves and encouraging each other. Together, we go further and imagine an even brighter future.

Whatever your career journey, we’ll help you find the right path. Through our training courses, mentorship, and continuous support, you’ll get everything you need to thrive. At Epicor, your success is our success. And that success really matters, because we’re the essential partners for the world’s most essential businesses—the hardworking companies who make, move, and sell the things the world needs.
Equal Opportunities and Accommodations Statement
At Epicor, we strive to create a welcoming, inclusive, and diverse workplace every day. Bring the whole and real you—that’s who we’re interested in. If you’re interested in this role but your experience and current skillset don’t match every qualification of the job description, that’s okay! We encourage you to apply anyway. Learning and sharing our knowledge keeps us all moving forward. You just might be the right fit and gain new skills new along the way.
#LI-MB2 #LI-Remote","$125,000 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,2014,Unknown / Non-Applicable
"Acuity Brands
3.6",3.6,"Conyers, GA",Data Engineer Senior,"We Light the Way!

Acuity Brands, Inc. (NYSE: AYI) is a market-leading industrial technology company. We use technology to solve problems in spaces and light. Through our two business segments, Acuity Brands Lighting and Lighting Controls (“ABL”) and the Intelligent Spaces Group (“ISG”), we design, manufacture, and bring to market products and services that make the world more brilliant, productive, and connected. We achieve growth through the development of innovative new products and services, including lighting, lighting controls, building management systems, and location-aware applications.
Job Summary
Acuity’s Business Intelligence team is comprised of people who are passionate about data. We believe accurate, timely and understandable data is vital to a data driven culture. We are devoted to aligning Acuity’s data to serve the needs of the enterprise and its customers. You will be joining a team of seasoned Business Intelligence professionals well versed in architecting bespoke BI (Business Intelligence) applications and implementing Microsoft Azure BI products.
We are seeking a talented and enthusiastic individual to be a Senor Data Engineer on our Business Intelligence Team as we transform Acuity Brands analytics and migrate our BI platform to Azure and Power BI. This position will work closely with BI Architects to bring to deliver end to end BI solutions that are innovative, scalable, and responsive.
Key Tasks & Responsibilities (Essential Functions)
Research, architect, drive, and deploy scalable, resilient cloud agnostic BI solutions to address Acuity current and future business needs and obligations
Partner with Data Architect, Solution Architect, and BI Product Managers to drive technology transformation on the BI platform to ensure the BI platform remains current and responsive
Mentor and guide Senior BI Developers to ensure adherence to BI standards and procedures
Partner with Data Architect to deliver integrated end to end data engineering solutions
Write application and cloud-based data processing code to transform inbound data to meet business requirements
Design and develop data models leveraging advanced modeling techniques to handle large and or complex data
Modify and optimize data engineering processes to handle ever-growing, complex, diverse data formats, sources, and pipelines
Work with infrastructure partners to tune and optimize code and BI resources such as but not limited to (index tuning, partitioning, caching, buffer tuning, and data archiving strategies.
Proactively estimate and plan development work and track performance to deliver work on schedule
Create and maintain current documentation in GIT (c4 and Plant UML)
Education (minimum education required)
Bachelor of Science
Preferred Education (i.e. type of degree)
Bachelor of Science in Computer Science or Information Systems
Experience (minimum experience required)
Bachelor’s Degree in Computer Science, MIS (Management Information System), or other technical/analytical field (or equivalent experience)
2 Azure Certifications
Working knowledge of data warehousing principles (Kimball, Inmon, Hybrid)
4-7 years or more database programming experience (SQL (preferred), Oracle, DB2)
4-7 years or more of BI experience (Power BI, Tableau, Qlik Sense/View, D3.js, SAP Business Objects, IBM Cognos)
4-7 years or more of working with Microsoft BI stack (SSIS, SSAS, T-SQL)
4-7 years or more of developing and enhancing ETL packages
4-7 years or more of working with and or constructing API (Push, Get, Post)
4-7 years or more of advanced experience identifying and optimizing database objects
2 years or more of application development (C# or .NET)
3 years or more experience working in Python or Scala
We invite you to apply today to join us as We Light the Way to a Brilliant, Productive, and Connected World!

We value diversity and are an equal opportunity employer. All qualified applicants will be considered for employment without regards to race, color, age, gender, sexual orientation, gender identity and expression, ethnicity or national origin, disability, pregnancy, religion, covered veteran status, protected genetic information, or any other characteristic protected by law.
Please click here and here for more information.

Accommodation for Applicants with Disabilities: As an equal opportunity employer, Acuity Brands is committed to providing reasonable accommodations in its application process for qualified individuals with disabilities and disabled veterans. If you have difficulty using our online system due to a disability and need an accommodation, you may contact us at (770) 922-9000. Please clearly indicate what type of accommodation you are requesting and for what requisition.

Any unsolicited resumes sent to Acuity Brands from a third party, such as an Agency recruiter, including unsolicited resumes sent to an Acuity Brands mailing address, fax machine or email address, directly to Acuity Brands employees, or to Acuity Brands resume database will be considered Acuity Brands property. Acuity Brands will NOT pay a fee for any placement resulting from the receipt of an unsolicited resume.

Acuity Brands will consider any candidate for whom an Agency has submitted an unsolicited resume to have been referred by the Agency free of any charges or fees. This includes any Agency that is an approved/engaged vendor, but does not have the appropriate approvals to be engaged on a search.

E-Verify Participation Poster
e-verify.gov
eeoc.gov","$92,338 /yr (est.)",10000+ Employees,Company - Public,Manufacturing,Electronics Manufacturing,2001,$1 to $5 billion (USD)
"Iron Systems
3.4",3.4,"Menlo Park, CA",Data Engineer III,"Date Posted:
6/1/2023

Job Function:
Software Development

Location:
Menlo Park CA - USA

Offered Salary:
USD 68 Hourly


Iron Systems is an innovative, customer-focused provider of custom-built computing infrastructure platforms such as network servers, storage, OEM/ODM appliances & embedded systems. For more than 15 years, customers have trusted us for our innovative problem-solving combined with holistic design, engineering, manufacturing, logistic, and global support services.

Job Title: Data Engineer III
Location: US - CA - Menlo Park

Summary:
The main function of the Data Engineer is to develop, evaluate, test, and maintain architectures and data solutions within our organization.
The typical Data Engineer executes plans, policies, and practices that control, protect, deliver, and enhance the value of the organization’s data assets.
Job Responsibilities:
Design, construct, install, test, and maintain highly scalable data management systems.
Ensure systems meet business requirements and industry practices.
Design, implement, automate, and maintain large-scale enterprise data ETL processes.
Build high-performance algorithms, prototypes, predictive models, and proof of concepts.
Skills:
Ability to work as part of a team, as well as work independently or with minimal direction.
Excellent written, presentation, and verbal communication skills.
Collaborate with data architects, modelers, and IT team members on project goals.
Strong PC skills including knowledge of Microsoft SharePoint.
Education/Experience:
Bachelor's degree in a technical field such as computer science, computer engineering, or related field required.
Process certification, such as Six Sigma, CBPP, BPM, ISO 20000, ITIL, and CMMI.",$68.00 /hr (est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1987,$25 to $100 million (USD)
"Calhoun International, LLC
3.7",3.7,"Washington, DC",Data Engineer (Senior),"About Us:
Calhoun International is a Professional Services company providing innovative solutions to our clients. Our expertise ranges from strategic intelligence analysis, expert instruction on intelligence analysis and sensors, cyberspace operations, information systems training, and knowledge management services among others. Calhoun International is located in Tampa, FL with employees in Florida, Virginia, Maryland, Washington, D.C. and overseas. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, or national origin.
Responsibilities:
Executes and on occasion, leads research, analysis, and evaluation efforts as well as studies, analyses, assessments, and technical reports, which may include the use of existing documents, databases, models, architectures and simulations. Deliverables produced shall be in a variety of formats in response to a wide range of requirements and delivery schedules. Provides mid to high-level analytical assessments and advice on complex issues, which require extensive knowledge of the subject matter. May attend various types of symposia and meetings at the ARSTAF and DOD level.
Requirements:
Minimum Education: bachelor’s degree; advanced intelligence discipline training; or other equivalent DoD or service Intelligence experience.
Minimum Experience: Fourteen (14) years of experience as an Army Intelligence analyst with experience from tactical to strategic. Experience shall have been within Two (2) years of starting on this contract.
Has served as a staff action officer at the HQDA (DCS, G-2 preferred) or Joint or a closely related DOD organization/agency.
Demonstrated SME level of knowledge of intelligence fusion systems, capabilities / employment, training, associated R&D efforts and program budget processes.
Demonstrated knowledge of Joint and Army processes –JCIDS, TAA, PED, JUONS/ONS, and CONOPS.
Experience in preparation and presentation of briefings on projects, studies and analysis to senior leaders and other ARSTAF action officers.
Desired:
Graduate from the Command and General Staff College or similar Senior Staff College
Security Clearance:
Active Top Secret with SCI eligibility required.","$127,546 /yr (est.)",1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,#N/A,$1 to $5 million (USD)
"Gridiron IT
4.5",4.5,Remote,Sr. Data Engineer,"GridironIT is seeking a Data Engineer.
Responsibilities:
We are looking for seasoned Data Engineer to work with our team and our clients to develop enterprise grade data platforms, services, and pipelines. We are looking for a more than just a ""Data Engineer"", but a technologist with excellent communication and customer service skills and a passion for data and problem solving.
Lead and architect migration of data environments with performance and reliability.
Assess and understand the ETL jobs, workflows, BI tools, and reports
Address technical inquiries concerning customization, integration, enterprise architecture and general feature / functionality of data products
Experience in crafting database / data warehouse solutions in cloud (Preferably AWS. Alternatively Azure, GCP).
Key must have skill sets – Python, AWS
Support an Agile software development lifecycle
You will contribute to the growth of our Data Exploitation Practice!
Qualifications:
US Citizen Only
Ability to hold a position of public trust with the US government.
8+ years industry experience coding commercial software and a passion for solving complex problems.
8+ years direct experience in Data Engineering with experience in tools such as:Big data tools: Hadoop, Spark, Kafka, etc.
Relational SQL and NoSQL databases, including Postgres and Cassandra
Data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
AWS cloud services: EC2, EMR, RDS, Redshift
Data streaming systems: Storm, Spark-Streaming, etc.
Search tools: Solr, Lucene, Elasticsearch
Object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Amazon S3, Athena, Redshift Spectrum, AWS Glue, AWS Glue Catalog, AWS Functions, and Amazon EC2 with SQL Server Developer
Advanced working SQL knowledge and experience working with relational databases, query authoring and optimization (SQL) as well as working familiarity with a variety of databases.
Experience with message queuing, stream processing, and highly scalable ‘big data’ data stores.
Experience manipulating, processing, and extracting value from large, disconnected datasets.
Experience manipulating structured and unstructured data for analysis
Experience constructing complex queries to analyze results using databases or in a data processing development environment
Experience with data modeling tools and process
Experience architecting data systems (transactional and warehouses)
Experience aggregating results and/or compiling information for reporting from multiple datasets
Experience working in an Agile environment
Experience supporting project teams of developers and data scientists who build web-based interfaces, dashboards, reports, and analytics/machine learning models
Experience with Informix and Data Stage
Job Type: Full-time
Pay: $84,436.60 - $150,799.40 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Monday to Friday
Experience:
AWS Glue: 5 years (Required)
SQL: 8 years (Required)
Data warehouse: 5 years (Required)
Work Location: Remote","$117,618 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable
Capitaltechsolutions Inc,#N/A,"Washington, DC",Looking for 3+ years of Voice/Data Comm Engineer,"Role :Voice/Data Comm Engineer
Client : State Of DC
Location : Washington, DC (Hybrid)
Description:
Duties:
1. As part of the OCFO technology team, this position is responsible for
providing support to internal and remote users by installing, configuring,
and upgrading OCFO telecommunication products, including OCFO call center which uses Alvaria Unified Platform, Mobile devices managed by AirWatch (MDM), Avaya Desk phones, CISCO desk phones, Webex Audio\Video conferencing, VoIP, SIP, analog lines, eFax, and take part in the CCaaS deployment initiative.
Serve as the first point of contact for customers seeking Telecom related
technical assistance over the phone or email
Monitoring and maintaining of OCFO Voice Network and reporting
issues to Telecom ISP/Cloud Hosting Vendor (OCTO) using remedy
Portal ticketing system.
Work with Telecommunications Partner (Aspect) on upgrades and patch
management of voice products.
Managing Helpdesk Tickets of Telecom related issues using Zendesk.
Configuring new hires user profiles for Voicemail and Display name
change on desk phones in timely manner
Update Equipment Inventory documentation of the telephony
infrastructure and Voice network infrastructure.
Maintain Voice network cabling closets and cable location inventory.
Perform technology refreshes, mobile devices iOS update in accordance
with OCTO AirWatch policy
Ability to document work activities into meaningful incidents or tasks in
the Zendesk system.
Performs all duties in accordance with OCFO policies and procedures
Maintain inventories of all OCFO Telecom assets using the FCMS
inventory and Verizon Portal to secure assets
Participate in the development of the documentation of Telecom
infrastructure and practices by providing written and/or verbal
communications to effectively maintain a resource of standard practices.
Participate in meetings as required and directed to insure clear
communication within IT Operations.
Install and move assets as required according to OCFO IT Operation
processes.
Responsibilities:
Provides technical direction and engineering knowledge for
communications activities including planning, designing,
developing, testing, installing and maintaining large communications
networks.
Ensures that adequate and appropriate planning is provided to direct
building architects and planners in building communications spaces
and media pathways meet industry standards.
Develops, operates, and maintains voice, wireless, video, and data
communications systems.
Provides complex engineering or analytical tasks and activities
associated with one or more technical areas within the
communications function.
Qualifications:
1-5 years of experience developing, operating and Required maintaining
voice, wireless video, and data comm. Systems
1-5 years of experience providing direction for communications activities related to large comm. networks
Troubleshoot daily telecom related issues in the areas of desk phones, call center applications, mobile devices, and data \ voice ports
- Proven experience with Mobile Device Management (MDM) AirWatch or other
- Proven experience with analyzing call center reports
- Proven knowledge of Call Center setup, writing call routing, reporting, and supporting agents
NICE CXone CCaaS deployment and support
Genesys CCaaS deployment and support
Proven experience with telecom system PBX\ACD\UIP
Proven experience in asset management in the areas of hardware and software
Skills
Experience developing, operating and Required maintaining voice, wireless video. 3 Years
Troubleshoot daily telecom related issues in the areas of desk phones, call center applications, mobile devices, and data \ voice ports. 3 Years
Proven experience with telecom system PBX\ACD\UIP. 2 Years
Proven experience with Mobile Device Management (MDM) AirWatch or other. 3 Years
NICE CXone CCaaS & Genesys CCaaS deployment and support. 3 Years
TELECOM EXPERIENCE. 4 Years
Job Type: Contract
Salary: $40.00 - $45.00 per hour
Benefits:
Health insurance
Schedule:
8 hour shift
Ability to commute/relocate:
Washington, DC 20001: Reliably commute or planning to relocate before starting work (Required)
Experience:
TELECOM EXPERIENCE (Preferred)
maintaining voice, wireless video (Preferred)
Troubleshoot daily telecom related issues (Preferred)
experience with telecom system PBX\ACD\UIP (Preferred)
Mobile Device Management (MDM) AirWatch or other (Preferred)
NICE CXone CCaaS & Genesys CCaaS deployment and support (Preferred)
Work Location: One location",$42.50 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Procore Technologies
4.5",4.5,Oregon,Staff Data Engineer,"Job Description

What if you could use your technology skills to develop a product that impacts the way communities' hospitals, homes, sports stadiums, and schools across the world are built? Construction impacts the lives of nearly everyone in the world, and yet it's also one of the world's least digitized industries, not to mention one of the most dangerous. That's why we're looking for a talented Staff Data Engineer to join Procore's journey to revolutionize a historically underserved industry.
As a Staff Data Engineer, you'll design and develop data products for Procore Data Platform data management area. You'll be part of the high-performance team of Data Engineers and will collaborate with platform engineers and product leaders.
This position will report to our Senior Manager of Data Engineering, and can be based remotely from any US location. We're looking for someone to join our team immediately.
What you'll do:
Lead the design and development of big data predictive analytics using object-oriented analysis, design and programming skills, and design patterns
Implement ETL workflows for data matching, data cleansing, data integration, and management
Maintain existing data pipelines and develop new data pipelines using big data technologies
Develop and maintain tables and data models in SQL, abstracting multiple sources and historical data across varied schemas to a format suitable for further analysis
Responsible for leading the effort to continuously improve the reliability, scalability, and stability of the enterprise data platform
Contribute to and lead the continuous improvement of the software development framework and processes by collaborating with Quality Assurance engineers
Deliver observable, reliable, and secure software, embracing the ""you build it, you run it"" mentality, focusing on automation and GitOps
Participate in daily standups, team meetings, sprint planning, and demo/retrospectives while working cross-functionality with other teams to drive the innovation of our products
Apply data governance framework, including the management of data, data compliance operating model, data policies, and standards
What we're looking for:
BS degree in Computer Science, a similar technical field of study, or equivalent practical experience; MS or Ph.D. degree in Computer Science or a related field is preferred
5+ years of experience in a Data Engineering position
Strong expertise with 3+ years of experience building enterprise techniques for large-scale distributed system design and data processing, including:
Building data pipelines with Databricks as the source
Building and maintaining data warehouses in support of BI tools (Snowflake, dbt, Tableau)
Building data pipeline framework for data workflow to process large data sets and Real-Time & Batch Data Pipeline development
Experience in processing structured and unstructured data into a form suitable for analysis and reporting with integration with a variety of data metrics providers ranging from advertising, web analytics, and consumer devices
Desire to be actively hands-on with code, using Java, Python (80%), and SQL, along with willingness and passion for mentoring junior engineers and performing code reviews
Possess familiarity with AWS-managed services for data (Glue, Athena, Data Pipeline, Flink, Spark) and Snowflake

Additional Information

Base Pay Range $147,200-$202,400. Eligible for Bonus Incentive Compensation. Eligible for Equity Compensation. Procore is committed to offering competitive, fair, and commensurate compensation, and has provided an estimated pay range for this role. Actual compensation will be based on a candidate’s job-related skills, experience, education or training, and location.
Perks & Benefits
At Procore, we invest in our employees and provide a full range of benefits and perks to help you grow and thrive. From generous paid time off and healthcare coverage to career enrichment and development programs, learn more details about what we offer and how we empower you to be your best.
About Us
Procore Technologies is building the software that builds the world. We provide cloud-based construction management software that helps clients more efficiently build skyscrapers, hospitals, retail centers, airports, housing complexes, and more. At Procore, we have worked hard to create and maintain a culture where you can own your work and are encouraged and given resources to try new ideas. Check us out on Glassdoor to see what others are saying about working at Procore.
We are an equal-opportunity employer and welcome builders of all backgrounds. We thrive in a diverse, dynamic, and inclusive environment. We do not tolerate discrimination against employees on the basis of age, color, disability, gender, gender identity or expression, marital status, national origin, political affiliation, race, religion, sexual orientation, veteran status, or any other classification protected by law.
If you'd like to stay in touch and be the first to hear about new roles at Procore, join our Talent Community.","$174,800 /yr (est.)",1001 to 5000 Employees,Company - Public,Information Technology,Enterprise Software & Network Solutions,2002,Unknown / Non-Applicable
"JPMorgan Chase Bank, N.A.
3.8",3.8,"Plano, TX",Senior Lead Data Engineer,"Embrace this pivotal role as an essential member of a high performing team dedicated to reaching new heights in data engineering. Your contributions will be instrumental in shaping the future of one of the world's largest and most influential companies.

As a Senior Lead Data Engineer at JPMorgan Chase- Global Technology Strategy team, you are an integral part of an agile team that works to enhance, build, and deliver data collection, storage, access, and analytics in a secure, stable, and scalable way. Leverage your deep technical expertise and problem solving capabilities to drive significant business impact and tackle a diverse array of challenges that span multiple data pipelines, data architectures, and other data consumers.

Job Responsibilities
Provides recommendationsandinsight on data managementandgovernance proceduresandintricacies applicable totheacquisition,maintenance,validation, andutilization of data
Designs and delivers trusted data collection, storage, access, and analytics data platform solutions in a secure, stable, and scalable way.
Defines database back-up,recovery, and archivingstrategy
Generates advanced data models for one or more teams using firmwide tooling, linear algebra, statistical and geometrical algorithms
Approves data analysis toolsandprocesses and creates functional andtechnicaldocumentationsupporting bestpractices
Leads and mentors'datavisualization analysts ininformationpresentation anddelivery and adds to team culture of diversity, equity, inclusion, and respect
Evaluates and reports onaccesscontrol processes todetermineeffectiveness of dataasset security

Required Qualifications, Skills, and Capabilities
Working experience with bothrelational and NoSQL databases
Advanced understanding of database back-up, recovery, and archiving strategy
Advanced knowledge of linear algebra, statistical and geometrical algorithms
Experience presenting and delivering visual data
JPMorgan Chase & Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.
We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.
The health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the ""WELL Health-Safety Rating"" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.
As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm's current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm's vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.

We offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, discretionary incentive compensation which may be awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.
Equal Opportunity Employer/Disability/Veterans
Base Pay/Salary
Seattle,WA $156,750.00 - $190,000.00 / year","$173,375 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Banking & Lending,1799,$10+ billion (USD)
"MeridianLink
3.8",3.8,United States,Data Engineer - 1483,"JOB SUMMARY
We are looking for an accomplished Data Engineer to join our quickly growing Analytics team. The hire will be responsible for expanding and improving our data and data pipeline architecture, as well as optimizing data flow and MDM for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data pipelines to support our next generation of products and data initiatives.
RESPONSIBILITIES
Design, develop, and operate large scale data pipelines to support internal and external consumers
Improve and automate internal processes
Integrate data sources to meet business requirements
Write robust, maintainable, well documented code
QUALIFICATIONS
2-4 years professional Data Engineering and Data warehousing experience
Extremely strong implementation experience in
Python, Spark, Azure Databricks, Delta Lake, and Databricks Data Warehouse.
SQL development knowledge – Stored procedures, triggers, jobs, indexes, partitioning etc.
Be able to write/debug complex SQL queries
Azure Data factory or Azure Synapse Analytics
ETL/ELT and Data-warehousing techniques and best practices
Experience with MS-SQL server and Databricks Data warehouse.
Experience building, maintaining, and scaling ETL/ELT processes and infrastructure
Knowledge of being able to work with a variety of Ingestion patterns such as API/SQL servers etc.
Experience with cloud infrastructure (Azure strongly preferred)
Knowledge of Master Data Management
Implementation experience with various data modelling techniques
Implementation experience working with a BI visualization tool (Sisense is a plus)
Experience with CI/CD tools (Preferred Gitlab, Jenkins)
Pluses for experience with
oUI development frameworks such as java script, Django, REACT etc.
Experience working in a fast-paced product environment, with an attitude of getting the job done with the least amount of tech debt
Prior Financial industry experience a plus.
Be able to navigate ambiguity and pivot based on business priorities with ease.
Strong communication, negotiating and estimating skills.
Be a team player and should be able to collaborate well.
Enables entrepreneurs and consumers to achieve the American dream by creating technological solutions that fuel the engine for financial growth. Our top-notch solutions create the premier customer experience every time. We believe in the principles of empowerment, collaboration, individual achievement, and innovation.
At MeridianLink, we work together to design, implement, test, and deliver state-of-the-art web applications for the financial services industry, using the latest technologies including cloud computing, mobile development, responsive design, ASP.NET, JavaScript, C#, VB.NET, and SQL Server.
OUR CULTURE
Our low turnover is a testament to our wonderful culture where people value the work they do and appreciate each other for their contributions. MeridianLink develops our employees so they can grow professionally by preferring to promote from within. We have an open door policy with direct access to executives; we want to hear your ideas and what you think. Our company believes that to be productive in the long term, we must have a genuine work-life balance. We understand that employees have families and full lives outside of the office. To that end, we honor their personal commitments.
MeridianLink is an Equal Opportunity Employer. We do not discriminate on the basis of race, religion, color, sex, age, national origin, disability or any other characteristic protected by applicable law.
MeridianLink runs a comprehensive background check, credit check and drug test as part of our offer process.

MeridianLink has a wonderful culture where people value the work they do and appreciate each other for their contributions. We develop our employees so they can grow professionally by preferring to promote from within. We have an open-door policy with direct access to executives; we want to hear your ideas and what you think. Our company believes that to be productive in the long term, we must have a genuine work-life balance. We understand that employees have families and full lives outside of the office. To that end, we honor their personal commitments.
MeridianLink is an Equal Opportunity Employer. We do not discriminate based on race, religion, color, sex, age, national origin, disability, or any other characteristic protected by applicable law.
MeridianLink runs a comprehensive background check, credit check, and drug test as part of our offer process.
Salary range of $94,500-$133,400. [It is not typical for offers to be made at or near the top of the range.] The actual salary will be determined based on experience and other job-related factors permitted by law.
Meridianlink offers:

Potential For Equity-Based Awards
Insurance coverage (medical, dental, vision, life, and disability)
Flexible paid time off
Paid holidays
401(k) plan with company match
Remote work
All compensation and benefits are subject to the terms and conditions of the underlying plans or programs, as applicable and as may be amended, terminated, or superseded from time to time.
#LI-REMOTE","$113,950 /yr (est.)",501 to 1000 Employees,Company - Public,Information Technology,Computer Hardware Development,1998,Unknown / Non-Applicable
"Epicor
4.2",4.2,"San Francisco, CA",Azure Data Engineer,"General information
Office (s)
, Albany, NY, Atlanta, GA, Austin, TX, Cincinnati, OH, Cleveland / Akron, OH, Minneapolis, MN, Philadelphia, PA, Salt Lake City, UT, San Diego, CA, San Francisco Bay Area, CA
Date Published
Friday, June 2, 2023
Country
United States
Job ID
22303
Function
Finance
Salary Range
70,000 - 180,000
Description & Requirements
Azure Data Engineer
Job Description
As a member of the Epicor Cloud Data team, you are joining a team that invests in your success by providing comprehensive learning and mentorship programs. You will be the Principal engineer driving innovation and success of our next-generation Microsoft Azure-based Data Lake/DataMart and Reporting platforms. You will be innovating and mentoring the rest of the engineering org as we grow our teams.
What You'll Do
Create and maintain data pipeline architecture on the Azure platform. Ensure that system designs adhere to solution architecture design and are traceable to functional and non-functional requirements.
Leverage Azure Data Factory and Databricks to assemble large, complex data sets. Design new solutions and services to improve overall user experience.
Identify, design, and implement internal process improvements such as automating manual processes and optimizing data delivery. Define system design standards to improve and sustain standardization.
Build the infrastructure required for optimal data extraction, transformation, and loading from a wide variety of data sources.
Design relational and non-relational data stores on Azure.
Assist Manager-Data Administration in leading coordination with other staff to ensure data handling meets organizational objectives for data quality, business process management, and risk management.
Provide technical mentoring to team members
Perform all duties and maintain all standards in accordance with company policies, procedures, and internal controls such as SOX.
What You Bring:
4 years of experience working as part of an IT Data team using Azure-based technologies to solve business problems and build solutions.
Azure PaaS, Data analytics, Data warehousing, and Data science.
Azure Synapse Analytics, SQL Pool
Azure Spark, ADF, T-SQL Scripting, Stored Procs, Data bricks, Python
Microsoft related certifications
Experience with visualization tools such as Tableau (preferred)
The Team:
We put a high value on work-life balance. It isn’t about how many hours you spend at home or at work; it’s about the flow you establish that brings energy to both parts of your life. Most of our engineers have been with us for 5+ years growing in their respective roles. We believe striking the right balance between your personal and professional life is critical to life-long happiness and fulfillment while offering flexibility in working hours and encouraging you to find your own balance.
Why Epicor?

At Epicor, we know that success comes from working together. Everyone has a role to play, and it’s the essential partnerships across our company that are crucial to our customer’s success and our growth as a business.

We’re truly a team. Working in close partnership, we bring wide-ranging talents together in powerful collaborations. We think innovatively, share our knowledge generously, and constantly learn from our colleagues. We’re proud of the success we achieve every day, but we never stop challenging ourselves and encouraging each other. Together, we go further and imagine an even brighter future.

Whatever your career journey, we’ll help you find the right path. Through our training courses, mentorship, and continuous support, you’ll get everything you need to thrive. At Epicor, your success is our success. And that success really matters, because we’re the essential partners for the world’s most essential businesses—the hardworking companies who make, move, and sell the things the world needs.
Equal Opportunities and Accommodations Statement
At Epicor, we strive to create a welcoming, inclusive, and diverse workplace every day. Bring the whole and real you—that’s who we’re interested in. If you’re interested in this role but your experience and current skillset don’t match every qualification of the job description, that’s okay! We encourage you to apply anyway. Learning and sharing our knowledge keeps us all moving forward. You just might be the right fit and gain new skills new along the way.
#LI-MB2 #LI-Remote","$125,000 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Software Development,1972,$1 to $5 billion (USD)
"Dropbox
4.6",4.6,Remote,"Senior Data Engineer, New Initiatives","Company Description

Dropbox is a special place where we are all seeking to fulfill our mission to design a more enlightened way of working. We’re looking for innovative talent to join us on our journey. The words shared by our founders at the start of Dropbox still ring true today.

Wouldn’t it be great if our working environment—and the tools we use—were designed with people’s actual needs in mind? Imagine if every minute at work were well spent—if we could focus and spend our time on the things that matter. This is possible, and Dropbox is connecting the dots.

The nearly 3,000 Dropboxers around the world have helped make Dropbox a living workspace - the place where people come together and their ideas come to life. Our 700+ million global users have been some of our best salespeople, and they have helped us acquire customers with incredible efficiency. As a result, we reached a billion dollar revenue run rate faster than any software-as-a-service company in history.

Dropbox is making the dream of a fulfilling and seamless work life a reality. We hope you’ll join us on the journey.

Team Description

Our Engineering team is working to simplify the way people work together. They’re building a family of products that handle over a billion files a day for people around the world. With our broad mission and massive scale, there are countless opportunities to make an impact.

Role Description

In this role you will build very large, scalable platforms using cutting edge data technologies. This is not a “maintain existing platform” or “make minor tweaks to current code base” kind of role. We are effectively building from the ground up and plan to leverage the most recent Big Data technologies. If you enjoy building new things without being constrained by technical debt, this is the job for you!
Responsibilities

Help define company data assets (data model), spark, sparkSQL and hiveSQL jobs to populate data models
Help define and design data integrations, data quality frameworks and design and evaluate open source/vendor tools for data lineage
Work closely with Dropbox business units and engineering teams to develop strategy for long term Data Platform architecture to be efficient, reliable and scalable
Conceptualize and own the data architecture for multiple large-scale projects, while evaluating design and operational cost-benefit tradeoffs within systems
Create and contribute to frameworks that improve the efficacy of logging data, while working with data infrastructure to triage issues and resolve
Collaborate with engineers, product managers, and data scientists to understand data needs, representing key data insights in a meaningful way
Define and manage SLA for all data sets in allocated areas of ownership
Determine and implement the security model based on privacy requirements, confirm safeguards are followed, address data quality issues, and evolve governance processes within allocated areas of ownership
Design, build, and launch collections of sophisticated data models and visualizations that support multiple use cases across different products or domains
Solve our most challenging data integration problems, utilizing optimal ETL patterns, frameworks, query techniques, sourcing from structured and unstructured data sources
Optimize pipelines, dashboards, frameworks, and systems to facilitate easier development of data artifacts
Requirements

Startup mentality with strong ownership to solve 0-1 problems with minimal guidance and being comfortable with ambiguities
Excellent product strategic thinking and communications to influence product and cross-functional teams by identifying the data opportunities to drive impact
BS degree in Computer Science or related technical field involving coding (e.g., physics or mathematics), or equivalent technical experience
5+ years of Python or Java, C++, Scale development experience
7+ years of SQL experience (No-SQL experience is a plus)
5+ years of experience with schema design and dimensional data modeling
Proven ability in regards to managing and communicating data warehouse plans to internal clients.
Experience designing, building and maintaining data processing systems
Experience working with either a Map Reduce or a MPP system on any size/scale
High tech experiences are preferred

Total Rewards

Our Engineering Career Framework is viewable by anyone outside the company and describes what’s expected for our engineers at each of our career levels. Check out our blog post on this topic and more here .

For candidates hired in San Francisco metro, New York City metro, or Seattle metro, the expected salary/On-Target Earnings (OTE) range for the role is currently $178,500 - $210,000 - $241,500.

For candidates hired in the following locations: Austin (TX) metro, Chicago metro, California (outside SF metro), Colorado, Connecticut (outside NYC metro), Delaware, Massachusetts, New Hampshire, New York (outside NYC metro), Oregon, Pennsylvania (outside NYC or DC metro), Washington (outside Seattle metro) and Washington DC metro, the expected salary/On-Target Earnings (OTE) range for the role is currently $160,700 - $189,000 - $217,400.

For candidates hired in all other US locations, the expected salary/On-Target Earnings (OTE) range for this role is currently $142,800 - $168,000 - $193,200.

Range(s) is subject to change. Dropbox takes a number of factors into account when determining individual starting pay, including job and level they are hired into, location/metropolitan area, skillset, and peer compensation. Dropbox uses the zip code of an employee’s remote work location to determine which metropolitan pay range we use.

Salary/OTE is just one component of Dropbox’s total rewards package. All regular employees are also eligible for the corporate bonus program or a sales incentive (target included in OTE) as well as stock in the form of Restricted Stock Units (RSUs).

Dropbox is committed to investing in the holistic health and wellbeing of all Dropboxers and their families. Our benefits and perks programs include, but are not limited to:

Competitive medical, dental and vision coverage

(US Only) Competitive 401(k) Plan with a generous company match and immediate vesting

Flexible Time Off/Paid Time Off, paid holidays, 11 Company-wide PTO days, Volunteer time off and more

Protection Plans including; Life Insurance, Disability Insurance and Travel benefit plans

Perks Allowance to be used on what matters most to you, whether that’s wellness, learning and development, food & groceries, and much more

Parental benefits including; Parental Leave, Child and Adult Care, Day Care FSA (US Only), Fertility Benefits (US Only), Adoption and Surrogacy support and Lactation Support

Mental Health and Wellness benefits Free Dropbox space for your friends and family

Additional benefits details are available upon request.

Benefits

Dropbox is committed to investing in the holistic health and wellbeing of all Dropboxers and their families. Our benefits and perks programs include, but are not limited to:

Competitive medical, dental and vision coverage*

Retirement Savings through a defined contribution pension or savings plan**

Dropbox provides a Flexible PTO Policy in addition to your statutory holidays allowing you to unplug, unwind, and refresh

Dropbox also provides exclusive additional paid time off for all FTE employees across the Globe, in addition to any relevant statutory holidays

Protection Plans including Life and Disability Insurance*

A Perks Allowance to be used on what matters most to you, whether that’s wellness, learning and development, food & groceries, and much more

Parental benefits including; Parental Leave, Fertility Benefits, Adoptions and Surrogacy support, and Lactation support

Additional benefits details are available upon request.

Where group plans are not available, allowances are provided
**Benefit, amount, and type are dependent on geographical location, based upon applicable law or company policy

Dropbox is an equal opportunity employer. We are a welcoming place for everyone, and we do our best to make sure all people feel supported and connected at work. A big part of that effort is our support for members and allies of internal groups like Asians at Dropbox, BlackDropboxers, Latinx, Pridebox (LGBTQ), Vets at Dropbox, Women at Dropbox, ATX Diversity (based in Austin, Texas) and the Dropbox Empowerment Network (based in Dublin, Ireland).",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,2007,Unknown / Non-Applicable
"Caire
2.7",2.7,United States,Senior Data Engineer,"Sr. Data Engineer (Remote)
About Us
Caire is building a next-generation, virtual-first care platform that puts the patient at the center. This platform will deliver best-in-class integrated shared services and technology to support the launch of multiple new virtual care companies over time. Caire was born out of a novel joint venture between Aegis Ventures, a venture capital studio creating transformative healthcare companies, and Northwell Health, New York's largest health system. Through this unique arrangement, Caire operates as an independent health tech start up but enjoys the benefits of existing within this larger healthcare ecosystem.
Caire is currently focused on building the scalable infrastructure which will serve as the backbone for a network of in-house virtual care companies to come. Through the creation of a shared operating system across every core function (e.g., technology, product, provider recruiting, healthcare operations), this infrastructure will minimize time on operations while maximizing time on what really matters – high quality patient care that delivers great outcomes. Over time the Caire platform will position us as a market leader in redefining how virtual care companies exist in a broader ecosystem of in-person and virtual care.
In parallel, Caire is leading the development and launch of a set of market-leading women's health virtual care companies that are coming to market in 2022-2023. The first Caire women's health company is Upliv Health (launched November 2022). Upliv is a health tech company that aims to empower women in perimenopause and menopause to take charge of their mental, physical, and emotional health through holistic care services and evidence-based information.
Caire is supported by a multidisciplinary team of clinical advisors, product experts, experienced engineers, and business strategists who are passionate about improving healthcare for all. Come reimagine the future of healthcare with us!
About The Role
Caire is seeking a passionate and experienced Data Engineer to build our data platform from the ground up in support of Caire's virtual care platform launch. The candidate will work closely with the CTO to execute against the product roadmap while collaborating effectively within their team.
What You Will Do
Help define Caire's data strategy, including designing and launching our data lakehouse
Build ETL/ELT pipelines to ingest data from our platform hosted on AWS
Setup BI tooling to provide data analytics capabilities to our business and product teams
Help manage our application and data cloud infrastructure using Terraform
Store and prepare semi-structured and unstructured data for ML use cases
Work closely with our product team to prioritize features in support of the Caire platform launch
What You Bring
At least five years of data engineering experience
Experience configuring data solutions in a cloud environment such as AWS
Experience using cloud storage and computing technologies such as Snowflake, Redshift, or Oracle ADW
Experience developing complex ETL/ELT pipelines using streaming solutions such as Kafka
DevOps experience creating and managing cloud infrastructure using Terraform
Advanced experience working with SQL
Experience working in all phases of full life cycle data analytics development including requirements, architecture, design, testing, and deployment
Familiarity with Machine Learning concepts and Data Science languages such as Python or R is a plus
Experience working in an agile environment (standups, sprint planning, retrospectives, etc.)
Ability to collaborate and communicate effectively within a team environment
Experience at a fast-paced, high-growth company is a plus
Experience working in highly-secure or highly regulated industries such as healthcare is a plus
Benefits
Comprehensive Health, Dental, & Vision benefits
Generous Vacation and Sick time
Parental leave
Tax-free commuter benefit
Free membership to One Medical and Talkspac
One-time $250 Work from Home (WFH) reimbursement
Additional Details
For this role, the base salary range is $170,000 - $200,000 with an eligible cash bonus. A salary offer will be determined by a number of factors including experience, skill level, education, internal pay equity, and other relevant business considerations.
Total compensation also includes a stock option grant and a competitive benefits package
This role is fully remote and open to candidates in the U.S.
Applicants must be currently authorized to work in the United States on a full-time basis
Caire is a proud Equal Opportunity Employer — we recruit, train, compensate and promote our team members based on qualifications. We encourage you to apply regardless of your race, religion, national origin, sex, gender identity, sexual orientation, disability, age, veteran status, or any other applicable legally protected characteristics.","$185,000 /yr (est.)",201 to 500 Employees,Company - Public,Manufacturing,Health Care Products Manufacturing,1993,$5 to $25 million (USD)
"Braintrust
4.6",4.6,"San Francisco, CA",Data Visualization Engineer - Tableau,"ABOUT US:
Braintrust is a user-owned talent network that connects you with great jobs with no fees or membership costs—so you keep 100% of what you earn.

JOB TYPE: Freelance, Contract Position (no agencies/C2C - see notes below)
LOCATION: Remote - Work from anywhere
HOURLY RANGE: Our client is looking to pay $75 – $100/hr
ESTIMATED DURATION: 20h/week - Long term

ABOUT THE HIRING PROCESS:
When you join Braintrust, you will be invited to a screening process for Braintrust to learn more about your previous work experiences. Once completed, you will have access to the employer for this role and other top companies that seek high-quality talent. Apply to this job to kick off the process.
THE OPPORTUNITY
Skills Required:
Expertise in handling Tableau products and building in Tableau
Problem-solving skills
Proficiency in handling SQL and databases
Should be good in written & verbal communication
Good to Have Skills:
English and French speaking
SaaS industry experience
Roles & Responsibilities:
Elicit business requirements and develop measurement plans for strategic programs
Collect and analyze data through manual and automated collection methods
Translate abstract data into easily-digestible and highly-visual dashboard applications and executive-level report deliverables
Collaborate with other analysts, data engineers, and data scientists to provide exceptional customer support to our internal stakeholders
Analyze large data sets and translate abstract data into highly visual and easily-digestible
What you’ll be working on
As a Senior Developer, you are responsible for the development, support, maintenance, and implementation of a complex project module.
You should have good experience in the application of standard software development principles. You should be able to work as an independent team member, capable of applying judgment to plan and execute your tasks. You should have in-depth knowledge of at least one development technology/programming language.
Apply Now!
Braintrust Job ID: 6721
C2C Candidates: This role is not available to C2C candidates working with an agency. If you are a professional contractor who has created an LLC/corp around their consulting practice, this is well aligned with Braintrust and we’d welcome your application.

Braintrust values the multitude of talents and perspectives that a diverse workforce brings. All qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status.

This is a remote position.",$87.50 /hr (est.),1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,2018,$100 to $500 million (USD)
Suprha Svc LLC,#N/A,"Horsham, PA",Lead Data Engineer,"Job Title : Lead Data Engineer
Job Location: Horsham, PA/Hybrid
Job Description
This role will be responsible for transforming extensive and complex data into consumable business capabilities.
Create system architecture, design, and specification using in-depth engineering skills and knowledge to solve complex development problems and achieve engineering goals.
Determine and source appropriate data for a given analysis.
Work with data modelers/analysts to understand the business problems they are trying to solve, then create or augment data assets to feed their analysis.
Acts as a resource and mentor for colleagues with less experience.
Lead a team of data engineers through a modernization project of moving on-premise big data implementation to cloud.
Skillset Required:
MUST have Lead experience, Data Analysis and Architecture, Spark, PySpark, Python, Sqoop, Hive, Azure, Google Dataproc, Databricks, No SQL Data Stores, Object Store, Design and Development of APIs, Kafka, Agile skills.
Must have Databricks, Snowflake, Azure/Google Dataproc experience.
Your expertise is deep and broad; you’re hands-on, producing both detailed technical work and high-level architectural designs.
8+ years of recent hands-on in an object-oriented language (Java, Scala, Python).
8+ years of experience designing and building data pipelines and data-intensive applications.
Experience using Big Data frameworks (e.g., Hadoop, Spark), databases for complex data assembly and transformation.
Experience designing and implementing complex big data implementation using Databricks, Snowflake, Azure, Google Dataproc.
Experience working with Healthcare data would be preferabble.
Job Type: Full-time
Pay: $110,000.00 - $120,000.00 per year
Experience level:
8 years
Schedule:
Monday to Friday
Ability to commute/relocate:
Horsham, PA 19044: Reliably commute or planning to relocate before starting work (Required)
Experience:
Hadoop: 4 years (Required)
Adobe Spark: 4 years (Required)
Databricks: 4 years (Preferred)
Snowflake: 4 years (Required)
Python: 5 years (Required)
Work Location: One location","$115,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"MyFitnessPal
4.2",4.2,United States,Data Engineer 2,"At MyFitnessPal, our vision is to be the global catalyst for every ""body"" to achieve their healthy. We believe good health starts with what you eat. We provide the tools and resources to reach your fitness goals.
We are looking for a Data Engineer to join the MyFitnessPal Data Engineering team. Our users rely on MyFitnessPal to power their health and fitness journeys every day. As a member of our MyFitnessPal Engineering team, you'll have the opportunity to positively impact those users with your expertise in the backend systems that drive the MyFitnessPal ecosystem. In addition to technical expertise, you'll find that your teammates value collaboration, mentorship, and inclusive environments.
What you'll be doing:
Build and maintain pipelines that provide data critical to operational reporting and data-driven initiatives
Integrate with services and systems across the MyFitnessPal engineering teams
Evaluate and improve existing pipelines, data models, and processes to provide more robust solutions that allow others to move quickly and efficiently
Support resolution of production issues across the Data Platform stack
Engage in code-reviews, working with teammates to learn and grow
Live our core values in all you do:
Be Kind and Care
Live Good Health
Be Data-Inspired
Champion Change
Leave it Better than You Found It
Make It Happen
Qualifications to be successful in this role:
2-4 years of data engineering experience working with large datasets and complex data environments, processes, and associated solutions
Experience with a variety of data stores (e.g. Snowflake, MySQL, MongoDB, DynamoDB, etc.)
Experience with data orchestration tooling (e.g. Airflow, Data Factory, etc.)
Experience with development languages (e.g. Python, SQL, Scala, etc.)
Understanding of data modeling for analysis by business intelligence or data science teams
Experience with a variety of API design patterns, such as REST
Experience developing validation and data integrity frameworks
Triaging and debugging production data issues
Experience building high volume data pipelines for downstream analysis supporting operational indicators
Experience with data at scale
Worked alongside client teams to support integration efforts
Familiarity with AWS and/or other cloud computing platforms

Please consider applying even if you don't meet 100% of the qualifications. Research shows you can still be considered for a position if you meet some of the requirements. At MyFitnessPal, we're building a fitness product for everyone and believe our team should reflect that. We encourage people of different backgrounds, experiences, abilities, and perspectives to apply.
Full Time Employee Perks, Benefits, and Culture:
Remote equal philosophy enabling you to work from any state in which we have operations in the continental U.S.
Want to work in an office? We also have a physical office in Austin, TX
Annual, in-person company retreats to work, bond, and enjoy team-building activities
Opportunities for team members to meet and connect in person for company paid lunches or working sessions
Flexible time-off policy + flexible working hours (Unlimited PTO Plan)
Competitive medical, dental, and vision benefits
Safe Harbor 401K program
Paid maternity and parental leave
Monthly Wellness Allowance to assist team members to focus on their own physical and mental wellbeing and select wellness initiatives of their own choice
Reward & recognition platform enabling peers to recognize and reward their peers for all the great work they do
MyFitnessPal Premium
Modern Virtual Learning and Development Library
DEI Committee dedicated to ongoing efforts to foster a diverse and inclusive workplace by setting actionable goals and evaluating progress
Diversity training for employees
A dynamic, motivating, and fun work environment
At MyFitnessPal, our mission is to enable people to make healthy choices. And it wouldn't be possible without our team. We celebrate the unique POV that each person brings to the table and believe in a collaborative and inclusive environment. As an equal opportunity employer, we prohibit any unlawful discrimination on the basis of race, color, religion, military or veteran status, sex, gender, gender identity or expression, sexual orientation, national origin, age, disability or genetic information. These are our guiding ideologies and apply across all aspects of employment.
MyFitnessPal participates in E-Verify.",#N/A,51 to 200 Employees,Company - Private,Information Technology,Internet & Web Services,2005,$100 to $500 million (USD)
"HCA Healthcare
3.3",3.3,"Nashville, TN",Senior Data Engineer,"Introduction
Last year our HCA Healthcare colleagues invested over 156,000 hours volunteering in our communities. As a Sr Database Admin with HCA Healthcare you can be a part of an organization that is devoted to giving back!
Benefits
HCA Healthcare offers a total rewards package that supports the health, life, career and retirement of our colleagues. The available plans and programs include:
Comprehensive medical coverage that covers many common services at no cost or for a low copay. Plans include prescription drug and behavioral health coverage as well as free telemedicine services and free AirMed medical transportation.
Additional options for dental and vision benefits, life and disability coverage, flexible spending accounts, supplemental health protection plans (accident, critical illness, hospital indemnity), auto and home insurance, identity theft protection, legal counseling, long-term care coverage, moving assistance, pet insurance and more.
Free counseling services and resources for emotional, physical and financial wellbeing
401(k) Plan with a 100% match on 3% to 9% of pay (based on years of service)
Employee Stock Purchase Plan with 10% off HCA Healthcare stock
Family support through fertility and family building benefits with Progyny and adoption assistance.
Referral services for child, elder and pet care, home and auto repair, event planning and more
Consumer discounts through Abenity and Consumer Discounts
Retirement readiness, rollover assistance services and preferred banking partnerships
Education assistance (tuition, student loan, certification support, dependent scholarships)
Colleague recognition program
Time Away From Work Program (paid time off, paid family leave, long- and short-term disability coverage and leaves of absence)
Employee Health Assistance Fund that offers free employee-only coverage to full-time and part-time colleagues based on income.
Learn more about Employee Benefits
Note: Eligibility for benefits may vary by location.

Would you like to unlock your potential with a leading healthcare provider dedicated to the growth and development of our colleagues? Join the HCA Healthcare family! We will give you the tools and resources you need to succeed in our organization. We are looking for an enthusiastic Sr Database Admin to help us reach our goals. Unlock your potential!
Job Summary and Qualifications
As a Sr. Data Engineer you will have the opportunity to grow and leverage your technical skills in MPP systems like that of Teradata’s to perform physical database design, SQL tuning and support our Teradata Data Warehouse platform on-prem and in the public cloud. You will be performing various operational tasks, including, but not limited to, system monitoring, object deployment, backup and recovery, user provisioning, query optimization and system maintenance. The job requires communicating with data architecture, integration and reporting teams for adherence to best practices and process standardization.
The role requires self-starters who are proficient in problem solving and communicating complicated, technical issues with clarity to the other technology teams and stakeholders. The culture of our organization places an emphasis on teamwork, so social and interpersonal skills are equally important as technical capability.

Assist in developing automation of various operational tasks performed by a DBA.
Work with cross-functional teams to build the physical databases and provide technical guidance during all phases of the development process.
Work with team in researching, evaluating and implementing new technologies as needed.
Provide regular, clear, and consistent communication (written and oral) on the status of projects, issues, and deliverables to team leadership.
Work with vendor technical support to facilitate analysis of and resolution to technical issues.
Provide planning input to leadership – operational and tactical – in order to drive success for team and company goals.
Participate in planned system maintenance tasks.
Provide rotational on-call support
What qualifications you will need:
Bachelors Degree preferred
Five or more years of relevant work experience
Other/Special Qualifications
Teradata development and/or administration (5+ years).
Teradata certification(s).
Shell Scripting and/or python or any development experience using C/C++/Java
Exposure to Cloud technologies
HCA Healthcare has been recognized as one of the World’s Most Ethical Companies® by the Ethisphere Institute more than ten times. In recent years, HCA Healthcare spent an estimated $3.7 billion in cost for the delivery of charitable care, uninsured discounts, and other uncompensated expenses.
""There is so much good to do in the world and so many different ways to do it.""- Dr. Thomas Frist, Sr.
HCA Healthcare Co-Founder
Be a part of an organization that invests in you! We are reviewing applications for our Sr Database Admin opening. Qualified candidates will be contacted for interviews. Submit your application and help us raise the bar in patient care!
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.","$115,561 /yr (est.)",10000+ Employees,Company - Public,Healthcare,Health Care Services & Hospitals,1968,$10+ billion (USD)
"Spectrum Communications & Consulting Inc.
4.3",4.3,"Chicago, IL",Data Engineer,"What do incubators and Spectrum have in common? Well, they’re great for growth, and even better for stability. As an innovative software development and digital marketing company pioneering the field of artificial intelligence, we can offer our newest Data Engineer the best of both words – a high energy, forward-thinking start-up culture, inside of a well-established, profitable, and stable structure . if you’re interested in getting your hands dirty and inciting change into a larger organization with a vision to change the world uses data today, then please read on.
Responsibilities
Beyond working with state of the art technology you will have many different fantastic projects to work on as a Data Engineer at Spectrum. Here are just a few different responsibilities you can expect off the bat:
Design, build and maintain both new and existing data infrastructures using tools including (but not limited to) Microsoft SQL Server, Azure Tables, Azure Blobs, and Azure Cosmos
Determine the right database system for a provided data set
Drafting database/table schemas
Properly index tables when applicable to ensure query performance
Create data pipelines for transforming new datasets into consistent, reliable data systems that are usable across multiple platforms
Identify optimizations and improvements in existing data pipelines
Create solutions for data validation to ensure data integrity, accuracy, and consistency
Ensure data is readily available for analysis and development usage
Shifting existing big data pipelines
Come up with data validation solutions for both initial data imports as well as results
Audit the automation process to make sure there are no gaps, redundancies, or faults in the data pipeline that would cause damage to data integrity
Planning query efficient data store schemas with respect to the data requirements of our internal data warehouse
Communicate with data scientists and other software engineers to build proper data pipelines for efficient and cost-effective querying
Develop, construct, test, and maintain architectures with the help and feedback of software engineers
Determine and implement proper data schemas that align with Spectrum’s and Spectrum’s client’s business objectives
Recommend and implement ways to optimize data reliability, efficiency, and quality
Consult and suggest different technologies for our team to test and utilize together
Incorporate the business objectives of key stakeholders in the architecture of our data warehouse
Research new and interesting data acquisition opportunities
Some Characteristics That Define You
We understand that as a Data Engineer for Spectrum, you have many different professional goals and personal interests. As such here are just a few different things that typically define our team members on the Data Science team:
Self-Starter. Building a data warehouse is no simple task. You will need to come in with a self-starter attitude to not only make this data engineering role what you want it to be, but make a stellar and efficient data warehouse while you’re at it.
Analytical. In order to solve problems and build innovative new digital marketing campaigns, it is essential that you know how to take an idea and analyze it from all of its angles.
Developer. In the ever changing world of artificial intelligence, it’s not enough just to build models. You bring to the table an eye for data patterns along with a knack for relational database engineering.
Patient. As a data engineer, you know that you work with extremely large data sets on a daily basis. As such we are looking for someone who is not only meticulous, but patient enough to sit and sift through that data in a thorough way.
Creative. Beyond just analyzing data sets, you are an explorer and a puzzle solver. Pulling insights out of your data and understanding how those insights can better shape our tools is something that you live to do.
Student. More so than most industries, the field of data science is always changing and evolving. As such, you are always looking to learn new things and gain new skills.
Required Skills and Experience
On top of the many intangible skills you bring to the table, there are many skills that can help improve the efficiencies and success of your work at Spectrum. Here are a few of those required skills and experience that you will come in with as a Data Engineer on our team:
A bachelor’s degree/pursuing a bachelor’s degree in computer science, mathematics, statistics, information systems, or a related field
Experience with statistical modeling
1-3 years working experience with Python and/or SAS languages
1-3 years working experience with SQL databases and database querying languages
1-3 years working experience with C#/.NET
Familiarity with Microsoft Azure
Familiarity with Graph database structures
Experience with both RDBMS and TDMS
Experience with data mining and data cleaning
Experience with data visualization and reporting techniques
Written and verbal expression
Benefits
As a Data Engineer at Spectrum there are a ton of fantastic perks and benefits that come along with your work. Here are just a few of the benefits you can expect when joining the Spectrum family:
Comprehensive medical & dental insurance
Retirement planning & company matching
Generous PTO, including sick days & holidays
Coworking space access for remote and hybrid work options
Year-round gym memberships
Paid continuing education
Hybrid work from Home , meetings a few times a month to strategize new projects
About Us
Our mission at Spectrum has always been the same— growth. Whether that be helping our customers find quality new business, developing and challenging our team members, or evolving our products and services with advancements in technology and best practices, we have always been looking towards bettering ourselves for the future. Now as we continue to grow, we find ourselves as not only the nation’s leading digital marketing and software provider for the home services industry, but an innovator and ground-shaker for the world of artificial intelligence as well. From marketing automation software powered by AI, to top notch digital marketing services via those same AI insights, we love what we do and are excited to continue to innovate for the future together.
INDSPCI
Responsibilities
Design, build and maintain both new and existing data infrastructures using tools including (but not limited to) Microsoft SQL Server, Azure Tables, Azure Blobs, and Azure Cosmos
Determine the right database system for a provided data set
Drafting database/table schemas
Properly index tables when applicable to ensure query performance
Create data pipelines for transforming new datasets into consistent, reliable data systems that are usable across multiple platforms
Identify optimizations and improvements in existing data pipelines
Create solutions for data validation to ensure data integrity, accuracy, and consistency
Ensure data is readily available for analysis and development usage
Shifting existing big data pipelines
Come up with data validation solutions for both initial data imports as well as results
Audit the automation process to make sure there are no gaps, redundancies, or faults in the data pipeline that would cause damage to data integrity
Beyond working with state of the art technology you will have many different fantastic projects to work on as a Data Engineer at Spectrum. Here are just a few different responsibilities you can expect off the bat:
Planning query efficient data store schemas with respect to the data requirements of our internal data warehouse
Communicate with data scientists and other software engineers to build proper data pipelines for efficient and cost-effective querying
Develop, construct, test, and maintain architectures with the help and feedback of software engineers
Determine and implement proper data schemas that align with Spectrum’s and Spectrum’s client’s business objectives
Recommend and implement ways to optimize data reliability, efficiency, and quality
Consult and suggest different technologies for our team to test and utilize together
Incorporate the business objectives of key stakeholders in the architecture of our data warehouse
Research new and interesting data acquisition opportunities
Some Characteristics That Define You
We understand that as a Data Engineer for Spectrum, you have many different professional goals and personal interests. As such here are just a few different things that typically define our team members on the Data Science team:
Self-Starter. Building a data warehouse is no simple task. You will need to come in with a self-starter attitude to not only make this data engineering role what you want it to be, but make a stellar and efficient data warehouse while you’re at it.
Analytical. In order to solve problems and build innovative new digital marketing campaigns, it is essential that you know how to take an idea and analyze it from all of its angles.
Developer. In the ever changing world of artificial intelligence, it’s not enough just to build models. You bring to the table an eye for data patterns along with a knack for relational database engineering.
Patient. As a data engineer, you know that you work with extremely large data sets on a daily basis. As such we are looking for someone who is not only meticulous, but patient enough to sit and sift through that data in a thorough way.
Creative. Beyond just analyzing data sets, you are an explorer and a puzzle solver. Pulling insights out of your data and understanding how those insights can better shape our tools is something that you live to do.
Student. More so than most industries, the field of data science is always changing and evolving. As such, you are always looking to learn new things and gain new skills.
Required Skills and Experience
On top of the many intangible skills you bring to the table, there are many skills that can help improve the efficiencies and success of your work at Spectrum. Here are a few of those required skills and experience that you will come in with as a Data Engineer on our team:
A bachelor’s degree/pursuing a bachelor’s degree in computer science, mathematics, statistics, information systems, or a related field
Experience with statistical modeling
1-3 years working experience with Python and/or SAS languages
1-3 years working experience with SQL databases and database querying languages
1-3 years working experience with C#/.NET
Familiarity with Microsoft Azure
Familiarity with Graph database structures
Experience with both RDBMS and TDMS
Experience with data mining and data cleaning
Experience with data visualization and reporting techniques
Written and verbal expression
Benefits
As a Data Engineer at Spectrum there are a ton of fantastic perks and benefits that come along with your work. Here are just a few of the benefits you can expect when joining the Spectrum family:
Comprehensive medical & dental insurance
Retirement planning & company matching
Generous PTO, including sick days & holidays
Coworking space access for remote and hybrid work options
Year-round gym memberships
Paid continuing education
Hybrid work from Home , meetings a few times a month to strategize new projects
About Us
Our mission at Spectrum has always been the same— growth. Whether that be helping our customers find quality new business, developing and challenging our team members, or evolving our products and services with advancements in technology and best practices, we have always been looking towards bettering ourselves for the future. Now as we continue to grow, we find ourselves as not only the nation’s leading digital marketing and software provider for the home services industry, but an innovator and ground-shaker for the world of artificial intelligence as well. From marketing automation software powered by AI, to top notch digital marketing services via those same AI insights, we love what we do and are excited to continue to innovate for the future together.
INDSPCI
Hours
40","$92,696 /yr (est.)",51 to 200 Employees,Company - Private,Media & Communication,Advertising & Public Relations,1992,$5 to $25 million (USD)
"CNA Insurance
3.9",3.9,"Chicago, IL",Senior Data Engineer,"You have a clear vision of where your career can go. And we have the leadership to help you get there. At CNA, we strive to create a culture in which people know they matter and are part of something important, ensuring the abilities of all employees are used to their fullest potential.
CNA seeks to offer a comprehensive and competitive benefits package to our employees that helps them — and their family members — achieve their physical, financial, emotional and social wellbeing goals.

For a detailed look at CNA’s benefits, check out our
Candidate’s Guide
.
JOB DESCRIPTION:
Essential Duties & Responsibilities
Performs a combination of duties in accordance with departmental guidelines:
Lead the design and build data solutions and applications that enable reporting, analytics, data science, and data management.
Design, develop and implement data integration projects using Informatica and SSIS.
Provide Azure application insights and Cloud-based integration using Python and PowerShell Scripts.
Create analytical Business Intelligence (BI) reports using Google Analytics for web applications.
Lead the design, implementation and automation of data pipelines, including sourcing data from internal and external systems and transforming the data for the optimal needs of various systems and business requirements.
Lead robust unit testing to ensure deliverables match the design and provide expertise to support subsequent release testing.
Apply machine learning concepts to development work.
Adhere to and establish quality and reliability standards, and ensure team adheres to the same quality and standards working in an Agile development environment.
Design complex physical data models, projects and cloud-based data lake constructs including SQL/NoSQL database systems.
Research, identify and implement process improvements that address complex technology gaps and build strong knowledge of technology enablers.
Maintain professional and technical knowledge by attending educational workshops, reviewing professional publications, establishing personal networks, and participating in professional societies.
Reporting Relationship
Typically Manager or above
Education & Experience
Bachelor’s degree in computer science, information technology or related and 5 (five) years of experience in data analytics or application development.
Must have work experience with each of the following:
Design, develop and implement data integration projects using Informatica and SSIS;
Provide Azure application insights and Cloud-based integration using Python and PowerShell Scripts;
Create analytical Business Intelligence (BI) reports using Google Analytics for web applications.
Primary Location United States – Illinois – Chicago
Organization – IT
Mon-Fri., 8:30am – 4:45pm, 37.5 hours/week, $140,046 to $148,800 per year, overtime exempt. This position qualifies for CNA’s employee referral policy program.
Apply: Submit cover letter and resume at
www.cna.com
.
CNA is committed to providing reasonable accommodations to qualified individuals with disabilities in the recruitment process. To request an accommodation, please contact
leaveadministration@cna.com
.","$144,423 /yr (est.)",1001 to 5000 Employees,Company - Public,Insurance,Insurance Carriers,1897,$10+ billion (USD)
"Dentsu Aegis Network
3.6",3.6,"Raleigh, NC",Data Engineer - BI Developer,"Company Description

Dentsu International is a modern marketing solutions company. Our mission is to help clients navigate, progress and thrive in a world of change. Businesses rely on our integrated network of agencies and specialized practices to champion meaningful progress through creative, media, commerce, data, and technology. Part of Dentsu Group, our global network comprises 66,000 diverse people in 143 countries, who are dedicated to teaming for growth and good. Some of our award-winning agencies include 360i, Carat, dentsumcgarrybowen, DEG, dentsuX, iProspect and Merkle. Follow us on Twitter @DentsuUSA and visit dentsu.com/us.
Part of Dentsu International, Dentsu Creative is a Global Creative Network that transforms brands and businesses through the power of Modern Creativity. Led by Global Chief Creative Officer Fred Levron, 9,000 experts across the globe work seamlessly together to deliver ideas that Create Culture, Shape Society and Invent the Future. Dentsu Creative was launched in June 2022 to address a client need for simplicity and will be Dentsu International’s sole creative network by the end of 2022.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.

Job Description

The Data Engineer is a new key role within the Business Platforms Americas team. We are looking for a well-rounded Senior BI Developer expertise with strong knowledge of MS SQL, PowerBI, and data warehousing techniques. This is a unique opportunity to be involved in delivering leading-edge business analytics using the latest and greatest cutting-edge BI tools, such as cloud-based databases, self-service analytics and leading visualization tools enabling the company’s aim to become a fully digital organization.
Key Responsibilities
Collaborate with the BI Dev team members to evaluate, design, develop BI reports and dashboards according to functional specifications while maintaining data integrity and data quality
Deliver Technical Design Document capturing specific processes and data flows, data definitions and relevant business rules
Apply best practices of data integration for data quality and automation
Work with business analysts to understand business requirements and use cases to write and assign technical stories and tasks
Work independently within guidelines, responsible for initiating, planning, executing, controlling, and implementation of projects using a formal project management and agile methodology
Work collaboratively with key stakeholders to translate business information needs into well-defined data requirements to implement the BI solutions
Work with team to provide support for existing analytics and PowerBI reporting platforms
Coaching, mentoring, and providing technical direction and training to other IT personnel
Working with BI & Analytic teams to develop and establish BI road Map/Vision

Qualifications

Experience:
Excellent communication skills
Over 7-10 years of experience in Data warehousing and Business Intelligence
Over 5 years’ experience in a Business Intelligence Analyst or Developer roles
Over 4 years’ experience using ADF for data warehousing
Experience in designing and performance tuning data warehouses and data lakes.
2+ years experience in developing data models and dashboards using Power BI within an IT department
Being delivery-focused with a can-do attitude in a sometimes-challenging environment is essential.
Experience using Power BI to visualize data held in SQL Server
Experience working with finance data highly desirable
Other key Competencies:
Strong communications skills and ability to turn business requirements into technical solutions
Experience in developing data lakes and data warehouses using Microsoft Azure
Demonstrable experience designing high-quality dashboards using Power BI
Strong database design skills, including an understanding of both normalized form and dimensional form databases.
In-depth knowledge and experience of data-warehousing strategies and techniques e.g., Kimble Data warehousing
Experience in Cloud based data integration tools like Azure Data Factory
Experience in power bi data modelling and DAX is preferred
Experience in Azure Dev Ops or JIRA is a plus
Familiarity with agile development techniques and objectives

Additional Information

The anticipated salary range for this position is $94,000-146K. Salary is based on a wide range of factors that include relevant experience, knowledge, skills, other job-related qualifications, and geography. A range of medical, dental, vision, 401(k) matching, paid time off, and/or other benefits also are available. For more information regarding dentsu benefits, please visit https://dentsubenefitsplus.com/.

Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact recruiting@dentsuaegis.com if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.

#LI-AJ1
About dentsu
Dentsu is the network designed for what’s next, helping clients predict and plan for disruptive future opportunities in the sustainable economy. Taking a people-centered approach to business transformation, dentsu combines Japanese innovation with a diverse, global perspective to drive client growth and to shape society www.dentsu.com.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.
Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact your recruiter if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.","$120,000 /yr (est.)",10000+ Employees,Company - Public,Media & Communication,Advertising & Public Relations,#N/A,Unknown / Non-Applicable
Bekhealth Corp,#N/A,"Branford, CT",Senior Software Engineer (Data),"About Us
BEKHealth Corporation is a leading clinical technology company that operates an AI-powered patient-matching software platform serving the clinical trial industry. Our platform allows life sciences and healthcare organizations to speed up trial feasibility, site selection, and patient recruitment by extracting data from electronic medical records (EMRs), which includes structured and unstructured clinical data that captures three times more trial criteria. We specialize in providing software for patient population analysis, site feasibility and selection, study participant identification, and study participant matching to both site networks.

About the Opportunity
We are seeking an experienced and high-achieving Software Engineer to enhance BEKTranslate, our AI-driven data processing platform. The selected individual will play a critical role in driving the BEKHealth mission, developing and improving data pipelines for processing Electronic Medical Record (EMR) data. Responsibilities include ensuring the reliability and scalability of BEKTranslate for handling petabytes of medical data, collaborating across BEKHealth teams to support machine learning and trial recruitment efforts through product development. This position reports to the Engineering Manager for the Data Processing Services team.
Responsibilities:
Design, develop, enhance, and maintain data pipelines for processing EMR data
Produce high-quality, maintainable and well documented code
Collaborate with other developers, product owners, and stakeholders to gather requirements and develop solutions that meet business needs
Mentor and guide junior developers to improve their technical skills and knowledge
Be proactive and solution-oriented, and driven to get things done in high-complexity environments
Work with the Research Services team to launch the BEKHealth platform with new customers

Requirements:
Undergraduate or graduate degree in a technical or scientific field, such as Computer Science, Engineering, Mathematics, or similar.
4+ years of professional experience as a software engineer
Experience designing and writing Python for ETL pipelines required
Senior level experience with Python and SQL required
Well versed in data processing optimization in the areas of
Streaming
ACID
Memory usage
Parallelization
Fault tolerance
Experience with data modeling and understanding of both relational and document-based databases
Experience working with pandas, polars, celery and pyodbc strongly preferred
Experience required with relational databases such as Postgres, MySQL, SQLServer, and Snowflake
Experience working with REST APIs is required
Experience working with distributed data processing frameworks required
Such as AWS Glue, AWS Step Functions, Airflow/Prefect
Strong experience operating in Linux and working with Kubernetes, Docker Swarm or other container orchestration technologies
Experience working with Healthcare Data Systems
Experience with EMR systems is preferred
Experience with FHIR is a plus
Experience dealing with protected health information (PHI) is a plus
Experience with medical terminology or medical ontologies/semantics is a plus
Cloud oriented and familiar with SaaS models
Ability to work in a fast-paced, dynamic environment

Personality:
The ideal candidate is one whom others see as humble, transparent, intelligent, honest, confident, creative, passionate, and someone who possesses a strong ability to get things done; and who has been described as focused and entrepreneurial, with great follow-through and attention to detail. Someone who exhibits character, and presence without arrogance, while also being highly credible both internally with the team and externally with customers will be an excellent fit.

Benefits:
We offer competitive salary and equity packages, health insurance, and other benefits. You will have the opportunity to work with a talented and passionate team and make a significant impact on the healthcare industry.

Additional Information:
Immigration sponsorship is not available for this position","$118,941 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"HP
4.2",4.2,"Vancouver, WA",Data Engineer,"Applies basic foundation of a function's principles, theories and concepts to assignments of limited scope. Uses professional concepts and theoretical knowledge acquired through specialized training, education or previous experience. Develops expertise and practical knowledge of applications within business environment. Acts as team member by providing information, analysis and recommendations in support of team efforts. Exercises independent judgment within defined parameters.
Responsibilities
Codes limited enhancements, updates, and programming changes for portions and subsystems of data pipelines, repositories or models for structured/unstructured data.
Analyzes design and determines coding, programming, and integration activities required based on objectives and guidance from more senior project team members.
Executes established portions of testing plans, protocols, and documentation for assigned portion of application; identifies and debugs issues with code and suggests changes or improvements.
Participates as a member of a project team of other data science professionals to develop reliable, cost effective and high-quality solutions for assigned data system, model, or component.
Knowledge & Skills
Using data engineering tools, languages, frameworks to cleanse, mine and explore data.
Basic understanding of SQL and NoSQL & relational based systems along with complex, distributed and massively parallel systems.
Ability to apply analytical and problem-solving skills.
Ability to understand complex data structures.
Understanding of database technologies and management systems.
Understanding of database architecture testing methodology, including execution of test plans, debugging, and testing scripts and tools.
Strong written and verbal communication skills; mastery in English and local language.
Scope & Impact
Collaborates with peers, senior engineers, data scientists and project team.
Typically partners with more senior Individual Contributors.
Supports projects requiring data engineering solutions expertise.
Education & Experience
Bachelor's or Master's degree in Computer Science, Information Systems, Engineering, or equivalent.

About HP

You’re out to reimagine and reinvent what’s possible—in your career as well as the world around you.
So are we. We love taking on tough challenges, disrupting the status quo, and creating what’s next. We’re in search of talented people who are inspired by big challenges, driven to learn and grow, and dedicated to making a meaningful difference.

HP is a technology company that operates in more than 170 countries around the world united in creating technology that makes life better for everyone, everywhere.

Our history: HP’s commitment to diversity, equity and inclusion – it's just who we are.
From the boardroom to factory floor, we create a culture where everyone is respected and where people can be themselves, while being a part of something bigger than themselves. We celebrate the notion that you can belong at HP and bring your authentic self to work each and every day. When you do that, you’re more innovative and that helps grow our bottom line. Come to HP and thrive!",#N/A,10000+ Employees,Company - Public,Information Technology,Computer Hardware Development,1939,Unknown / Non-Applicable
"Latitude Inc
4.3",4.3,"Arlington, VA",AWS Data Engineer (US Citizen),"What You’ll Do:
Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies
Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems
Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake
Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community
Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment
Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance
Basic Qualifications:
Bachelor’s Degree
At least 4 years of experience in application development (Internship experience does not apply)
At least 1 year of experience in big data technologies
Preferred Qualifications:
5 years of experience in application development including Python, SQL, Scala, or Java
2 years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)
3 years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)
2 year experience working on real-time data and streaming applications
2 years of experience with NoSQL implementation (Mongo, Cassandra)
2 years of data warehousing experience (Redshift or Snowflake)
3 years of experience with UNIX/Linux including basic commands and shell scripting
2 years of experience with Agile engineering practices
Job Type: Full-time
Pay: $70,000.00 - $100,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Yearly pay
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Arlington, VA 22202: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Preferred)
Experience:
AWS: 1 year (Preferred)
SQL: 1 year (Preferred)
Python: 1 year (Preferred)
Security clearance:
Confidential (Preferred)
Work Location: Hybrid remote in Arlington, VA 22202","$85,000 /yr (est.)",1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,#N/A,$1 to $5 million (USD)
"AT&T
3.7",3.7,"Plano, TX",Professional Big Data Software Engineer,"DUTIES: Professional-Big Data Software-Engineer needed by AT&T Services, Inc. in Plano, TX to plan for the development of high performance, distributed computing and data processing tasks using big data technologies including Hadoop and NoSQL, text mining and in the areas of finance and Risk and Fraud Systems. Experience in Microsoft Azure and Palantir Foundry. Analyze structured and unstructured data, work with clients to understand requirements, and develop data pipelines and applications to produce data science features required to develop analytics and data to run the business more efficiently. Apply strong software engineering ability in developing, deploying, and testing code in a modern DevSecOps environment and tools for data science development and deployments in the cloud. Utilize java vm (jvm) based function languages including Scala, Hadoop query languages including pig, hive, along with alternative hdfs-based computing frameworks including spark and storm. Use big data programming languages and technology, write code, complete programming and documentation, and perform testing and debugging of applications. Analyze, design, program, debug and modify software enhancements and new products used in distributed, large scale analytics and visualization solutions. Interact with data scientists and industry experts to understand how data needs to be converted, loaded and presented. Work in a highly agile environment while fully functioning technical professional. Develops new concepts, methods, techniques. Object Oriented Analysis and Design using Python Programming language and Agile Methodologies and Scrums.. Execute problem solving to solve non routine problems based on analysis of multiple factors. Identify key issues, patterns or deviations from norm. Work on problems requiring judgment and in depth evaluation of multiple factors. Analyze and interpret research to evaluate and recommend solutions while applying independence guided by team goals and operational objectives. Apply judgment to determine appropriate processes and technical area standard. Provide technical direction to others in own work area while contributing to AT&T technology key contributor on diverse projects of moderate scope. Recommend new procedures to drive desired results. Build productive internal/external relationships and collaborate with others in own team or across teams.

Requirement: Requires a Bachelor’s degree, or foreign equivalent degree in Computer Engineering or Computer Science and two (2) years of experience in the job offered or two (2) years of experience in a related occupation planning for the development of high performance, distributed computing and data processing tasks using big data technologies including Hadoop and NoSQL in the areas of finance and Risk and Fraud Systems; utilizing java vm (jvm) based function languages including Scala, Hadoop query languages including pig, hive, along with alternative hdfs-based computing frameworks including spark; analyzing, designing, programing, debugging and modifying software enhancements and new products used in distributed, large scale analytics and visualization solutions; and Objecting Oriented Analysis and Design using Python Programming language and Agile Methodologies and Scrums.

Our Professional-Big Data Software-Engineers earn between $121,952 - $175,100 yearly. Not to mention all the other amazing rewards that working at AT&T offers.

Joining our team comes with amazing perks and benefits:
Medical/Dental/Vision coverage
401(k) plan
Tuition reimbursement program
Paid Time Off and Holidays (based on date of hire, at least 23 days of vacation each year and 9 company-designated holidays)
Paid Parental Leave
Paid Caregiver Leave
Additional sick leave beyond what state and local law require may be available but is unprotected
Adoption Reimbursement
Disability Benefits (short term and long term)
Life and Accidental Death Insurance
Supplemental benefit programs: critical illness/accident hospital indemnity/group legal
Employee Assistance Programs (EAP)
Extensive employee wellness programs
Employee discounts up to 50% off on eligible AT&T mobility plans and accessories, AT&T internet (and fiber where available) and AT&T phone

AT&T is an Affirmative Action/Equal Opportunity Employer, and we are committed to hiring a diverse and talented workforce. EOE/AA/M/F/D/V
*np*","$148,526 /yr (est.)",10000+ Employees,Company - Public,Telecommunications,Telecommunications Services,1876,$10+ billion (USD)
"Par Government Systems Corporation
4.4",4.4,"Bethesda, MD",Senior Data Engineer,"PAR Government is excited to welcome a Senior Data Engineer to the our Intelligence and Readiness Operations. As a Senior Data Engineer you will support a large-scale intelligence processing system that is focused on digital document exploitation.

Establish a data engineering processes to support the understanding of information needs to maximize the use and value of data and information assets by consumers. You will manage information consistently across the enterprise and align data management efforts and technologies with business needs. Key task areas on the program include forensic image processing, machine learning model production, knowledge graph construction and reasoning, agile development, system security, technology transition, and system operations.

Responsibilities and Duties:
Analyze, design, build, test, deploy, operate, and maintain solutions, capabilities, and services to meet data needs.
Plan and lead major technology assignments, evaluate performance results, and recommend major changes affecting short-term project growth and success.
Conduct requirements analysis.
Conduct requirements design.
Implementation solutions
Maintenance databases of related solution components


Required Skills/Experience:
Must have an active or in scope US Top Secret Clearance with SCI Eligibility
Expertise in analyzing. designing, building, testing, deploying. operating, and maintaining solutions, capabilities, and services to meet data needs, including requirements analysis, and design, implementation, and maintenance of databases' related solution components.
Bachelors degree or equivalent with a minimum of 10 years of experience as a Data Engineer
Experience with defining and recording data requirements and delivering data requirements specifications.
Experience with developing and maintaining conceptual data models and delivering conceptual data model diagrams, logical data models, physical data models, and physical databases .
Experience with managing data model versions and integrating and delivering data model libraries.
Expertise in designing data integration services and delivering source-to-target maps, data extract-transform- load (ETL) design specifications, and data conversion designs.
Expertise in writing software code and scripts to distributed the processing of information extraction tasks to identify entities, events, and relationships from large corpus of structured and unstructured data and multimedia stored in a distributed file system ox object store.
Experience with applying data cleansing, transformation, and augmentation methods to measure and improve data quality.
Experience building, testing, and delivering data integration services
Experience with establishing Golden Records and delivering reliable reference and master data.
Experience defining, delivering, and maintaining hierarchies and affiliations that define the meaning of data within the context of its interrelationships with other data
Experience with importing and exporting data between an external RDBMS and a Hadoop cluster, including the ability to import specific subsets, change the delimiter and file format of imported data during ingest, and alter the data access patten or privileges.
Experience ingesting real-time and near-real time (NRT) Streaming data into the Hadoop File System (HDFS), including the ability to distribute to multiple data sources and convert data on ingest from one format to another.
Expertise in loading data into and out of the Hadoop File System (HDFS) using the HDFS command line interface; converting sets of data values in a given format at stored in Hadoop File System (HDFS) into new data values and/or a new data format and writing them into HDFS or Hive/HCatalog.
Expertise in filtering, sorting, joining, aggregating, and transforming one or more data sets in a given format (e.g., Parquet, Avro, JSON, delimited text, and natural language text) stored in the Hadoop Distributed Filesystem (HDFS)

It is the policy of PAR to prohibit all forms of discrimination and to affirmatively implement equal opportunity to all qualified employees and applicants for employment without regard to race, color, creed, religion, sex, age, veteran status, national origin, disability, marital status, predisposing genetic characteristics, sexual orientation, gender identity, or other legally protected status and positive action shall be taken to insure the fulfillment of this policy.
If you require reasonable accommodation in the application process, call Human Resources at 315.356.2260. All other applications must be submitted online.***

Required Skills

Required Experience","$122,700 /yr (est.)",201 to 500 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
#N/A,#N/A,"Englewood, CO",Senior Data Engineer,"Department Summary

DISH is a Fortune 200 company that continues to redefine the communications industry. Our legacy is innovation and a willingness to challenge the status quo, including reinventing ourselves. We disrupted the pay-TV industry in the mid-90s with the launch of the DISH satellite TV service, taking on some of the largest U.S. corporations in the process, and grew to be the fourth-largest pay-TV provider. We are doing it again with the first live, internet-delivered TV service – Sling TV – that bucks traditional pay-TV norms and gives consumers a truly new way to access and watch television.
Now we have our sights set on upending the wireless industry and unseating the entrenched incumbent carriers.
We are driven by curiosity, pride, adventure, and a desire to win – it’s in our DNA. We’re looking for people with boundless energy, intelligence, and an overwhelming need to achieve, to join our team as we embark on the next chapter of our story.
Opportunity is here. We are DISH.


Job Duties and Responsibilities

The Data Architecture team in DISH IT Digital Solutions seeks a Data Engineer who will develop and maintain a variety of data solutions for supporting our core business and modernizing DISH IT’s transactional data persistence systems.
Key responsibilities:
Develop scalable and high-performing data update applications for on-premises and Cloud-based databases.
Deliver, optimize, and automate data updates for existing business processes.
Provide on-call support for data persistence systems in Production.

Work attire: Business casual.

Working hours: This is a full-time position: 40 hours/week. Days and hours of work are typically Monday through Friday; 8:00 a.m. to 5:00 p.m. or 9:00 a.m. to 6:00 p.m.


Skills, Experience and Requirements

Education: Bachelor’s degree in Computer Science, Computer Engineering, or a related technical degree, or a combination of education and experience.

Experience: 4+ years of experience as a Data Engineer or similar role.

Skills and qualifications:

Strong desire to learn new skills.
Familiar with one programming language.
Hands-on SQL development in a relational or NoSQL database.
Strong problems solving and analytical skills.
Experiences in Python or Java.
SQL/PLSQL development experiences in Oracle or MySql.
Familiar with Linux.

Salary Range

Compensation: $106,250.00/Year - $125,000.00/Year
Compensation and Benefits

We also offer versatile health perks, including flexible spending accounts, HSA, a 401(k) Plan with company match, ESPP, career opportunities, and a flexible time away plan; all benefits can be viewed here: DISH Benefits.

The base pay range shown is a guideline. Individual total compensation will vary based on factors such as qualifications, skill level, and competencies; compensation is based on the role's location and is subject to change based on work location. Candidates need to successfully complete a pre-employment screen, which may include a drug test and DMV check.","$115,625 /yr (est.)",10000+ Employees,Company - Public,Telecommunications,"Cable, Internet & Telephone Providers",1980,$10+ billion (USD)
"Tremco Incorporated
3.7",3.7,United States,Data Engineer,"GENERAL PURPOSE OF THE JOB:
*100% REMOTE / TELEWORK*
Division - Tremco Roofing & Building Maintenance
We are seeking an experienced and skilled Data Engineer to join our team! We are looking for a candidate that thrives in a collaborative environment, is a self-starter, and is passionate about data science. Our data science team is the foundation for data-driven business decisions and is leading the way for continued growth in innovative markets within the construction industry.
On the Data Science team, the Data Engineer’s purpose is to design, develop, and maintain the company's data infrastructure, pipelines, and workflows. They are responsible for merging predictive and prescriptive modeling to ensure it stays consistent with data flowing across the organization. They work closely with data scientists, analysts, and other stakeholders to ensure the data is properly collected, stored, processed, and analyzed to drive informed business decisions.
If you are passionate about data science and want to work with a dynamic team of professionals, please apply today!
ESSENTIAL DUTIES AND RESPONSIBILITIES:
Design, develop, build, and maintain the company's data infrastructure, pipelines, and workflows and all associated engineering tasks.
Develop and maintain ETL (Extract, Transform, Load) processes to collect and integrate data from various sources.
Build and maintain data APIs to enable data access across the organization.
Develop and implement scalable data solutions to optimize data processing, storage, and retrieval.
Develop and maintain documentation for data pipelines, including data dictionaries, standard operating procedures, and data flow diagrams.
Work with unstructured data and develop data models to enable data analysis and insights.
Identify any hidden patterns or data inconsistencies and work along with similar ad-hoc analysis
Ensure data quality, consistency, and accuracy and is properly structured and formatted to support analyses.
Ensure data security, integrity, and compliance with data privacy regulations.
Troubleshoot and resolve data-related issues, including data quality, integrity, and performance.
Continuously monitor, maintain, and optimize the health and performance of the data infrastructure, pipelines, and workflows
Collaborate with data scientists, analysts, and other stakeholders to understand data requirements.
Stay up to date with the latest advancements in data engineering and recommend new technologies, tools, and processes to improve efficiency and productivity.
EDUCATION:
Bachelor's or Master's degree in Information Technology, Computer Science, or a related field
EXPERIENCE:
3+ years of experience in a data science or related role
CERTIFICATES, LICENSES, REGISTRATIONS:
Not Required but beneficial:
Certified SQL
Certified SQL, Advanced Queries
Python for Data Science & Machine Learning
R for Data Science & Machine Learning
Databricks Lakehouse Fundamentals
OTHER SKILLS AND ABILITIES:
Proficiency in programming languages such as Python, R, and SQL
Strong understanding of database technologies and SQL queries
Strong experience with ETL processes, data integration, and data modeling
Experience with cloud-based data storage and computing services, specifically Azure
Excellent problem-solving and analytical skills
Experience with data visualization tools such as Tableau or Power BI
Experience with data lakehouse tools such as Synapse (data lake) or databricks
Excellent communication and collaboration skills
Ability to work independently and prioritize tasks in a fast-paced & dynamic environment

Qualified applicants will receive consideration for employment without regard to their race, color, religion, national origin, sex, sexual orientation, gender identity, protected veteran status or disability.",#N/A,1001 to 5000 Employees,Subsidiary or Business Segment,Manufacturing,Chemical Manufacturing,1997,$500 million to $1 billion (USD)
"Ocean Health Initiatives
2.2",2.2,"Neptune City, NJ",Data Engineer,"Company Intro: Ocean Health Initiatives, Inc. (OHI) is a Federally Qualified Health Center (FQHC) dedicated to providing quality, accessible and comprehensive primary health care to the residents of Ocean and Monmouth County; regardless of economic status.

Our Health Center Locations: Brick, Freehold, Lakewood, Little Egg Harbor, Manahawkin, Manchester and Toms River; with our school-based Wellness Programs located within the Clifton Ave Grade School, Lakewood; and Lakewood High School.

OHI services include family and internal medicine, pediatrics, behavioral health, nutrition, OB/GYN women’s health, dental, family planning, specialty care, pharmacies, PrEP, and a STI clinic. Hours: Full-time hours are M-F 8:00am- 4:30pm with rotating evening/Saturday shifts (subject to seasonal changes and business/site needs)
Position Summary
Reporting to the Director of Informatics, the Data Engineer will be integrated within the Informatics Team. This individual will play a key role in managing and optimizing Ocean Health Initiatives’ data systems, particularly relating to our Athenahealth Electronic Medical Records (EMR) system and Microsoft Azure platform, to support data-driven decision-making and enhance the quality of healthcare services for our communities. The Data Engineer will collaborate with other members of the IT Department to include: IT Systems Analyst, IT Helpdesk Specialist, Data Analyst, as well as other Informatics team members.
Responsibilities
Collaborates with the Informatics team to design, construct, install, test and maintain highly scalable data management systems.
Works with Athenahealth EMR and Microsoft Azure platforms to enhance data collection procedures, ensure data integrity, and optimize data delivery for each project.
Extracts and integrates data from various sources, including the Athenahealth EMR system and Microsoft Azure, while ensuring data privacy and adherence to HIPAA guidelines.
Develops, tests, and maintains architectures, including databases and large-scale processing systems, on Microsoft Azure.
Employ a variety of languages and tools to marry systems together or to create data interfaces.
Recommends and implements ways to improve data reliability, efficiency, and quality.
Collaborates with Informatics teams to strive for greater functionality within OHI’s data systems.
Stays informed about industry trends and emerging technologies in the fields of data engineering, healthcare informatics, and information technology.
Is highly accountable for one's own productivity. In this era of transparency, all projects must be represented clearly on a project board in Monday.com. Responsible for ensuring the board is always updated and accurate and that information is accessible to those that need it.
Maintains a passing monthly scorecard threshold at 80%.
Adheres to Corporate Compliance policies.
Other duties as assigned.
Education/Experience/Licensure
Bachelor degree in Computer Science, Information Systems, or a related field is required. Advanced degrees and certifications in Microsoft Azure or Athenahealth are a plus.
Two to three years of experience in a data engineering role, with specific experience in Athenahealth EMR and Microsoft Azure environments is required.
Proficiency in understanding and optimizing complex data systems, including healthcare EMR systems and cloud platforms like Microsoft Azure is required.
Strong understanding of data warehousing concepts and ETL (Extract, Transform, Load) tools and processes is required.
Familiarity with healthcare data standards, such as HL7, FHIR, and ICD-10 is required.
Knowledge of data security and privacy regulations, such as HIPAA is required.
Excellent problem-solving, analytical, and communication skills, with the ability to translate complex data concepts for non-technical stakeholders is required.
The ability to work both independently and collaboratively in a fast-paced, dynamic environment is required.
Proficiency in Microsoft Office 365 is required.
Benefits
Paid Time Off (PTO)
Holidays (8)
Health Insurance
Dental Benefits
401(k) + match
Group Term Life Insurance
Flexible Spending Account
Pre-Employment Requirements
Physical
Criminal Background Checks
Drug Screening
Tuberculosis Screening
Ocean Health Initiatives is firmly committed to creating a diverse workplace and is proud to provide equal employment opportunities to all applicants. Therefore, Ocean Health does not discriminate on the basis of creed, color, national origin, sex, gender identity, sexual orientation, age, religion, marital or parental status, alienage, disability, political affiliation or belief, military or military discharge status.
In accordance with New Jersey Executive order (COVID-19 No.283) in conjunction with the Federal CDC guidelines, COVID vaccinations are a requirement for Ocean Health Initiatives as well as many other Healthcare Organizations. Proof of full vaccination and the booster shot is required prior to the beginning date of employment. If you have a medical or religious contraindication, please inform Human Resources when the offer is extended","$91,048 /yr (est.)",201 to 500 Employees,Nonprofit Organization,Healthcare,Health Care Services & Hospitals,2003,Unknown / Non-Applicable
Zllius Inc.,#N/A,"North Chicago, IL",Data Engineer,"Hi,
Greetings from Zllius Inc!
We have an urgent opening for the role Data Engineer. If interested, revert me back with your Updated resume, along with contact details, work authorization.
Job Role: Data Engineer
Duration: Long-Term
Position Type: W2/ 1099
Visa: Any
Job Description:
Technical/Functional Skills
Successful candidates will have:
Hands on experience in Dremio and Scripting
Hands on Scripting experience in Python and Flask
Hands on experience in Integrating data from multiple source systems (PostgreSQL and CSV)
Experience with Apache Superset
Ability to write complex SQL queries
Experience in building data pipelines using PostgreSQL, Oracle database, Dremio, AWS Open Search
Experience with Docker Containerization of Code
Work as a part of a scrum team in Agile methodology
ETL, Data Pipelines, PostgreSQL, SQL Queries, Scripting
Thanks & Regards
Zllius Inc.
Job Types: Full-time, Contract
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
North Chicago, IL 60064: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person","$100,218 /yr (est.)",Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Acuity Brands
3.6",3.6,"Conyers, GA",Data Engineer Senior,"We Light the Way!

Acuity Brands, Inc. (NYSE: AYI) is a market-leading industrial technology company. We use technology to solve problems in spaces and light. Through our two business segments, Acuity Brands Lighting and Lighting Controls (“ABL”) and the Intelligent Spaces Group (“ISG”), we design, manufacture, and bring to market products and services that make the world more brilliant, productive, and connected. We achieve growth through the development of innovative new products and services, including lighting, lighting controls, building management systems, and location-aware applications.
Job Summary
Acuity’s Business Intelligence team is comprised of people who are passionate about data. We believe accurate, timely and understandable data is vital to a data driven culture. We are devoted to aligning Acuity’s data to serve the needs of the enterprise and its customers. You will be joining a team of seasoned Business Intelligence professionals well versed in architecting bespoke BI (Business Intelligence) applications and implementing Microsoft Azure BI products.
We are seeking a talented and enthusiastic individual to be a Senor Data Engineer on our Business Intelligence Team as we transform Acuity Brands analytics and migrate our BI platform to Azure and Power BI. This position will work closely with BI Architects to bring to deliver end to end BI solutions that are innovative, scalable, and responsive.
Key Tasks & Responsibilities (Essential Functions)
Research, architect, drive, and deploy scalable, resilient cloud agnostic BI solutions to address Acuity current and future business needs and obligations
Partner with Data Architect, Solution Architect, and BI Product Managers to drive technology transformation on the BI platform to ensure the BI platform remains current and responsive
Mentor and guide Senior BI Developers to ensure adherence to BI standards and procedures
Partner with Data Architect to deliver integrated end to end data engineering solutions
Write application and cloud-based data processing code to transform inbound data to meet business requirements
Design and develop data models leveraging advanced modeling techniques to handle large and or complex data
Modify and optimize data engineering processes to handle ever-growing, complex, diverse data formats, sources, and pipelines
Work with infrastructure partners to tune and optimize code and BI resources such as but not limited to (index tuning, partitioning, caching, buffer tuning, and data archiving strategies.
Proactively estimate and plan development work and track performance to deliver work on schedule
Create and maintain current documentation in GIT (c4 and Plant UML)
Education (minimum education required)
Bachelor of Science
Preferred Education (i.e. type of degree)
Bachelor of Science in Computer Science or Information Systems
Experience (minimum experience required)
Bachelor’s Degree in Computer Science, MIS (Management Information System), or other technical/analytical field (or equivalent experience)
2 Azure Certifications
Working knowledge of data warehousing principles (Kimball, Inmon, Hybrid)
4-7 years or more database programming experience (SQL (preferred), Oracle, DB2)
4-7 years or more of BI experience (Power BI, Tableau, Qlik Sense/View, D3.js, SAP Business Objects, IBM Cognos)
4-7 years or more of working with Microsoft BI stack (SSIS, SSAS, T-SQL)
4-7 years or more of developing and enhancing ETL packages
4-7 years or more of working with and or constructing API (Push, Get, Post)
4-7 years or more of advanced experience identifying and optimizing database objects
2 years or more of application development (C# or .NET)
3 years or more experience working in Python or Scala
We invite you to apply today to join us as We Light the Way to a Brilliant, Productive, and Connected World!

We value diversity and are an equal opportunity employer. All qualified applicants will be considered for employment without regards to race, color, age, gender, sexual orientation, gender identity and expression, ethnicity or national origin, disability, pregnancy, religion, covered veteran status, protected genetic information, or any other characteristic protected by law.
Please click here and here for more information.

Accommodation for Applicants with Disabilities: As an equal opportunity employer, Acuity Brands is committed to providing reasonable accommodations in its application process for qualified individuals with disabilities and disabled veterans. If you have difficulty using our online system due to a disability and need an accommodation, you may contact us at (770) 922-9000. Please clearly indicate what type of accommodation you are requesting and for what requisition.

Any unsolicited resumes sent to Acuity Brands from a third party, such as an Agency recruiter, including unsolicited resumes sent to an Acuity Brands mailing address, fax machine or email address, directly to Acuity Brands employees, or to Acuity Brands resume database will be considered Acuity Brands property. Acuity Brands will NOT pay a fee for any placement resulting from the receipt of an unsolicited resume.

Acuity Brands will consider any candidate for whom an Agency has submitted an unsolicited resume to have been referred by the Agency free of any charges or fees. This includes any Agency that is an approved/engaged vendor, but does not have the appropriate approvals to be engaged on a search.

E-Verify Participation Poster
e-verify.gov
eeoc.gov","$92,338 /yr (est.)",10000+ Employees,Company - Public,Manufacturing,Electronics Manufacturing,2001,$1 to $5 billion (USD)
"AgileEngine
5.0",5.0,Remote,Senior Big Data SDET Engineer,"Must haves
Bachelor's degree in IT or Computer Science or other related discipline. Master’s degree preferred
Competency in Spark and Scala
SQL or data query background (e.g. panda, koalas, pyspark dataframes)
Experience with AWS
Strong quality mindset
Ability to automate based on functional and data requirements
Mid to senior level experience
Nice to haves
Experience with Java
Familiarity with data pipelines (ETL, ELT) or desire to work with them
Desire to leave the space better than they found it: improve frameworks and processes
The benefits of joining us
Professional growth: accelerate your professional journey with mentorship, TechTalks, and personalized growth roadmaps
Competitive compensation: we match your ever-growing skills, talent, and contributions with competitive USD-based compensation and budgets for education, fitness, and team activities
A selection of exciting projects: join projects with modern solutions development and top-tier clients that include Fortune 500 enterprises and leading product brands
Flextime: tailor your schedule for an optimal work-life balance, by having the options of working from home and going to the office – whatever makes you the happiest and most productive.
Job Types: Full-time, Contract
Experience level:
4 years
Work Location: Remote",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2010,Unknown / Non-Applicable
"Geopaq Logic
4.7",4.7,"Ridgefield, NJ",Sr. Cloud Engineer--data migrate --W2,"Role: Cloud Engineer
Level: Middle (must have 5+ year of related experience)
Contract period: 6 to 8 months upon project period
Work Location: Ridgefield Park, NJ (must be able to work onsite 3 day/wk currently but not permanently hybrid)
- Travel: up to 30% (domestic)
Responsibilities:
Migrate multi-tiered workloads from on-prem to company cloud
Implement new workload/service using company private cloud
Support requirements gathering for migration/implementation
Test and implement cloud-based services to support ongoing cloud migration
Develop and maintain secure and reliable cloud infrastructure (backup, monitoring and secure logging)
Provide support for security policies and procedures
Develop and maintain technical documentation
Troubleshoot technical issues
Requirements/Qualifications:
Must have hands-on experience with cluster configuration (Windows, Linux)
Worked on data/DB migration
At least 5 years of experience in the field of cloud computing
Knowledge on cloud computing technology (AWS, Azure, GCP)
Experience with onsite to cloud migration assessment and implementation
Worked and hands-on experience in cloud migration and in a multi-cloud environment
Experience with networking, compute infrastructure such as servers, database, firewall, load balance
Knowledge of system monitoring, capacity planning and understand architecture principles across infrastructure platform
Hands-on experience with administrator on variety of Linux/Windows systems
Experience with information security practices and procedures
Documentation skill using MS Office products.
Job Types: Full-time, Contract
Salary: $9,500.00 - $10,000.00 per month
Benefits:
Health insurance
Schedule:
8 hour shift
Experience:
Azure: 1 year (Preferred)
AWS: 1 year (Preferred)
Security clearance:
Confidential (Preferred)
Work Location: On the road","$9,750 /mo (est.)",1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,2014,$1 to $5 million (USD)
"Calhoun International, LLC
3.7",3.7,"Washington, DC",Data Engineer (Senior),"About Us:
Calhoun International is a Professional Services company providing innovative solutions to our clients. Our expertise ranges from strategic intelligence analysis, expert instruction on intelligence analysis and sensors, cyberspace operations, information systems training, and knowledge management services among others. Calhoun International is located in Tampa, FL with employees in Florida, Virginia, Maryland, Washington, D.C. and overseas. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, or national origin.
Responsibilities:
Executes and on occasion, leads research, analysis, and evaluation efforts as well as studies, analyses, assessments, and technical reports, which may include the use of existing documents, databases, models, architectures and simulations. Deliverables produced shall be in a variety of formats in response to a wide range of requirements and delivery schedules. Provides mid to high-level analytical assessments and advice on complex issues, which require extensive knowledge of the subject matter. May attend various types of symposia and meetings at the ARSTAF and DOD level.
Requirements:
Minimum Education: bachelor’s degree; advanced intelligence discipline training; or other equivalent DoD or service Intelligence experience.
Minimum Experience: Fourteen (14) years of experience as an Army Intelligence analyst with experience from tactical to strategic. Experience shall have been within Two (2) years of starting on this contract.
Has served as a staff action officer at the HQDA (DCS, G-2 preferred) or Joint or a closely related DOD organization/agency.
Demonstrated SME level of knowledge of intelligence fusion systems, capabilities / employment, training, associated R&D efforts and program budget processes.
Demonstrated knowledge of Joint and Army processes –JCIDS, TAA, PED, JUONS/ONS, and CONOPS.
Experience in preparation and presentation of briefings on projects, studies and analysis to senior leaders and other ARSTAF action officers.
Desired:
Graduate from the Command and General Staff College or similar Senior Staff College
Security Clearance:
Active Top Secret with SCI eligibility required.","$127,546 /yr (est.)",1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,#N/A,$1 to $5 million (USD)
"Gridiron IT
4.5",4.5,Remote,Sr. Data Engineer,"GridironIT is seeking a Data Engineer.
Responsibilities:
We are looking for seasoned Data Engineer to work with our team and our clients to develop enterprise grade data platforms, services, and pipelines. We are looking for a more than just a ""Data Engineer"", but a technologist with excellent communication and customer service skills and a passion for data and problem solving.
Lead and architect migration of data environments with performance and reliability.
Assess and understand the ETL jobs, workflows, BI tools, and reports
Address technical inquiries concerning customization, integration, enterprise architecture and general feature / functionality of data products
Experience in crafting database / data warehouse solutions in cloud (Preferably AWS. Alternatively Azure, GCP).
Key must have skill sets – Python, AWS
Support an Agile software development lifecycle
You will contribute to the growth of our Data Exploitation Practice!
Qualifications:
US Citizen Only
Ability to hold a position of public trust with the US government.
8+ years industry experience coding commercial software and a passion for solving complex problems.
8+ years direct experience in Data Engineering with experience in tools such as:Big data tools: Hadoop, Spark, Kafka, etc.
Relational SQL and NoSQL databases, including Postgres and Cassandra
Data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
AWS cloud services: EC2, EMR, RDS, Redshift
Data streaming systems: Storm, Spark-Streaming, etc.
Search tools: Solr, Lucene, Elasticsearch
Object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Amazon S3, Athena, Redshift Spectrum, AWS Glue, AWS Glue Catalog, AWS Functions, and Amazon EC2 with SQL Server Developer
Advanced working SQL knowledge and experience working with relational databases, query authoring and optimization (SQL) as well as working familiarity with a variety of databases.
Experience with message queuing, stream processing, and highly scalable ‘big data’ data stores.
Experience manipulating, processing, and extracting value from large, disconnected datasets.
Experience manipulating structured and unstructured data for analysis
Experience constructing complex queries to analyze results using databases or in a data processing development environment
Experience with data modeling tools and process
Experience architecting data systems (transactional and warehouses)
Experience aggregating results and/or compiling information for reporting from multiple datasets
Experience working in an Agile environment
Experience supporting project teams of developers and data scientists who build web-based interfaces, dashboards, reports, and analytics/machine learning models
Experience with Informix and Data Stage
Job Type: Full-time
Pay: $84,436.60 - $150,799.40 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Monday to Friday
Experience:
AWS Glue: 5 years (Required)
SQL: 8 years (Required)
Data warehouse: 5 years (Required)
Work Location: Remote","$117,618 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable
"Dentsu Aegis Network
3.6",3.6,"Raleigh, NC",Data Engineer - BI Developer,"Company Description

Dentsu International is a modern marketing solutions company. Our mission is to help clients navigate, progress and thrive in a world of change. Businesses rely on our integrated network of agencies and specialized practices to champion meaningful progress through creative, media, commerce, data, and technology. Part of Dentsu Group, our global network comprises 66,000 diverse people in 143 countries, who are dedicated to teaming for growth and good. Some of our award-winning agencies include 360i, Carat, dentsumcgarrybowen, DEG, dentsuX, iProspect and Merkle. Follow us on Twitter @DentsuUSA and visit dentsu.com/us.
Part of Dentsu International, Dentsu Creative is a Global Creative Network that transforms brands and businesses through the power of Modern Creativity. Led by Global Chief Creative Officer Fred Levron, 9,000 experts across the globe work seamlessly together to deliver ideas that Create Culture, Shape Society and Invent the Future. Dentsu Creative was launched in June 2022 to address a client need for simplicity and will be Dentsu International’s sole creative network by the end of 2022.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.

Job Description

The Data Engineer is a new key role within the Business Platforms Americas team. We are looking for a well-rounded Senior BI Developer expertise with strong knowledge of MS SQL, PowerBI, and data warehousing techniques. This is a unique opportunity to be involved in delivering leading-edge business analytics using the latest and greatest cutting-edge BI tools, such as cloud-based databases, self-service analytics and leading visualization tools enabling the company’s aim to become a fully digital organization.
Key Responsibilities
Collaborate with the BI Dev team members to evaluate, design, develop BI reports and dashboards according to functional specifications while maintaining data integrity and data quality
Deliver Technical Design Document capturing specific processes and data flows, data definitions and relevant business rules
Apply best practices of data integration for data quality and automation
Work with business analysts to understand business requirements and use cases to write and assign technical stories and tasks
Work independently within guidelines, responsible for initiating, planning, executing, controlling, and implementation of projects using a formal project management and agile methodology
Work collaboratively with key stakeholders to translate business information needs into well-defined data requirements to implement the BI solutions
Work with team to provide support for existing analytics and PowerBI reporting platforms
Coaching, mentoring, and providing technical direction and training to other IT personnel
Working with BI & Analytic teams to develop and establish BI road Map/Vision

Qualifications

Experience:
Excellent communication skills
Over 7-10 years of experience in Data warehousing and Business Intelligence
Over 5 years’ experience in a Business Intelligence Analyst or Developer roles
Over 4 years’ experience using ADF for data warehousing
Experience in designing and performance tuning data warehouses and data lakes.
2+ years experience in developing data models and dashboards using Power BI within an IT department
Being delivery-focused with a can-do attitude in a sometimes-challenging environment is essential.
Experience using Power BI to visualize data held in SQL Server
Experience working with finance data highly desirable
Other key Competencies:
Strong communications skills and ability to turn business requirements into technical solutions
Experience in developing data lakes and data warehouses using Microsoft Azure
Demonstrable experience designing high-quality dashboards using Power BI
Strong database design skills, including an understanding of both normalized form and dimensional form databases.
In-depth knowledge and experience of data-warehousing strategies and techniques e.g., Kimble Data warehousing
Experience in Cloud based data integration tools like Azure Data Factory
Experience in power bi data modelling and DAX is preferred
Experience in Azure Dev Ops or JIRA is a plus
Familiarity with agile development techniques and objectives

Additional Information

The anticipated salary range for this position is $94,000-146K. Salary is based on a wide range of factors that include relevant experience, knowledge, skills, other job-related qualifications, and geography. A range of medical, dental, vision, 401(k) matching, paid time off, and/or other benefits also are available. For more information regarding dentsu benefits, please visit https://dentsubenefitsplus.com/.

Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact recruiting@dentsuaegis.com if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.

#LI-AJ1
About dentsu
Dentsu is the network designed for what’s next, helping clients predict and plan for disruptive future opportunities in the sustainable economy. Taking a people-centered approach to business transformation, dentsu combines Japanese innovation with a diverse, global perspective to drive client growth and to shape society www.dentsu.com.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.
Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact your recruiter if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.","$120,000 /yr (est.)",10000+ Employees,Company - Public,Media & Communication,Advertising & Public Relations,#N/A,Unknown / Non-Applicable
"Braintrust
4.6",4.6,"San Francisco, CA",Data Visualization Engineer - Tableau,"ABOUT US:
Braintrust is a user-owned talent network that connects you with great jobs with no fees or membership costs—so you keep 100% of what you earn.

JOB TYPE: Freelance, Contract Position (no agencies/C2C - see notes below)
LOCATION: Remote - Work from anywhere
HOURLY RANGE: Our client is looking to pay $75 – $100/hr
ESTIMATED DURATION: 20h/week - Long term

ABOUT THE HIRING PROCESS:
When you join Braintrust, you will be invited to a screening process for Braintrust to learn more about your previous work experiences. Once completed, you will have access to the employer for this role and other top companies that seek high-quality talent. Apply to this job to kick off the process.
THE OPPORTUNITY
Skills Required:
Expertise in handling Tableau products and building in Tableau
Problem-solving skills
Proficiency in handling SQL and databases
Should be good in written & verbal communication
Good to Have Skills:
English and French speaking
SaaS industry experience
Roles & Responsibilities:
Elicit business requirements and develop measurement plans for strategic programs
Collect and analyze data through manual and automated collection methods
Translate abstract data into easily-digestible and highly-visual dashboard applications and executive-level report deliverables
Collaborate with other analysts, data engineers, and data scientists to provide exceptional customer support to our internal stakeholders
Analyze large data sets and translate abstract data into highly visual and easily-digestible
What you’ll be working on
As a Senior Developer, you are responsible for the development, support, maintenance, and implementation of a complex project module.
You should have good experience in the application of standard software development principles. You should be able to work as an independent team member, capable of applying judgment to plan and execute your tasks. You should have in-depth knowledge of at least one development technology/programming language.
Apply Now!
Braintrust Job ID: 6721
C2C Candidates: This role is not available to C2C candidates working with an agency. If you are a professional contractor who has created an LLC/corp around their consulting practice, this is well aligned with Braintrust and we’d welcome your application.

Braintrust values the multitude of talents and perspectives that a diverse workforce brings. All qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status.

This is a remote position.",$87.50 /hr (est.),1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,2018,$100 to $500 million (USD)
"Karius
3.5",3.5,"Redwood City, CA",SENIOR STAFF DATA ENGINEER,"About Karius
Karius is a venture-backed life science startup that is transforming the way pathogens
and other microbes are observed throughout the body. By unlocking the information
present in microbial cell-free DNA, helping doctors quickly solve their most
challenging cases, providing industry partners with access to 1000’s of biomarkers to
accelerate clinical trials, discover new microbes, and reduce patient suffering worldwide.
Karius aims to conquer infectious diseases through innovations around genomic
sequencing and machine learning. The company’s platform is already delivering
unprecedented insights into the microbial landscape, providing clinicians with a
a comprehensive test capable of identifying more than a thousand pathogens directly
from blood, and helping the industry accelerate the development of therapeutic
solutions. The Karius test we provide today is one of the most advanced solutions
available to physicians who aim to deliver better care to many otherwise ineffectively
treated patients.

Position Summary
Karius is building AI-driven data analytics pipelines to deliver life-saving results in the
highly complex infectious disease landscape. We are seeking a seasoned Senior Staff
Data Engineer in Redwood City, CA to lead the design and development of a scalable
data platform to meet our rapid business growth. Senior Staff Data Engineer will be
responsible for defining the technology roadmap, and developing and optimizing the
data platform to enable us to extract values from large amounts of genomic, clinical,
operation and clinical data to provide actionable insights to serve the patients and
develop innovative products. In this regard, the Senior Staff Data Engineer will work
with key stakeholders within the company to understand our data landscape and the
core needs for data governance and usage.

Primary Responsibilities
Design, develop, and operate a scalable data platform that ingests, stores, and
aggregates various datasets to meet the defined requirements;
As the primary subject matter expert in the data engineering domain, evaluate
technology trends in the data industry, identify those technologies relevant to
the company’s business objectives, and develop a roadmap to update the
company’s data platform;
Provide Machine Learning (“ML”) data platform capabilities for R&D and
Analytics teams to perform data preparation, model training and management,
and run experiments against clinical and genomic datasets;
Train the R&D and Analytics teams on using Karius data toolsets and mentor
and support them throughout their research and development efforts;
Build and maintain data ETL/ELT pipelines to source and aggregate the
required internal data to calculate operational and commercial Key Performance
Indicators (“KPIs”) and various data analysis and reporting needs;
Develop integrations with Karius and 3rd party systems to source, qualify and
ingest various datasets; work closely with cross-functional groups and
stakeholders, such as the product, engineering, medical, and scientiﬁc teams,
for data modeling and general life cycle management;
Provide data analytics and visualization tools to extract valuable insights from
the data and enable data-driven decisions; and
Work closely with the Security and Compliance teams, and deploy necessary
data governance to meet the regulatory and legal requirements.

Position Minimum Requirements
At least a Bachelor’s degree in Computer Science, Data Science, or Software
Engineering, Electrical Engineering, or Bio-Engineering (or its foreign equivalent);
plus
At least 10 years of experience as a Software or Data Engineer or similar
position, including at least 5 years in a senior or higher-level position;

AND (or experience must include):

4+ years of hands-on design, development and operation of data solutions using
the following data technologies: Spark and Spark Streaming, Presto, Parquet,
MLﬂow, Kafka, and ETL tools such as Stitch or FiveTran;
4+ years of hands-on experience with design, development and maintenance of
structured, semi and non-structured (NoSQL) data stores, such as MySQL,
PostgreSQL, AWS Redshift, Teradata, Graph databases like Neo4j, and
Databricks Lakehouse;
4+ years of hands-on development and operation of workflows and jobs using
task orchestration engines such as Airﬂow, Argo, NextFlow, Dollar U and Tidal;
4+ years of hands-on experience building and operating data solutions on
operating systems such as Linux and Unix hosted in Amazon Web Services
(AWS) cloud;
5+ years of hands-on building and operation of scalable infrastructure to support
batch, micro-batch, and stream data processing for large volumes of data;
5+ years of hands-on experience designing and implementing enterprise data
warehouse/Lakehouse solutions to house business and technical datasets and
derive KPI dimensions for consumption;
Demonstrated experience with enterprise data modeling in healthcare and/or life
science sectors;
Demonstrated experience with the development and operation of visualization
and dashboards for business KPI reporting using tools such as Tableau or
Looker;
Proficiency in Python and PySpark;
Automation of Data Testing using scripting;
Experience developing and managing technical and administrative controls for
data governance and regulatory compliance in the healthcare and/or life sciences
sectors;
Experience mentoring and coaching junior data engineers; and
Cross-functional project management experience.

Travel: No travel is required.

Reports to: VP, Engineering
$180,000 - $240,000 a year","$210,000 /yr (est.)",51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,2014,Unknown / Non-Applicable
"HP
4.2",4.2,"Vancouver, WA",Data Engineer,"Applies basic foundation of a function's principles, theories and concepts to assignments of limited scope. Uses professional concepts and theoretical knowledge acquired through specialized training, education or previous experience. Develops expertise and practical knowledge of applications within business environment. Acts as team member by providing information, analysis and recommendations in support of team efforts. Exercises independent judgment within defined parameters.
Responsibilities
Codes limited enhancements, updates, and programming changes for portions and subsystems of data pipelines, repositories or models for structured/unstructured data.
Analyzes design and determines coding, programming, and integration activities required based on objectives and guidance from more senior project team members.
Executes established portions of testing plans, protocols, and documentation for assigned portion of application; identifies and debugs issues with code and suggests changes or improvements.
Participates as a member of a project team of other data science professionals to develop reliable, cost effective and high-quality solutions for assigned data system, model, or component.
Knowledge & Skills
Using data engineering tools, languages, frameworks to cleanse, mine and explore data.
Basic understanding of SQL and NoSQL & relational based systems along with complex, distributed and massively parallel systems.
Ability to apply analytical and problem-solving skills.
Ability to understand complex data structures.
Understanding of database technologies and management systems.
Understanding of database architecture testing methodology, including execution of test plans, debugging, and testing scripts and tools.
Strong written and verbal communication skills; mastery in English and local language.
Scope & Impact
Collaborates with peers, senior engineers, data scientists and project team.
Typically partners with more senior Individual Contributors.
Supports projects requiring data engineering solutions expertise.
Education & Experience
Bachelor's or Master's degree in Computer Science, Information Systems, Engineering, or equivalent.

About HP

You’re out to reimagine and reinvent what’s possible—in your career as well as the world around you.
So are we. We love taking on tough challenges, disrupting the status quo, and creating what’s next. We’re in search of talented people who are inspired by big challenges, driven to learn and grow, and dedicated to making a meaningful difference.

HP is a technology company that operates in more than 170 countries around the world united in creating technology that makes life better for everyone, everywhere.

Our history: HP’s commitment to diversity, equity and inclusion – it's just who we are.
From the boardroom to factory floor, we create a culture where everyone is respected and where people can be themselves, while being a part of something bigger than themselves. We celebrate the notion that you can belong at HP and bring your authentic self to work each and every day. When you do that, you’re more innovative and that helps grow our bottom line. Come to HP and thrive!",#N/A,10000+ Employees,Company - Public,Information Technology,Computer Hardware Development,1939,Unknown / Non-Applicable
"Royalty Staffing
3.8",3.8,"Moline, IL",Data Engineer,"Senior software engineers specializing in Data Engineering and Databricks to work on projects for one of its top clients (John Deere).
Required Skills (MUST HAVE):
Python, AWS, Apache Spark
3 years of Data Engineering work experience building Pipelines and ETL processes.
Client project experience in Databricks implementation
Client project experience working with Big Data
5 years or more experience in building backend systems in AWS / JAVA J2EE in the design, development, testing, and integration of highly complex backend
Experience with CI/CD build processes and configuration","$82,819 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Tremco Incorporated
3.7",3.7,United States,Data Engineer,"GENERAL PURPOSE OF THE JOB:
*100% REMOTE / TELEWORK*
Division - Tremco Roofing & Building Maintenance
We are seeking an experienced and skilled Data Engineer to join our team! We are looking for a candidate that thrives in a collaborative environment, is a self-starter, and is passionate about data science. Our data science team is the foundation for data-driven business decisions and is leading the way for continued growth in innovative markets within the construction industry.
On the Data Science team, the Data Engineer’s purpose is to design, develop, and maintain the company's data infrastructure, pipelines, and workflows. They are responsible for merging predictive and prescriptive modeling to ensure it stays consistent with data flowing across the organization. They work closely with data scientists, analysts, and other stakeholders to ensure the data is properly collected, stored, processed, and analyzed to drive informed business decisions.
If you are passionate about data science and want to work with a dynamic team of professionals, please apply today!
ESSENTIAL DUTIES AND RESPONSIBILITIES:
Design, develop, build, and maintain the company's data infrastructure, pipelines, and workflows and all associated engineering tasks.
Develop and maintain ETL (Extract, Transform, Load) processes to collect and integrate data from various sources.
Build and maintain data APIs to enable data access across the organization.
Develop and implement scalable data solutions to optimize data processing, storage, and retrieval.
Develop and maintain documentation for data pipelines, including data dictionaries, standard operating procedures, and data flow diagrams.
Work with unstructured data and develop data models to enable data analysis and insights.
Identify any hidden patterns or data inconsistencies and work along with similar ad-hoc analysis
Ensure data quality, consistency, and accuracy and is properly structured and formatted to support analyses.
Ensure data security, integrity, and compliance with data privacy regulations.
Troubleshoot and resolve data-related issues, including data quality, integrity, and performance.
Continuously monitor, maintain, and optimize the health and performance of the data infrastructure, pipelines, and workflows
Collaborate with data scientists, analysts, and other stakeholders to understand data requirements.
Stay up to date with the latest advancements in data engineering and recommend new technologies, tools, and processes to improve efficiency and productivity.
EDUCATION:
Bachelor's or Master's degree in Information Technology, Computer Science, or a related field
EXPERIENCE:
3+ years of experience in a data science or related role
CERTIFICATES, LICENSES, REGISTRATIONS:
Not Required but beneficial:
Certified SQL
Certified SQL, Advanced Queries
Python for Data Science & Machine Learning
R for Data Science & Machine Learning
Databricks Lakehouse Fundamentals
OTHER SKILLS AND ABILITIES:
Proficiency in programming languages such as Python, R, and SQL
Strong understanding of database technologies and SQL queries
Strong experience with ETL processes, data integration, and data modeling
Experience with cloud-based data storage and computing services, specifically Azure
Excellent problem-solving and analytical skills
Experience with data visualization tools such as Tableau or Power BI
Experience with data lakehouse tools such as Synapse (data lake) or databricks
Excellent communication and collaboration skills
Ability to work independently and prioritize tasks in a fast-paced & dynamic environment

Qualified applicants will receive consideration for employment without regard to their race, color, religion, national origin, sex, sexual orientation, gender identity, protected veteran status or disability.",#N/A,1001 to 5000 Employees,Subsidiary or Business Segment,Manufacturing,Chemical Manufacturing,1997,$500 million to $1 billion (USD)
"Spectrum Communications & Consulting Inc.
4.3",4.3,"Chicago, IL",Data Engineer,"What do incubators and Spectrum have in common? Well, they’re great for growth, and even better for stability. As an innovative software development and digital marketing company pioneering the field of artificial intelligence, we can offer our newest Data Engineer the best of both words – a high energy, forward-thinking start-up culture, inside of a well-established, profitable, and stable structure . if you’re interested in getting your hands dirty and inciting change into a larger organization with a vision to change the world uses data today, then please read on.
Responsibilities
Beyond working with state of the art technology you will have many different fantastic projects to work on as a Data Engineer at Spectrum. Here are just a few different responsibilities you can expect off the bat:
Design, build and maintain both new and existing data infrastructures using tools including (but not limited to) Microsoft SQL Server, Azure Tables, Azure Blobs, and Azure Cosmos
Determine the right database system for a provided data set
Drafting database/table schemas
Properly index tables when applicable to ensure query performance
Create data pipelines for transforming new datasets into consistent, reliable data systems that are usable across multiple platforms
Identify optimizations and improvements in existing data pipelines
Create solutions for data validation to ensure data integrity, accuracy, and consistency
Ensure data is readily available for analysis and development usage
Shifting existing big data pipelines
Come up with data validation solutions for both initial data imports as well as results
Audit the automation process to make sure there are no gaps, redundancies, or faults in the data pipeline that would cause damage to data integrity
Planning query efficient data store schemas with respect to the data requirements of our internal data warehouse
Communicate with data scientists and other software engineers to build proper data pipelines for efficient and cost-effective querying
Develop, construct, test, and maintain architectures with the help and feedback of software engineers
Determine and implement proper data schemas that align with Spectrum’s and Spectrum’s client’s business objectives
Recommend and implement ways to optimize data reliability, efficiency, and quality
Consult and suggest different technologies for our team to test and utilize together
Incorporate the business objectives of key stakeholders in the architecture of our data warehouse
Research new and interesting data acquisition opportunities
Some Characteristics That Define You
We understand that as a Data Engineer for Spectrum, you have many different professional goals and personal interests. As such here are just a few different things that typically define our team members on the Data Science team:
Self-Starter. Building a data warehouse is no simple task. You will need to come in with a self-starter attitude to not only make this data engineering role what you want it to be, but make a stellar and efficient data warehouse while you’re at it.
Analytical. In order to solve problems and build innovative new digital marketing campaigns, it is essential that you know how to take an idea and analyze it from all of its angles.
Developer. In the ever changing world of artificial intelligence, it’s not enough just to build models. You bring to the table an eye for data patterns along with a knack for relational database engineering.
Patient. As a data engineer, you know that you work with extremely large data sets on a daily basis. As such we are looking for someone who is not only meticulous, but patient enough to sit and sift through that data in a thorough way.
Creative. Beyond just analyzing data sets, you are an explorer and a puzzle solver. Pulling insights out of your data and understanding how those insights can better shape our tools is something that you live to do.
Student. More so than most industries, the field of data science is always changing and evolving. As such, you are always looking to learn new things and gain new skills.
Required Skills and Experience
On top of the many intangible skills you bring to the table, there are many skills that can help improve the efficiencies and success of your work at Spectrum. Here are a few of those required skills and experience that you will come in with as a Data Engineer on our team:
A bachelor’s degree/pursuing a bachelor’s degree in computer science, mathematics, statistics, information systems, or a related field
Experience with statistical modeling
1-3 years working experience with Python and/or SAS languages
1-3 years working experience with SQL databases and database querying languages
1-3 years working experience with C#/.NET
Familiarity with Microsoft Azure
Familiarity with Graph database structures
Experience with both RDBMS and TDMS
Experience with data mining and data cleaning
Experience with data visualization and reporting techniques
Written and verbal expression
Benefits
As a Data Engineer at Spectrum there are a ton of fantastic perks and benefits that come along with your work. Here are just a few of the benefits you can expect when joining the Spectrum family:
Comprehensive medical & dental insurance
Retirement planning & company matching
Generous PTO, including sick days & holidays
Coworking space access for remote and hybrid work options
Year-round gym memberships
Paid continuing education
Hybrid work from Home , meetings a few times a month to strategize new projects
About Us
Our mission at Spectrum has always been the same— growth. Whether that be helping our customers find quality new business, developing and challenging our team members, or evolving our products and services with advancements in technology and best practices, we have always been looking towards bettering ourselves for the future. Now as we continue to grow, we find ourselves as not only the nation’s leading digital marketing and software provider for the home services industry, but an innovator and ground-shaker for the world of artificial intelligence as well. From marketing automation software powered by AI, to top notch digital marketing services via those same AI insights, we love what we do and are excited to continue to innovate for the future together.
INDSPCI
Responsibilities
Design, build and maintain both new and existing data infrastructures using tools including (but not limited to) Microsoft SQL Server, Azure Tables, Azure Blobs, and Azure Cosmos
Determine the right database system for a provided data set
Drafting database/table schemas
Properly index tables when applicable to ensure query performance
Create data pipelines for transforming new datasets into consistent, reliable data systems that are usable across multiple platforms
Identify optimizations and improvements in existing data pipelines
Create solutions for data validation to ensure data integrity, accuracy, and consistency
Ensure data is readily available for analysis and development usage
Shifting existing big data pipelines
Come up with data validation solutions for both initial data imports as well as results
Audit the automation process to make sure there are no gaps, redundancies, or faults in the data pipeline that would cause damage to data integrity
Beyond working with state of the art technology you will have many different fantastic projects to work on as a Data Engineer at Spectrum. Here are just a few different responsibilities you can expect off the bat:
Planning query efficient data store schemas with respect to the data requirements of our internal data warehouse
Communicate with data scientists and other software engineers to build proper data pipelines for efficient and cost-effective querying
Develop, construct, test, and maintain architectures with the help and feedback of software engineers
Determine and implement proper data schemas that align with Spectrum’s and Spectrum’s client’s business objectives
Recommend and implement ways to optimize data reliability, efficiency, and quality
Consult and suggest different technologies for our team to test and utilize together
Incorporate the business objectives of key stakeholders in the architecture of our data warehouse
Research new and interesting data acquisition opportunities
Some Characteristics That Define You
We understand that as a Data Engineer for Spectrum, you have many different professional goals and personal interests. As such here are just a few different things that typically define our team members on the Data Science team:
Self-Starter. Building a data warehouse is no simple task. You will need to come in with a self-starter attitude to not only make this data engineering role what you want it to be, but make a stellar and efficient data warehouse while you’re at it.
Analytical. In order to solve problems and build innovative new digital marketing campaigns, it is essential that you know how to take an idea and analyze it from all of its angles.
Developer. In the ever changing world of artificial intelligence, it’s not enough just to build models. You bring to the table an eye for data patterns along with a knack for relational database engineering.
Patient. As a data engineer, you know that you work with extremely large data sets on a daily basis. As such we are looking for someone who is not only meticulous, but patient enough to sit and sift through that data in a thorough way.
Creative. Beyond just analyzing data sets, you are an explorer and a puzzle solver. Pulling insights out of your data and understanding how those insights can better shape our tools is something that you live to do.
Student. More so than most industries, the field of data science is always changing and evolving. As such, you are always looking to learn new things and gain new skills.
Required Skills and Experience
On top of the many intangible skills you bring to the table, there are many skills that can help improve the efficiencies and success of your work at Spectrum. Here are a few of those required skills and experience that you will come in with as a Data Engineer on our team:
A bachelor’s degree/pursuing a bachelor’s degree in computer science, mathematics, statistics, information systems, or a related field
Experience with statistical modeling
1-3 years working experience with Python and/or SAS languages
1-3 years working experience with SQL databases and database querying languages
1-3 years working experience with C#/.NET
Familiarity with Microsoft Azure
Familiarity with Graph database structures
Experience with both RDBMS and TDMS
Experience with data mining and data cleaning
Experience with data visualization and reporting techniques
Written and verbal expression
Benefits
As a Data Engineer at Spectrum there are a ton of fantastic perks and benefits that come along with your work. Here are just a few of the benefits you can expect when joining the Spectrum family:
Comprehensive medical & dental insurance
Retirement planning & company matching
Generous PTO, including sick days & holidays
Coworking space access for remote and hybrid work options
Year-round gym memberships
Paid continuing education
Hybrid work from Home , meetings a few times a month to strategize new projects
About Us
Our mission at Spectrum has always been the same— growth. Whether that be helping our customers find quality new business, developing and challenging our team members, or evolving our products and services with advancements in technology and best practices, we have always been looking towards bettering ourselves for the future. Now as we continue to grow, we find ourselves as not only the nation’s leading digital marketing and software provider for the home services industry, but an innovator and ground-shaker for the world of artificial intelligence as well. From marketing automation software powered by AI, to top notch digital marketing services via those same AI insights, we love what we do and are excited to continue to innovate for the future together.
INDSPCI
Hours
40","$92,696 /yr (est.)",51 to 200 Employees,Company - Private,Media & Communication,Advertising & Public Relations,1992,$5 to $25 million (USD)
"CEP Global
5.0",5.0,"San Francisco, CA",Associate Data Systems Engineer,"(Data Systems Engineer I)
San Francisco, CA / Cambridge, MA
We believe, and research shows, that student and stakeholder feedback matters for school improvement and student success. That’s why, in 2008, we created YouthTruth – to help educators harness student and stakeholder feedback to accelerate improvements. We’ve heard from over two million students as well as hundreds of thousands of family and staff members across 39 states. When you join YouthTruth, you join a small and collaborative team that has a big impact on schools across the U.S. and the education sector.
As one of the go-to technical experts in the room, the Associate Data Systems Engineer will be instrumental in projects that improve the organization’s use of data, identify and drive efficiencies in our processes, and work closely with staff to maintain and upgrade our in-house online reporting system. To that end, the Associate Data Systems Engineer will build and develop cloud-based data pipelines and architecture while maintaining and improving back-end Python applications for this system, which are used to develop and deliver YouthTruth’s student survey reports to schools, districts, and networks nation-wide and the Center for Effective Philanthropy’s (CEP’s) assessment tools to foundation leaders.
This position is available in either our San Francisco, CA or Cambridge, MA office.
In this role, you will lead and execute a variety of tasks, including:
Developing and maintaining (debugging, improving, testing, and deploying) Python scripts
Ensuring smooth functioning of our development, quality assurance, and production environments
Supporting the creation and rollout of our cloud-based data ingestion and translation architecture
Building integrations applications and solving integrations issues
Creating and executing unit test plans based on system and validation requirements
Documenting changes in software for end users
Maintaining and building upon our relational databases
Serving as a thought partner with colleagues and internal stakeholders about the feasibility and time intensiveness of proposed projects
As an integral part of CEP’s tech team, your time will be spent on CEP’s technology infrastructure, touching both the YouthTruth and Assessment and Advisory Services teams’ projects.
The Ideal Candidate:
You exhibit excellent judgment and have the ability to work with low direction to support your data and client analyst colleagues in a fast-paced environment while being thorough and results-oriented.
You take pride in your work and employ the necessary steps to ensure the production of reliable and sustainable code.
You have exceptional communication skills. You are collaborative and excited to work cross-functionally while building relationships and generating influence across the company.
You bring an aptitude for continuous improvement within processes and disciplines, helping to evolve from the current state into leveraging new mindsets and approaches.
You are tolerant of ambiguity and have the desire to create structures, evolve systems, and build new capabilities to better serve our customers.


Your Background, Experience, and Interests:
Experience with the following languages: Python and SQL
At least 2 years’ experience as a software developer, data analyst, engineer, or similar, in a full-time role.
Experience using Python with datasets
Experience with SQL language and tools to help build data transformation models
Understanding of cloud architectures and their applications within a data-driven environment
Integration among different applications through APIs
Experience with code management and review such as Github
A degree (or related practical experience) in computer programming, or related field
Knowledge and practice of common SDLC process methodologies (e.g., AGILE, SCRUM, Waterfall) to job function
Experience with AWS, data modeling and implementation, and/or Ruby on Rails a plus but not necessary


Benefits:
Our nonprofit model is central to our identity: our bottom line is impact, not profit. Yet even as a nonprofit, we successfully compete for top talent across both the public and private sectors and offer competitive compensation and benefits, including:
A commitment to pay parity and salary equity. The annual base salary for this position is $113,636 (Cambridge)/ $125,000 (SF). In addition, this role is eligible for a competitive Performance Based Incentive Compensation.
Comprehensive health, vision, and dental insurance plans.
Generous paid time off plan, including up to 15 holidays, three weeks of accrued vacation, and two personal days per year.
401(k) plan with a 1:1 Employer Match up to 5% of total compensation.
Generous annual personal professional development allowance.
Flexible spending and dependent care tax free savings plans.
Life insurance covered 100% by the organization.
We believe diversity and inclusion are key drivers of creativity and innovation, and we actively seek out candidates from many types of diverse backgrounds to apply for this exciting role.

About CEP:
For more than 20 years, CEP has led the movement to improve philanthropy through a powerful combination of dispassionate analysis and a passionate commitment to improving lives. Today, over 350 foundations have used CEP’s assessment tools to gather honest feedback from their stakeholders in an effort to learn how to be even more effective. CEOs and trustees have come to rely on our research for insights into foundation effectiveness on a wide range of topics, from assessing performance to developing strategy to managing stakeholder relationships. Our highly regarded programming—including our biennial conference—gives foundation leaders an exclusive and unprecedented opportunity to connect with their peers. YouthTruth harnesses student and stakeholder feedback to help school leaders, school system leaders, and funders accelerate improvements. Through our validated surveys and tailored advisory services, we help schools, districts, states, YouthTruth partners with schools, districts, and funders to enhance learning for all students. Since 2008, YouthTruth has surveyed over 2.6 million students and nearly 600,000 family and school staff members across 39 states. Strengths of CEP’s work culture are entrepreneurialism, accountability, teamwork, collegiality, diversity, and mutual respect. CEP is based in Cambridge, Massachusetts, with a second office in San Francisco, California.
Location:
We have offices in Cambridge, Massachusetts and San Francisco, California. The YouthTruth team resides alongside staff in other departments at The Center for Effective Philanthropy (CEP), YouthTruth’s parent nonprofit. This role can be based in either our Cambridge, MA or San Francisco, CA office.
We strive to balance in-person time with flexibility and the needs of each person, their team, and the larger organization. We believe culture, communication, trust, training, and certain kinds of creative work benefit from in-office interactions – and we believe that the flexibility of remote work also has many advantages. Moving forward we expect to ask staff to be in the office one to two days per week, and everyone will enjoy the option to work fully remotely from anywhere four weeks per year.
Our Process:
Our rolling process includes three interview rounds: an initial phone conversation, a skills assessment, and a team interview round. Our process also includes the checking of references in final stages. Check out CEP’s careers page to learn more about how we hire.
To Apply:
Please fill out our application for employment and attach a resume and thoughtful cover letter, outlining how your skills and experience meet the qualifications of the position.
If you have any questions, please contact Leaha Wynn, Senior Manager, People and Culture; Diversity and Inclusion Strategist or Alyse d’Amico, Vice President, People & Culture at jobs [at] cep[dot] org. Applications will be reviewed on a rolling basis.
We believe that a diversity of thoughts, experiences, backgrounds, personalities, and identities helps us think bigger and better, and enables us to reach our goals more effectively. We are committed to building a diverse staff and encourage individuals from all backgrounds to apply.
Once a candidate begins our process, we discourage outreach to our staff for interviews or conversations that are outside our standard interview process. We have worked hard to design a process that is fair and rigorous and achieves a good match between candidates and CEP.
CEP evaluates candidates based on their merits. We strongly discourage unsolicited references. We will ask for references if and when a candidate reaches a finalist stage.","$113,636 /yr (est.)",Unknown,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Dropbox
4.6",4.6,Remote,"Senior Data Engineer, New Initiatives","Company Description

Dropbox is a special place where we are all seeking to fulfill our mission to design a more enlightened way of working. We’re looking for innovative talent to join us on our journey. The words shared by our founders at the start of Dropbox still ring true today.

Wouldn’t it be great if our working environment—and the tools we use—were designed with people’s actual needs in mind? Imagine if every minute at work were well spent—if we could focus and spend our time on the things that matter. This is possible, and Dropbox is connecting the dots.

The nearly 3,000 Dropboxers around the world have helped make Dropbox a living workspace - the place where people come together and their ideas come to life. Our 700+ million global users have been some of our best salespeople, and they have helped us acquire customers with incredible efficiency. As a result, we reached a billion dollar revenue run rate faster than any software-as-a-service company in history.

Dropbox is making the dream of a fulfilling and seamless work life a reality. We hope you’ll join us on the journey.

Team Description

Our Engineering team is working to simplify the way people work together. They’re building a family of products that handle over a billion files a day for people around the world. With our broad mission and massive scale, there are countless opportunities to make an impact.

Role Description

In this role you will build very large, scalable platforms using cutting edge data technologies. This is not a “maintain existing platform” or “make minor tweaks to current code base” kind of role. We are effectively building from the ground up and plan to leverage the most recent Big Data technologies. If you enjoy building new things without being constrained by technical debt, this is the job for you!
Responsibilities

Help define company data assets (data model), spark, sparkSQL and hiveSQL jobs to populate data models
Help define and design data integrations, data quality frameworks and design and evaluate open source/vendor tools for data lineage
Work closely with Dropbox business units and engineering teams to develop strategy for long term Data Platform architecture to be efficient, reliable and scalable
Conceptualize and own the data architecture for multiple large-scale projects, while evaluating design and operational cost-benefit tradeoffs within systems
Create and contribute to frameworks that improve the efficacy of logging data, while working with data infrastructure to triage issues and resolve
Collaborate with engineers, product managers, and data scientists to understand data needs, representing key data insights in a meaningful way
Define and manage SLA for all data sets in allocated areas of ownership
Determine and implement the security model based on privacy requirements, confirm safeguards are followed, address data quality issues, and evolve governance processes within allocated areas of ownership
Design, build, and launch collections of sophisticated data models and visualizations that support multiple use cases across different products or domains
Solve our most challenging data integration problems, utilizing optimal ETL patterns, frameworks, query techniques, sourcing from structured and unstructured data sources
Optimize pipelines, dashboards, frameworks, and systems to facilitate easier development of data artifacts
Requirements

Startup mentality with strong ownership to solve 0-1 problems with minimal guidance and being comfortable with ambiguities
Excellent product strategic thinking and communications to influence product and cross-functional teams by identifying the data opportunities to drive impact
BS degree in Computer Science or related technical field involving coding (e.g., physics or mathematics), or equivalent technical experience
5+ years of Python or Java, C++, Scale development experience
7+ years of SQL experience (No-SQL experience is a plus)
5+ years of experience with schema design and dimensional data modeling
Proven ability in regards to managing and communicating data warehouse plans to internal clients.
Experience designing, building and maintaining data processing systems
Experience working with either a Map Reduce or a MPP system on any size/scale
High tech experiences are preferred

Total Rewards

Our Engineering Career Framework is viewable by anyone outside the company and describes what’s expected for our engineers at each of our career levels. Check out our blog post on this topic and more here .

For candidates hired in San Francisco metro, New York City metro, or Seattle metro, the expected salary/On-Target Earnings (OTE) range for the role is currently $178,500 - $210,000 - $241,500.

For candidates hired in the following locations: Austin (TX) metro, Chicago metro, California (outside SF metro), Colorado, Connecticut (outside NYC metro), Delaware, Massachusetts, New Hampshire, New York (outside NYC metro), Oregon, Pennsylvania (outside NYC or DC metro), Washington (outside Seattle metro) and Washington DC metro, the expected salary/On-Target Earnings (OTE) range for the role is currently $160,700 - $189,000 - $217,400.

For candidates hired in all other US locations, the expected salary/On-Target Earnings (OTE) range for this role is currently $142,800 - $168,000 - $193,200.

Range(s) is subject to change. Dropbox takes a number of factors into account when determining individual starting pay, including job and level they are hired into, location/metropolitan area, skillset, and peer compensation. Dropbox uses the zip code of an employee’s remote work location to determine which metropolitan pay range we use.

Salary/OTE is just one component of Dropbox’s total rewards package. All regular employees are also eligible for the corporate bonus program or a sales incentive (target included in OTE) as well as stock in the form of Restricted Stock Units (RSUs).

Dropbox is committed to investing in the holistic health and wellbeing of all Dropboxers and their families. Our benefits and perks programs include, but are not limited to:

Competitive medical, dental and vision coverage

(US Only) Competitive 401(k) Plan with a generous company match and immediate vesting

Flexible Time Off/Paid Time Off, paid holidays, 11 Company-wide PTO days, Volunteer time off and more

Protection Plans including; Life Insurance, Disability Insurance and Travel benefit plans

Perks Allowance to be used on what matters most to you, whether that’s wellness, learning and development, food & groceries, and much more

Parental benefits including; Parental Leave, Child and Adult Care, Day Care FSA (US Only), Fertility Benefits (US Only), Adoption and Surrogacy support and Lactation Support

Mental Health and Wellness benefits Free Dropbox space for your friends and family

Additional benefits details are available upon request.

Benefits

Dropbox is committed to investing in the holistic health and wellbeing of all Dropboxers and their families. Our benefits and perks programs include, but are not limited to:

Competitive medical, dental and vision coverage*

Retirement Savings through a defined contribution pension or savings plan**

Dropbox provides a Flexible PTO Policy in addition to your statutory holidays allowing you to unplug, unwind, and refresh

Dropbox also provides exclusive additional paid time off for all FTE employees across the Globe, in addition to any relevant statutory holidays

Protection Plans including Life and Disability Insurance*

A Perks Allowance to be used on what matters most to you, whether that’s wellness, learning and development, food & groceries, and much more

Parental benefits including; Parental Leave, Fertility Benefits, Adoptions and Surrogacy support, and Lactation support

Additional benefits details are available upon request.

Where group plans are not available, allowances are provided
**Benefit, amount, and type are dependent on geographical location, based upon applicable law or company policy

Dropbox is an equal opportunity employer. We are a welcoming place for everyone, and we do our best to make sure all people feel supported and connected at work. A big part of that effort is our support for members and allies of internal groups like Asians at Dropbox, BlackDropboxers, Latinx, Pridebox (LGBTQ), Vets at Dropbox, Women at Dropbox, ATX Diversity (based in Austin, Texas) and the Dropbox Empowerment Network (based in Dublin, Ireland).",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,2007,Unknown / Non-Applicable
"Epicor
4.2",4.2,"San Francisco, CA",Azure Data Engineer,"General information
Office (s)
, Albany, NY, Atlanta, GA, Austin, TX, Cincinnati, OH, Cleveland / Akron, OH, Minneapolis, MN, Philadelphia, PA, Salt Lake City, UT, San Diego, CA, San Francisco Bay Area, CA
Date Published
Friday, June 2, 2023
Country
United States
Job ID
22303
Function
Finance
Salary Range
70,000 - 180,000
Description & Requirements
Azure Data Engineer
Job Description
As a member of the Epicor Cloud Data team, you are joining a team that invests in your success by providing comprehensive learning and mentorship programs. You will be the Principal engineer driving innovation and success of our next-generation Microsoft Azure-based Data Lake/DataMart and Reporting platforms. You will be innovating and mentoring the rest of the engineering org as we grow our teams.
What You'll Do
Create and maintain data pipeline architecture on the Azure platform. Ensure that system designs adhere to solution architecture design and are traceable to functional and non-functional requirements.
Leverage Azure Data Factory and Databricks to assemble large, complex data sets. Design new solutions and services to improve overall user experience.
Identify, design, and implement internal process improvements such as automating manual processes and optimizing data delivery. Define system design standards to improve and sustain standardization.
Build the infrastructure required for optimal data extraction, transformation, and loading from a wide variety of data sources.
Design relational and non-relational data stores on Azure.
Assist Manager-Data Administration in leading coordination with other staff to ensure data handling meets organizational objectives for data quality, business process management, and risk management.
Provide technical mentoring to team members
Perform all duties and maintain all standards in accordance with company policies, procedures, and internal controls such as SOX.
What You Bring:
4 years of experience working as part of an IT Data team using Azure-based technologies to solve business problems and build solutions.
Azure PaaS, Data analytics, Data warehousing, and Data science.
Azure Synapse Analytics, SQL Pool
Azure Spark, ADF, T-SQL Scripting, Stored Procs, Data bricks, Python
Microsoft related certifications
Experience with visualization tools such as Tableau (preferred)
The Team:
We put a high value on work-life balance. It isn’t about how many hours you spend at home or at work; it’s about the flow you establish that brings energy to both parts of your life. Most of our engineers have been with us for 5+ years growing in their respective roles. We believe striking the right balance between your personal and professional life is critical to life-long happiness and fulfillment while offering flexibility in working hours and encouraging you to find your own balance.
Why Epicor?

At Epicor, we know that success comes from working together. Everyone has a role to play, and it’s the essential partnerships across our company that are crucial to our customer’s success and our growth as a business.

We’re truly a team. Working in close partnership, we bring wide-ranging talents together in powerful collaborations. We think innovatively, share our knowledge generously, and constantly learn from our colleagues. We’re proud of the success we achieve every day, but we never stop challenging ourselves and encouraging each other. Together, we go further and imagine an even brighter future.

Whatever your career journey, we’ll help you find the right path. Through our training courses, mentorship, and continuous support, you’ll get everything you need to thrive. At Epicor, your success is our success. And that success really matters, because we’re the essential partners for the world’s most essential businesses—the hardworking companies who make, move, and sell the things the world needs.
Equal Opportunities and Accommodations Statement
At Epicor, we strive to create a welcoming, inclusive, and diverse workplace every day. Bring the whole and real you—that’s who we’re interested in. If you’re interested in this role but your experience and current skillset don’t match every qualification of the job description, that’s okay! We encourage you to apply anyway. Learning and sharing our knowledge keeps us all moving forward. You just might be the right fit and gain new skills new along the way.
#LI-MB2 #LI-Remote","$125,000 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Software Development,1972,$1 to $5 billion (USD)
"Dobbs Defense Solutions, LLC",#N/A,"Fort Meade, MD",Data/NiFi Engineer,"Our Mission
At Dobbs Defense, we deliver mission-centric IT, Cyber, and data analytics solutions for our government and commercial clients through the convergence of automation, innovation, training, and education. Delivering high-quality IT, cybersecurity, and data analytics solutions through proven and innovative methods is our vision.

Job Description
Dobbs Defense Solutions is seeking a Senior Data/NiFi Engineer to design, develop, implement, test, secure, and support data transport and brokering for multiple classified networks and between networks.

Duties
Design, develop, implement, test, secure, and support data transport and brokering.
Data flow engineering for one of DISA’s premier boundary defense systems.
Perform system administration of Niagara Files (NiFi) on Linux VMs.
Support backup/failover/load balancing of data brokering to improve availability.
Monitor availability & amp, usage.
Respond to outages, take corrective action, and issue outage reports.
Familiarity with DISA STIGs and SCAP.
Familiarity with Java programing.

Qualifications
Required education and experience:
5+ years of Data Engineering experience.
Significant experience with Niagara Files (NiFi) and MiNiFi or similar data engineering tools.
Excellent written and verbal communication skills.
IAM or IAT Level II Certification (e.g., GSEC, SEC+, SCNP, SSCP, etc.).
Active Secret clearance.

Required Clearance
Secret

Working Environment
Onsite

Our Equal Employment Opportunity Policy
Dobbs Defense Solutions is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, creed, sex (including pregnancy, childbirth, breastfeeding, or medical conditions related to pregnancy, childbirth or breastfeeding), sexual orientation, gender or gender identity (except where gender is a bona fide occupational qualification), gender expression and transgender status, national origin, ancestry, age, disability, military or veteran status, marital or domestic partner status, genetic information, citizenship, low-income status or any other status or characteristic protected by applicable law. We are committed to equal employment opportunity in all decisions related to employment, promotion, wages, benefits, and all other privileges, terms, and conditions of employment.
OE5clsBZDc","$84,032 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Par Government Systems Corporation
4.4",4.4,"Bethesda, MD",Senior Data Engineer,"PAR Government is excited to welcome a Senior Data Engineer to the our Intelligence and Readiness Operations. As a Senior Data Engineer you will support a large-scale intelligence processing system that is focused on digital document exploitation.

Establish a data engineering processes to support the understanding of information needs to maximize the use and value of data and information assets by consumers. You will manage information consistently across the enterprise and align data management efforts and technologies with business needs. Key task areas on the program include forensic image processing, machine learning model production, knowledge graph construction and reasoning, agile development, system security, technology transition, and system operations.

Responsibilities and Duties:
Analyze, design, build, test, deploy, operate, and maintain solutions, capabilities, and services to meet data needs.
Plan and lead major technology assignments, evaluate performance results, and recommend major changes affecting short-term project growth and success.
Conduct requirements analysis.
Conduct requirements design.
Implementation solutions
Maintenance databases of related solution components


Required Skills/Experience:
Must have an active or in scope US Top Secret Clearance with SCI Eligibility
Expertise in analyzing. designing, building, testing, deploying. operating, and maintaining solutions, capabilities, and services to meet data needs, including requirements analysis, and design, implementation, and maintenance of databases' related solution components.
Bachelors degree or equivalent with a minimum of 10 years of experience as a Data Engineer
Experience with defining and recording data requirements and delivering data requirements specifications.
Experience with developing and maintaining conceptual data models and delivering conceptual data model diagrams, logical data models, physical data models, and physical databases .
Experience with managing data model versions and integrating and delivering data model libraries.
Expertise in designing data integration services and delivering source-to-target maps, data extract-transform- load (ETL) design specifications, and data conversion designs.
Expertise in writing software code and scripts to distributed the processing of information extraction tasks to identify entities, events, and relationships from large corpus of structured and unstructured data and multimedia stored in a distributed file system ox object store.
Experience with applying data cleansing, transformation, and augmentation methods to measure and improve data quality.
Experience building, testing, and delivering data integration services
Experience with establishing Golden Records and delivering reliable reference and master data.
Experience defining, delivering, and maintaining hierarchies and affiliations that define the meaning of data within the context of its interrelationships with other data
Experience with importing and exporting data between an external RDBMS and a Hadoop cluster, including the ability to import specific subsets, change the delimiter and file format of imported data during ingest, and alter the data access patten or privileges.
Experience ingesting real-time and near-real time (NRT) Streaming data into the Hadoop File System (HDFS), including the ability to distribute to multiple data sources and convert data on ingest from one format to another.
Expertise in loading data into and out of the Hadoop File System (HDFS) using the HDFS command line interface; converting sets of data values in a given format at stored in Hadoop File System (HDFS) into new data values and/or a new data format and writing them into HDFS or Hive/HCatalog.
Expertise in filtering, sorting, joining, aggregating, and transforming one or more data sets in a given format (e.g., Parquet, Avro, JSON, delimited text, and natural language text) stored in the Hadoop Distributed Filesystem (HDFS)

It is the policy of PAR to prohibit all forms of discrimination and to affirmatively implement equal opportunity to all qualified employees and applicants for employment without regard to race, color, creed, religion, sex, age, veteran status, national origin, disability, marital status, predisposing genetic characteristics, sexual orientation, gender identity, or other legally protected status and positive action shall be taken to insure the fulfillment of this policy.
If you require reasonable accommodation in the application process, call Human Resources at 315.356.2260. All other applications must be submitted online.***

Required Skills

Required Experience","$122,700 /yr (est.)",201 to 500 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"K&A Engineering Consulting PC
4.6",4.6,United States,Data Verification Engineer,"Header:
K&A Engineering Consulting P.C. (K&A) – is a people-first organization. We value knowledge and learning. We believe in the power of connection, and collaboration, and we seek to build relationships that mean something. We employ some of the best and brightest in our industry and we work hard to maintain a culture that our people can be proud of. Our company has earned its reputation through challenging work, uncompromising precision and continuous learning and adapting.

K&A Engineering is a privately held, providing comprehensive consulting, engineering and design, program/project management, construction management, and support services to power utility and industrial clients. K&A's experience includes all aspects of generation, transmission, substations, distribution, energy efficiency, and distributed energy resource (DER) projects, from planning and environmental and regulatory analyses through engineering and design, construction, program management, project controls, and commissioning. The utility industry is a vast market, and K&A quickly established our name and reputation as a top-tier service provider. We were recently ranked #126 on the Inc. 5000 List of America's Fastest-Growing Private Companies and the top engineering firm with a three-year growth rate of 3,166%.
Job Overview::
Due to Company growth, we are actively seeking an experienced Data Verification Engineer to join our Distribution Engineering team remotely. Must be in Central or Eastern time zones.
Responsibilities & Duties::
Work with project team on a large-scale Distribution Grid Data project
Analysis and review of large amounts of distribution image data
Review images of distribution backbone and extended backbone circuits, use the images to validate existing GIS system data and provide corrections as necessary to update the GIS data
Review the images for conductor damage and assessment
Ability to analyze numerous drone images of distribution backbone circuits daily to validate GIS data for equipment, conductors and phasing for each circuit
Verify assigned circuits in a timely manner with a high degree of accuracy, quality, and completeness to avoid rework and impacts to the project schedule
Coordinate with project team leads and foster positive relationships with team member
Report deficiencies and risks immediately to project manager, team lead or other identified affected parties
Be open to feedback and develop and implement methods to improve efficiency and quality
Perform other duties and responsibilities as assigned
Qualifications & Skills::
5-8 years experience as a distribution lineman or equivalent combination of education and experience in electrical distribution
Strong working knowledge and experience in the inspection, installation, replacement, and repair of distribution system equipment and lines
Ability to identify distribution equipment and basic knowledge of their function; i.e. electronic reclosers, capacitors, transformers, disconnects, cut-outs, etc
General knowledge of GIS systems; Ability to navigate mapping systems and identify equipment on the system
Familiar with Microsoft Office Applications, Office 365
Experience with screenshot or graphic applications
General knowledge of GIS systems; Ability to navigate mapping systems and identify equipment on the system.
Good communication skills, both written and oral are required
Ability to organize and handle multiple project tasks
Detail oriented with strong people and team skills",#N/A,51 to 200 Employees,Company - Public,"Energy, Mining & Utilities",Energy & Utilities,2016,Unknown / Non-Applicable
"Iron Service Global Inc
3.4",3.4,"Menlo Park, CA","Data Engineer (Python, SQL)","Data Analytics & Engineering - Data Engineer III
Job Description: Onsite at Menlo Park location only.
Summary:
The main function of the Data Engineer is to develop, evaluate, test, and maintain architectures and data solutions within our organization. The typical Data Engineer executes plans, policies, and practices that control, protect, deliver, and enhance the value of the organization’s data assets.
Job Responsibilities:
Design, construct, install, test, and maintain highly scalable data management systems.
Ensure systems meet business requirements and industry practices.
Design, implement, automate and maintain large scale enterprise data ETL processes.
Build high-performance algorithms, prototypes, predictive models and proof of concepts.Skills:
Ability to work as part of a team, as well as work independently or with minimal direction.
Excellent written, presentation, and verbal communication skills.
Collaborate with data architects, modelers and IT team members on project goals.
Strong PC skills including knowledge of Microsoft SharePoint
Strong Python/SQL experience
Education/Experience:
Bachelor's degree in a technical field such as computer science, computer engineering or related field required.
Process certification, such as, Six Sigma, CBPP, BPM, ISO 20000, ITIL, CMMI.
Job Types: Full-time, Contract
Salary: $110,000.00 - $135,000.00 per year
Benefits:
Dental insurance
Health insurance
Life insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Menlo Park, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
ETL: 5 years (Preferred)
Data management: 5 years (Preferred)
Enterprise data ETL processes: 5 years (Preferred)
Business process modeling: 5 years (Preferred)
License/Certification:
Six Sigma (Preferred)
ISO 20000 (Preferred)
ITIL (Preferred)
Work Location: One location","$122,500 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1987,$25 to $100 million (USD)
"CNA Insurance
3.9",3.9,"Chicago, IL",Senior Data Engineer,"You have a clear vision of where your career can go. And we have the leadership to help you get there. At CNA, we strive to create a culture in which people know they matter and are part of something important, ensuring the abilities of all employees are used to their fullest potential.
CNA seeks to offer a comprehensive and competitive benefits package to our employees that helps them — and their family members — achieve their physical, financial, emotional and social wellbeing goals.

For a detailed look at CNA’s benefits, check out our
Candidate’s Guide
.
JOB DESCRIPTION:
Essential Duties & Responsibilities
Performs a combination of duties in accordance with departmental guidelines:
Lead the design and build data solutions and applications that enable reporting, analytics, data science, and data management.
Design, develop and implement data integration projects using Informatica and SSIS.
Provide Azure application insights and Cloud-based integration using Python and PowerShell Scripts.
Create analytical Business Intelligence (BI) reports using Google Analytics for web applications.
Lead the design, implementation and automation of data pipelines, including sourcing data from internal and external systems and transforming the data for the optimal needs of various systems and business requirements.
Lead robust unit testing to ensure deliverables match the design and provide expertise to support subsequent release testing.
Apply machine learning concepts to development work.
Adhere to and establish quality and reliability standards, and ensure team adheres to the same quality and standards working in an Agile development environment.
Design complex physical data models, projects and cloud-based data lake constructs including SQL/NoSQL database systems.
Research, identify and implement process improvements that address complex technology gaps and build strong knowledge of technology enablers.
Maintain professional and technical knowledge by attending educational workshops, reviewing professional publications, establishing personal networks, and participating in professional societies.
Reporting Relationship
Typically Manager or above
Education & Experience
Bachelor’s degree in computer science, information technology or related and 5 (five) years of experience in data analytics or application development.
Must have work experience with each of the following:
Design, develop and implement data integration projects using Informatica and SSIS;
Provide Azure application insights and Cloud-based integration using Python and PowerShell Scripts;
Create analytical Business Intelligence (BI) reports using Google Analytics for web applications.
Primary Location United States – Illinois – Chicago
Organization – IT
Mon-Fri., 8:30am – 4:45pm, 37.5 hours/week, $140,046 to $148,800 per year, overtime exempt. This position qualifies for CNA’s employee referral policy program.
Apply: Submit cover letter and resume at
www.cna.com
.
CNA is committed to providing reasonable accommodations to qualified individuals with disabilities in the recruitment process. To request an accommodation, please contact
leaveadministration@cna.com
.","$144,423 /yr (est.)",1001 to 5000 Employees,Company - Public,Insurance,Insurance Carriers,1897,$10+ billion (USD)
"HCA Healthcare
3.3",3.3,"Nashville, TN",Senior Data Engineer,"Introduction
Last year our HCA Healthcare colleagues invested over 156,000 hours volunteering in our communities. As a Sr Database Admin with HCA Healthcare you can be a part of an organization that is devoted to giving back!
Benefits
HCA Healthcare offers a total rewards package that supports the health, life, career and retirement of our colleagues. The available plans and programs include:
Comprehensive medical coverage that covers many common services at no cost or for a low copay. Plans include prescription drug and behavioral health coverage as well as free telemedicine services and free AirMed medical transportation.
Additional options for dental and vision benefits, life and disability coverage, flexible spending accounts, supplemental health protection plans (accident, critical illness, hospital indemnity), auto and home insurance, identity theft protection, legal counseling, long-term care coverage, moving assistance, pet insurance and more.
Free counseling services and resources for emotional, physical and financial wellbeing
401(k) Plan with a 100% match on 3% to 9% of pay (based on years of service)
Employee Stock Purchase Plan with 10% off HCA Healthcare stock
Family support through fertility and family building benefits with Progyny and adoption assistance.
Referral services for child, elder and pet care, home and auto repair, event planning and more
Consumer discounts through Abenity and Consumer Discounts
Retirement readiness, rollover assistance services and preferred banking partnerships
Education assistance (tuition, student loan, certification support, dependent scholarships)
Colleague recognition program
Time Away From Work Program (paid time off, paid family leave, long- and short-term disability coverage and leaves of absence)
Employee Health Assistance Fund that offers free employee-only coverage to full-time and part-time colleagues based on income.
Learn more about Employee Benefits
Note: Eligibility for benefits may vary by location.

Would you like to unlock your potential with a leading healthcare provider dedicated to the growth and development of our colleagues? Join the HCA Healthcare family! We will give you the tools and resources you need to succeed in our organization. We are looking for an enthusiastic Sr Database Admin to help us reach our goals. Unlock your potential!
Job Summary and Qualifications
As a Sr. Data Engineer you will have the opportunity to grow and leverage your technical skills in MPP systems like that of Teradata’s to perform physical database design, SQL tuning and support our Teradata Data Warehouse platform on-prem and in the public cloud. You will be performing various operational tasks, including, but not limited to, system monitoring, object deployment, backup and recovery, user provisioning, query optimization and system maintenance. The job requires communicating with data architecture, integration and reporting teams for adherence to best practices and process standardization.
The role requires self-starters who are proficient in problem solving and communicating complicated, technical issues with clarity to the other technology teams and stakeholders. The culture of our organization places an emphasis on teamwork, so social and interpersonal skills are equally important as technical capability.

Assist in developing automation of various operational tasks performed by a DBA.
Work with cross-functional teams to build the physical databases and provide technical guidance during all phases of the development process.
Work with team in researching, evaluating and implementing new technologies as needed.
Provide regular, clear, and consistent communication (written and oral) on the status of projects, issues, and deliverables to team leadership.
Work with vendor technical support to facilitate analysis of and resolution to technical issues.
Provide planning input to leadership – operational and tactical – in order to drive success for team and company goals.
Participate in planned system maintenance tasks.
Provide rotational on-call support
What qualifications you will need:
Bachelors Degree preferred
Five or more years of relevant work experience
Other/Special Qualifications
Teradata development and/or administration (5+ years).
Teradata certification(s).
Shell Scripting and/or python or any development experience using C/C++/Java
Exposure to Cloud technologies
HCA Healthcare has been recognized as one of the World’s Most Ethical Companies® by the Ethisphere Institute more than ten times. In recent years, HCA Healthcare spent an estimated $3.7 billion in cost for the delivery of charitable care, uninsured discounts, and other uncompensated expenses.
""There is so much good to do in the world and so many different ways to do it.""- Dr. Thomas Frist, Sr.
HCA Healthcare Co-Founder
Be a part of an organization that invests in you! We are reviewing applications for our Sr Database Admin opening. Qualified candidates will be contacted for interviews. Submit your application and help us raise the bar in patient care!
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.","$115,561 /yr (est.)",10000+ Employees,Company - Public,Healthcare,Health Care Services & Hospitals,1968,$10+ billion (USD)
#N/A,#N/A,"Englewood, CO",Senior Data Engineer,"Department Summary

DISH is a Fortune 200 company that continues to redefine the communications industry. Our legacy is innovation and a willingness to challenge the status quo, including reinventing ourselves. We disrupted the pay-TV industry in the mid-90s with the launch of the DISH satellite TV service, taking on some of the largest U.S. corporations in the process, and grew to be the fourth-largest pay-TV provider. We are doing it again with the first live, internet-delivered TV service – Sling TV – that bucks traditional pay-TV norms and gives consumers a truly new way to access and watch television.
Now we have our sights set on upending the wireless industry and unseating the entrenched incumbent carriers.
We are driven by curiosity, pride, adventure, and a desire to win – it’s in our DNA. We’re looking for people with boundless energy, intelligence, and an overwhelming need to achieve, to join our team as we embark on the next chapter of our story.
Opportunity is here. We are DISH.


Job Duties and Responsibilities

The Data Architecture team in DISH IT Digital Solutions seeks a Data Engineer who will develop and maintain a variety of data solutions for supporting our core business and modernizing DISH IT’s transactional data persistence systems.
Key responsibilities:
Develop scalable and high-performing data update applications for on-premises and Cloud-based databases.
Deliver, optimize, and automate data updates for existing business processes.
Provide on-call support for data persistence systems in Production.

Work attire: Business casual.

Working hours: This is a full-time position: 40 hours/week. Days and hours of work are typically Monday through Friday; 8:00 a.m. to 5:00 p.m. or 9:00 a.m. to 6:00 p.m.


Skills, Experience and Requirements

Education: Bachelor’s degree in Computer Science, Computer Engineering, or a related technical degree, or a combination of education and experience.

Experience: 4+ years of experience as a Data Engineer or similar role.

Skills and qualifications:

Strong desire to learn new skills.
Familiar with one programming language.
Hands-on SQL development in a relational or NoSQL database.
Strong problems solving and analytical skills.
Experiences in Python or Java.
SQL/PLSQL development experiences in Oracle or MySql.
Familiar with Linux.

Salary Range

Compensation: $106,250.00/Year - $125,000.00/Year
Compensation and Benefits

We also offer versatile health perks, including flexible spending accounts, HSA, a 401(k) Plan with company match, ESPP, career opportunities, and a flexible time away plan; all benefits can be viewed here: DISH Benefits.

The base pay range shown is a guideline. Individual total compensation will vary based on factors such as qualifications, skill level, and competencies; compensation is based on the role's location and is subject to change based on work location. Candidates need to successfully complete a pre-employment screen, which may include a drug test and DMV check.","$115,625 /yr (est.)",10000+ Employees,Company - Public,Telecommunications,"Cable, Internet & Telephone Providers",1980,$10+ billion (USD)
"CHS Inc
3.9",3.9,"Inver Grove Heights, MN",Senior Big Data Engineer,"CHS Inc. is a leading global agribusiness owned by farmers, ranchers and cooperatives across the United States that provides grain, food and energy resources to businesses and consumers around the world. We serve agriculture customers and consumers across the United States and around the world. Most of our 10,000 employees are in the United States, but today we have employees in 19 countries. At CHS, we are creating connections to empower agriculture.
CHS Inc.
Senior Big Data Engineer
Location: Inver Grove Heights, MN

Job Description
Analyze, Model, Develop the ELT framework for ingestion, transformation and distribution of data streams to Snowflake, AWS and PowerBI Reports. Implement various data modelling techniques using DBT tool. Design and Develop python lambda to load various API data into Snowflake to perform analytics for business requirements. Use tidal to automate the job schedule. Perform continuous integrations and deployment using Azure DevOps, Git, Octopus and Terraform. Collaborate with team to troubleshoot and develop standards and best practices. Integration testing to validate data between Cloudera and Snowflake. Implemented Change Data Capture (CDC) technology in HVR to load the deltas to Snowflake. Experience with Snowflake and AWS S3 bucket for integrating data from multiple sources with nested JSON formatted data into Snowflake table. Position allows working from home within commuting distance of worksite location.

Job Requirements
The qualified candidate must have at least a Bachelor’s degree or foreign equivalent degree in Computer Science, Information Technology, Management Information Systems (MIS), Business Intelligence, or closely related technical field. The qualified candidate must have at least 4 years (48 months) of experience with all the following: (a) Data Integration, Data Modeling, and ETL/ELT and SQL Development; (b) developing solutions related to Big Data, and Data Sciences from end-to-end (data ingestion to consumption); (c) developing and maintaining scalable data pipelines that will ingest, transform, and distribute data streams and batches; and (d) utilizing all the following tools/technologies: Java, C++ and C#, Object Oriented Design, Python 2.7 and 3, NoSQL Databases (Hive, Spark), AWS Native Tools (Glue, DMS, S3, Athena), Snowflake, Software Development Lifecycle (SDLC) Cloudera CDP and Databricks. All experience may be gained concurrently. Position allows working from home within commuting distance of worksite location.","$113,398 /yr (est.)",10000+ Employees,Company - Public,Agriculture,Farm Support,1929,$10+ billion (USD)
"EXL Services
3.8",3.8,"Hartford, CT",Azure GCP Cloud Data Migration Engineer,"Company Overview and Culture
EXL (NASDAQ: EXLS) is a global analytics and digital solutions company that partners with clients to improve business outcomes and unlock growth. Bringing together deep domain expertise with robust data, powerful analytics, cloud, and AI, we create agile, scalable solutions and execute complex operations for the world’s leading corporations in industries including insurance, healthcare, banking and financial services, media, and retail, among others. Focused on creating value from data for driving faster decision-making and transforming operating models, EXL was founded on the core values of innovation, collaboration, excellence, integrity and respect. Headquartered in New York, our team is over 40,000 strong, with more than 50 offices spanning six continents. For information, visit www.exlservice.com.

For the past 20 years, EXL has worked as a strategic partner and won awards in its approach to helping its clients solve business challenges such as digital transformation, improving customer experience, streamlining business operations, taking products to market faster, improving corporate finance, building models to become compliant more quickly with new regulations, turning volumes of data into business opportunities, creating new channels for growth and better adapting to change. The business operates within four business units: Insurance, Health, Analytics, and Emerging businesses.

Job Description

Azure/ GCP Cloud Data Migration Engineer:
More than 6+ years of experience in SAS & SQL Reporting
Prior experience leading Cloud technologies including: GCP, Azure and Azure Data Factory
Understand the business requirement and develop as per the technical design outlined.
Builds data marts and data models to support clients and other internal customers.
Identifying delivery issues and ensuring corrective action plans
Experience writing code in Python, PySpark and SQL
Experience developing ETL workflows.
Experience with database performance tuning and database management tools for backups, recovery and snapshot management.
Collaborates with client team to transform data and integrate algorithms and models into automated processes
Uses expertise, judgment, and precedents to contribute to the resolution of moderately complex problems
Skill set:
GCP, Azure, Azure Data Factory, Python, PySpark, SQL and SAS

EEO/Minorities/Females/Vets/Disabilities

Base Salary Range Disclaimer: The base salary range represents the low and high end of the EXL base salary range for this position. Actual salaries will vary depending on factors including but not limited to: location and experience. The base salary range listed is just one component of EXL's total compensation package for employees. Other rewards may include bonuses, as well as a Paid Time Off policy, and many region specific benefits.

Please also note that the data shared through the job application will be stored and processed by EXL in accordance with the EXL Privacy Policy.","$90,000 /yr (est.)",10000+ Employees,Company - Public,Management & Consulting,Business Consulting,1999,$500 million to $1 billion (USD)
"Synopsys
4.1",4.1,"Mountain View, CA",Data Engineer Leader,"44867BR
USA - California - Mountain View/Sunnyvale
Job Description and Requirements
The Synopsys Central Engineering team is tasked to digitize Synopsys product development activities. We will achieve it by identifying areas of improvement and correlating various aspects of the product development, and providing all levels of management visibility for action. The domain areas we are focused on relate to Quality, Productivity, and Operational Efficiency,

We are looking for an experienced Data Engineer who will contribute to building the next-generation Data Platform. As a Data Engineer, you will be working on modern, large-scale big data technologies to build data platforms on Snowflake and toolset. The goal is to create an effective and efficient data pipeline to facilitate data exchange between various applications.

In this role, you will be partnering with data providers to enable them to make available volumes of data and integrate in a common data platform. You will also interact with Data analysts to transform data into information and insights driving data-based strategic outcomes. This is a hands-on role, and you are joining the team near the beginning of our journey where you can help shape our way to manage big data.

Responsibilities:
Drive strategic goal of data consolidation for the whole of engineering to enable cross-domain analytics.
Own and establish Center of Excellence for Data Engineering practices by defining architecture, rules, and setting guardrails for data processing capabilities. This includes data Ingestion, quality control, transformation, and high availability.
Incorporate state-of-the-art practices in Data Engineering to scale the value we deliver in transforming data into insights.
Identify, design and implement internal process improvements, including data infrastructure, for scalability, optimizing data delivery, and automating manual processes.
Leverage your experience and proficiency in all aspects of data management, data cataloging, analytics solution architecture & design, and implementation roadmap.
Build data cataloging infrastructure and metadata platform to enable data discovery, data observability, and federated governance.
Drive cross-team projects to integrate data from numerous separate sources into a unified data environment.

Expertise & Skills:
Must be proficient in ELT (Extract-Load-Transform) process with hands-on experience.
Must be detailed-oriented with a passion for data accuracy and reliable solution development.
Subject Matter Resource in designing and building high performance data pipelines to move and process data using modern tools.
Experience in Data Engineering Architecture and Design.
Subject Matter Resource in SQL (advanced).
Experience in at least one prominent programming language, such as Python (preferred), or Java.
The ability to work with other team members, drive projects to completion, and work autonomously.
Excellent written and verbal communication, work autonomously, and have proven organizational and planning skills.
We also value:
Prior experience in leading Data or Analytics teams.
Experience in database design and management, such as MS SQL Server, Oracle Database, MySQL Database, Cassandra, MongoDB, etc
Familiarity and experience in Snowflake toolset is a proven asset for this position.
Experience with Power BI and/or Tableau or other visualization tools.
Experience with HVR and Fivetran a plus.
Experience with dbt Cloud a plus.

Requirements:
Bachelor's or master's Degree in a quantitative field
> 5-10 years of relevant experience
5+ years’ experience engineering and operationalizing data pipelines with large and complex datasets
3+ years’ experience working with Cloud technologies such as Snowflake

At Synopsys, we’re at the heart of the innovations that change the way we work and play. Self-driving cars. Artificial Intelligence. The cloud. 5G. The Internet of Things. These breakthroughs are ushering in the Era of Smart Everything. And we’re powering it all with the world’s most advanced technologies for chip design and software security. If you share our passion for innovation, we want to meet you.

Stay Connected: Join our Talent Community

The hourly range across the U.S. for this role is between $117,000 - $204,000. In addition, this role may be eligible for an annual bonus, equity, and other discretionary bonuses. Synopsys offers comprehensive health, wellness, and financial benefits as part of a comparative total rewards package. The actual compensation offered will be based on a number of job-related factors, including location, skills, experience, and education. Your recruiter can provide more specific details on the total rewards package upon request.

Inclusion and Diversity are important to us. Synopsys considers all applicants for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity, age, military veteran status, or disability.

Job Category
Engineering
Country
United States
Job Subcategory
R&D Engineering
Hire Type
Employee
Base Salary Range
$117,000 - $204,000","$160,500 /yr (est.)",10000+ Employees,Company - Public,Information Technology,Computer Hardware Development,#N/A,$1 to $5 billion (USD)
"Procore Technologies
4.5",4.5,Oregon,Staff Data Engineer,"Job Description

What if you could use your technology skills to develop a product that impacts the way communities' hospitals, homes, sports stadiums, and schools across the world are built? Construction impacts the lives of nearly everyone in the world, and yet it's also one of the world's least digitized industries, not to mention one of the most dangerous. That's why we're looking for a talented Staff Data Engineer to join Procore's journey to revolutionize a historically underserved industry.
As a Staff Data Engineer, you'll design and develop data products for Procore Data Platform data management area. You'll be part of the high-performance team of Data Engineers and will collaborate with platform engineers and product leaders.
This position will report to our Senior Manager of Data Engineering, and can be based remotely from any US location. We're looking for someone to join our team immediately.
What you'll do:
Lead the design and development of big data predictive analytics using object-oriented analysis, design and programming skills, and design patterns
Implement ETL workflows for data matching, data cleansing, data integration, and management
Maintain existing data pipelines and develop new data pipelines using big data technologies
Develop and maintain tables and data models in SQL, abstracting multiple sources and historical data across varied schemas to a format suitable for further analysis
Responsible for leading the effort to continuously improve the reliability, scalability, and stability of the enterprise data platform
Contribute to and lead the continuous improvement of the software development framework and processes by collaborating with Quality Assurance engineers
Deliver observable, reliable, and secure software, embracing the ""you build it, you run it"" mentality, focusing on automation and GitOps
Participate in daily standups, team meetings, sprint planning, and demo/retrospectives while working cross-functionality with other teams to drive the innovation of our products
Apply data governance framework, including the management of data, data compliance operating model, data policies, and standards
What we're looking for:
BS degree in Computer Science, a similar technical field of study, or equivalent practical experience; MS or Ph.D. degree in Computer Science or a related field is preferred
5+ years of experience in a Data Engineering position
Strong expertise with 3+ years of experience building enterprise techniques for large-scale distributed system design and data processing, including:
Building data pipelines with Databricks as the source
Building and maintaining data warehouses in support of BI tools (Snowflake, dbt, Tableau)
Building data pipeline framework for data workflow to process large data sets and Real-Time & Batch Data Pipeline development
Experience in processing structured and unstructured data into a form suitable for analysis and reporting with integration with a variety of data metrics providers ranging from advertising, web analytics, and consumer devices
Desire to be actively hands-on with code, using Java, Python (80%), and SQL, along with willingness and passion for mentoring junior engineers and performing code reviews
Possess familiarity with AWS-managed services for data (Glue, Athena, Data Pipeline, Flink, Spark) and Snowflake

Additional Information

Base Pay Range $147,200-$202,400. Eligible for Bonus Incentive Compensation. Eligible for Equity Compensation. Procore is committed to offering competitive, fair, and commensurate compensation, and has provided an estimated pay range for this role. Actual compensation will be based on a candidate’s job-related skills, experience, education or training, and location.
Perks & Benefits
At Procore, we invest in our employees and provide a full range of benefits and perks to help you grow and thrive. From generous paid time off and healthcare coverage to career enrichment and development programs, learn more details about what we offer and how we empower you to be your best.
About Us
Procore Technologies is building the software that builds the world. We provide cloud-based construction management software that helps clients more efficiently build skyscrapers, hospitals, retail centers, airports, housing complexes, and more. At Procore, we have worked hard to create and maintain a culture where you can own your work and are encouraged and given resources to try new ideas. Check us out on Glassdoor to see what others are saying about working at Procore.
We are an equal-opportunity employer and welcome builders of all backgrounds. We thrive in a diverse, dynamic, and inclusive environment. We do not tolerate discrimination against employees on the basis of age, color, disability, gender, gender identity or expression, marital status, national origin, political affiliation, race, religion, sexual orientation, veteran status, or any other classification protected by law.
If you'd like to stay in touch and be the first to hear about new roles at Procore, join our Talent Community.","$174,800 /yr (est.)",1001 to 5000 Employees,Company - Public,Information Technology,Enterprise Software & Network Solutions,2002,Unknown / Non-Applicable
"AT&T
3.7",3.7,"Plano, TX",Professional Big Data Software Engineer,"DUTIES: Professional-Big Data Software-Engineer needed by AT&T Services, Inc. in Plano, TX to plan for the development of high performance, distributed computing and data processing tasks using big data technologies including Hadoop and NoSQL, text mining and in the areas of finance and Risk and Fraud Systems. Experience in Microsoft Azure and Palantir Foundry. Analyze structured and unstructured data, work with clients to understand requirements, and develop data pipelines and applications to produce data science features required to develop analytics and data to run the business more efficiently. Apply strong software engineering ability in developing, deploying, and testing code in a modern DevSecOps environment and tools for data science development and deployments in the cloud. Utilize java vm (jvm) based function languages including Scala, Hadoop query languages including pig, hive, along with alternative hdfs-based computing frameworks including spark and storm. Use big data programming languages and technology, write code, complete programming and documentation, and perform testing and debugging of applications. Analyze, design, program, debug and modify software enhancements and new products used in distributed, large scale analytics and visualization solutions. Interact with data scientists and industry experts to understand how data needs to be converted, loaded and presented. Work in a highly agile environment while fully functioning technical professional. Develops new concepts, methods, techniques. Object Oriented Analysis and Design using Python Programming language and Agile Methodologies and Scrums.. Execute problem solving to solve non routine problems based on analysis of multiple factors. Identify key issues, patterns or deviations from norm. Work on problems requiring judgment and in depth evaluation of multiple factors. Analyze and interpret research to evaluate and recommend solutions while applying independence guided by team goals and operational objectives. Apply judgment to determine appropriate processes and technical area standard. Provide technical direction to others in own work area while contributing to AT&T technology key contributor on diverse projects of moderate scope. Recommend new procedures to drive desired results. Build productive internal/external relationships and collaborate with others in own team or across teams.

Requirement: Requires a Bachelor’s degree, or foreign equivalent degree in Computer Engineering or Computer Science and two (2) years of experience in the job offered or two (2) years of experience in a related occupation planning for the development of high performance, distributed computing and data processing tasks using big data technologies including Hadoop and NoSQL in the areas of finance and Risk and Fraud Systems; utilizing java vm (jvm) based function languages including Scala, Hadoop query languages including pig, hive, along with alternative hdfs-based computing frameworks including spark; analyzing, designing, programing, debugging and modifying software enhancements and new products used in distributed, large scale analytics and visualization solutions; and Objecting Oriented Analysis and Design using Python Programming language and Agile Methodologies and Scrums.

Our Professional-Big Data Software-Engineers earn between $121,952 - $175,100 yearly. Not to mention all the other amazing rewards that working at AT&T offers.

Joining our team comes with amazing perks and benefits:
Medical/Dental/Vision coverage
401(k) plan
Tuition reimbursement program
Paid Time Off and Holidays (based on date of hire, at least 23 days of vacation each year and 9 company-designated holidays)
Paid Parental Leave
Paid Caregiver Leave
Additional sick leave beyond what state and local law require may be available but is unprotected
Adoption Reimbursement
Disability Benefits (short term and long term)
Life and Accidental Death Insurance
Supplemental benefit programs: critical illness/accident hospital indemnity/group legal
Employee Assistance Programs (EAP)
Extensive employee wellness programs
Employee discounts up to 50% off on eligible AT&T mobility plans and accessories, AT&T internet (and fiber where available) and AT&T phone

AT&T is an Affirmative Action/Equal Opportunity Employer, and we are committed to hiring a diverse and talented workforce. EOE/AA/M/F/D/V
*np*","$148,526 /yr (est.)",10000+ Employees,Company - Public,Telecommunications,Telecommunications Services,1876,$10+ billion (USD)
"BigLynx Computer Software
4.9",4.9,"Redmond, WA",AWS Data Engineer(Tech Lead),"BigLynx, Inc is an American multinational technology corporation headquartered in Seattle, Washington, with operations in the United States, Canada, and India. The company began in 2016, as a product development company specializing in AI/ML Data Engineering in the Retail vertical space with its products warehouse & fast. Post Pandemic in 2022, BigLynx added a business division of boutique technology consulting, specializing inData Engineering, Full Stack , and Microsoft Dynamics helping clients build the next generation data platform and big data pipelines.
Design and architect data solutions: Work closely with stakeholders to understand business requirements and design scalable and efficient data solutions on AWS. Create data architectures, data models, and data flow diagrams.
Data pipeline development: Develop, implement, and manage data pipelines to extract, transform, and load (ETL) data from various sources into AWS. Utilize AWS services such as AWS Glue, AWS Data Pipeline, or Apache Airflow for data integration and orchestration.
Data transformation and processing: Transform raw data into structured and usable formats for analytics and reporting purposes. Apply data manipulation techniques and develop data transformation workflows using AWS services like AWS Glue, AWS Lambda, or Apache Spark.
Data storage and management: Design and implement scalable data storage solutions on AWS, such as Amazon S3, Amazon Redshift, or Amazon DynamoDB. Optimize data storage and retrieval for performance and cost efficiency.
Data quality and governance: Ensure data quality, consistency, and accuracy through data cleansing, validation, and standardization processes. Implement data governance practices and adhere to data privacy and security standards.
Performance optimization: Identify and resolve performance bottlenecks in data pipelines and data processing workflows. Optimize query performance and data processing capabilities using AWS tools and techniques.
Team leadership and collaboration: Lead a team of data engineers, providing technical guidance, mentoring, and driving best practices. Collaborate with cross-functional teams, including data scientists, analysts, and stakeholders, to understand data requirements and deliver high-quality data solutions.
Cloud infrastructure management: Configure and manage AWS infrastructure components related to data engineering, such as EC2 instances, VPCs, IAM roles, and security groups. Monitor and troubleshoot issues related to infrastructure and data services on AWS.
Documentation and knowledge sharing: Create and maintain technical documentation, including design documents, architecture diagrams, and standard operating procedures. Share knowledge and provide training to team members and stakeholders.
Qualifications:
Extensive experience in data engineering: Minimum of [X] years of experience in data engineering roles, with a focus on AWS cloud-based solutions.
AWS expertise: In-depth knowledge and hands-on experience with AWS services related to data engineering, including AWS Glue, AWS Data Pipeline, Amazon S3, Amazon Redshift, or Amazon DynamoDB.
ETL and data integration: Strong proficiency in ETL processes, data integration techniques, and data transformation workflows. Familiarity with tools like Apache Spark, Apache Airflow, or AWS Glue for data manipulation and processing.
Database and data warehouse technologies: Solid understanding of relational databases, data warehousing concepts, and SQL. Experience with Amazon Redshift or other data warehousing solutions is preferred.
Programming and scripting: Proficiency in Python, SQL, and shell scripting for data engineering tasks. Knowledge of other programming languages such as Java or Scala is a plus.
Data modeling and schema design: Experience in designing and implementing data models, database schemas, and dimensional modeling concepts. Familiarity with schema design optimization for analytical workloads.
Cloud infrastructure and security: Strong understanding of AWS cloud infrastructure components, security controls, and best practices. Experience in managing AWS resources and implementing security measures for data.
Leadership and teamwork: Proven exper
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Experience level:
8 years
Schedule:
Monday to Friday
Work Location: In person",$75.00 /hr (est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"JPMorgan Chase Bank, N.A.
3.8",3.8,"Jersey City, NJ",Software Engineer III - Data Engineering,"We have an exciting and rewarding opportunity for you to take your software engineering career to the next level.

As a Software Engineer III at JPMorgan Chase, you serve as a seasoned member of an agile team to design and deliver trusted market-leading technology products in a secure, stable, and scalable way. You are responsible for carrying out critical technology solutions across multiple technical areas within various business functions in support of the firm's business objectives.

Job responsibilities
Executes software solutions, design, development, and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions or break down technical problems
Creates secure and high-quality production code and maintains algorithms that run synchronously with appropriate systems
Produces architecture and design artifacts for complex applications while being accountable for ensuring design constraints are met by software code development
Gathers, analyzes, synthesizes, and develops visualizations and reporting from large, diverse data sets in service of continuous improvement of software applications and systems
Proactively identifies hidden problems and patterns in data and uses these insights to drive improvements to coding hygiene and system architecture
Contributes to software engineering communities of practice and events that explore new and emerging technologies
Adds to team culture of diversity, equity, inclusion, and respect

Required qualifications, capabilities, and skills
Hands-on practical experience in system design, application development, testing, and operational stability using Scala, Core Java, and/or Python
Solid understanding of agile methodologies such as CI/CD, Applicant Resiliency, and Security
Experience with big data technologies such as Spark (ideally with Scala), Hadoop, Databricks or related technologies
Knowledge of Unix shell and SQL as well as NoSQL DBs is required.
Experience in developing, debugging, and maintaining code in a large corporate environment with one or more modern programming languages and database querying languages
Hands-on practical experience in system design, application development, testing, and operational stability
Proficient in coding in one or more languages

Preferred qualifications, capabilities, and skills
Experience performance tuning and applying modelling concepts with data (SQL and/or no-SQL)
Exposure to cloud technologies (AWS EMR, EC2, Snowflake)
Liquidity/Capital Markets/Prime Brokerage Risk domain knowledge/skills
JPMorgan Chase & Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.
We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.
The health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the ""WELL Health-Safety Rating"" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.
As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm's current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm's vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.

We offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, discretionary incentive compensation which may be awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.
Equal Opportunity Employer/Disability/Veterans
Base Pay/Salary
Jersey City,NJ $123,500.00 - $180,000.00 / year","$151,750 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Banking & Lending,1799,$10+ billion (USD)
Zllius Inc.,#N/A,"Canton, MI",Data Engineer AUG76,"Hi,
Greetings from Zllius Inc!
We have an urgent opening for the roleData Engineer. If interested, revert me back with your Updated resume, along with contact details, work authorization.
Job Role: Data Engineer
Location: Hartford, CT ( Initially Remote )
Duration: Long-term
Position: W2/1099
Visa: Any (Except H1B)
Job Ref: AUG76
Job Description:
Good experience on designing and developing data pipelines for data ingestion and transformation using Spark.
Distributed computing experience using Pyspark.
Good understanding of spark framework and spark architecture.
Experience working in Cloud based big data infrastructure.
Excellent in trouble shooting the performance and data skew issues.
Must have good understanding of spark run time metrics and tune applications based on metrics.
Deep knowledge in partitioning, bucketing concepts of data ingestion.
Good understanding of AWS services like Glue, Athena, S3, Lambda, Cloud formation.
Preferred working knowledge on the implementation of datalake ETL using AWS glue, Databricks etc.
Experience with data modelling techniques for cloud data stores and on prem databases like Teradata, Teradata Vantage (TDV) etc.
Preferred working experience in ETL development in Teradata vantage and data migration from on prem to Teradata vantage.
Proficiency in SQL, relational and non-relational databases, query optimization and data modelling.
Experience with source code control systems like Gitlab.
Experience with large scale distributed relational and NoSQL database systems.
Experience : 9+years
Thanks & Regards:
Zllius Inc.
844 495 5487
Job Types: Full-time, Contract
Schedule:
8 hour shift
Work Location: In person","$86,440 /yr (est.)",Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Augeo Affinity Marketing, Inc.
3.7",3.7,United States,Data Analytics Engineer,"Hello, we're Augeo-architects of enterprise engagement and loyalty platforms, delivering compelling experiences and fostering meaningful connections with employees, customers, members and channel partners across industries. We serve thousands of clients, including more than 70 of the top Fortune 500 companies, representing millions of end users. We are thought leaders and disruptors who think differently and creatively, built by entrepreneurs, operators and innovators.
At Augeo, we're passionate about providing an inclusive workplace that values diversity. Everyone is welcome, and our employees are comfortable bringing their authentic whole selves to work. Be you.
We work hard, we play hard-and most importantly, we care to our CORE about our teams and each other. We over-communicate around everything, especially while we are all connected virtually.
Summary/Objective
A data analytics engineer is responsible for creating and implementing personalized user recommendations. This involves using data and analytics to understand user behavior and preferences, and developing algorithms and models to recommend personalized content, products, and services to each user.
The main responsibilities may include:
Collaborating with designers, product managers, and other stakeholders to understand user needs and develop personalized solutions.
Designing and implementing personalization algorithms and models using machine learning, data mining, and other relevant techniques.
Analyzing user data and behavior to identify patterns and develop insights that can be used to improve the personalized user experience.
Keeping up to date with the latest trends and technologies in personalization and machine learning and applying these insights to enhance the personalization capabilities of the platform.
The ideal candidate for this role should have a strong background in computer science, software engineering, data science, or a related field. They should have experience with the AWS cloud platform, preferably the AWS Personalize service, and be familiar with machine learning and data mining techniques. Additionally, strong analytical and problem-solving skills are essential, as well as excellent communication and collaboration abilities to work in a team.
Requirements
Coding proficiency in at least one modern programming language (Python, R, Java, etc)
Relational database experience in SQL, AWS RDS
Experience with ML Services in AWS (Personalize, SageMaker) or equivalent a plus.
Strong critical thinking, communication, and problem solving skills and a quick learner.
Experience with cloud-based platforms (particularly AWS)
Experience working in multi-developer environment, using version control (i.e. Git)
Write Extract-Transform-Load (ETL) jobs to calculate business metrics
Own the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions
Monitor and troubleshoot operational or data issues in the data pipelines
Strong verbal and written communication skills.
Ability to operate independently and in teams.
Experience working within an Agile environment and JIRA preferred
Education and Experience
Bachelor’s Degree in Computer Science or related area
1-3 years of relevant work experience in analytics, data engineering, business intelligence or related field.
Experience using SQL queries, experience in writing and optimizing SQL queries in a business environment with large-scale, complex datasets
Knowledge of data warehouse technical architecture, infrastructure components, ETL and reporting/analytic tools and environment
Benefits of joining our team
Medical, Dental & Vision Insurance
Employer-sponsored Long-term disability and Life Insurance
Paid Time Off and flexible work schedule
401(k) Plan
Fun and casual work environment
Career growth opportunities
Rewards & Incentives",#N/A,201 to 500 Employees,Company - Private,Media & Communication,Advertising & Public Relations,1999,$100 to $500 million (USD)
"Acuity Brands
3.6",3.6,"Conyers, GA",Data Engineer Senior,"We Light the Way!

Acuity Brands, Inc. (NYSE: AYI) is a market-leading industrial technology company. We use technology to solve problems in spaces and light. Through our two business segments, Acuity Brands Lighting and Lighting Controls (“ABL”) and the Intelligent Spaces Group (“ISG”), we design, manufacture, and bring to market products and services that make the world more brilliant, productive, and connected. We achieve growth through the development of innovative new products and services, including lighting, lighting controls, building management systems, and location-aware applications.
Job Summary
Acuity’s Business Intelligence team is comprised of people who are passionate about data. We believe accurate, timely and understandable data is vital to a data driven culture. We are devoted to aligning Acuity’s data to serve the needs of the enterprise and its customers. You will be joining a team of seasoned Business Intelligence professionals well versed in architecting bespoke BI (Business Intelligence) applications and implementing Microsoft Azure BI products.
We are seeking a talented and enthusiastic individual to be a Senor Data Engineer on our Business Intelligence Team as we transform Acuity Brands analytics and migrate our BI platform to Azure and Power BI. This position will work closely with BI Architects to bring to deliver end to end BI solutions that are innovative, scalable, and responsive.
Key Tasks & Responsibilities (Essential Functions)
Research, architect, drive, and deploy scalable, resilient cloud agnostic BI solutions to address Acuity current and future business needs and obligations
Partner with Data Architect, Solution Architect, and BI Product Managers to drive technology transformation on the BI platform to ensure the BI platform remains current and responsive
Mentor and guide Senior BI Developers to ensure adherence to BI standards and procedures
Partner with Data Architect to deliver integrated end to end data engineering solutions
Write application and cloud-based data processing code to transform inbound data to meet business requirements
Design and develop data models leveraging advanced modeling techniques to handle large and or complex data
Modify and optimize data engineering processes to handle ever-growing, complex, diverse data formats, sources, and pipelines
Work with infrastructure partners to tune and optimize code and BI resources such as but not limited to (index tuning, partitioning, caching, buffer tuning, and data archiving strategies.
Proactively estimate and plan development work and track performance to deliver work on schedule
Create and maintain current documentation in GIT (c4 and Plant UML)
Education (minimum education required)
Bachelor of Science
Preferred Education (i.e. type of degree)
Bachelor of Science in Computer Science or Information Systems
Experience (minimum experience required)
Bachelor’s Degree in Computer Science, MIS (Management Information System), or other technical/analytical field (or equivalent experience)
2 Azure Certifications
Working knowledge of data warehousing principles (Kimball, Inmon, Hybrid)
4-7 years or more database programming experience (SQL (preferred), Oracle, DB2)
4-7 years or more of BI experience (Power BI, Tableau, Qlik Sense/View, D3.js, SAP Business Objects, IBM Cognos)
4-7 years or more of working with Microsoft BI stack (SSIS, SSAS, T-SQL)
4-7 years or more of developing and enhancing ETL packages
4-7 years or more of working with and or constructing API (Push, Get, Post)
4-7 years or more of advanced experience identifying and optimizing database objects
2 years or more of application development (C# or .NET)
3 years or more experience working in Python or Scala
We invite you to apply today to join us as We Light the Way to a Brilliant, Productive, and Connected World!

We value diversity and are an equal opportunity employer. All qualified applicants will be considered for employment without regards to race, color, age, gender, sexual orientation, gender identity and expression, ethnicity or national origin, disability, pregnancy, religion, covered veteran status, protected genetic information, or any other characteristic protected by law.
Please click here and here for more information.

Accommodation for Applicants with Disabilities: As an equal opportunity employer, Acuity Brands is committed to providing reasonable accommodations in its application process for qualified individuals with disabilities and disabled veterans. If you have difficulty using our online system due to a disability and need an accommodation, you may contact us at (770) 922-9000. Please clearly indicate what type of accommodation you are requesting and for what requisition.

Any unsolicited resumes sent to Acuity Brands from a third party, such as an Agency recruiter, including unsolicited resumes sent to an Acuity Brands mailing address, fax machine or email address, directly to Acuity Brands employees, or to Acuity Brands resume database will be considered Acuity Brands property. Acuity Brands will NOT pay a fee for any placement resulting from the receipt of an unsolicited resume.

Acuity Brands will consider any candidate for whom an Agency has submitted an unsolicited resume to have been referred by the Agency free of any charges or fees. This includes any Agency that is an approved/engaged vendor, but does not have the appropriate approvals to be engaged on a search.

E-Verify Participation Poster
e-verify.gov
eeoc.gov","$92,338 /yr (est.)",10000+ Employees,Company - Public,Manufacturing,Electronics Manufacturing,2001,$1 to $5 billion (USD)
"Braintrust
4.6",4.6,"San Francisco, CA",Data Visualization Engineer - Tableau,"ABOUT US:
Braintrust is a user-owned talent network that connects you with great jobs with no fees or membership costs—so you keep 100% of what you earn.

JOB TYPE: Freelance, Contract Position (no agencies/C2C - see notes below)
LOCATION: Remote - Work from anywhere
HOURLY RANGE: Our client is looking to pay $75 – $100/hr
ESTIMATED DURATION: 20h/week - Long term

ABOUT THE HIRING PROCESS:
When you join Braintrust, you will be invited to a screening process for Braintrust to learn more about your previous work experiences. Once completed, you will have access to the employer for this role and other top companies that seek high-quality talent. Apply to this job to kick off the process.
THE OPPORTUNITY
Skills Required:
Expertise in handling Tableau products and building in Tableau
Problem-solving skills
Proficiency in handling SQL and databases
Should be good in written & verbal communication
Good to Have Skills:
English and French speaking
SaaS industry experience
Roles & Responsibilities:
Elicit business requirements and develop measurement plans for strategic programs
Collect and analyze data through manual and automated collection methods
Translate abstract data into easily-digestible and highly-visual dashboard applications and executive-level report deliverables
Collaborate with other analysts, data engineers, and data scientists to provide exceptional customer support to our internal stakeholders
Analyze large data sets and translate abstract data into highly visual and easily-digestible
What you’ll be working on
As a Senior Developer, you are responsible for the development, support, maintenance, and implementation of a complex project module.
You should have good experience in the application of standard software development principles. You should be able to work as an independent team member, capable of applying judgment to plan and execute your tasks. You should have in-depth knowledge of at least one development technology/programming language.
Apply Now!
Braintrust Job ID: 6721
C2C Candidates: This role is not available to C2C candidates working with an agency. If you are a professional contractor who has created an LLC/corp around their consulting practice, this is well aligned with Braintrust and we’d welcome your application.

Braintrust values the multitude of talents and perspectives that a diverse workforce brings. All qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status.

This is a remote position.",$87.50 /hr (est.),1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,2018,$100 to $500 million (USD)
"Calhoun International, LLC
3.7",3.7,"Washington, DC",Data Engineer (Senior),"About Us:
Calhoun International is a Professional Services company providing innovative solutions to our clients. Our expertise ranges from strategic intelligence analysis, expert instruction on intelligence analysis and sensors, cyberspace operations, information systems training, and knowledge management services among others. Calhoun International is located in Tampa, FL with employees in Florida, Virginia, Maryland, Washington, D.C. and overseas. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, or national origin.
Responsibilities:
Executes and on occasion, leads research, analysis, and evaluation efforts as well as studies, analyses, assessments, and technical reports, which may include the use of existing documents, databases, models, architectures and simulations. Deliverables produced shall be in a variety of formats in response to a wide range of requirements and delivery schedules. Provides mid to high-level analytical assessments and advice on complex issues, which require extensive knowledge of the subject matter. May attend various types of symposia and meetings at the ARSTAF and DOD level.
Requirements:
Minimum Education: bachelor’s degree; advanced intelligence discipline training; or other equivalent DoD or service Intelligence experience.
Minimum Experience: Fourteen (14) years of experience as an Army Intelligence analyst with experience from tactical to strategic. Experience shall have been within Two (2) years of starting on this contract.
Has served as a staff action officer at the HQDA (DCS, G-2 preferred) or Joint or a closely related DOD organization/agency.
Demonstrated SME level of knowledge of intelligence fusion systems, capabilities / employment, training, associated R&D efforts and program budget processes.
Demonstrated knowledge of Joint and Army processes –JCIDS, TAA, PED, JUONS/ONS, and CONOPS.
Experience in preparation and presentation of briefings on projects, studies and analysis to senior leaders and other ARSTAF action officers.
Desired:
Graduate from the Command and General Staff College or similar Senior Staff College
Security Clearance:
Active Top Secret with SCI eligibility required.","$127,546 /yr (est.)",1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,#N/A,$1 to $5 million (USD)
"HCA Healthcare
3.3",3.3,"Nashville, TN",Senior Data Engineer,"Introduction
Last year our HCA Healthcare colleagues invested over 156,000 hours volunteering in our communities. As a Sr Database Admin with HCA Healthcare you can be a part of an organization that is devoted to giving back!
Benefits
HCA Healthcare offers a total rewards package that supports the health, life, career and retirement of our colleagues. The available plans and programs include:
Comprehensive medical coverage that covers many common services at no cost or for a low copay. Plans include prescription drug and behavioral health coverage as well as free telemedicine services and free AirMed medical transportation.
Additional options for dental and vision benefits, life and disability coverage, flexible spending accounts, supplemental health protection plans (accident, critical illness, hospital indemnity), auto and home insurance, identity theft protection, legal counseling, long-term care coverage, moving assistance, pet insurance and more.
Free counseling services and resources for emotional, physical and financial wellbeing
401(k) Plan with a 100% match on 3% to 9% of pay (based on years of service)
Employee Stock Purchase Plan with 10% off HCA Healthcare stock
Family support through fertility and family building benefits with Progyny and adoption assistance.
Referral services for child, elder and pet care, home and auto repair, event planning and more
Consumer discounts through Abenity and Consumer Discounts
Retirement readiness, rollover assistance services and preferred banking partnerships
Education assistance (tuition, student loan, certification support, dependent scholarships)
Colleague recognition program
Time Away From Work Program (paid time off, paid family leave, long- and short-term disability coverage and leaves of absence)
Employee Health Assistance Fund that offers free employee-only coverage to full-time and part-time colleagues based on income.
Learn more about Employee Benefits
Note: Eligibility for benefits may vary by location.

Would you like to unlock your potential with a leading healthcare provider dedicated to the growth and development of our colleagues? Join the HCA Healthcare family! We will give you the tools and resources you need to succeed in our organization. We are looking for an enthusiastic Sr Database Admin to help us reach our goals. Unlock your potential!
Job Summary and Qualifications
As a Sr. Data Engineer you will have the opportunity to grow and leverage your technical skills in MPP systems like that of Teradata’s to perform physical database design, SQL tuning and support our Teradata Data Warehouse platform on-prem and in the public cloud. You will be performing various operational tasks, including, but not limited to, system monitoring, object deployment, backup and recovery, user provisioning, query optimization and system maintenance. The job requires communicating with data architecture, integration and reporting teams for adherence to best practices and process standardization.
The role requires self-starters who are proficient in problem solving and communicating complicated, technical issues with clarity to the other technology teams and stakeholders. The culture of our organization places an emphasis on teamwork, so social and interpersonal skills are equally important as technical capability.

Assist in developing automation of various operational tasks performed by a DBA.
Work with cross-functional teams to build the physical databases and provide technical guidance during all phases of the development process.
Work with team in researching, evaluating and implementing new technologies as needed.
Provide regular, clear, and consistent communication (written and oral) on the status of projects, issues, and deliverables to team leadership.
Work with vendor technical support to facilitate analysis of and resolution to technical issues.
Provide planning input to leadership – operational and tactical – in order to drive success for team and company goals.
Participate in planned system maintenance tasks.
Provide rotational on-call support
What qualifications you will need:
Bachelors Degree preferred
Five or more years of relevant work experience
Other/Special Qualifications
Teradata development and/or administration (5+ years).
Teradata certification(s).
Shell Scripting and/or python or any development experience using C/C++/Java
Exposure to Cloud technologies
HCA Healthcare has been recognized as one of the World’s Most Ethical Companies® by the Ethisphere Institute more than ten times. In recent years, HCA Healthcare spent an estimated $3.7 billion in cost for the delivery of charitable care, uninsured discounts, and other uncompensated expenses.
""There is so much good to do in the world and so many different ways to do it.""- Dr. Thomas Frist, Sr.
HCA Healthcare Co-Founder
Be a part of an organization that invests in you! We are reviewing applications for our Sr Database Admin opening. Qualified candidates will be contacted for interviews. Submit your application and help us raise the bar in patient care!
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.","$115,561 /yr (est.)",10000+ Employees,Company - Public,Healthcare,Health Care Services & Hospitals,1968,$10+ billion (USD)
"Gridiron IT
4.5",4.5,Remote,Sr. Data Engineer,"GridironIT is seeking a Data Engineer.
Responsibilities:
We are looking for seasoned Data Engineer to work with our team and our clients to develop enterprise grade data platforms, services, and pipelines. We are looking for a more than just a ""Data Engineer"", but a technologist with excellent communication and customer service skills and a passion for data and problem solving.
Lead and architect migration of data environments with performance and reliability.
Assess and understand the ETL jobs, workflows, BI tools, and reports
Address technical inquiries concerning customization, integration, enterprise architecture and general feature / functionality of data products
Experience in crafting database / data warehouse solutions in cloud (Preferably AWS. Alternatively Azure, GCP).
Key must have skill sets – Python, AWS
Support an Agile software development lifecycle
You will contribute to the growth of our Data Exploitation Practice!
Qualifications:
US Citizen Only
Ability to hold a position of public trust with the US government.
8+ years industry experience coding commercial software and a passion for solving complex problems.
8+ years direct experience in Data Engineering with experience in tools such as:Big data tools: Hadoop, Spark, Kafka, etc.
Relational SQL and NoSQL databases, including Postgres and Cassandra
Data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
AWS cloud services: EC2, EMR, RDS, Redshift
Data streaming systems: Storm, Spark-Streaming, etc.
Search tools: Solr, Lucene, Elasticsearch
Object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Amazon S3, Athena, Redshift Spectrum, AWS Glue, AWS Glue Catalog, AWS Functions, and Amazon EC2 with SQL Server Developer
Advanced working SQL knowledge and experience working with relational databases, query authoring and optimization (SQL) as well as working familiarity with a variety of databases.
Experience with message queuing, stream processing, and highly scalable ‘big data’ data stores.
Experience manipulating, processing, and extracting value from large, disconnected datasets.
Experience manipulating structured and unstructured data for analysis
Experience constructing complex queries to analyze results using databases or in a data processing development environment
Experience with data modeling tools and process
Experience architecting data systems (transactional and warehouses)
Experience aggregating results and/or compiling information for reporting from multiple datasets
Experience working in an Agile environment
Experience supporting project teams of developers and data scientists who build web-based interfaces, dashboards, reports, and analytics/machine learning models
Experience with Informix and Data Stage
Job Type: Full-time
Pay: $84,436.60 - $150,799.40 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Monday to Friday
Experience:
AWS Glue: 5 years (Required)
SQL: 8 years (Required)
Data warehouse: 5 years (Required)
Work Location: Remote","$117,618 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable
"Par Government Systems Corporation
4.4",4.4,"Bethesda, MD",Senior Data Engineer,"PAR Government is excited to welcome a Senior Data Engineer to the our Intelligence and Readiness Operations. As a Senior Data Engineer you will support a large-scale intelligence processing system that is focused on digital document exploitation.

Establish a data engineering processes to support the understanding of information needs to maximize the use and value of data and information assets by consumers. You will manage information consistently across the enterprise and align data management efforts and technologies with business needs. Key task areas on the program include forensic image processing, machine learning model production, knowledge graph construction and reasoning, agile development, system security, technology transition, and system operations.

Responsibilities and Duties:
Analyze, design, build, test, deploy, operate, and maintain solutions, capabilities, and services to meet data needs.
Plan and lead major technology assignments, evaluate performance results, and recommend major changes affecting short-term project growth and success.
Conduct requirements analysis.
Conduct requirements design.
Implementation solutions
Maintenance databases of related solution components


Required Skills/Experience:
Must have an active or in scope US Top Secret Clearance with SCI Eligibility
Expertise in analyzing. designing, building, testing, deploying. operating, and maintaining solutions, capabilities, and services to meet data needs, including requirements analysis, and design, implementation, and maintenance of databases' related solution components.
Bachelors degree or equivalent with a minimum of 10 years of experience as a Data Engineer
Experience with defining and recording data requirements and delivering data requirements specifications.
Experience with developing and maintaining conceptual data models and delivering conceptual data model diagrams, logical data models, physical data models, and physical databases .
Experience with managing data model versions and integrating and delivering data model libraries.
Expertise in designing data integration services and delivering source-to-target maps, data extract-transform- load (ETL) design specifications, and data conversion designs.
Expertise in writing software code and scripts to distributed the processing of information extraction tasks to identify entities, events, and relationships from large corpus of structured and unstructured data and multimedia stored in a distributed file system ox object store.
Experience with applying data cleansing, transformation, and augmentation methods to measure and improve data quality.
Experience building, testing, and delivering data integration services
Experience with establishing Golden Records and delivering reliable reference and master data.
Experience defining, delivering, and maintaining hierarchies and affiliations that define the meaning of data within the context of its interrelationships with other data
Experience with importing and exporting data between an external RDBMS and a Hadoop cluster, including the ability to import specific subsets, change the delimiter and file format of imported data during ingest, and alter the data access patten or privileges.
Experience ingesting real-time and near-real time (NRT) Streaming data into the Hadoop File System (HDFS), including the ability to distribute to multiple data sources and convert data on ingest from one format to another.
Expertise in loading data into and out of the Hadoop File System (HDFS) using the HDFS command line interface; converting sets of data values in a given format at stored in Hadoop File System (HDFS) into new data values and/or a new data format and writing them into HDFS or Hive/HCatalog.
Expertise in filtering, sorting, joining, aggregating, and transforming one or more data sets in a given format (e.g., Parquet, Avro, JSON, delimited text, and natural language text) stored in the Hadoop Distributed Filesystem (HDFS)

It is the policy of PAR to prohibit all forms of discrimination and to affirmatively implement equal opportunity to all qualified employees and applicants for employment without regard to race, color, creed, religion, sex, age, veteran status, national origin, disability, marital status, predisposing genetic characteristics, sexual orientation, gender identity, or other legally protected status and positive action shall be taken to insure the fulfillment of this policy.
If you require reasonable accommodation in the application process, call Human Resources at 315.356.2260. All other applications must be submitted online.***

Required Skills

Required Experience","$122,700 /yr (est.)",201 to 500 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"CEP Global
5.0",5.0,"San Francisco, CA",Associate Data Systems Engineer,"(Data Systems Engineer I)
San Francisco, CA / Cambridge, MA
We believe, and research shows, that student and stakeholder feedback matters for school improvement and student success. That’s why, in 2008, we created YouthTruth – to help educators harness student and stakeholder feedback to accelerate improvements. We’ve heard from over two million students as well as hundreds of thousands of family and staff members across 39 states. When you join YouthTruth, you join a small and collaborative team that has a big impact on schools across the U.S. and the education sector.
As one of the go-to technical experts in the room, the Associate Data Systems Engineer will be instrumental in projects that improve the organization’s use of data, identify and drive efficiencies in our processes, and work closely with staff to maintain and upgrade our in-house online reporting system. To that end, the Associate Data Systems Engineer will build and develop cloud-based data pipelines and architecture while maintaining and improving back-end Python applications for this system, which are used to develop and deliver YouthTruth’s student survey reports to schools, districts, and networks nation-wide and the Center for Effective Philanthropy’s (CEP’s) assessment tools to foundation leaders.
This position is available in either our San Francisco, CA or Cambridge, MA office.
In this role, you will lead and execute a variety of tasks, including:
Developing and maintaining (debugging, improving, testing, and deploying) Python scripts
Ensuring smooth functioning of our development, quality assurance, and production environments
Supporting the creation and rollout of our cloud-based data ingestion and translation architecture
Building integrations applications and solving integrations issues
Creating and executing unit test plans based on system and validation requirements
Documenting changes in software for end users
Maintaining and building upon our relational databases
Serving as a thought partner with colleagues and internal stakeholders about the feasibility and time intensiveness of proposed projects
As an integral part of CEP’s tech team, your time will be spent on CEP’s technology infrastructure, touching both the YouthTruth and Assessment and Advisory Services teams’ projects.
The Ideal Candidate:
You exhibit excellent judgment and have the ability to work with low direction to support your data and client analyst colleagues in a fast-paced environment while being thorough and results-oriented.
You take pride in your work and employ the necessary steps to ensure the production of reliable and sustainable code.
You have exceptional communication skills. You are collaborative and excited to work cross-functionally while building relationships and generating influence across the company.
You bring an aptitude for continuous improvement within processes and disciplines, helping to evolve from the current state into leveraging new mindsets and approaches.
You are tolerant of ambiguity and have the desire to create structures, evolve systems, and build new capabilities to better serve our customers.


Your Background, Experience, and Interests:
Experience with the following languages: Python and SQL
At least 2 years’ experience as a software developer, data analyst, engineer, or similar, in a full-time role.
Experience using Python with datasets
Experience with SQL language and tools to help build data transformation models
Understanding of cloud architectures and their applications within a data-driven environment
Integration among different applications through APIs
Experience with code management and review such as Github
A degree (or related practical experience) in computer programming, or related field
Knowledge and practice of common SDLC process methodologies (e.g., AGILE, SCRUM, Waterfall) to job function
Experience with AWS, data modeling and implementation, and/or Ruby on Rails a plus but not necessary


Benefits:
Our nonprofit model is central to our identity: our bottom line is impact, not profit. Yet even as a nonprofit, we successfully compete for top talent across both the public and private sectors and offer competitive compensation and benefits, including:
A commitment to pay parity and salary equity. The annual base salary for this position is $113,636 (Cambridge)/ $125,000 (SF). In addition, this role is eligible for a competitive Performance Based Incentive Compensation.
Comprehensive health, vision, and dental insurance plans.
Generous paid time off plan, including up to 15 holidays, three weeks of accrued vacation, and two personal days per year.
401(k) plan with a 1:1 Employer Match up to 5% of total compensation.
Generous annual personal professional development allowance.
Flexible spending and dependent care tax free savings plans.
Life insurance covered 100% by the organization.
We believe diversity and inclusion are key drivers of creativity and innovation, and we actively seek out candidates from many types of diverse backgrounds to apply for this exciting role.

About CEP:
For more than 20 years, CEP has led the movement to improve philanthropy through a powerful combination of dispassionate analysis and a passionate commitment to improving lives. Today, over 350 foundations have used CEP’s assessment tools to gather honest feedback from their stakeholders in an effort to learn how to be even more effective. CEOs and trustees have come to rely on our research for insights into foundation effectiveness on a wide range of topics, from assessing performance to developing strategy to managing stakeholder relationships. Our highly regarded programming—including our biennial conference—gives foundation leaders an exclusive and unprecedented opportunity to connect with their peers. YouthTruth harnesses student and stakeholder feedback to help school leaders, school system leaders, and funders accelerate improvements. Through our validated surveys and tailored advisory services, we help schools, districts, states, YouthTruth partners with schools, districts, and funders to enhance learning for all students. Since 2008, YouthTruth has surveyed over 2.6 million students and nearly 600,000 family and school staff members across 39 states. Strengths of CEP’s work culture are entrepreneurialism, accountability, teamwork, collegiality, diversity, and mutual respect. CEP is based in Cambridge, Massachusetts, with a second office in San Francisco, California.
Location:
We have offices in Cambridge, Massachusetts and San Francisco, California. The YouthTruth team resides alongside staff in other departments at The Center for Effective Philanthropy (CEP), YouthTruth’s parent nonprofit. This role can be based in either our Cambridge, MA or San Francisco, CA office.
We strive to balance in-person time with flexibility and the needs of each person, their team, and the larger organization. We believe culture, communication, trust, training, and certain kinds of creative work benefit from in-office interactions – and we believe that the flexibility of remote work also has many advantages. Moving forward we expect to ask staff to be in the office one to two days per week, and everyone will enjoy the option to work fully remotely from anywhere four weeks per year.
Our Process:
Our rolling process includes three interview rounds: an initial phone conversation, a skills assessment, and a team interview round. Our process also includes the checking of references in final stages. Check out CEP’s careers page to learn more about how we hire.
To Apply:
Please fill out our application for employment and attach a resume and thoughtful cover letter, outlining how your skills and experience meet the qualifications of the position.
If you have any questions, please contact Leaha Wynn, Senior Manager, People and Culture; Diversity and Inclusion Strategist or Alyse d’Amico, Vice President, People & Culture at jobs [at] cep[dot] org. Applications will be reviewed on a rolling basis.
We believe that a diversity of thoughts, experiences, backgrounds, personalities, and identities helps us think bigger and better, and enables us to reach our goals more effectively. We are committed to building a diverse staff and encourage individuals from all backgrounds to apply.
Once a candidate begins our process, we discourage outreach to our staff for interviews or conversations that are outside our standard interview process. We have worked hard to design a process that is fair and rigorous and achieves a good match between candidates and CEP.
CEP evaluates candidates based on their merits. We strongly discourage unsolicited references. We will ask for references if and when a candidate reaches a finalist stage.","$113,636 /yr (est.)",Unknown,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"JPMorgan Chase Bank, N.A.
3.8",3.8,"Jersey City, NJ",Software Engineer II - Data Engineering,"You're ready to gain the skills and experience needed to grow within your role and advance your career - and we have the perfect software engineering opportunity for you.

As a Software Engineer at JPMorgan Chase, within the Corporate Sector, Finance Technology team, you are part of an agile that works to enhance, design, and deliver the software components of the firm's state-of-the-art technology products in a secure, stable, and scalable way. As an emerging member of a software engineering team, you execute software solutions through the design, development, and technical troubleshooting of multiple components within a technical product, application, or system, while gaining the skills and experience needed to grow within your role.
Job responsibilities

Executes standard software solutions, design, development, and technical troubleshooting
Writes secure and high-quality code using the syntax of at least one programming language with limited guidance
Designs, develops, codes, and troubleshoots with consideration of upstream and downstream systems and technical implications
Applies knowledge of tools within the Software Development Life Cycle toolchain to improve the value realized by automation
Applies technical troubleshooting to break down solutions and solve technical problems of basic complexity
Gathers, analyzes, and draws conclusions from large, diverse data sets to identify problems and contribute to decision-making in service of secure, stable application development
Learns and applies system processes, methodologies, and skills for the development of secure, stable code and systems
Adds to team culture of diversity, equity, inclusion, and respect
Required qualifications, capabilities, and skills

Looking for top-notch lead engineering talent who will be able to thrive in an entrepreneurial environment that demands quick turnaround for mission critical technical solutions, have extremely high standards with a low tolerance for low quality output.
Strong background in computer science concepts.
Minimum of 3+ years of server side development using Java, Scala and/or Python.
Excellent oral and written, and problem-solving skills are required.
Candidate should be comfortable working in a fast-paced environment and can help build APIs, Calculators, on new cutting edge cloud and big data technologies such as AWS EMR, EC2, Scala Spark, Scala, Snowflake
Ideal candidates should have strong analytical skills and a penchant for tackling complex problems and designs of scale.
Knowledge of Unix shell and SQL as well as NoSQL DBs is required.
Preferred qualifications, capabilities, and skills

Proactively improve support services by building upon best practice and tools
Cloud certification.
JPMorgan Chase & Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.
We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.
The health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the ""WELL Health-Safety Rating"" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.
As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm's current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm's vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.

We offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, discretionary incentive compensation which may be awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.
Equal Opportunity Employer/Disability/Veterans
Base Pay/Salary
Jersey City,NJ $109,250.00 - $145,000.00 / year","$127,125 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Banking & Lending,1799,$10+ billion (USD)
"Dentsu Aegis Network
3.6",3.6,"Raleigh, NC",Data Engineer - BI Developer,"Company Description

Dentsu International is a modern marketing solutions company. Our mission is to help clients navigate, progress and thrive in a world of change. Businesses rely on our integrated network of agencies and specialized practices to champion meaningful progress through creative, media, commerce, data, and technology. Part of Dentsu Group, our global network comprises 66,000 diverse people in 143 countries, who are dedicated to teaming for growth and good. Some of our award-winning agencies include 360i, Carat, dentsumcgarrybowen, DEG, dentsuX, iProspect and Merkle. Follow us on Twitter @DentsuUSA and visit dentsu.com/us.
Part of Dentsu International, Dentsu Creative is a Global Creative Network that transforms brands and businesses through the power of Modern Creativity. Led by Global Chief Creative Officer Fred Levron, 9,000 experts across the globe work seamlessly together to deliver ideas that Create Culture, Shape Society and Invent the Future. Dentsu Creative was launched in June 2022 to address a client need for simplicity and will be Dentsu International’s sole creative network by the end of 2022.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.

Job Description

The Data Engineer is a new key role within the Business Platforms Americas team. We are looking for a well-rounded Senior BI Developer expertise with strong knowledge of MS SQL, PowerBI, and data warehousing techniques. This is a unique opportunity to be involved in delivering leading-edge business analytics using the latest and greatest cutting-edge BI tools, such as cloud-based databases, self-service analytics and leading visualization tools enabling the company’s aim to become a fully digital organization.
Key Responsibilities
Collaborate with the BI Dev team members to evaluate, design, develop BI reports and dashboards according to functional specifications while maintaining data integrity and data quality
Deliver Technical Design Document capturing specific processes and data flows, data definitions and relevant business rules
Apply best practices of data integration for data quality and automation
Work with business analysts to understand business requirements and use cases to write and assign technical stories and tasks
Work independently within guidelines, responsible for initiating, planning, executing, controlling, and implementation of projects using a formal project management and agile methodology
Work collaboratively with key stakeholders to translate business information needs into well-defined data requirements to implement the BI solutions
Work with team to provide support for existing analytics and PowerBI reporting platforms
Coaching, mentoring, and providing technical direction and training to other IT personnel
Working with BI & Analytic teams to develop and establish BI road Map/Vision

Qualifications

Experience:
Excellent communication skills
Over 7-10 years of experience in Data warehousing and Business Intelligence
Over 5 years’ experience in a Business Intelligence Analyst or Developer roles
Over 4 years’ experience using ADF for data warehousing
Experience in designing and performance tuning data warehouses and data lakes.
2+ years experience in developing data models and dashboards using Power BI within an IT department
Being delivery-focused with a can-do attitude in a sometimes-challenging environment is essential.
Experience using Power BI to visualize data held in SQL Server
Experience working with finance data highly desirable
Other key Competencies:
Strong communications skills and ability to turn business requirements into technical solutions
Experience in developing data lakes and data warehouses using Microsoft Azure
Demonstrable experience designing high-quality dashboards using Power BI
Strong database design skills, including an understanding of both normalized form and dimensional form databases.
In-depth knowledge and experience of data-warehousing strategies and techniques e.g., Kimble Data warehousing
Experience in Cloud based data integration tools like Azure Data Factory
Experience in power bi data modelling and DAX is preferred
Experience in Azure Dev Ops or JIRA is a plus
Familiarity with agile development techniques and objectives

Additional Information

The anticipated salary range for this position is $94,000-146K. Salary is based on a wide range of factors that include relevant experience, knowledge, skills, other job-related qualifications, and geography. A range of medical, dental, vision, 401(k) matching, paid time off, and/or other benefits also are available. For more information regarding dentsu benefits, please visit https://dentsubenefitsplus.com/.

Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact recruiting@dentsuaegis.com if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.

#LI-AJ1
About dentsu
Dentsu is the network designed for what’s next, helping clients predict and plan for disruptive future opportunities in the sustainable economy. Taking a people-centered approach to business transformation, dentsu combines Japanese innovation with a diverse, global perspective to drive client growth and to shape society www.dentsu.com.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.
Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact your recruiter if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.","$120,000 /yr (est.)",10000+ Employees,Company - Public,Media & Communication,Advertising & Public Relations,#N/A,Unknown / Non-Applicable
"CNA Insurance
3.9",3.9,"Chicago, IL",Senior Data Engineer,"You have a clear vision of where your career can go. And we have the leadership to help you get there. At CNA, we strive to create a culture in which people know they matter and are part of something important, ensuring the abilities of all employees are used to their fullest potential.
CNA seeks to offer a comprehensive and competitive benefits package to our employees that helps them — and their family members — achieve their physical, financial, emotional and social wellbeing goals.

For a detailed look at CNA’s benefits, check out our
Candidate’s Guide
.
JOB DESCRIPTION:
Essential Duties & Responsibilities
Performs a combination of duties in accordance with departmental guidelines:
Lead the design and build data solutions and applications that enable reporting, analytics, data science, and data management.
Design, develop and implement data integration projects using Informatica and SSIS.
Provide Azure application insights and Cloud-based integration using Python and PowerShell Scripts.
Create analytical Business Intelligence (BI) reports using Google Analytics for web applications.
Lead the design, implementation and automation of data pipelines, including sourcing data from internal and external systems and transforming the data for the optimal needs of various systems and business requirements.
Lead robust unit testing to ensure deliverables match the design and provide expertise to support subsequent release testing.
Apply machine learning concepts to development work.
Adhere to and establish quality and reliability standards, and ensure team adheres to the same quality and standards working in an Agile development environment.
Design complex physical data models, projects and cloud-based data lake constructs including SQL/NoSQL database systems.
Research, identify and implement process improvements that address complex technology gaps and build strong knowledge of technology enablers.
Maintain professional and technical knowledge by attending educational workshops, reviewing professional publications, establishing personal networks, and participating in professional societies.
Reporting Relationship
Typically Manager or above
Education & Experience
Bachelor’s degree in computer science, information technology or related and 5 (five) years of experience in data analytics or application development.
Must have work experience with each of the following:
Design, develop and implement data integration projects using Informatica and SSIS;
Provide Azure application insights and Cloud-based integration using Python and PowerShell Scripts;
Create analytical Business Intelligence (BI) reports using Google Analytics for web applications.
Primary Location United States – Illinois – Chicago
Organization – IT
Mon-Fri., 8:30am – 4:45pm, 37.5 hours/week, $140,046 to $148,800 per year, overtime exempt. This position qualifies for CNA’s employee referral policy program.
Apply: Submit cover letter and resume at
www.cna.com
.
CNA is committed to providing reasonable accommodations to qualified individuals with disabilities in the recruitment process. To request an accommodation, please contact
leaveadministration@cna.com
.","$144,423 /yr (est.)",1001 to 5000 Employees,Company - Public,Insurance,Insurance Carriers,1897,$10+ billion (USD)
#N/A,#N/A,"Englewood, CO",Senior Data Engineer,"Department Summary

DISH is a Fortune 200 company that continues to redefine the communications industry. Our legacy is innovation and a willingness to challenge the status quo, including reinventing ourselves. We disrupted the pay-TV industry in the mid-90s with the launch of the DISH satellite TV service, taking on some of the largest U.S. corporations in the process, and grew to be the fourth-largest pay-TV provider. We are doing it again with the first live, internet-delivered TV service – Sling TV – that bucks traditional pay-TV norms and gives consumers a truly new way to access and watch television.
Now we have our sights set on upending the wireless industry and unseating the entrenched incumbent carriers.
We are driven by curiosity, pride, adventure, and a desire to win – it’s in our DNA. We’re looking for people with boundless energy, intelligence, and an overwhelming need to achieve, to join our team as we embark on the next chapter of our story.
Opportunity is here. We are DISH.


Job Duties and Responsibilities

The Data Architecture team in DISH IT Digital Solutions seeks a Data Engineer who will develop and maintain a variety of data solutions for supporting our core business and modernizing DISH IT’s transactional data persistence systems.
Key responsibilities:
Develop scalable and high-performing data update applications for on-premises and Cloud-based databases.
Deliver, optimize, and automate data updates for existing business processes.
Provide on-call support for data persistence systems in Production.

Work attire: Business casual.

Working hours: This is a full-time position: 40 hours/week. Days and hours of work are typically Monday through Friday; 8:00 a.m. to 5:00 p.m. or 9:00 a.m. to 6:00 p.m.


Skills, Experience and Requirements

Education: Bachelor’s degree in Computer Science, Computer Engineering, or a related technical degree, or a combination of education and experience.

Experience: 4+ years of experience as a Data Engineer or similar role.

Skills and qualifications:

Strong desire to learn new skills.
Familiar with one programming language.
Hands-on SQL development in a relational or NoSQL database.
Strong problems solving and analytical skills.
Experiences in Python or Java.
SQL/PLSQL development experiences in Oracle or MySql.
Familiar with Linux.

Salary Range

Compensation: $106,250.00/Year - $125,000.00/Year
Compensation and Benefits

We also offer versatile health perks, including flexible spending accounts, HSA, a 401(k) Plan with company match, ESPP, career opportunities, and a flexible time away plan; all benefits can be viewed here: DISH Benefits.

The base pay range shown is a guideline. Individual total compensation will vary based on factors such as qualifications, skill level, and competencies; compensation is based on the role's location and is subject to change based on work location. Candidates need to successfully complete a pre-employment screen, which may include a drug test and DMV check.","$115,625 /yr (est.)",10000+ Employees,Company - Public,Telecommunications,"Cable, Internet & Telephone Providers",1980,$10+ billion (USD)
"Oak Street Health
3.7",3.7,"Chicago, IL","Sr. Engineer I, Data Engineering","Company: Oak Street Health
Title: Sr. Engineer I, Data Engineering
Oak Street Health is a rapidly growing public company of primary care centers for adults on Medicare in medically-underserved communities where there is little to no quality healthcare. Oak Street’s care is based on a unique model that is focused on value for its patients, not on volume of services. The company is accountable for its patients’ health, spending more than twice as long with its patients and taking on the risks and costs of their care. For more information, visit http://www.oakstreethealth.com.
For more information, visit www.oakstreethealth.com.

Role Description:
The Data Engineer will be responsible for delivering high quality modern data solutions through collaboration with our engineering, analysts, and product teams in a fast-paced, agile environment leveraging cutting-edge technology to reimagine how Healthcare is provided. You will be instrumental in designing, integrating, and implementing solutions as well supporting migrations of existing workloads to Azure cloud. The Data Engineer is expected to have extensive knowledge of modern programming languages, designing and developing data solutions

Core Responsibilities:
Develop and automate solutions to consume data from multiple data sources including external API
Programming and modifying code in languages like Java, Json, and Python to support and implement Data Warehouse solutions
Design and deploy enterprise-scale cloud infrastructure solutions
Research, analyze, recommend and select technical approaches for solving difficult and meaningful development and integration problems
Work closely with the Data and Engineering teams to design best in class Azure implementations
Participate in efforts to develop and execute testing, training, and documentation across applications
Design, develop and deliver customized ETL and Database solutions
What are we looking for?
3+ years of relevant working experience with Azure
3+ years of experience working with SQL
3+ years Hands-on experience with cloud orchestration and automation tools, CI/CD pipeline creation
3+ Experience in provisioning, configuring, and developing solutions in Azure Data Lake, Azure Data Factory, Azure SQL Data Warehouse, Azure Synapse and Cosmos DB
Hands-on experience working with PaaS/ IaaS/ SaaS products and solutions
Hands-on experience with Python, Javascript or PySpark
Understanding of Distributed Data Processing of big data batch or streaming pipelines
A desire to work within a fast-paced, collaborative, and team-based support environment
Ability to work independently as well as function as part of a team
Willingness to identify and implement process improvements, and best practices as well as ability to take ownership
Familiarity with healthcare data and healthcare insurance feeds is a plus
Excellent oral and written communication skills
US work authorization

What does being “Oaky” look like?
Radiating positive energy
Assuming good intentions
Creating an unmatched patient experience
Driving clinical excellence
Taking ownership and delivering results
Being scrappy

Why Oak Street?

Oak Street Health offers our coworkers the opportunity to be at the forefront of a revolution in healthcare, as well as:
Collaborative and energetic culture
Fast-paced and innovative environment
Competitive benefits including paid vacation and sick time, generous 401K match with immediate vesting, and health benefits
Oak Street Health is an equal opportunity employer. We embrace diversity and encourage all interested readers to apply to oakstreethealth.com/careers.","$112,274 /yr (est.)",1001 to 5000 Employees,Company - Public,Healthcare,Health Care Services & Hospitals,2012,Unknown / Non-Applicable
"Dobbs Defense Solutions, LLC",#N/A,"Fort Meade, MD",Data/NiFi Engineer,"Our Mission
At Dobbs Defense, we deliver mission-centric IT, Cyber, and data analytics solutions for our government and commercial clients through the convergence of automation, innovation, training, and education. Delivering high-quality IT, cybersecurity, and data analytics solutions through proven and innovative methods is our vision.

Job Description
Dobbs Defense Solutions is seeking a Senior Data/NiFi Engineer to design, develop, implement, test, secure, and support data transport and brokering for multiple classified networks and between networks.

Duties
Design, develop, implement, test, secure, and support data transport and brokering.
Data flow engineering for one of DISA’s premier boundary defense systems.
Perform system administration of Niagara Files (NiFi) on Linux VMs.
Support backup/failover/load balancing of data brokering to improve availability.
Monitor availability & amp, usage.
Respond to outages, take corrective action, and issue outage reports.
Familiarity with DISA STIGs and SCAP.
Familiarity with Java programing.

Qualifications
Required education and experience:
5+ years of Data Engineering experience.
Significant experience with Niagara Files (NiFi) and MiNiFi or similar data engineering tools.
Excellent written and verbal communication skills.
IAM or IAT Level II Certification (e.g., GSEC, SEC+, SCNP, SSCP, etc.).
Active Secret clearance.

Required Clearance
Secret

Working Environment
Onsite

Our Equal Employment Opportunity Policy
Dobbs Defense Solutions is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, creed, sex (including pregnancy, childbirth, breastfeeding, or medical conditions related to pregnancy, childbirth or breastfeeding), sexual orientation, gender or gender identity (except where gender is a bona fide occupational qualification), gender expression and transgender status, national origin, ancestry, age, disability, military or veteran status, marital or domestic partner status, genetic information, citizenship, low-income status or any other status or characteristic protected by applicable law. We are committed to equal employment opportunity in all decisions related to employment, promotion, wages, benefits, and all other privileges, terms, and conditions of employment.
OE5clsBZDc","$84,032 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Labviva, Inc.
2.8",2.8,"Boston, MA",Senior Data Engineer,"Senior Data Engineer
Location: Boston, MA
Category: Data/Analytics
Type: Full-time, Hybrid Boston Office

About the Role
As a Senior Data Engineer at Labviva, you will be charting the blueprint of data infrastructure for a leading marketplace in life science. You will be the first hire in the data engineering function and will join our growing team of analytics and data professionals, reporting to the VP Analytics and Data Science. You would be a great fit for the role if you are a data engineering expert knowledgeable of data architecture in Big Data. You have worked in a scaled e-commerce platform, preferably having experienced its growth stages. You are a self-starter who thrives in an ambiguous, fast-paced startup environment. Success in this position will mature Labviva's approach to data to meet standards that are high quality (accurate, complete, consistent), comprehensive to meaningful business activity, easy to use properly and difficult to use incorrectly to drive the business forward.

How You Will Contribute
Work with internal/external resources to design and build a centralized business intelligence environment
Develop and implement data pipelines to ETL/ELT data from multiple sources into a data warehouse or data lake and monitor for data accuracy
Maintain data infrastructure in production and development environments
Provide data engineering support throughout product/solution lifecycle from research, development, testing to production
Improve operational efficiency by automating and streamlining data processes
Collaborate with stakeholders to develop and implement data governance procedures to ensure data security, privacy and compliance

What You Bring to the Team
Bachelor's degree in computer science, engineering or a related field
5+ years of experience with database design, data modeling, data pipelines, data warehousing/data lake
Retail/ecommerce data management expertise; e-Marketplace, supply chain management and/or data platform experience is a plus
Data governance and compliance experience in regulated industries are preferred
Public cloud experience; AWS preferred
Event driven architecture and microservices
Big Data technologies such as Kafka, Spark, Hadoop
Excellent working knowledge of SQL; proven ability to debug and optimize queries & procedures
PostgreSQL, MySQL, Microsoft SQL Server
NoSQL DBs such as MongoDB
Python, Java
Exposure to scaled enterprises andp mentality
Strong intellectual curiosity and self-motivation

About the Company
Labviva is on a mission to accelerate the pace of life science research. We connect researchers with suppliers of reagents, chemicals and instrumentation in an intuitive user-friendly platform that supports the priorities of scientists while staying compliant with purchasing rules. We are a startup that acknowledges the unique contributions of each team member drive our success. We commit to creating a diverse and inclusive workspace where people can make a positive impact. Labviva does not discriminate based on race, religion, national origin, sexual orientation, gender identity or expression, age, disability, marital, veteran status and classifications protected by discrimination laws.","$118,922 /yr (est.)",Unknown,Company - Private,Pharmaceutical & Biotechnology,Biotechnology,#N/A,Unknown / Non-Applicable
"Inspire Medical Systems I
4.4",4.4,"Golden Valley, MN",Azure Data Engineer,"Inspire Medical Systems has developed the only FDA-approved neurostimulation technology that transforms the lives of people with moderate to severe sleep apnea. We are a ground-breaking, fast-growing company where the patient’s outcome is first and foremost our top priority.
If you want to become part of a purpose-driven company and directly help to transform lives, this is the perfect career opportunity for you!
Position Summary:
The Azure Data Engineer should be experienced building in the Big Data space, using traditional, new, and emerging technologies. A good understanding of data modeling and SQL coding best practices is expected.
We are looking for a highly energetic and collaborative Azure Data Engineer with experience leading enterprise data modeling projects with Business and IT operations using Azure Analytics, Azure Data Warehousing and Azure Big Data products.

MAIN DUTIES/RESPONSIBILITIES:

You will produce high-quality, secure, and maintainable code in an agile environment
Learn and understand business processes with limited guidance
Work collaboratively as a member of the development team to build best-in-class software solutions in an agile environment
Support and research issues across all application layers and database
You will identify areas to improve and scale our Azure architecture and application design
Ensure code can be deployed using Azure DevOps
Design and query database tables, views, functions, stored procedures and batch processes
Develop, implement, and support interfaces that connect our websites, back-end systems, and various 3rd party cloud solutions
Required Qualifications:
Bachelor’s degree from an accredited college or University
Experience with private and public cloud architectures, pros/cons, and migration considerations
Minimum of 5 years of RDBMS experience
Experience with JSON, JSON-LD, XML data structures
Experience implementing data pipelines using latest technologies and techniques
Experience with SDLC products (JIRA, Confluence, Github, etc) or similar agile project management tools
5+ years of hands-on experience in programming languages such as Java 8, c#, node.js, python, SQL, Unix shell/Perl scripting etc.
At least 5 years of consulting or client service delivery experience on Azure
Bachelor’s or higher degree in Computer Science or related discipline
Experience handling structured and unstructured datasets
Expert in USQL, Java, Python, Hive SQL, Spark SQL, DataBricks or Snowflake
Strong t-SQL skills with experience in Azure SQL DW
Experience in Data Modeling and Advanced SQL techniques
Cloud migration methodologies and processes including tools like Azure Data Factory, Azure Synapse, Event Hub, etc.
Excellent problem solving, analytical, and critical thinking skills
Preferred Qualifications:
Master’s degree from an accredited college or University
Experience ingesting data from MS Dynamics CRM a big plus
Microsoft Azure certifications are a plus

Inspire Medical Systems provides equal employment opportunity (EEO) to all employees and applicants without regard to race, color, religion, creed, sex, national origin, age, disability, marital status, familial status, sexual orientation, status with regard to public assistance, membership or activity in a local commission, military or veteran status, genetic information, or any other status protected by applicable federal, state and local laws. This policy applies to all aspects of the employment relationship, including recruitment, hiring, compensation, promotion, transfer, disciplinary action, layoff, return from layoff, training and social and recreational programs. Inspire Medical Systems complies with applicable laws governing non-discrimination in employment in every location in which Inspire Medical Systems has facilities. All such employment decisions will be made without unlawfully discriminating on any prohibited basis.

Inspire Medical Systems is an equal opportunity employer with recruitment efforts focused on ensuring a diverse workforce. Applicants with a disability that are in need of accommodations to complete the Inspire Medical Systems application process should contact Human Resources at 844-672-4357 or email careers@inspiresleep dot com.

Inspire Medical Systems participates in E-Verify.","$108,551 /yr (est.)",51 to 200 Employees,Company - Public,Healthcare,Health Care Services & Hospitals,2007,$5 to $25 million (USD)
"Iron Service Global Inc
3.4",3.4,"Menlo Park, CA","Data Engineer (Python, SQL)","Data Analytics & Engineering - Data Engineer III
Job Description: Onsite at Menlo Park location only.
Summary:
The main function of the Data Engineer is to develop, evaluate, test, and maintain architectures and data solutions within our organization. The typical Data Engineer executes plans, policies, and practices that control, protect, deliver, and enhance the value of the organization’s data assets.
Job Responsibilities:
Design, construct, install, test, and maintain highly scalable data management systems.
Ensure systems meet business requirements and industry practices.
Design, implement, automate and maintain large scale enterprise data ETL processes.
Build high-performance algorithms, prototypes, predictive models and proof of concepts.Skills:
Ability to work as part of a team, as well as work independently or with minimal direction.
Excellent written, presentation, and verbal communication skills.
Collaborate with data architects, modelers and IT team members on project goals.
Strong PC skills including knowledge of Microsoft SharePoint
Strong Python/SQL experience
Education/Experience:
Bachelor's degree in a technical field such as computer science, computer engineering or related field required.
Process certification, such as, Six Sigma, CBPP, BPM, ISO 20000, ITIL, CMMI.
Job Types: Full-time, Contract
Salary: $110,000.00 - $135,000.00 per year
Benefits:
Dental insurance
Health insurance
Life insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Menlo Park, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
ETL: 5 years (Preferred)
Data management: 5 years (Preferred)
Enterprise data ETL processes: 5 years (Preferred)
Business process modeling: 5 years (Preferred)
License/Certification:
Six Sigma (Preferred)
ISO 20000 (Preferred)
ITIL (Preferred)
Work Location: One location","$122,500 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1987,$25 to $100 million (USD)
"International Sports Sciences Association
4.1",4.1,"Phoenix, AZ",Sr. Data Engineer,"ISSA is looking for Sr Data Engineer!
REMOTE, but must reside in an ISSA-eligible state (AZ, UT, NV, ID, OR, TX, IL, IN, MN, OH, FL, SC, GA, TN)
About ISSA: ISSA is an organization that operates as an education and certification company for fitness trainers, personal trainers, strength and conditioning coaches, nutritionists, nutrition coaches, aerobic instructors, and medical professionals. For more than 35 years, ISSA has created a personal fitness training program to merge gym experience with practical and applied sciences, we have certified nearly 500,000 trainers in 174 countries. With over 200 plus employees nationwide. We have been voted a Best Place to Work by The Phoenix Business Journal in 2022!
We are looking to add Sr. Data Engineer to our effective relationship-builders with a laser focus on providing exceptional service. We are tenacious and resilient in our pursuit of personal and professional success, while we incorporate fun into every day!
Job Summary:
We are seeking an experienced Senior Data Engineer with expertise in Azure SQL, Data Factory, and the Azure Cloud Environment to join our dynamic team. As a Senior Data Engineer, you will play a crucial role in designing, implementing, and maintaining our data infrastructure to support our organization's data-driven initiatives. You will be responsible for building and optimizing data pipelines, ensuring data quality and integrity, and collaborating with cross-functional teams to deliver scalable and efficient data solutions.
Responsibilities and duties:
Design, develop, and maintain data pipelines and workflows using Azure Data Factory to extract, transform, and load data from various sources into Azure SQL databases.
Collaborate with data architects, data scientists, and business analysts to understand data requirements and translate them into technical specifications.
Develop and optimize SQL queries, stored procedures, and functions to ensure efficient data retrieval and processing.
Monitor and troubleshoot performance issues, data quality problems, and other data-related anomalies in Azure SQL databases.
Implement data security and access controls in compliance with industry best practices and regulatory requirements.
Develop and maintain data governance policies, standards, and procedures to ensure data integrity, privacy, and compliance.
Collaborate with Azure Cloud Administrators to provision and manage Azure SQL resources, including server administration, backups, and disaster recovery strategies.
Leverage general cloud experience to design scalable, reliable, and cost-effective data solutions within the Azure Cloud Environment.
Provide guidance and mentorship to junior data engineers, promoting best practices and ensuring high-quality deliverables following industry standards.
Stay up to date with the latest advancements in Azure SQL and related technologies and assess their potential impact on our data engineering processes.
What is ISSA looking for:
Minimum of 5+ years of experience as a Data Engineer, with a strong focus on Azure SQL and related technologies.
Proficiency in designing and implementing data pipelines using Azure Data Factory.
Extensive experience in Azure SQL Database, Azure SQL Managed Instance, and Azure SQL Data Warehouse.
Strong knowledge of SQL programming, query optimization, and performance tuning.
Familiarity with Azure Cloud environment and associated services, including Azure Storage, Azure Data Lake, and Azure Key Vault.
Experience with general cloud concepts, such as virtual machines, networking, and identity management.
Experience with data modeling, data warehousing concepts, and dimensional data modeling techniques.
Knowledge of ETL (Extract, Transform, Load) processes and data integration patterns.
Understanding of data governance, data security, and privacy practices.
Excellent problem-solving skills and the ability to work effectively in a fast-paced, collaborative environment.
Strong communication and interpersonal skills, with the ability to explain complex technical concepts to non-technical stakeholders.
Computer Science or Information Systems Degree Preferred
Bonus Skills:
Microsoft Azure Certifications, such as Azure Data Engineer or Azure Administrator.
Experience with other Azure services like Azure Databricks, Azure Synapse Analytics, or Azure Analysis Services.
Familiarity with big data technologies, such as Apache Spark or Hadoop.
Knowledge of programming languages such as Power shell, Python or Scala.
Experience with version control systems, such as Git and GitHub.
Anything else? Absolutely! Benefits and Perks:
ISSA is a place where every day we are inspired by our teammates, encouraging each other to be our best. The environment is relaxed, friendly, and upbeat! And we feel it’s important to reward our team with competitive pay and benefits. Here are some of the highlights:
Free certifications: Who wants to be their best self? WE DO! As an ISSA team member, you have access to all 25 certifications…at no expense! Additionally, you'll receive exclusive discounts on leading products in the fitness industry. Cool, right?
Work-Life Synergy: Our Core Team members have 4 weeks of PTO. If you join ISSA as a People Leader you get 5 weeks of PTO and our Directors and Above have Discretionary Time Off! All PTO starts accruing on day one! And everyone at ISSA gets 8 paid national holidays every year! Work hard, play hard!
Parental Time Off: We support our team members as they grow their families! Once you qualify for FMLA, you are eligible to receive up to 6 weeks of paid paternity time to bond with a new child.
Well-rounded support: We are proud to offer Maven, a global platform that provides on-demand care! Maven provides an option for inclusive, comprehensive support for the whole employee and their family journey!
Medical, Dental, and Vision Insurance: A key component to living a healthy lifestyle is having access to the care we (and our families) need. We offer 2 medical insurance plans accompanied by Health Saving Account (HSA) and Flexible Savings Account (FSA) options, as well Dental and Vision coverage. And you don’t have to wait forever! Eligibility begins the 1st of the month after you start.
Team Building: We may be remote, but we don’t have to be alone! We host a monthly schedule of virtual team activities, town halls, weekly gratitude and motivational sessions, and other special events…so many fun ways for us to connect and support one another.
Growth opportunities: When our people grow…we grow! We offer leadership training, development journeys for each role and career path, and many coaching/mentoring opportunities. Constant growth and development are inherent in our culture.
Tools to do the job: We ensure you are hooked up with the tools, equipment, and systems you need. We begin the process prior to your start date, so you are ready to rock ‘n roll on your first day. We also provide a monthly work-from-home allowance!
What else? We have a 401(k) company match and 100% company-paid life/AD&D insurance/short-term disability. We also offer an employee assistance program to assist with work-life concerns (legal, financial, and mental health). Our ultimate goal is to support you and your overall wellness.
Transform your career at ISSA!","$92,457 /yr (est.)",51 to 200 Employees,Company - Private,Education,Education & Training Services,1988,$25 to $100 million (USD)
"BigLynx Computer Software
4.9",4.9,"Redmond, WA",AWS Data Engineer(Tech Lead),"BigLynx, Inc is an American multinational technology corporation headquartered in Seattle, Washington, with operations in the United States, Canada, and India. The company began in 2016, as a product development company specializing in AI/ML Data Engineering in the Retail vertical space with its products warehouse & fast. Post Pandemic in 2022, BigLynx added a business division of boutique technology consulting, specializing inData Engineering, Full Stack , and Microsoft Dynamics helping clients build the next generation data platform and big data pipelines.
Design and architect data solutions: Work closely with stakeholders to understand business requirements and design scalable and efficient data solutions on AWS. Create data architectures, data models, and data flow diagrams.
Data pipeline development: Develop, implement, and manage data pipelines to extract, transform, and load (ETL) data from various sources into AWS. Utilize AWS services such as AWS Glue, AWS Data Pipeline, or Apache Airflow for data integration and orchestration.
Data transformation and processing: Transform raw data into structured and usable formats for analytics and reporting purposes. Apply data manipulation techniques and develop data transformation workflows using AWS services like AWS Glue, AWS Lambda, or Apache Spark.
Data storage and management: Design and implement scalable data storage solutions on AWS, such as Amazon S3, Amazon Redshift, or Amazon DynamoDB. Optimize data storage and retrieval for performance and cost efficiency.
Data quality and governance: Ensure data quality, consistency, and accuracy through data cleansing, validation, and standardization processes. Implement data governance practices and adhere to data privacy and security standards.
Performance optimization: Identify and resolve performance bottlenecks in data pipelines and data processing workflows. Optimize query performance and data processing capabilities using AWS tools and techniques.
Team leadership and collaboration: Lead a team of data engineers, providing technical guidance, mentoring, and driving best practices. Collaborate with cross-functional teams, including data scientists, analysts, and stakeholders, to understand data requirements and deliver high-quality data solutions.
Cloud infrastructure management: Configure and manage AWS infrastructure components related to data engineering, such as EC2 instances, VPCs, IAM roles, and security groups. Monitor and troubleshoot issues related to infrastructure and data services on AWS.
Documentation and knowledge sharing: Create and maintain technical documentation, including design documents, architecture diagrams, and standard operating procedures. Share knowledge and provide training to team members and stakeholders.
Qualifications:
Extensive experience in data engineering: Minimum of [X] years of experience in data engineering roles, with a focus on AWS cloud-based solutions.
AWS expertise: In-depth knowledge and hands-on experience with AWS services related to data engineering, including AWS Glue, AWS Data Pipeline, Amazon S3, Amazon Redshift, or Amazon DynamoDB.
ETL and data integration: Strong proficiency in ETL processes, data integration techniques, and data transformation workflows. Familiarity with tools like Apache Spark, Apache Airflow, or AWS Glue for data manipulation and processing.
Database and data warehouse technologies: Solid understanding of relational databases, data warehousing concepts, and SQL. Experience with Amazon Redshift or other data warehousing solutions is preferred.
Programming and scripting: Proficiency in Python, SQL, and shell scripting for data engineering tasks. Knowledge of other programming languages such as Java or Scala is a plus.
Data modeling and schema design: Experience in designing and implementing data models, database schemas, and dimensional modeling concepts. Familiarity with schema design optimization for analytical workloads.
Cloud infrastructure and security: Strong understanding of AWS cloud infrastructure components, security controls, and best practices. Experience in managing AWS resources and implementing security measures for data.
Leadership and teamwork: Proven exper
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Experience level:
8 years
Schedule:
Monday to Friday
Work Location: In person",$75.00 /hr (est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"The Hartford
3.9",3.9,"Hartford, CT",Sr. Data Engineer,"You are a driven and motivated problem solver ready to pursue meaningful work. You strive to make an impact every day & not only at work, but in your personal life and community too. If that sounds like you, then you've landed in the right place.
Strong candidate has been identified
The Property & Casualty Support team within the Actuarial Information Services department is seeking a talented professional Senior Data Engineer to join our team. We are looking for a talented professional with a proven track record of data engineering and operationalizing a next generation analytics solution suite using Oracle, Cloud (Snowflake) and Big Data Technologies.
Our ideal candidate will leverage deep technical expertise and strong communication skills to deliver both invest and maintenance projects within the Actuarial portfolio. Responsibilities include but are not limited to:
Responsibilities:
Architect innovative solutions that continue to advance our actuarial reserving capabilities while maintaining focus on delivering what is truly needed to our business partners in a timely fashion
Provides highly technical consulting and leadership in identifying and implementing new uses of information technologies that assist the functional business units in meeting their strategic objectives
Identify opportunities to optimize processes by creatively harnessing available data/tools, reusing strategic components, and reducing system complexities where appropriate
Solve problems in an efficient and structured manner; understand the desired outcome, assess a root cause, produce a viable and realistic solution for our business partners
When appropriate, leverage Agile methodology to work iteratively pairing closely with our business partners
Possesses functional knowledge and skills reflective of a competent practitioner with the ability to deliver on work of highest technical complexity
Formulates logical statements of business problems and devises, tests and implements efficient application program solutions (e.g., codes and/or reuses existing code using program development software alternatives and/or integrates purchased solutions)
Use Toad for Oracle to connect to multiple Oracle databases and access needed datasets
Maintain documentation and other data design artifacts that define business data requirements and handling rules.
Perform data or statistical analysis across multiple business units and processes.
Experience & Skills Qualifications:
Candidates must have the technical skills to transform, manipulate and store data, the analytical skills to relate the data to the business processes that generates it, and the communication skills to document & disseminate information regarding the availability, quality, and other characteristics of the data to a diverse audience.
These varied skills may be demonstrated through the following:
Bachelor’s degree with at least 7 or more years of applicable work experience with respect to Data Analysis, manipulation, and technical development
Experience accessing and retrieving data from large data sources, by creating and tuning SQL queries. Understanding of data modeling concepts, data warehousing tools and databases (e.g., Oracle, AWS, Snowflake and R)
Demonstrated ability to create and deliver high quality PL/SQL code using software engineering best practices. Experience with object-oriented programming and software development a plus.
Ability to analyze data sources and provide technical solutions
Determine business recommendations and translate into actionable steps
Critical thinking skills that lead to findings, solutions, and positive business outcomes
Self-starter with curiosity and a willingness to become a data expert
Forward-looking and continuous improvement mindset
Detail and results driven with commitment to managing multiple deadlines in a fast-paced environment.
Strong relationship building / interpersonal and influencing skills
Results oriented with the ability to multi-task and adjust priorities when necessary
Ability to work both independently and in a team environment with internal customers
Ability to effectively communicate through written and verbal means
Experience in Reserving is an added advantage
Compensation
The listed annualized base pay range is primarily based on analysis of similar positions in the external market. Actual base pay could vary and may be above or below the listed range based on factors including but not limited to performance, proficiency and demonstration of competencies required for the role. The base pay is just one component of The Hartford’s total compensation package for employees. Other rewards may include short-term or annual bonuses, long-term incentives, and on-the-spot recognition. The annualized base pay range for this role is:
$110,560 - $165,840
Equal Opportunity Employer/Females/Minorities/Veterans/Disability/Sexual Orientation/Gender Identity or Expression/Religion/Age
About Us
|
Culture & Employee Insights
|
Diversity, Equity and Inclusion
|
Benefits
Sr Data Engineer - GE07BE",#N/A,10000+ Employees,Company - Public,Insurance,Insurance Carriers,1810,$10+ billion (USD)
"Karius
3.5",3.5,"Redwood City, CA",SENIOR STAFF DATA ENGINEER,"About Karius
Karius is a venture-backed life science startup that is transforming the way pathogens
and other microbes are observed throughout the body. By unlocking the information
present in microbial cell-free DNA, helping doctors quickly solve their most
challenging cases, providing industry partners with access to 1000’s of biomarkers to
accelerate clinical trials, discover new microbes, and reduce patient suffering worldwide.
Karius aims to conquer infectious diseases through innovations around genomic
sequencing and machine learning. The company’s platform is already delivering
unprecedented insights into the microbial landscape, providing clinicians with a
a comprehensive test capable of identifying more than a thousand pathogens directly
from blood, and helping the industry accelerate the development of therapeutic
solutions. The Karius test we provide today is one of the most advanced solutions
available to physicians who aim to deliver better care to many otherwise ineffectively
treated patients.

Position Summary
Karius is building AI-driven data analytics pipelines to deliver life-saving results in the
highly complex infectious disease landscape. We are seeking a seasoned Senior Staff
Data Engineer in Redwood City, CA to lead the design and development of a scalable
data platform to meet our rapid business growth. Senior Staff Data Engineer will be
responsible for defining the technology roadmap, and developing and optimizing the
data platform to enable us to extract values from large amounts of genomic, clinical,
operation and clinical data to provide actionable insights to serve the patients and
develop innovative products. In this regard, the Senior Staff Data Engineer will work
with key stakeholders within the company to understand our data landscape and the
core needs for data governance and usage.

Primary Responsibilities
Design, develop, and operate a scalable data platform that ingests, stores, and
aggregates various datasets to meet the defined requirements;
As the primary subject matter expert in the data engineering domain, evaluate
technology trends in the data industry, identify those technologies relevant to
the company’s business objectives, and develop a roadmap to update the
company’s data platform;
Provide Machine Learning (“ML”) data platform capabilities for R&D and
Analytics teams to perform data preparation, model training and management,
and run experiments against clinical and genomic datasets;
Train the R&D and Analytics teams on using Karius data toolsets and mentor
and support them throughout their research and development efforts;
Build and maintain data ETL/ELT pipelines to source and aggregate the
required internal data to calculate operational and commercial Key Performance
Indicators (“KPIs”) and various data analysis and reporting needs;
Develop integrations with Karius and 3rd party systems to source, qualify and
ingest various datasets; work closely with cross-functional groups and
stakeholders, such as the product, engineering, medical, and scientiﬁc teams,
for data modeling and general life cycle management;
Provide data analytics and visualization tools to extract valuable insights from
the data and enable data-driven decisions; and
Work closely with the Security and Compliance teams, and deploy necessary
data governance to meet the regulatory and legal requirements.

Position Minimum Requirements
At least a Bachelor’s degree in Computer Science, Data Science, or Software
Engineering, Electrical Engineering, or Bio-Engineering (or its foreign equivalent);
plus
At least 10 years of experience as a Software or Data Engineer or similar
position, including at least 5 years in a senior or higher-level position;

AND (or experience must include):

4+ years of hands-on design, development and operation of data solutions using
the following data technologies: Spark and Spark Streaming, Presto, Parquet,
MLﬂow, Kafka, and ETL tools such as Stitch or FiveTran;
4+ years of hands-on experience with design, development and maintenance of
structured, semi and non-structured (NoSQL) data stores, such as MySQL,
PostgreSQL, AWS Redshift, Teradata, Graph databases like Neo4j, and
Databricks Lakehouse;
4+ years of hands-on development and operation of workflows and jobs using
task orchestration engines such as Airﬂow, Argo, NextFlow, Dollar U and Tidal;
4+ years of hands-on experience building and operating data solutions on
operating systems such as Linux and Unix hosted in Amazon Web Services
(AWS) cloud;
5+ years of hands-on building and operation of scalable infrastructure to support
batch, micro-batch, and stream data processing for large volumes of data;
5+ years of hands-on experience designing and implementing enterprise data
warehouse/Lakehouse solutions to house business and technical datasets and
derive KPI dimensions for consumption;
Demonstrated experience with enterprise data modeling in healthcare and/or life
science sectors;
Demonstrated experience with the development and operation of visualization
and dashboards for business KPI reporting using tools such as Tableau or
Looker;
Proficiency in Python and PySpark;
Automation of Data Testing using scripting;
Experience developing and managing technical and administrative controls for
data governance and regulatory compliance in the healthcare and/or life sciences
sectors;
Experience mentoring and coaching junior data engineers; and
Cross-functional project management experience.

Travel: No travel is required.

Reports to: VP, Engineering
$180,000 - $240,000 a year","$210,000 /yr (est.)",51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,2014,Unknown / Non-Applicable
"D.A. Davidson Companies
3.8",3.8,Remote,Data Engineer,"Job Description:

DA Davidson is looking for an energetic, creative Data Engineer to join our expanding team of analytics professionals. This role will be responsible for expanding and optimizing our data and data pipeline architecture, as well as monitoring, troubleshooting and performance tuning data flows used in the collection, integration and provisioning of data. Candidates should have experience building data pipelines and enjoy optimizing entire data systems as well as building them from the ground up. The Data Engineer will work alongside our software development teams, database architects, data analysts and end users on data initiatives, and will ensure our data delivery architecture is consistent with best practices. Candidates must be comfortable working in an Agile team in support of multiple teams, across various systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our growing portfolio of data initiatives.
D.A. Davidson Companies is an independent, employee-owned company with a rich history spanning more than 80 years. We are dedicated to conducting our business in accordance with the highest standards of integrity and ethics, and delivering outstanding service to our clients and each other. We support a friendly, open and adaptive culture, and encourage candid communication and productive collaboration across our firm. Just as we work to improve our clients and employees’ well-being, we also work to strengthen local communities-and giving back is one of our core values.
Experience and Skills:
Employee Growth, Job Qualifications, and Responsibilities
Growth
D.A. Davidson places significant importance on career growth and employee development
Position includes opportunities to work with cloud strategies, branding, standards, and emerging technologies
Numerous employee training programs that include tuition credit, conferences, technical certifications, field training, and broker-dealer series licensing
This position includes working with a global workforce and subject matter experts across many IT and business domains
Position offers employee ownership that includes profit sharing and access to ESOP programs
Professional culture with healthy work / life balance, along with options and technology to support working remotely
D.A. Davidson maintains an Agile IT culture that with communities of practice and centers of excellence to broadly share knowledge and insights

Qualifications
We are looking for a candidate with 3+ years of experience in a Data Engineering role, who has attained a degree in Computer Science, Information Systems, Business Intelligence or comparable work history
Prior experience in the Financial Services industry is beneficial but not required
A flexible, growth-based mindset focused on teamwork, collective project management and knowledge-forward collaboration
Advanced working TSQL knowledge and experience working with relational databases, query authoring, stored procedure development, end-to-end data pipeline development
Experience performing operational analysis on internal and external data and processes to uncover failure points and identify opportunities for improvement
Strong analytic skills related to working with complex, multi-layered data pipelines and dependent datasets
Holistic awareness of processes supporting data preparation, metadata, dependencies, lineages, compliance and workload management
Understanding of logical data modeling and data normalization
Understanding of meta-model and metadata management requirements
Specific skills include:
o Experience with relational and dimensional database modeling
o Experience with relational database platforms: preferably MS SQLServer
o Experience with data pipeline tools such as: SSIS
o Experience using orchestration tools such as: ActiveBatch
o Experience with web services and API integrations
o Experience with scripting languages such as: Python and Powershell
o Experience with Agile, DevOps and DataOps methods and technologies: Source Code Control, Versioning, Test Automation, Continuous Integration, Automated Deployment, Recoverability, Telemetry, etc.
o Familiarity with cloud data services such as: Azure, AWS, Snowflake
o Familiarity with data visualization technologies such as: Tableau, PowerBI
Job Responsibilities
Design, implement and maintain a robust, efficient and compliant data pipeline architecture
Assemble data sets that meet functional business requirements
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal acquisition, transformation, and provisioning of data from a variety of data sources
Build and maintain a service-based architecture for the publication of consumable data and information objects
Work with data architects and data analysts to build analytics products that utilize the data pipelines to provide business-relevant information
Work with Business and IT stakeholders to assist with data-related technical issues and support their data and information requirements
Contribute to the development of frameworks to ensure data structures and integration processes are accurate, compliant and secure
Facilitate the development and implementation of data quality standards, data protection standards and adoption requirements across the enterprise
Define indicators of performance and quality metrics and ensure compliance with data related policies, standards, roles and responsibilities, and adoption requirements
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.
From: D.A. Davidson Companies",#N/A,1001 to 5000 Employees,Company - Private,Financial Services,Investment & Asset Management,1935,$100 to $500 million (USD)
"CHS Inc
3.9",3.9,"Inver Grove Heights, MN",Senior Big Data Engineer,"CHS Inc. is a leading global agribusiness owned by farmers, ranchers and cooperatives across the United States that provides grain, food and energy resources to businesses and consumers around the world. We serve agriculture customers and consumers across the United States and around the world. Most of our 10,000 employees are in the United States, but today we have employees in 19 countries. At CHS, we are creating connections to empower agriculture.
CHS Inc.
Senior Big Data Engineer
Location: Inver Grove Heights, MN

Job Description
Analyze, Model, Develop the ELT framework for ingestion, transformation and distribution of data streams to Snowflake, AWS and PowerBI Reports. Implement various data modelling techniques using DBT tool. Design and Develop python lambda to load various API data into Snowflake to perform analytics for business requirements. Use tidal to automate the job schedule. Perform continuous integrations and deployment using Azure DevOps, Git, Octopus and Terraform. Collaborate with team to troubleshoot and develop standards and best practices. Integration testing to validate data between Cloudera and Snowflake. Implemented Change Data Capture (CDC) technology in HVR to load the deltas to Snowflake. Experience with Snowflake and AWS S3 bucket for integrating data from multiple sources with nested JSON formatted data into Snowflake table. Position allows working from home within commuting distance of worksite location.

Job Requirements
The qualified candidate must have at least a Bachelor’s degree or foreign equivalent degree in Computer Science, Information Technology, Management Information Systems (MIS), Business Intelligence, or closely related technical field. The qualified candidate must have at least 4 years (48 months) of experience with all the following: (a) Data Integration, Data Modeling, and ETL/ELT and SQL Development; (b) developing solutions related to Big Data, and Data Sciences from end-to-end (data ingestion to consumption); (c) developing and maintaining scalable data pipelines that will ingest, transform, and distribute data streams and batches; and (d) utilizing all the following tools/technologies: Java, C++ and C#, Object Oriented Design, Python 2.7 and 3, NoSQL Databases (Hive, Spark), AWS Native Tools (Glue, DMS, S3, Athena), Snowflake, Software Development Lifecycle (SDLC) Cloudera CDP and Databricks. All experience may be gained concurrently. Position allows working from home within commuting distance of worksite location.","$113,398 /yr (est.)",10000+ Employees,Company - Public,Agriculture,Farm Support,1929,$10+ billion (USD)
"Booz Allen Hamilton
4.2",4.2,"Norfolk, VA","Tech Excellence Data Engineer, Junior","The Opportunity:
Ever-expanding technology like IoT, machine learning, and artificial intelligence means that there are more structured and unstructured data available today than ever before. As a data engineer, you know that organizing big data can yield pivotal insights when it’s gathered from disparate sources. We need a data professional like you to help our clients find answers in their big data to impact important missions—from fraud detection to cancer research to national intelligence.
As a big data engineer at Booz Allen, you’ll use your skills to implement data engineering activities on some of the most mission-driven projects in the industry. You’ll help develop and deploy the pipelines and platforms that organize and make disparate data meaningful.
Here, you’ll learn from a multi-disciplinary team of analysts, data engineers, developers, and data consumers in a fast-paced, Agile environment. You’ll grow your skills in analytical exploration and data examination while you support the assessment, design, building, and maintenance of scalable platforms for your clients.
Work with us to use big data for good.
Join us. The world can’t wait.
You Have:
Experience with querying or analyzing data to answer questions and solve problems
Experience with visualizing data to identify or communicate key insights
Knowledge of basic concepts in mathematics and statistics
Knowledge of Cloud environments
Ability to obtain a security clearance
Bachelor’s degree
Nice If You Have:
Experience with systems engineering or systems administration
Experience with Amazon Web Services
Experience with Azure
Experience with Docker, Kubernetes, and Ansible
Experience with Python or GitHub in Data Science
Ability to learn a programming language
Bachelor’s degree in Data Science, Mathematics, Engineering, Physics, Statistics, or CS
Clearance:
Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information.
Create Your Career:
Grow With Us
Your growth matters to us—that’s why we offer a variety of ways for you to develop your career. With professional and leadership development opportunities like upskilling programs, tuition reimbursement, mentoring, and firm-sponsored networking, you can chart a unique and fulfilling career path on your own terms.
A Place Where You Belong
Diverse perspectives cultivate collective ingenuity. Booz Allen’s culture of respect, equity, and opportunity means that, here, you are free to bring your whole self to work. With an array of business resource groups and other opportunities for connection, you’ll develop your community in no time.
Support Your Well-Being
Our comprehensive benefits package includes wellness programs with HSA contributions, paid holidays, paid parental leave, a generous 401(k) match, and more. With these benefits, plus the option for flexible schedules and remote and hybrid locations, we’ll support you as you pursue a balanced, fulfilling life—at work and at home.
Your Candidate Journey
At Booz Allen, we know our people are what propel us forward, and we value relationships most of all. Here, we’ve compiled a list of resources so you’ll know what to expect as we forge a connection with you during your journey as a candidate with us.
Compensation
At Booz Allen, we celebrate your contributions, provide you with opportunities and choices, and support your total well-being. Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, work-life programs, and dependent care. Our recognition awards program acknowledges employees for exceptional performance and superior demonstration of our values. Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in Booz Allen’s benefit programs. Individuals that do not meet the threshold are only eligible for select offerings, not inclusive of health benefits. We encourage you to learn more about our total benefits by visiting the Resource page on our Careers site and reviewing Our Employee Benefits page.
Salary at Booz Allen is determined by various factors, including but not limited to location, the individual’s particular combination of education, knowledge, skills, competencies, and experience, as well as contract-specific affordability and organizational requirements. The projected compensation range for this position is $45,300.00 to $93,000.00 (annualized USD). The estimate displayed represents the typical salary range for this position and is just one component of Booz Allen’s total compensation package for employees.
Work Model
Our people-first culture prioritizes the benefits of flexibility and collaboration, whether that happens in person or remotely.
If this position is listed as remote or hybrid, you’ll periodically work from a Booz Allen or client site facility.
If this position is listed as onsite, you’ll work with colleagues and clients in person, as needed for the specific role.
EEO Commitment
We’re an equal employment opportunity/affirmative action employer that empowers our people to fearlessly drive change – no matter their race, color, ethnicity, religion, sex (including pregnancy, childbirth, lactation, or related medical conditions), national origin, ancestry, age, marital status, sexual orientation, gender identity and expression, disability, veteran status, military or uniformed service member status, genetic information, or any other status protected by applicable federal, state, local, or international law.","$69,150 /yr (est.)",10000+ Employees,Company - Public,Management & Consulting,Business Consulting,1914,$5 to $10 billion (USD)
"McLane Company
3.2",3.2,"Temple, TX",BI Data Engineer,"JOB SUMMARY / GENERAL DESCRIPTION:

Cleans, prepares, and optimizes data for further analysis and modelling. Designs, develops, optimizes, and maintains data architecture and pipelines that adhere to Data Pipeline (ie ELT) principles and business goals.

ESSENTIAL JOB FUNCTIONS / PRINCIPAL ACCOUNTABILITIES:
Designs, develops, optimizes, and maintains data architecture and pipelines that adhere to ELT principles and business goals.
Solves complex data problems to delivers insights that helps business achieve its goals.
Creates data products for engineer, analyst, and data scientist team members to accelerate their productivity.
Engineer effective features for modelling in close collaboration with data scientists and businesses
Leads the evaluation, implementation and deployment of emerging tools and process for analytics data engineering to improve productivity and quality.
Partners with machine learning engineers, BI, and solutions architects to develop technical architectures for strategic enterprise projects and initiatives.
Fosters a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions.
Advises, consults, mentors, and coach other data and analytic professionals on data standards and practices.
Develops and delivers communication and education plans on analytic data engineering capabilities, standards, and processes.
Learns about machine learning, data science, computer vision, artificial intelligence, statistics, and/or applied mathematics as necessary to carry out role effectively.

MINIMUM SKILLS AND QUALIFICATION REQUIREMENTS:
Bachelor's degree in computer science, statistics, engineering, or a related field
5-10 years of experience required.
Experience with designing and maintaining data warehouses and/or data lakes with big data technologies such as Spark/Databricks, or distributed databases, like Redshift and Snowflake, and experience with housing, accessing, and transforming data in a variety of relational databases.
Experience in building data pipelines and deploying/maintaining them following modern DE best practices (e.g., DBT, Airflow, Spark, Python OSS Data Ecosystem)
Knowledge of Software Engineering fundamentals and software development tooling (e.g., Git, CI/CD, JIRA) and familiarity with the Linux operating system and the Bash/Z shell
Experience with cloud database technologies (e.g., Azure) and developing solutions on cloud computing services and infrastructure in the data and analytics space.
Basic familiarity with BI tools (e.g., Alteryx, Tableau, Power BI, Looker)
Expertise in ELT and data analysis, SQL primarily
Conceptual knowledge of data and analytics, such as dimensional modelling, reporting tools, data governance, and structured and unstructured data

WORKING CONDITIONS:
Office Environment

Pay and Benefits:

The pay range for this position is $90k to $110k annually based on qualifications and experience. This role is also eligible to participate in the annual incentive plan with a target incentive of 15% of your base annual salary. Full-time employees are offered benefits including health/RX, dental and vision insurance; flexible and health spending accounts (FSA/HSA); short and long-term disability coverage, supplemental life insurance; 401(k); paid time off and holiday pay for Company designated holidays","$100,000 /yr (est.)",10000+ Employees,Company - Public,Retail & Wholesale,Wholesale,1894,$10+ billion (USD)
"Synopsys
4.1",4.1,"Mountain View, CA",Data Engineer Leader,"44867BR
USA - California - Mountain View/Sunnyvale
Job Description and Requirements
The Synopsys Central Engineering team is tasked to digitize Synopsys product development activities. We will achieve it by identifying areas of improvement and correlating various aspects of the product development, and providing all levels of management visibility for action. The domain areas we are focused on relate to Quality, Productivity, and Operational Efficiency,

We are looking for an experienced Data Engineer who will contribute to building the next-generation Data Platform. As a Data Engineer, you will be working on modern, large-scale big data technologies to build data platforms on Snowflake and toolset. The goal is to create an effective and efficient data pipeline to facilitate data exchange between various applications.

In this role, you will be partnering with data providers to enable them to make available volumes of data and integrate in a common data platform. You will also interact with Data analysts to transform data into information and insights driving data-based strategic outcomes. This is a hands-on role, and you are joining the team near the beginning of our journey where you can help shape our way to manage big data.

Responsibilities:
Drive strategic goal of data consolidation for the whole of engineering to enable cross-domain analytics.
Own and establish Center of Excellence for Data Engineering practices by defining architecture, rules, and setting guardrails for data processing capabilities. This includes data Ingestion, quality control, transformation, and high availability.
Incorporate state-of-the-art practices in Data Engineering to scale the value we deliver in transforming data into insights.
Identify, design and implement internal process improvements, including data infrastructure, for scalability, optimizing data delivery, and automating manual processes.
Leverage your experience and proficiency in all aspects of data management, data cataloging, analytics solution architecture & design, and implementation roadmap.
Build data cataloging infrastructure and metadata platform to enable data discovery, data observability, and federated governance.
Drive cross-team projects to integrate data from numerous separate sources into a unified data environment.

Expertise & Skills:
Must be proficient in ELT (Extract-Load-Transform) process with hands-on experience.
Must be detailed-oriented with a passion for data accuracy and reliable solution development.
Subject Matter Resource in designing and building high performance data pipelines to move and process data using modern tools.
Experience in Data Engineering Architecture and Design.
Subject Matter Resource in SQL (advanced).
Experience in at least one prominent programming language, such as Python (preferred), or Java.
The ability to work with other team members, drive projects to completion, and work autonomously.
Excellent written and verbal communication, work autonomously, and have proven organizational and planning skills.
We also value:
Prior experience in leading Data or Analytics teams.
Experience in database design and management, such as MS SQL Server, Oracle Database, MySQL Database, Cassandra, MongoDB, etc
Familiarity and experience in Snowflake toolset is a proven asset for this position.
Experience with Power BI and/or Tableau or other visualization tools.
Experience with HVR and Fivetran a plus.
Experience with dbt Cloud a plus.

Requirements:
Bachelor's or master's Degree in a quantitative field
> 5-10 years of relevant experience
5+ years’ experience engineering and operationalizing data pipelines with large and complex datasets
3+ years’ experience working with Cloud technologies such as Snowflake

At Synopsys, we’re at the heart of the innovations that change the way we work and play. Self-driving cars. Artificial Intelligence. The cloud. 5G. The Internet of Things. These breakthroughs are ushering in the Era of Smart Everything. And we’re powering it all with the world’s most advanced technologies for chip design and software security. If you share our passion for innovation, we want to meet you.

Stay Connected: Join our Talent Community

The hourly range across the U.S. for this role is between $117,000 - $204,000. In addition, this role may be eligible for an annual bonus, equity, and other discretionary bonuses. Synopsys offers comprehensive health, wellness, and financial benefits as part of a comparative total rewards package. The actual compensation offered will be based on a number of job-related factors, including location, skills, experience, and education. Your recruiter can provide more specific details on the total rewards package upon request.

Inclusion and Diversity are important to us. Synopsys considers all applicants for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity, age, military veteran status, or disability.

Job Category
Engineering
Country
United States
Job Subcategory
R&D Engineering
Hire Type
Employee
Base Salary Range
$117,000 - $204,000","$160,500 /yr (est.)",10000+ Employees,Company - Public,Information Technology,Computer Hardware Development,#N/A,$1 to $5 billion (USD)
"Beacon Health System
3.7",3.7,"Granger, IN",Data Engineer,"Reports to the Manager, Business Intelligence. The Data Engineer delivers insights and supports data driven decision making at Beacon. On a daily basis, the Data Engineer will assist in maintaining the existing business intelligence platform and collaborate with team members on designing and implementing platform enhancements. The Data Engineer will use the business intelligence tools to build custom queries, integrate new data and data sources, and ensure accurate data. The Data Engineer is expected to work independently and as part of a team.
MISSION, VALUES and SERVICE GOALS
MISSION: We deliver outstanding care, inspire health, and connect with heart.
VALUES: Trust. Respect. Integrity. Compassion.
SERVICE GOALS: Personally connect. Keep everyone informed. Be on their team.
Supports the development and maintenance of a system-wide data analytics platform by:
Designing, building and maintaining ETL feeds for new and existing data sources.
Ensuring ongoing accuracy of data ETL feeds, monitoring for changes in the source systems which may alter inbound data.
Building appropriate linkages and relationships between data fields within EDW.
Creating and maintaining source-of-truth relationship tables for key fields to be referenced throughout the EDW.
Implementing a standard nomenclature for use within the EDW, in accordance with established Business Intelligence policies.
Serving as a subject matter expert on data sources, structure, or definitions.
Collaborate with members of Business Intelligence, Finance, and Information Systems teams to optimize data and information usage.
Partner with Information Systems teams to optimize the data warehouse through hardware or software upgrades or enhancements.
Structuring data in a way that is easy to understand, query, and display.
Provides information necessary to the financial and clinical success of Beacon organizations by:
Collecting, organizing, analyzing, and disseminating significant amounts of information with attention to detail and accuracy.
Creating analyses of strategic planning and market research data to provide insight to business development and growth opportunities for Beacon Health System.
Creating analyses of population health and value-based reimbursement contract performance to determine future contracting opportunities and opportunities for improvement in cost or quality performance.
Developing, designing, and automating regular reports accurately and on a timely basis.
Designing and building ad-hoc reports that provide actionable and meaningful information.
Responding to analytics inquiries from various departments of Beacon Health System by identifying data that will provide appropriate and actionable answers.
Identifying, analyzing, and interpreting trends or patterns in complex data sets and connecting those trends to actionable insights and business needs.
Interpreting reports or contractual language to analyze the impact on Beacon's overall operations and financials.
Communication of strategic priorities to Beacon departments based on insights gleaned from data analytics.
Assists the Business Intelligence Team in planning for the evolution of Beacon's data management initiative by:
Providing input into strategic direction and decisions for the Business Intelligence Department Researching new data management techniques or best practices.
Documenting data integration processes and data flows.
Documenting metadata regarding data source, field type, definition, etc. for each field and table created.
Actively promoting and supporting a culture of data-driven decision making within various departments of Beacon Health System. This may include innovative data visualization and data science methodologies.
Completing all other duties as assigned.
ORGANIZATIONAL RESPONSIBILITIES
Associate complies with the following organizational requirements:
Attends and participates in department meetings and is accountable for all information shared.
Completes mandatory education, annual competencies and department specific education within established timeframes.
Completes annual employee health requirements within established timeframes.
Maintains license/certification, registration in good standing throughout fiscal year.
Direct patient care providers are required to maintain current BCLS (CPR) and other certifications as required by position/department.
Consistently utilizes appropriate universal precautions, protective equipment, and ergonomic techniques to protect patient and self.
Adheres to regulatory agency requirements, survey process and compliance.
Complies with established organization and department policies.
Available to work overtime in addition to working additional or other shifts and schedules when required.

Commitment to Beacon's six-point Operating System, referred to as The Beacon Way:
Leverage innovation everywhere.
Cultivate human talent.
Embrace performance improvement.
Build greatness through accountability.
Use information to improve and advance.
Communicate clearly and continuously.

Education and Experience
The knowledge, skills and abilities as indicated below are normally acquired through the successful completion of a Bachelor's Degree in Business, Data Analytics, Engineering, Information Systems, Informatics, or a related field. Experience in application support, data management, programming, or database management, preferably in a healthcare setting, is preferred.
Knowledge & Skills
Requires the ability to troubleshoot issues and propose solutions.
Requires understanding relational database structures and concepts.
Requires ability to research, design, and implement data platform enhancements.
Requires the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail, urgency, and accuracy.
Requires basic knowledge of the healthcare business needs, including a basic knowledge and understanding of the healthcare revenue cycle and healthcare delivery systems.
Requires strong skills in organization and time management.
Demonstrates determination and perseverance through challenging or ambiguous work.
Demonstrates well-developed communication skills necessary to effectively communicate both verbally and in writing, and work within the Business Intelligence Team, organization, and third-party technical contacts.
Preferred Skills include: experience supporting ETL and databases, writing and supporting VBA in Excel, using Structured Query Languages (SQL), learning and using a programming language, using or maintaining data in a computer application, designing and developing data models and data analysis, hands on experience using database technology such as Dimensional Insight's Diver, SQL Server, Oracle, or other database and ETL software. 8. Experience with financial and patient medical record systems commonly found in a healthcare setting, such as: Cerner, Meditech, McKesson STAR, Allscripts, or Epic. 9. Knowledge of Cerner CCL a plus.
Working Conditions
Works in an office environment.
May be required to vary hours and days, and work on holidays, weekends, etc., depending upon the needs of the department.
Physical Demands
Requires the physical ability and stamina to perform the essential functions of the position.

Location: Beacon Health System · Business Intelligence
Schedule: Full-time, Day, 8:00 AM - 5:00 PM","$96,775 /yr (est.)",5001 to 10000 Employees,Hospital,Healthcare,Health Care Services & Hospitals,2011,$500 million to $1 billion (USD)
"National Grid
3.9",3.9,"Waltham, MA",Senior Data Engineer,"Senior Data Engineer
Location: Waltham, MA, US, 02451
Division: Global Head IT Service Delivery
Job Type:
Requisition Number: 44337
Department:
Job Function: Information Technology
About us

Come be a part of driving National Grid’s digital transformation! We are digital creators, continuous learners and daring innovators. We leverage digital innovative ways to create products and catalyze the transformation of National Grid's business units into more agile and digitally native organizations in our shared purpose of bringing energy to life. We need you
Job Purpose

This is an exciting opportunity to design and develop highly scalable and extensible data pipelines that enable collection, storage and distribution, modeling and analysis of large data sets from a variety of channels.
What you'll do

Build out new data integrations including APIs to support continuing increases in data volume and complexity
Develop, test, document and support scalable data pipelines.
Establish and follow data governance processes and guidelines to ensure data availability, usability, consistency, integrity, and security
Build and implement scalable solutions that align to our data governance standards and architectural road map for data integrations, data storage, reporting, and analytic solutions
Design, implement, and automate deployment of our distributed system for collecting and processing streaming events from multiple sources
Build out new data integrations including APIs to support continuing increases in data volume and complexity.
Establish and follow data governance processes and guidelines to ensure data availability, usability, consistency, integrity, and security.
Build and implement scalable solutions that align to our data governance standards and architectural road maps for data integrations, data storage, reporting, and analytic solutions.
Collaborate with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility, and fostering data-driven decision making across the organization.
Design and develop data integrations and a data quality framework. Write unit/integration/functional tests and document work.
Design, implement, and automate deployment of our distributed system for collecting and processing streaming events from multiple sources.
Perform data analysis needed to troubleshoot data-related issues and aid in the resolution of data issues.
Guide and mentor junior engineers on coding best practices and optimization.
What you'll need

5+ years of relevant work experience in data engineering, BI or related field
Must have strong experience with Python, PySpark and SWL
Strong experience with Databricks & Snowflake
Familiarity with any cloud platform GCP, AWS, or Azure.
A Bachelor's or more advanced degree is great, but not required
More Information

Our organization follows a hybrid work structure in our service territory (NY & MA and adjacent states) where employees can work remotely or from the office, as needed. Working from the office is encouraged when working on tasks that require a high degree of collaboration. We work with our employees to foster a work schedule that fits your flexible schedule.

At National Grid, we keep the lights on and homes warm. But it’s so much more than that. We keep people connected and society moving. This is no easy feat, and it takes all of us. But National Grid supplies us with the environment to make it happen. As we generate momentum in the energy transition for all, we don’t plan on leaving any of our customers in the dark. So, join us and help bring energy to life.

#LI-Hybrid
#LI-Remote
Salary
$112,000 - $158,000 a year
This position has a career path which provides for advancement opportunities within and across bands as you develop and evolve in the position; gaining experience, expertise and acquiring and applying technical skills. Candidates will be assessed and provided offers against the minimum qualifications of this role and their individual experience.

National Grid is an equal opportunity employer that values a broad diversity of talent, knowledge, experience and expertise. We foster a culture of inclusion that drives employee engagement to deliver superior performance to the communities we serve. National Grid is proud to be an affirmative action employer. We encourage minorities, women, individuals with disabilities and protected veterans to join the National Grid team.


Nearest Major Market: Waltham
Nearest Secondary Market: Boston
Recruitment Advisory","$135,000 /yr (est.)",10000+ Employees,Company - Public,"Energy, Mining & Utilities",Energy & Utilities,1990,$5 to $10 billion (USD)
"Aspen Insurance Holdings
3.7",3.7,"Jersey City, NJ",Data Engineer,"Reference: ASPUS00490
Jersey City, New Jersey
Permanent - Full Time
About us
Since Aspen was founded in 2002, we have become a leading, diversified specialty insurance and reinsurance company. We respond thoughtfully and creatively to find the best outcomes for our clients and business partners through carefully tailored solutions. We believe the way we work is just as important as the work we do, and we are guided by our core values of respect, honesty, trust and professionalism. Aspen is a great place to develop your career offering an exciting and challenging environment where achievement is rewarded.
The role

Aspen’s vision is to be the global reference point for quality in all its markets. To achieve this goal, Aspen has launched our Data and Analytics strategy which introduces a preconnected, 360 view of data to power company wide analytics and embed data focused decision making from every seat. Through the creation of our single version of data truth, we ensure that all data used for decision making is accurate and fit for purpose, building trust in reporting and provide client intimacy.
As a cloud data engineer you will have a passion for sourcing data of all types and building high quality, scalable data pipelines creating out single version of data truth to power analytics across the enterprise. Partnering with our internal leaders, users and clients, you will design and integrate data from customers, external data providers and our internal applications to create a 360-insurance data platform which will be the foundation for our ML/AI services and power the delivery of all insights used for decision making. As a member of our engineering unit, you will support the design and implementation of data integration capabilities making the most use of our Cloud Tech data services. You will have the autonomy to explore and find innovative ways or delivering our data landscape utilizing the tech and data we have access to.
If you have a real drive to make a difference, add value for our clients in an industry that creates resilience and sustainability globally then this is an opportunity that you don’t want to miss out on.

Key accountabilities
Contribute to Aspen's Data & Analytics Strategy to embrace tech and data to support our user’s demand for insights and informed decision making
Contributes to functional strategy and prioritizes deliverables to support delivery of business targets
Manages tactical plan/support to others to achieve positive results for business in line with strategy.
Ensure data pipelines are scalable, repeatable, and secure across enterprise
Explains technical considerations at meetings, including those with internal clients and less experienced team members.
Tests code thoroughly for accuracy of intended purpose.
Reviews end-product with client to ensure adequate understanding of data assets you and other data engineers are delivering.
Co-Create coding and delivery standards embedding a focus across the engineering unit on consistent, high quality data pipelines and access
Experience with integrating large scale data from a variety of sources for business partners to generate insight/decisions.
Translates business specifications into design specifications and code.
Ensures all code is well structured, includes sufficient documentation, and is easy to maintain and reuse.
Gains expertise in tools, technologies, and applications/databases in specific business areas and company-wide systems.
Create awareness across the business of data made available in Single Version of Truth
Works with key stakeholders/business managers to encourage adoption of single version of data truth.
Demonstrate extensive experience in building data pipelines in both Data Warehouse and Data Lake environments
Apply experience in working within cloud-based data infrastructure most notably Azure including tools such as Data Factory, Data Bricks
Apply experience coding in both python and SQL in spark-based environments
Support the design of our single version of data truth data model
Working with the Data Leadership Team, ensure Data Integration Framework for our single version of data truth is designed, supported, and managed in accordance with business needs
Skills & experience
Good knowledge of Data Governance and Data Architecture processes and standards
Knowledge and awareness of technology services which could the data and analytics space
Deep knowledge of data modelling for analytics – i.e. Data Warehousing, Data Lakes, Data Mesh architecture
Aware of data science methodologies and frameworks
Excellent understanding of Agile frameworks and processes
Knowledge of insurance industry’s processes
Sound knowledge of problem analysis, structure analysis, and design techniques
Strong understanding of underlying needs of the business and how own role contributes to these.
Strong coding experience using python, SQL
Experienced in building integration frameworks focused on reusability and consistency across data engineering
Experienced in creating engineering standards and monitoring the adoption and benefits across the engineering unit.
People management – ability to engage and lead a team.
Able to execute within agile processes, tracking capacity and deliverables in collaboration with team members
Resource and budget management
Specific professional qualifications at the level of degree or equivalent within topics such as computer or data science , information systems or similar
At least 5 years of experience of data engineering and design experience in Microsoft onprem, Microsoft Azure and its related data and analytical tools (ADF, DataBricks, PowerBI), Azure DevOps.
Experience within Financial Services (especially Insurance)
Evidence of supporting technical and operational strategy.
Working as part of a senior team within a complex organization (preferably within financial services industry)
Hands-on experience with computer networks, network administration and network installation.
Other
We are an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.
Apply now
Share","$109,532 /yr (est.)",1001 to 5000 Employees,Company - Private,Insurance,Insurance Carriers,2002,$1 to $5 billion (USD)
"VedaInfo Inc
4.1",4.1,"Phoenix, AZ",Big Data Engineer,"Hi,
I hope this note finds you well
I am reaching out to you regarding your career opportunity with us for “Big Data Engineer” for a “6+ Months Contract” @Phoenix, AZ
Please find the below requirement details.
Job Title: Big Data Engineer
Location: Phoenix, AZ (Onsite)
Duration: 6+ months
Rate: $60/hr C2C
Responsibilities:
Design, implement, and maintain big data systems handling large volumes of data.
Utilize Hadoop, Hive, and Spark for efficient data processing and analysis.
Collaborate with cross-functional teams to understand data requirements.
Develop and optimize data ingestion, storage, and transformation processes.
Build scalable data pipelines for seamless data processing and analysis.
Monitor and troubleshoot data processing and performance issues.
Stay updated with emerging big data technologies.
Requirements:
Bachelor's/Master's degree in CS, Engineering, or related field
7+ years of experience as a Big Data Engineer
Strong proficiency in Hadoop, Hive, and Spark
Extensive experience with AWS or Azure
Solid understanding of data ingestion, storage, and transformation
Excellent problem-solving and communication skills
Proactive and self-motivated
This is a 6+ month onsite contract position in Phoenix, AZ, with the possibility of extension. Compensation is $60/hr on C2C basis.
Please submit your updated resume highlighting relevant experience.
Thank you for your interest!
Thanks & Regards
Mohammed ZAIN
Job Type: Contract
Pay: $50.00 - $60.00 per hour
Experience level:
7 years
Ability to commute/relocate:
Phoenix, AZ: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
To improve your chances of getting Shortlisted, make sure to tailor your Profile to the Job Requirements, Highlight your Relevant Skills and Experience, and then upload your Resume.
Experience:
AWS (Amazon Web Services): 7 years (Required)
Azure: 7 years (Required)
Big data: 7 years (Required)
Hadoop: 7 years (Required)
Apache Hive: 7 years (Required)
Spark: 7 years (Required)
Work Location: One location",$55.00 /hr (est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$1 to $5 million (USD)
Sconcept,#N/A,Remote,Azure Data Engineer,"** W2 LONG TERM CONTRACT BASED....**(NO C2C)
JOB TYPE:- Long Term Contract(6 months)Extendable
JOB LOCATION:-USA
About Sconcept LLC:-Software Concepts Consulting was established in 2008. With our “hands-on” approach we are very successful in delivering high performance, secure IT solutions and IT services. With headquarters in Carrollton, Texas we are specialized in providing business solutions to the Fortune 500 clients. Our clients include Software, Banking, Financial, Healthcare, Retail, Consumer goods and Insurance sectors. We deliver candidates based on client requirements.
Required Skills:-
Must have worked on several (5+) projects building solutions for Data Pipelines, Data warehousing, Data Modelling on Azure Data Platform (AWS, Google Cloud Platform - good to have)
Expertise in design and development of Data pipelines and ETL using ADF, Databricks to move data from relational/structured/unstructured data from source to data lake to data warehouse, on Azure
Creating Pipelines and integrations of multiple data sources
Data processing and transformation using Databricks (Scala/Python)
Expertise in optimizing cost and performance of data pipelines
Experience working with Data Lake, Synapse on Azure
Designing Data Lake storage layer (Landing/staging, raw, trusted/curated zones)
Designing data storage DB schema, data models and processing on Azure Synapse DW
Experience with Synapse Spark Pool
Experience in Data Modelling
Building Data Models from structured and semi-structured data (Data Model, Common Data Model etc.)
Performance optimization of data model for high performance analytical and reporting workloads
Expertise with SQL Server and extensive experience with SQL programming
Programming and optimizing DB objects Views, Stored Procs, Functions
Good experience in defining data access policy and security for azure data services
Experience with Oracle EDW
Knowledge/Experience migrating from Oracle EDW to any cloud DW
Experience integrating AI skills/ML models with Data and reporting solutions
Experience in building API layer for downstream consumption
Job Type: Contract
Pay: $45.00 - $50.00 per hour
Benefits:
Health insurance
Compensation package:
Bonus pay
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$47.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Myticas Consulting
3.9",3.9,Arizona,BHJOB15656_20058 - Data Engineer,"Myticas's direct client based out of Phoenix, AZ is currently seeking a Data Engineer for a 100% remote contract position.
Pay Rate: Up to $34/hr

Job Description:
A Data Engineer can confidently understand, write, troubleshoot, optimize, and peer review SQL code. They understand business requirements and create code and reporting that exceeds specifications through anticipating underlying needs. They also understand relational database design, table structure, data types, and data models. They are experienced with and confident in development in reporting visualizations in Excel, PowerBI, Tableau, QuickSight, SSRS, or other data products. They can create reporting dashboards and products that inform the business. They easily grasp and retain complex data concepts and are able to explain them to others. Looking for engineers who are driven to innovate, use cutting edge technologies to solve problems and think outside the box.
What You’ll Do:
Digital Transformation into AWS
Data Warehouse related work (SQL, ETL, Replication, Databases)
Understand, write, troubleshoot, optimize, and peer review SQL code.
Understand business requirements and create code and reporting that exceeds specifications through anticipating underlying needs.
Understand relational database design, table structure, data types, and data models. 6.Experienced with and confidence in development in reporting visualizations in Excel, PowerBI, Tableau, QuickSight, SSRS, or other data products.
Data Warehouse related work (SQL, ETL, Replication, Databases
Perform other duties as assigned or apparent.
MINIMUM EDUCATION AND RELATED WORK EXPERIENCE:
Bachelor's degree in a Computer Science field and 2 years' work experience
2 years of experience in most phases of IT systems deployments in one or more of the following areas: design and deployment of cloud services, data migration to AWS cloud, Digital Transformation into AWS, and Data Warehouse related work
INDCHI
INDREM",$34.00 /hr (est.),1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"ProGrad
4.2",4.2,Remote,Data Engineer,"We are looking for data engineers that are excited about building and maintaining data pipelines for a variety of interests. The pipelines will ingest data from a variety of sources ranging from internal change data streams to external integrations with various SaaS platforms through webhooks and events. The ideal candidate is self driven and looking to grow as a data engineer while having an outsized impact on the future of data at the company. A love for database architecture and optimisation is a must as is the ability to work across diverse teams.
Requirements/Qualifications:
Bachelor's or Master's degree in Data Science (specialization)
Software development experience coding in a general-purpose programming language such as C, C++, Java, JavaScript, Golang, or Python
Experience working with data structures or algorithms (i. e., data structures /algorithms class, coursework/projects, research, internships, or other practical experience in/outside of school or work (including open source hobby coding)
Proficiency in SQL, and NoSQL Databases
Experience working with large datasets
Experience writing and debugging complex SQL queries
A strong understanding of cloud database best practices and performance optimisation
Experience building and maintaining data pipelines, coupled with knowledge of good programming practices such as unit and integration testing
. A strong understanding of cloud database best practices and performance optimisation
Good to have skills:
. Experience building and maintaining data pipelines, coupled with knowledge of good programming practices such as unit and integration testing
Job Types: Full-time, Permanent
Salary: Up to $1,200,000.00 per year
Experience level:
1 year
Under 1 year
Schedule:
Day shift
Work Location: Remote
Speak with the employer
+91 9057202434",#N/A,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"MARVEL TECHNOLOGIES INC
3.7",3.7,"Cincinnati, OH",Data Engineer,"Job Description
The data engineer designs and builds platforms tools and solutions that help the bank manage secure and generate value from its data. The person in this role creates scalable and reusable solutions for gathering collecting storing processing and serving data on both small and very large i.e. Big Data scales. These solutions can include on-premise and cloud-based data platforms and solutions in any of the following domains ETL business intelligence analytics persistence relational NoSQL data lakes search messaging data warehousing stream processing and machine learning.
Responsible and accountable for risk by openly exchanging ideas and opinions elevating concerns and personally following policies and procedures as defined. Accountable for always doing the right thing for customers and colleagues and ensures that actions and behaviors drive a positive customer experience. While operating within the Bank's risk appetite achieves results by consistently identifying assessing managing monitoring and reporting risks of all types.
ESSENTIAL DUTIES AND RESPONSIBILITIES:
Responsible for design Development and Support of data solutions APIs tools and processes to enable rapid delivery of business capabilities.
Work closely with IT application teams Enterprise architecture infrastructure information security and LOB stakeholders to translate business and technical strategies into data-driven solutions for the Bank.
Act as a technical Expert addressing problems related to system and application design performance integration security etc.
Conduct research and Development based on current trends and technologies related to the banking industry data engineering and architecture data security and related topics.
Work with developers to Build CI/CD pipelines Self-service Build tools and automated deployment processes.
Evaluate software products and Provide documented recommendations as needed.
Provide Support and troubleshooting for data platforms. Must be willing to Provide escalated on-call Support for complicated and/or critical incidents.
Participate in the planning process for hardware and software.
Plan and work on internal projects as needed including legacy system replacement Monitoring and analytics improvements tool Development and technical documentation.
Provide technical guidance and mentoring for other team members.
Manage and prioritize multiple assignments.
MINIMUM KNOWLEDGE SKILLS AND ABILITIES REQUIRED:
Bachelor's degree in Computer Science/Information Systems or equivalent combination of education and experience.
Must be able to communicate ideas both verbally and in writing to management business and IT sponsors and technical resources in language that is appropriate for each group.
Fundamental understanding of distributed computing principles
Knowledge of application and data security concepts best practices and common vulnerabilities.
Conceptual understanding of one or more of the following disciplines preferred big data technologies and distributions metadata management products commercial ETL tools Bi and reporting tools messaging systems data warehousing Java language and run time environment major version control systems continuous integration/delivery tools infrastructure automation and virtualization tools major cloud or rest API design and development.
Job Types: Full-time, Contract
Pay: $60.00 - $62.00 per hour
Compensation package:
Hourly pay
Yearly pay
Experience level:
9 years
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",$61.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,#N/A,$5 to $25 million (USD)
META FORCE IT LLC,#N/A,"Cincinnati, OH",Data Engineer,"Job Description
The data engineer designs and builds platforms tools and solutions that help the bank manage secure and generate value from its data. The person in this role creates scalable and reusable solutions for gathering collecting storing processing and serving data on both small and very large i.e. Big Data scales. These solutions can include on-premise and cloud-based data platforms and solutions in any of the following domains ETL business intelligence analytics persistence relational NoSQL data lakes search messaging data warehousing stream processing and machine learning.
Responsible and accountable for risk by openly exchanging ideas and opinions elevating concerns and personally following policies and procedures as defined. Accountable for always doing the right thing for customers and colleagues and ensures that actions and behaviors drive a positive customer experience. While operating within the Bank's risk appetite achieves results by consistently identifying assessing managing monitoring and reporting risks of all types.
ESSENTIAL DUTIES AND RESPONSIBILITIES:
Responsible for design Development and Support of data solutions APIs tools and processes to enable rapid delivery of business capabilities.
Work closely with IT application teams Enterprise architecture infrastructure information security and LOB stakeholders to translate business and technical strategies into data-driven solutions for the Bank.
Act as a technical Expert addressing problems related to system and application design performance integration security etc.
Conduct research and Development based on current trends and technologies related to the banking industry data engineering and architecture data security and related topics.
Work with developers to Build CI/CD pipelines Self-service Build tools and automated deployment processes.
Evaluate software products and Provide documented recommendations as needed.
Provide Support and troubleshooting for data platforms. Must be willing to Provide escalated on-call Support for complicated and/or critical incidents.
Participate in the planning process for hardware and software.
Plan and work on internal projects as needed including legacy system replacement Monitoring and analytics improvements tool Development and technical documentation.
Provide technical guidance and mentoring for other team members.
Manage and prioritize multiple assignments.
MINIMUM KNOWLEDGE SKILLS AND ABILITIES REQUIRED:
Bachelor's degree in Computer Science/Information Systems or equivalent combination of education and experience.
Must be able to communicate ideas both verbally and in writing to management business and IT sponsors and technical resources in language that is appropriate for each group.
Fundamental understanding of distributed computing principles
Knowledge of application and data security concepts best practices and common vulnerabilities.
Conceptual understanding of one or more of the following disciplines preferred big data technologies and distributions metadata management products commercial ETL tools Bi and reporting tools messaging systems data warehousing Java language and run time environment major version control systems continuous integration/delivery tools infrastructure automation and virtualization tools major cloud or rest API design and development.
Job Type: Contract
Pay: $45.96 - $50.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Cincinnati, OH 45215: Reliably commute or planning to relocate before starting work (Required)
Experience:
data engineer: 10 years (Preferred)
Big data: 3 years (Preferred)
Work Location: Hybrid remote in Cincinnati, OH 45215",$47.98 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
shreetek,#N/A,"Charlotte, NC",Senior Data Engineer,"Job Title : Data Engineer
ONLY USC
On W2 only
Job Location : Newjersey OR Charlotte,NC
Collect, manage, and convert raw data accurately and reliably
Organize data systems for subgroup access and analyses
Configure and sustain data cloud structures
Able to work with structured and unstructured data.
Validate outputs of data pipelines
Degree in Data Engineering preferred.
Two Programmers –
Convert existing SAS code to python/pyspark code for model operation in the cloud.
Create and sustain policy analysis models in the cloud.
Troubleshoot user interfaces in the cloud.
Create and sustain intuitive user interfaces in the cloud.
A degree in Computer Science is preferred.
Overall across resources keep following stack as reference
Visualization : Tableau
Data Modeling/Science : Python / SAS
Engg : AWS cloud native services, security, data pipeline
Job Type: Contract
Salary: $50.00 - $60.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Charlotte, NC 28202: Reliably commute or planning to relocate before starting work (Required)
Work Location: One location",$55.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Zllius Inc.,#N/A,"Chicago, IL",Data Engineer,"Hi,
Greetings from Zllius Inc!
We have an urgent opening for the role Data Engineer. If interested, revert me back with your Updated resume, along with contact details, work authorization.
Job Role: Data Engineer
Duration: Long-Term
Position Type: W2/ 1099
Visa: Any
Job Description:
Technical/Functional Skills
Successful candidates will have:
Hands on experience in Dremio and Scripting
Hands on Scripting experience in Python and Flask
Hands on experience in Integrating data from multiple source systems (PostgreSQL and CSV)
Experience with Apache Superset
Ability to write complex SQL queries
Experience in building data pipelines using PostgreSQL, Oracle database, Dremio, AWS Open Search
Experience with Docker Containerization of Code
Work as a part of a scrum team in Agile methodology
ETL, Data Pipelines, PostgreSQL, SQL Queries, Scripting
Job Types: Full-time, Contract
Salary: $111,076.11 - $133,769.08 per year
Schedule:
8 hour shift
Day shift
Monday to Friday
Work Location: On the road","$122,423 /yr (est.)",Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"BigLynx Computer Software
4.9",4.9,"Redmond, WA",Databricks Data Engineer,"BigLynx, Inc is an American multinational technology corporation headquartered in Seattle, Washington, with operations in the United States, Canada, and India. The company began in 2016, as a product development company specializing in AI/ML Data Engineering in the Retail vertical space with its products warehouse & fast. Post Pandemic in 2022, BigLynx added a business division of boutique technology consulting, specializing inData Engineering, Full Stack , and Microsoft Dynamics helping clients build the next generation data platform and big data pipelines.
Data pipeline development: Design, develop, and maintain scalable and efficient data pipelines using Databricks to ingest, transform, and load data from various sources. This includes data extraction, data cleansing, data transformation, and data loading processes.
Data modeling and schema design: Design and implement data models, database schemas, and data structures on Databricks. Optimize data models for performance, scalability, and ease of use.
ETL processes: Develop and maintain ETL (Extract, Transform, Load) processes using Databricks to transform and cleanse data. Implement efficient data integration and transformation logic using languages such as Python, SQL, or Scala.
Data integration: Integrate data from multiple systems and sources, ensuring data consistency, accuracy, and quality. Develop and maintain data connectors, APIs, and data ingestion processes.
Performance optimization: Identify and address performance bottlenecks in data pipelines and data models. Optimize query performance, data loading, and data processing capabilities on Databricks.
Data governance and security: Implement data governance practices, data privacy measures, and security controls on Databricks. Ensure compliance with data governance policies and regulations.
Monitoring and troubleshooting: Monitor the health and performance of Databricks data infrastructure, data pipelines, and data processing jobs. Troubleshoot issues and provide timely resolutions.
Collaboration and teamwork: Collaborate with cross-functional teams, including data scientists, data analysts, and business stakeholders, to understand data requirements, provide data engineering expertise, and support their data-related needs.
Qualifications:
Databricks expertise: Strong knowledge and hands-on experience with the Databricks platform, including Databricks notebooks, Databricks runtime, and Databricks clusters.
Data engineering skills: Proficiency in data engineering principles, ETL processes, data modeling, and data integration techniques. Experience with programming languages such as Python, SQL, or Scala.
Big data technologies: Experience with big data technologies, such as Apache Spark, Apache Hadoop, or related frameworks. Familiarity with distributed computing and data processing concepts.
Cloud platforms: Experience working with cloud platforms, preferably Azure Databricks, AWS Databricks, or Google Cloud Databricks. Knowledge of cloud storage, compute, and networking services.
Database and data warehouse concepts: Understanding of relational databases, data warehousing concepts, and SQL. Familiarity with data warehousing best practices and dimensional modeling.
Performance optimization: Strong skills in optimizing Spark jobs and queries on Databricks. Ability to identify and resolve performance bottlenecks.
Problem-solving skills: Strong analytical and problem-solving abilities to tackle complex data engineering challenges and troubleshoot issues.
Collaboration and communication: Excellent collaboration and communication skills to work effectively with cross-functional teams and stakeholders, translating business requirements into technical solutions and providing technical guidance.
Education: A bachelor's or master's degree in computer science, data engineering, or a related field is typically required. Relevant certifications, such as Databricks Certified Developer or similar, are h
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Experience level:
7 years
Schedule:
Monday to Friday
Work Location: In person",$75.00 /hr (est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Vedainfo
4.1",4.1,"Peoria, IL",Data Engineer,"Job Description
· Create and develop optimal data pipeline which fetches data from raw and apply business transformation and load derived.
· Build the AWS Fargate components required for optimal extraction, transformation, and loading of data-to-data lake.
· Develop data pipeline setup between base and derive values based on Caterpillar specification.
· Implement quality validation rules and follow process defined by stakeholder.
· Develop business events as defined by the solution team to generate CDC events for consumption layer (Kinesis)
· Create re-usable and configurable components wherever applicable.
· Create and execute unit, integration, and regression tests
· Ensure the code is developed satisfying all business rules, perform unit testing where applicable.
· Data pipelines development
· Python Development
· AWS cloud services
· Data quality validation rules
· Java (preferred)
· Implement CDC
· Transformation rules creation
· Message broker tools like Kinesis
· Azure DevOps – CI/CD pipelines
· Automated and Manual Testing experience
Job Type: Contract
Pay: Up to $60.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Peoria, IL 61602: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
This position is remote initially and you will be asked to work any of the client location once offer is made. Please confirm your current location.
Experience:
Data Engineer: 1 year (Preferred)
Python Development: 1 year (Preferred)
AWS: 1 year (Preferred)
Message broker tools: 1 year (Preferred)
Work Location: In person",$60.00 /hr (est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$1 to $5 million (USD)
"Deako
4.9",4.9,"Seattle, WA",Data Engineer II,"About Deako
Deako is delivering a revolutionary plug-n-play smart home platform with a laser focus on the untapped new home construction market. We make smart lighting so easy to upgrade that even those who would have never considered smart lighting are making it happen. We've built a company based on trust; where forming personal relationships is key to our success.
Software at Deako
The software team is a tight-knit team of smart, dedicated people. We are customer focused, we ship often, and are constantly asking “why.” We are passionate about the software we write. We work closely with other Deako teams. Developers at Deako are always looking for a better way and understand that no code is perfect.
Data at Deako
We believe that Data and Software go hand in hand. That's why the Data and Software teams work closely together to ensure we're collecting the right data, and getting it to the right people. Data is new at Deako, and we are working to make it a cornerstone of our company. We want every team at Deako to have a data-driven mindset.
Day to Day Expectations:
Participate in the code review process in our DBT repository
Build maintainable, testable SQL queries for various dashboards
Help maintain, contribute to and improve our data infrastructure and existing pipelines
Help maintain and improve our CI/CD pipelines
Responsible for obtaining, cleaning, and munging data and getting it into a form that our data analysts can access and analyze
Participate in meetings with stakeholders around data collection and cleaning

Must Have:
Demonstrable knowledge in SQL + Some Programming Language (Typescript, Python, Ect)
Demonstrates a high level autonomy and willingness to learn
1+ years of experience working as a Data Analyst/Engineer or Comparable field.
Experience with git, dbt
Nice to Have:
Experience in Snowflake, Fivetran, Hightouch
Experience with Gitlab CI
Experience with geospatial data
Stock Options
Hybrid work environment (Office + Remote)
Medical/Dental/Vision/Life/401K
Unlimited PTO
Free Snacks/Coffee/Drinks (Non-Alcoholic and Alcoholic)
Quarterly Company Parties","$115,000 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,2015,$5 to $25 million (USD)
Enterprise Minds,#N/A,Remote,Data Engineer,"Job Role: SQL Data Engineer
Location: Remote Opportunity - Need to work in PST hours
Visa: US Citizen Only
Duration: Long term
Roles and Responsibilities:
3+ years of experience with a complex build system in AWS using Starburst.
5+ years of experience with installation, maintenance, and administration of Oracle databases- PL/SQL and ANSI SQL
Oracle DBA + Oracle SQL to ANSI SQL conversion, performance/query tuning
3+ years of experience configuring, integrating, and securing multiple AWS database offerings (ex. RDS, Aurora, Casandra, and Dynamo)
2+ years of experience with supporting Linux systems engineering efforts in system design and evaluation, solution engineering, software development, or system administration.
Experience with Database Schema as Code tools
Ability to design and develop complex SQL queries and reports to retrieve required data.
Experience with database partitioning and microservices
Knowledge of Agile methodologies or the software development life cycle (SDLC)
US Citizenship is Mandatory.
Job Type: Full-time
Experience:
Data Engineer: 10 years (Preferred)
AWS using Starburst: 4 years (Preferred)
PL/SQL and ANSI SQL: 2 years (Preferred)
AWS RDS: 1 year (Preferred)
Work Location: Remote",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"DiamondPick
4.5",4.5,Remote,GCP Data Engineer,"Position: GCP Data Engineer (8+years)
Location: Remote
Type: Contract
Job Details:
· Relevant Industry Work Experience (6+ for Developer, preferred 9+ years)
· Experience extracting data from a variety of sources, and a desire to expand those skills (Excellent knowledge in SQL and Spark is mandatory)
· Strong knowledge of Google BigQuery and architecting data pipelines from on-prem to GCP.
· Experience building applications using Google Cloud Platform related frameworks such as DataProc and GCS at the minimum.
· Excellent Communication Skills to Understand and Pass on Requirements.
· Excellent Data Analysis skills. Must be comfortable with querying and analyzing large amount of data on Hadoop HDFS using Hive and Spark.
· Professional experience with a cloud hosting platform (GCP preferred)
· GCP certification is preferred.
Job Type: Contract
Salary: Up to $65.00 per hour
Experience level:
10 years
8 years
9 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
Google BigQuery: 4 years (Preferred)
DataProc: 2 years (Preferred)
Google Cloud Platform: 9 years (Required)
Spark: 8 years (Required)
SQL: 8 years (Required)
License/Certification:
Google Cloud Platform(GCP) Certification (Required)
Work Location: Remote",$65.00 /hr (est.),1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,#N/A,Unknown / Non-Applicable
"Lightcast
4.4",4.4,"Moscow, ID",Senior Data Engineer,"A Senior Data Engineer will architect big data analytical frameworks, and translate complex functional and technical requirements into detailed architecture design and high-performing software. A Senior Data Engineer will select data solution software and define hardware requirements. The position will be responsible for developing and implementing standards and processes for integration projects and initiatives.

Major Responsibilities:
Architect software applications, and test and build automated tools.
Select data solution software and define hardware requirements
Develop standards and processes for integration projects and initiatives.
Lead the design/development of software applications, testing, and building tools
Ensure database changes are reviewed and approved to standards
Lead and communicate to leadership on solution design
Provide technical assistance to junior members and to colleagues across the company
Skills & Abilities:
Analytical mind, personality and aptitude for working with data
Ability to look at the numbers, trends, and data to derive conclusions based on findings
Work closely with management to prioritize business and information needs
Proven ability to work effectively to meet goals and deadlines with minimal supervision
Highly creative problem-solving skills and an ability to tailor efforts based on the importance of the issue being addressed.
A hands-on, detail-oriented mindset with the ability to look beyond the box
Expert knowledge of object-oriented design, data structures, and algorithms
Demonstrated experience with agile or other rapid application development methods, object-oriented design, coding, testing patterns with a variety of languages
Significant knowledge of data structures, algorithms, data modeling and disaster recovery of data systems
Education & Experience:
Requires 5 years of data/software engineering/science experience
Bachelor's degree in computer science or other technical disciplines (masters preferred)
About Lightcast:
Lightcast is a global leader in labor market insights with headquarters in Moscow (ID) and Boston (MA) and offices in the United Kingdom, Europe, and India. We work with partners across six continents to help drive economic prosperity and mobility by providing the insights needed to build and develop our people, our institutions and companies, and our communities.

Lightcast is proud to be an equal opportunity workplace and is committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. Lightcast has always been, and always will be, committed to diversity, equity and inclusion. We seek dynamic professionals from all backgrounds to join our teams, and we encourage our employees to bring their authentic, original, and best selves to work.

#LI-SS1","$118,643 /yr (est.)",1 to 50 Employees,Company - Private,Government & Public Administration,State & Regional Agencies,2011,Unknown / Non-Applicable
"Stefanini, Inc
3.8",3.8,"Dearborn, MI",Data Engineer,"Stefanini Group is hiring!
Stefanini is looking for Data Engineer at Dearborn, MI (Hybrid)
For quick apply, please reach out to Rajat Baloria
Phone: (248) 728-2620
Email: Rajatsingh.Baloria@stefanini.com

Open to W2 candidates only!

Position Description:
Looking for a Software Engineer focused on delivering software leveraging Java and Python based on proven Lean/Agile methods. Knowledge of Big Data technologies like Hadoop and Spark is a plus.
The Software Engineer will work in a small, cross-functional, and co-located team. The Software Engineer will collaborate directly and continuously with business partners, product managers and designers, and will release early and often.
Position Responsibilities:
Work hands-on with the team and other stakeholders to deliver quality software products that meet our customer's requirements and needs.
Grow technical capabilities / expertise and provide guidance to other members on the team



Skills Required:
Exceptional software engineering knowledge; OO Design Principles
Basic understanding of Big Data and potential use cases
Strong desire to learn new skills and apply to solve business problems/opportunities “Spring Boot, Java, Angular, API, Micro Services, PCF, GCP (especially Cloud Run)”

Experience Required:
Overall 6 years of work experience in delivering customer facing products
Minimum 2 years of strong development experience in at least one of the following technologies: Java or Python on a Hadoop/Spark Platform

Education Required:
An associate's degree in Computer Science or similar technical discipline
Education Preferred:
Google Cloud Certification

Listed Salary Range may vary based on experience, qualifications, and local market, Also some positions may Include bonuses and other Incentives

About Stefanini Group
The Stefanini Group is a global provider of offshore, onshore and near shore outsourcing, IT digital consulting, systems integration, application and strategic staffing services to Fortune 1000 enterprises around the world. Our presence is in countries like Americas, Europe, Africa and Asia, and more than 400 clients across a broad spectrum of markets, including financial services, manufacturing, telecommunications, chemical services, technology, public sector, and utilities. Stefanini is a CMM level 5, IT consulting, company with global presence. We are CMM Level 5 company.",$82.00 /hr (est.),10000+ Employees,Subsidiary or Business Segment,Information Technology,Information Technology Support Services,1987,$1 to $5 billion (USD)
"Amazee Global Ventures Inc
5.0",5.0,Remote,Data Engineer,"(W2 ONLY)
Job Title: Data Engineer
Duration : 12 months
Location: Remote
6-7 years of experience
Healthcare experience is must
Design, develop, and maintain Tableau dashboards and reports
Develop and maintain data models and ETL processes
Analyze data to identify trends and patterns
Create and maintain data dictionaries and data models
Develop and maintain data pipelines
Collaborate with stakeholders to understand data requirements
Develop and maintain data quality processes
Monitor data quality and performance
Troubleshoot data issues
Develop and maintain documentation
Requirements:
Bachelor’s degree in Computer Science, Information Systems, or related field
4+ years of experience in data engineering and analysis
Expertise in Tableau
Experience with SQL and data modeling
Knowledge of ETL processes
Knowledge of data warehousing
Excellent problem-solving and communication skills
Ability to work independently and in a team environment\
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Type: Full-time
Salary: $40.33 per hour
Work Location: Remote",$40.33 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,2019,Less than $1 million (USD)
"JPMorgan Chase Bank, N.A.
3.8",3.8,"Wilmington, DE",Data Engineer III,"As a Data Engineer III under Consumer and Community Banking within the ADE Data Governance team at JPMorgan Chase, you serve as a seasoned member of an agile team to design and deliver trusted data collection, storage, access, and analytics solutions in a secure, stable, and scalable way. You are responsible for developing, testing, and maintaining critical data pipelines and architectures across multiple technical areas within various business functions in support of the firm's business objectives.

Job responsibilities
Supports review of controls to ensure sufficient protection of enterprise data
Responsible for advising and making custom configuration changes in one to two tools to generate a product at the business or customer request
Updates logical or physical data models based on new use cases
Frequently uses SQL and understands NoSQL databases and their niche in the marketplace
Adds to team culture of diversity, equity, inclusion, and respect

Required qualifications, capabilities, and skills
Experience across the data lifecycle
Advanced at SQL (e.g., joins and aggregations)
Working understanding of NoSQL databases
Proficiency with JSON/Java, Erwin, Oracle, Snowflake, AWS
Significant experience with statistical data analysis and ability to determine appropriate tools and data patterns to perform analysis, including data modeling
Experience customizing changes in a tool to generate product
Executes data architecture solutions and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions and break down problems
Produces data architecture and design artifacts for complex applications while being accountable for ensuring design constraints are met by software code development
Gathers, analyzes, synthesizes, and develops visualizations and reporting from large, diverse data sets in service of continuous improvement of data frameworks, applications, and systems that proactively identifies hidden problems and patterns in data and uses these insights to drive improvements to coding hygiene and system architecture

Preferred qualifications, capabilities, and skills
Familiarity with modern technologies
Exposure to cloud technologies
Chase is a leading financial services firm, helping nearly half of America's households and small businesses achieve their financial goals through a broad range of financial products. Our mission is to create engaged, lifelong relationships and put our customers at the heart of everything we do. We also help small businesses, nonprofits and cities grow, delivering solutions to solve all their financial needs.

We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.
The health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the ""WELL Health-Safety Rating"" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.
As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm's current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm's vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.
We offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, discretionary incentive compensation which may be awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.
Equal Opportunity Employer/Disability/Veterans
Base Pay/Salary
Jersey City,NJ $97,850.00 - $150,000.00 / year","$123,925 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Banking & Lending,1799,$10+ billion (USD)
"Optimal Inc.
3.6",3.6,"Dearborn, MI",Data Engineer - Google cloud platform,"Position Description:
As a Data Engineer you will be responsible for providing data support for enterprise data management tasks, standardization, enrichment, mastering and assembly of data products for downstream applications in Google Cloud Platform. Provide visibility to Data Quality issues and work with the business owners to fix the issues. Implement an Enterprise Data Governance model and actively promote the concept of data standardization, integration, fusion, and quality. Support the data requirements of the different functional teams like MS&S, PD, Quality, etc. and all the regional KPI / Metrics initiatives. Continuously increase Data Coverage by working closely with stakeholders and Data Scientists, understanding, and evaluating their data requirements to create meaningful, organized, and structured ""information""
Skills Required:
Candidates should have experience with using data analysis tools such as: Hive, PySpark, SQL, Python and ETL tools.
Experience of working within a complex business environment, including at least a year in a single function, with deep understanding of the information constructs of that business.
Demonstrated experience and expertise in conceptual thinking of how to apply information solutions to a business challenge.
Experience of applying problem solving capabilities. Proven capability to robustly examine large data sets and highlight patterns, anomalies, relationships, and trends.
Self-starter, demonstrating high levels of data integrity. Ability to manage deliverables according to a robust project plan.
Experience in Google Cloud Platform or other cloud platform is a plus.
Experience Required:
Minimum of a year experience in Google Cloud Platform Minimum of a year of experience in a Data Engineering role creating data products, writing codes/queries/scripts, and building data visualizations.
Minimum of a year of experience in data design, data architecture and data modeling (both transactional and analytic).
Experience in Google Cloud Platform or other cloud platform is a plus.
Education Required:
Bachelor's degree in computer science or related field from an accredited college or university
Prefer Master's degree in computer science or related field from an accredited college or university","$103,365 /yr (est.)",1 to 50 Employees,Nonprofit Organization,Education,Education & Training Services,2004,Unknown / Non-Applicable
"Jetty
4.2",4.2,United States,Data & Analytics Engineer,"Welcome to Jetty, the financial services platform on a mission to make renting a home more affordable and flexible. We've built multiple financial products that benefit both renters and property managers - and we're just getting started.

We are growing our data organization and looking to hire a Data and Analytics Engineer. As a Data and Analytics Engineer, your goal is to cultivate a data-informed culture and create insights that will be leveraged across the entire organization. You have experience executing at a high level, solving complex problems, and delivering solutions with real business impact - and you're excited by the opportunity to apply those principles to a new, best in class function.

Role & Responsibilities
Build / Support our modern data stack (Snowflake / Fivetran / DBT / Tableau)
Be an enthusiastic evangelist of our modern data stack (Fivetran / DBT / Snowflake / Tableau)
Develop analytics data models using SQL
Document our data models in a user friendly way for our business stakeholders
Implement the Five Pillars of Data Observability
Write ELT code using modern software engineering practices (Git, automated testing and deployments)
Build and maintain data pipelines to support various business processes and reporting (Fivetran / AWS Lambdas)
Partner with the Product Engineering team to ensure we are capturing the data we need from our applications for analytics and to iterate on our development practices for the data analytics team.
Be the resident resource on building standard reports and BI dashboards
Experience & Qualifications
3-5 years of experience working in a data / analytics engineering role
High proficiency in Snowflake / Fivetran / dbt / Tableau
High proficiency in SQL and Python
Ability to collect, interpret, and synthesize inputs from various parts of the business into data model requirements
Ability to simplify without being simplistic - ability to communicate complex topics and actionable insights in a compelling way that can be understood by a variety of audiences
Inherent curiosity and analytical follow-through — you can't help but ask ""why?"" and love using data and logic to explore potential solutions
Ability to balance ""Rigor"" and ""Scrappiness"" — you know the difference between 80/20 and giving something 110%; as well as when each is appropriate.
Deep understanding of the first and second order effects of reporting — you know the power of presenting the right data to the right people at the right time
Experience in a data/analytics function at a high-growth startup managing multiple stakeholders and delivering actionable insights
About Jetty
At Jetty, we know renting a home can be a financial challenge. That's why we're on a mission to make renting accessible to everyone. Jetty offers four financial products designed to help our members every step of the renting process: Jetty Deposit, a low-cost security deposit product that dramatically reduces move-in costs; Jetty Rent, a flexible rent payment program to eliminate pricey late rent fees; Jetty Credit, a credit building service that helps renters build credit just by paying rent; and Jetty Protect, an affordable renters insurance product that provides comprehensive coverage in just a few clicks.

Jetty has raised multiple rounds of venture capital from investors including Khosla Ventures, Ribbit Capital, Citi, Valar, and strategic investors. We've built a highly collaborative team working remotely around the country, and we believe in finding the best talent—regardless of where they live. To learn more about life at Jetty, visit jetty.com/careers.

Jetty is firmly committed to building a team as diverse as our Members. We are proud to provide equal employment opportunities for all candidates regardless of race, ancestry, citizenship, sex, gender identity or expression, religion, sexual orientation, marital status, age, disability, or veteran status.

Benefits & Perks
Health (with HSA and FSA options), dental, and vision insurance through Aetna & MetLife
401(k) retirement savings program
Optional life and disability coverage
20 days of PTO + 12 holidays, ""Jetty Winter Break,"" and flexible sick days
Generous parental leave policy
Flexible remote work in any US location (keeping east coast hours)
Stipends to cover WFH set-up, childcare, phone/internet bill, and optional co-working space",#N/A,51 to 200 Employees,Company - Private,Insurance,Insurance Agencies & Brokerages,2015,Unknown / Non-Applicable
"Geo Owl
4.6",4.6,"Fort Gordon, GA",Senior Data Engineer,"Geo Owl is currently looking for a motivated and qualified Senior Data Engineer to support our Department of Defense contract opportunity. To be qualified, you need knowledge of Army structure and defense level intelligence, intelligence collection, fusion, analysis, production, and dissemination for intelligence databases and products, and meet the requirements listed below. If interested, apply now, or contact one of our recruiters.
Location: Fort Gordon, GA
Clearance: TS/SCI
Requirements: Must meet all the requirements listed below.
Excellent written & oral communication, research, and analytic skills
Expert ability to manage personnel, requirements, and coordination of projects
Expert capabilities to research, create, develop, and deliver professional briefings, multimedia presentations, and written reports
Experience utilizing programming languages such as SAS, R, Java, C, MATLAB, ScaLa, or Python; experience accelerating large data transactions across industry-leading GPU architectures to answer analytic questions
Experience with assessments, enterprise data integration, governance, and metrics, including the application of metadata management techniques and ability to interrogate databases efficiently using SQL
Experience with tradecraft and publication; ability to coordinate and support cross-community meetings and working groups; assimilate large volumes of information, and independently produce reports using data science focused libraries such as Pandas, Scikit, TensorFlow and Gensim to answer analytical questions
Desired Requirements:
Knowledge of Army structure and defense level intelligence operations: intelligence collection, fusion,
analysis, production, and dissemination for intelligence databases and products
Knowledge and experience with intelligence operations and in assisting with drafting expert assessments
across operations priorities on behalf of the stakeholder
Specialized training from any intelligence collection and analysis school or certification to include GEOINT Professional Certification (GPC-F, GPC_IA-II, GPC_GA-II, GPC_IS-II, etc)
Knowledge and understanding of the National System for GEOINT (NSG) and Intelligence Community;
knowledge of private sector data science/analytics, machine learning, and data visualization communities
Education Requirements:
MA or MS in Data Science, Data Analytics, Informatics, Statistics, or related field AND 2 years CURRENT
Intelligence Analysis experience; OR
BA or BS in Data Science, Data Analytics, Informatics, Statistics, or related field AND 5 years CURRENT
Intelligence Analysis experience; OR
Undergraduate degree with graduate/professional certificate in Data Science, Data Analytics, Informatics,
Statistics, or related field AND at least 10 years of Intelligence Analysis experience

Benefits:
Health Insurance (Geo Owl pays 80%+ of the premium).
401k matching.
Dental, Vision, and other supplemental insurance plans available.
Company-paid short-term and long-term disability and life insurance.
Peer-to-Peer spot bonuses.
120 hours of PTO per year plus federal holidays.
Joining the Geo Owl Team | What to Expect
At Geo Owl, we highly value our team members. We offer challenging but rewarding opportunities for those who want to work hard to provide a great experience for the customer and strive to reach their professional goals. As a member of the Geo Owl family, you will be working alongside people who share this work ethic and are aiming to be the best partner for our customer. We are all proud to be a part of this company and we want you to be too.
Our Mission
Provide high quality solutions to our mission partners in the United States through our expert analysts.
Be recognized as the best at what we do by our customers.
Be a team our team members are proud and excited to be a part of.
Continually strive for excellence and seek to tackle the most difficult challenges our industry has to offer.
About Us
Geo Owl is a premiere provider of Full-Motion Video (FMV), Geospatial, ISR, Intelligence and IT services to the Department of Defense and Intelligence Community. We are vitalized by our engaged team of professionals that truly value each other and the important missions we support.
Equal Opportunities
Geo Owl is an equal opportunity employer and does not discriminate on the basis of race, color, religion, creed, sex, age, sexual orientation, national origin, disability, marital status, military status, genetic predisposition, or any other basis protected by law.
To stay up to date about new career opportunities:
Follow us on Twitter
Follow us on Instagram
Follow us on LinkedIn

I8pR3c9K0u","$126,295 /yr (est.)",51 to 200 Employees,Company - Private,Aerospace & Defense,Aerospace & Defense,2013,Unknown / Non-Applicable
"E-Business International INC
3.6",3.6,"Alexandria, VA",AWS Data Engineer,"Role: AWS Data Engineers
Location: Alexandria, VA
Full-Time/Permanent
AWS Engineers
We are seeking TWO (2) AWS Engineers to join our growing team. The qualified applicants will become part of the Enterprise Data Analytics Services (EDAS) program for a large federal agency in Alexandria, VA. Hybrid work options are available.
The AWS Engineer is responsible for the development and support of cloud-based solutions. Solutions include moving assets from on-premise to cloud environment, evaluation, and optimization. AWS Engineer will collaborate with the team effectively to ensure the team's success.
Responsibilities/Duties:
Work on automating the migration process in AWS from development to production.
Create CFT deploying/updating cloud resources.
Advise the engineering and software engineering team as they migrate from on-premise to cloud infrastructure.
Optimize cloud workloads for cost, scalability, availability, governance, compliance, etc.
Guide and/or provide hands-on support to administer production, staging, and deployment environments.
Partner with multi-disciplinary teams to understand requirements and plan architecture and solutions.
Apply insight and expertise across AWS services.
Apply knowledge of scripting and automation using tools like PowerShell, Python, Bash, Ruby, Perl, etc.
Experience and Qualifications:
Good working knowledge of AWS Services, including Compute, Storage Networking, Database, Management tools, Security, Identity, and Compliance.
3 years’ experience in AWS (EC2, Glue, S3, Redshift, Python programming).
Experience implementing and debugging workloads in AWS.
Good knowledge of creating CFTs.
Experience building infrastructure inside AWS via code. Familiarity with tools such as Terraform or CloudFormation.
Ability to proactively communicate, interact, and solve critical customer problems.
Ability to think and act strategically and proactively.
3 years of experience in software development tools and methodologies.
3 years of experience in infrastructure architecture, database architecture, and networking.
3 years of experience architecting/deploying operating solutions built on AWS.
Education/Certifications:
Bachelor's Degree in Computer Science, Information Technology, Engineering, or associated discipline.
AWS certification is a plus!
Clearance Requirements:
Ability to attain and maintain a Public Trust.
Job Type: Full-time
Salary: From $100,000.00 per year
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Alexandria, VA 20598: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Expected Salary Range (Must)?
Visa Status/ Work Authorization (Must)?
Experience:
AWS: 4 years (Required)
Work Location: One location","$100,000 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,1992,Unknown / Non-Applicable
NAVA TECH LLC,#N/A,Remote,Azure Data Engineer,"Azure Data Engineer
Remote
Long Term
Note: Need 10+ years of experience
Sound skills and hands on experience with Azure Data Lake, Azure Data Factory, SQL Data Warehouse Azure Blob, Azure Storage Explorer
Proficient in creating Data Factory pipelines for on-cloud ETL processing; copy activity, custom Azure development etc.
Knowledge of Azure Data Catalog, Event Grid, Service Bus, SQL and Synapse
Experience using Azure Databricks platform.
Experience with Python programming.
Experience using pandas and numpy for data engineering and data cleansing;
Ability to troubleshoot the job scheduler and compute clusters.
Experience with databricks CLI and secrets module.
Experience with Azure Blob storage access configuration.
p l e a s e s h a r e r e s u m e s t o m o h a m m e d (a t) n a v a t e c h (d o t) u s O R 4 4 3 5 3 7 9 8 3 5
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$62.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Cloudburst Technologies,#N/A,Remote,Data Collection Engineer,"About Us
Cloudburst is a seed-funded New York-based remote-first company helping customers with detection, prevention, and investigation into cryptocurrency market manipulation and fraud.
Cloudburst provides regulators, financial institutions, trading platforms, and others with access to a real-time, machine-readable crypto market monitoring tool, enabling advanced levels of diligence and customer/market protection. The Cloudburst team has multiple years of experience working together, plus it has worked directly with international law enforcement agencies and policymakers on cybercrime, terrorism, and high-level financial fraud.
What We’re Looking For
The ideal candidate
is an empathetic teammate who loves to help others, to bring order to chaos, and to document their path for others to follow
focuses primarily on web scraping, API integration, and collection of raw data
is capable of becoming fluent in Layer 7 protocols, blockchains, and emerging protocols
has high moral and ethical standards
is proficient in production-level Python
has worked in a threat intel or security environment
What You Will Do
Your primary responsibility will be to become expert at collecting open source data from the web and from other online protocols.
You will be comfortable building tools to bring in data from random files, varying degrees of quality of web sites, and new communications tools. You will have the opportunity to creatively apply your knowledge of software automation towards scaling data collection.
You will understand popular chat sites and how fraud actors use them. You will understand emerging technologies and will actively evaluate them for potential opportunities to collect data.
Why Cloudburst?
At Cloudburst we want to minimize our software engineers working on pixel-pushing and to maximize time spent understanding our domain and building products to help our customers investigate and prevent fraud.
We care about developing and sponsoring our engineers internally. The department adheres to these principles:
Rapid deployment of innovative new techniques for signal generation and attribution
KISS architecture that doesn’t get in developers’ or R&D’s way; we want code to be easy to understand, less abstract, easy to test, fast to deploy, and reducible to automation or simple manual processes
Building a diverse, unconventional team that cross-trains and grows together without ego or sacrifice to work-life balance
What You Will Be Using
Python
Cloud-based infrastructure
3rd party vendor integrations
Novel techniques for data collection using home-grown or discovered tools and frameworks
Hiring Process
Initial email
Initial phone screen with the hiring manager
Take-home test OR link to previous code
Panel interview
Call with CEO
Offer
Investors
Strategic Cyber Ventures
Coinbase Ventures
Bloccelerate
More Info:
https://www.crunchbase.com/organization/cloudburst-technologies-0a3e
https://www.prnewswire.com/news-releases/cloudburst-technologies-raises-3m-in-seed-funding-led-by-strategic-cyber-ventures-joined-by-coinbase-ventures-and-bloccelerate-301817742.html
https://burst.cloud/
https://open.spotify.com/episode/2cv1Is77s8jGLtHPPYRLPU
Job Type: Full-time
Pay: $115,000.00 - $150,000.00 per year
Benefits:
Dental insurance
Flexible schedule
Health insurance
Paid time off
Vision insurance
Compensation package:
Stock options
Experience level:
3 years
Schedule:
Choose your own hours
No nights
No weekends
Work Location: Remote","$132,500 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Fuge Technologies Inc
4.7",4.7,"Palmyra, NJ",Senior Data Engineer,"Job Description:
RESPONSIBILITIES
Basic Qualifications for consideration:
5+ Overall industry experience
3+ years' experience with building large scale big data applications development
Bachelor’s in computer science or related field
Provide technical leadership in developing data solutions and building frameworks
Experience building Data Lake, data applications and micro-services using Azure, AWS and Hands-on experience in cloud Data stack (preference is Azure)
Hands-on experience with Springboot framework, MVC architecture, Hibernate, API creation, Restful services, SOAP using Java
Java experience with OOPS concepts, multithreading
Experience deploying code on containers
Hands on experience with leveraging CI/CD to rapidly build & test application code
Conduct code reviews and strive for improvement in software engineering quality
Review data architecture and develop detailed implementation design
Hands-on experience in production rollout and infrastructure configuration
Demonstrable experience of successfully delivering big data projects using Kafka, Spark
Experience working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search
Experience working with PCI Data and working with data scientists is a plus
In depth knowledge of design principles and patterns
Able to tune big data solutions to improve performance
Qualifications
Bachelor’s degree in computer science or computer Engineering is required
Skills Set Mandate :
Java, Spark, and Azure cloud
Job Type: Contract
Salary: $55.00 - $60.00 per hour
Ability to commute/relocate:
Palmyra, NJ 08065: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",$57.50 /hr (est.),1 to 50 Employees,Contract,#N/A,#N/A,#N/A,Unknown / Non-Applicable
NLB Technology Services,#N/A,"Dallas, TX",CRM data Analytics Engineer,"MUST HAVE:
o Tableau CRMA & Einstein Discovery Certification
o Experience building Dashboards using CRMA (aka Tableau CRM and Einstein Analytics)
o Design, build, and support datasets, data recipes, dataflows, and dashboards using Salesforce and non-Salesforce data sources
o Troubleshoot functional, data, and performance issues with CRMA Dashboards
o Skilled in SAQL, JSON
o Develop and deploy AI/ML solutions using Einstein Discovery and deploy AI/ML Models built outside Salesforce
o Experience working with large data (1 B+ rows) and supporting large user base (10,000+)
o Mentor and coordinate the development efforts of other CRMA DevelopersAT&T
Job Type: Contract
Salary: $65.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Vision insurance
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
CRM ANalytics: 8 years (Required)
einstein: 1 year (Required)
Salesforce: 1 year (Required)
Work Location: One location",$70.00 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Wallero
4.4",4.4,"Seattle, WA",Data Engineer,"Hi,
Pretty much interested in your resume, we have an urgent opening for Data Engineer role with one of our clients based in Seattle WA.
Position: Data Engineer
Location: Seattle WA - Remote (100%)
Description:
Good Experience in reporting space including PowerBI, ADF, and possibly standing up a data warehouse/data mart.
The main scope centering is around reporting.
Experience with SQL, and agile framework including requirement gathering and story/feature writing skills.
Need someone strong who is comfortable leading and guiding teams in that capacity.
Migrating data from third party to Azure Cloud
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Seattle, WA 98101: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",$75.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2015,$1 to $5 million (USD)
"Fincons Group
4.0",4.0,Remote,Data Engineer – Databricks / Kafka / SQL,"Fincons Group is an IT business consulting company that has been designing the digital future of leading companies on international markets for 40 years. Fincons Group built its reputation on foreseeing and interpreting new business models and the rapid evolution of IT systems by building a complete range of services: from research and consulting to design and development, from system integration of leading vendor software solutions right up to application management, supporting clients step by step along their digital transformation journey. Fincons is a multinational with over 2,600 people and several offices worldwide (in Italy, Switzerland, Germany, France, the United Kingdom, Belgium and the USA), but above all a Family Company where the founders play a strategic role with commitment and passion, grounding the company in the same principles of a united and caring family.
We firmly believe in the value of cooperation and in the contribution that every idea and intuition can bring. We believe that everyone’s effort can make a difference. Transforming knowledge into a strategic resource is our company mission, and we pursue it with the help of our most strategic asset: our people.

Job Summary
Looking for an experienced data engineer with hands on experience in developing solutions based on Databricks. If you are a curious team-player with passion to learn and working for different clients to develop modern data streaming platforms, please reach out to us!

Work location: any US location might be considered
Employment type: freelance / contractor
Start date: about July 1th
Duration: about 6 months with possible renewal

Responsibilities and Duties
Design, develop and implement real time data ingestion pipelines from multiple sources using Databricks
Develop high-quality, scalable and re-usable frameworks for ingesting high volume and large data sets
Apply best practices to Integrate and ingest various big data formats like Protobuf, Parquet, AVRO and JSON
Work with event based / streaming technologies like Kafka, or Kinesis, to ingest and transform the data using KafkaStreams or KSQL
Working with other members of the project team to support delivery of additional project components (API interfaces, Search)
Work to load test the pipelines for quality and better performance using JMeter, or Gatling etc

Qualifications and skills
Bachelor’s degree and first experience designing, developing, deploying and/or supporting data pipelines using Databricks
Experience in designing and implementing Medallion architecture is preferred
Expertise in designing and deploying data applications on cloud solutions, such as Azure or AWS
Hands on experience in performance tuning and optimizing code running in Databricks environment
Proficient in programming languages like Pyspark and Python
Good understanding of SQL, T-SQL and/or PL/SQL
Demonstrated analytical and problem-solving skills particularly those that apply to a big data environment
Experience with agile development methodologies.
Ability to work effectively both independently and in a group or team environment

Authorization to Work
Applicants for employment in the US must be authorized to work in the US. Fincons Group will not sponsor applicants for work visas.
All qualified applicants will receive consideration for employment without regard to race, colour, religion, sex, sexual orientation, gender identity, national origin, age, disability, or protected veteran status, or any other legally protected basis, in accordance with applicable law.
The data will be processed and stored exclusively for the purposes of this or future selections, guaranteeing the rights referred to in art. 13 Legislative Decree 196/03 and EU regulation 679/2016 (GDPR)
2yrKJdtT13",#N/A,201 to 500 Employees,Company - Private,Financial Services,Accounting & Tax,#N/A,Unknown / Non-Applicable
"Central City Concern
3.4",3.4,"Portland, OR",Data Engineer I,"Central City Concern (CCC) is an innovative nonprofit agency providing comprehensive services to single adults and families impacted by homelessness, poverty, and addictions in the Portland metro area. We hire skilled and passionate people to meet our mission to end homelessness through innovative outcome-based strategies that support personal and community transformation.
Data Engineer I will assist with designing, implementing, and supporting a data platform while interfacing with business partners across CCC, including Health Services, Housing, Employment, Finance, HR, and other shared service functions. The Data Engineer I will assist with Extract, Transformation, and Load (ETL) strategies, perform data modeling to meet customers’ data needs, and continually improve ongoing reporting and analysis processes while automating or simplifying self-service support for datasets. Data Engineer I will work closely with the Decision Support team to deliver timely reports. This position will look to the Data Engineer II and Data Engineer Senior for coaching/mentoring and join them on projects and development tasks.
Schedule: Monday - Friday 8.00 am-5.00 pm
RESPONSIBILITIES:
Assist in maintaining/developing the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and cloud technologies
Assist with designing, optimizing, testing, and maintaining architectures for our client databases, data pipelines, and processing systems, as well as optimizing data flow and collection for cross-functional teams.
Assist in serving relevant data to all stakeholders, internally and externally, and maintain the infrastructure so the decision support team can present their work to the organization.
Assist in maintaining/developing a scalable data warehouse by connecting disparate data housed across numerous organizational systems and business lines.
Assist in collecting and documenting user requirements, development of user stories, and time estimates.
Supports key meetings and events (governance user groups, discovery events, etc.).
Partners with other Data Engineers to facilitate technical review meetings.
Generates runbook documentation of supported systems.
Actively audits and monitors system performance.
Supports during system and unit testing events.
Assists during code and system upgrades.
Develops and implements new technology with review by DE2 or DE Senior.
Escalates issues to DE2 and/or DE Senior, as needed.
Adhere to all state and federal privacy regulations, including HIPAA and 42 CFR Part 2, and CCC policies and agreements regarding confidentiality, privacy, and security. Support compliance with all privacy and security requirements pursuant to community partners' and outside providers’ patient confidentiality agreements, including privacy and security requirements for EMR access. This includes immediately reporting any breach of protected health information or personal identification information of any person receiving CCC services by CCC or an outside provider to the CCC Compliance Department, as well as to your supervisor or their designee.
Provide the highest standard of customer service to internal and external stakeholders, including CCC clients, CCC staff, and community members.
Attend all mandatory CCC training promptly.
Other duties as assigned.
QUALIFICATIONS:
Bachelor’s degree in related field required and 1+ years of recent data engineer experience that includes Python, Java and/or other object-oriented script language and experience using SQL and relational databases, including query authoring
OR Associate’s degree or trade school certificate in an IT related field with 2+ years of recent data engineer experience that includes experience with Python, Java and/or other object-oriented script language and experience using SQL and relational databases, including query authoring
OR 3+ years of recent data engineer experience that includes experience with Python, Java, and/or other object-oriented script language experience using SQL and relational databases, including query authoring
Familiarity with a variety of databases
Experience with manipulating, processing, and extracting value from large, disconnected datasets
Familiar with gathering data through multiple sources through API calls and scripting languages
Understanding reporting tools, like Power Bl, SQL Server Analysis Services, and SQL Server Report Services.
Understanding of the Agile principles and methodologies
Must be able to work within an integrated, multidisciplinary setting
Adhering to Central City Concern’s drug-free workplace encourages a safe, healthy, and productive work environment and strictly complies with the Drug-Free Work Place Act of 1988. An employee shall not, in the workplace, unlawfully manufacture, distribute, dispense, possess, or use a controlled substance or alcohol.
Must pass a pre-employment drug screen, TB Test, and background check.
Must adhere to the agency’s non-discrimination policies.
Ability to effectively interact with co-workers and clients with diverse ethnic backgrounds, religious views, cultural backgrounds, lifestyles, and sexual orientations and treat each individual with respect and dignity.
BENEFITS:
Central City Concern offers an incredible benefits package to our employees!
Generous paid time off plan beginning at 4 weeks per year at the time of hire. Accrual increases with longevity.
Amazing 403(b) Retirement Savings plan with an employer match of 4.25% in your 1styear, 6% in the 2nd year, and 8% in your 3rd year!
11 paid Holidays PLUS 2 Personal Holidays to be used at the employee’s discretion.
Comprehensive Medical, Vision, and Dental insurance coverage.
Employer Paid Life, Short-Term Disability, AND Long-Term Disability Insurance!
Sabbatical Program is offers extended time off in years 7, 14, and 21.
This description is intended to provide a snapshot of the work performed. It is not designed to contain a comprehensive inventory of all duties, responsibilities, and qualifications required for the position.
Central City Concern is a second-chance employer and complies with applicable laws regarding considering criminal background for employment purposes. Government regulations, contractual requirements, or the duties of this particular job may require CCC to conduct a background check and take appropriate action to address prior criminal convictions.
Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities
The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information. 41 CFR 60-1.35(c)","$77,601 /yr (est.)",Unknown,Nonprofit Organization,Nonprofit & NGO,Civic & Social Services,1979,Unknown / Non-Applicable
SecurePro,#N/A,"Arlington, VA",Data Engineer,"Collect, manage, and convert raw data accurately and reliably
Organize data systems for subgroup access and analyses
Configure and sustain data cloud structures
Able to work with structured and unstructured data.
Validate outputs of data pipelines
Convert existing SAS code to python/pyspark code for model operation in the cloud
Create and sustain policy analysis models in the cloud
Troubleshoot user interfaces in the cloud
Create and sustain intuitive user interfaces in the cloud
Degree in Data Engineering preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $100.00 per hour
Experience level:
3 years
Schedule:
8 hour shift
Application Question(s):
Are you a US citizen?
Must have DOD secret Clearance?
Work Location: Remote",$90.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"New York Technology Partners
3.9",3.9,"Charlotte, NC",AWS Data engineer,"Job Title: AWS Cloud Engineer
Location: Charlotte NC
Position: Contract
This is a major platform initiative which will set the pace and direction for Analytics in Vanguard.
Work with serverless cloud services to create integrations, APIs endpoints and related components
· Work on features and POCs with industry-leading technologies and tools
· Work on greenfield space to shape the modernization efforts for the analytics ecosystem
· Create automated tests for functional, performance, and security in order to ensure flexibility over time.
· Work alongside multiple lines of business and platform teams
· Create custom integrations between existing, AWS components, and vendor products.
About you:
· You care about software quality and maintainability
· You have an interest in serverless architecture
· You want to shape the Analytics modernization effort enterprise wide
· Qualifications
· Experience and technologies that you will work with:
· Scripting & Programming: Python – 5+ years’ experience
· 5+ years’ experience with Infrastructure as a Service
· 3+ years’ experience with building serverless applications
· AWS Core Services: Cloudformation, Step Functions, DynamoDB, S3, Glue, Athena
· AWS Serverless Infrastructure: Lambda, SQS, SNS, EventBridge, API Gateway, Application Load Balancer
Nice to have:
· AWS Associate or Professional Certification
· EMR knowledge
· Docker & Kubernetes
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Charlotte, NC 28202: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 5 years (Required)
IaaS: 3 years (Required)
serverless application: 3 years (Required)
Work Location: One location",$60.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,1999,$25 to $100 million (USD)
"VedaInfo Inc
4.1",4.1,"Phoenix, AZ",Big Data Engineer,"Hi,
I hope this note finds you well
I am reaching out to you regarding your career opportunity with us for “Big Data Engineer” for a “6+ Months Contract” @Phoenix, AZ
Please find the below requirement details.
Job Title: Big Data Engineer
Location: Phoenix, AZ (Onsite)
Duration: 6+ months
Rate: $60/hr C2C
Responsibilities:
Design, implement, and maintain big data systems handling large volumes of data.
Utilize Hadoop, Hive, and Spark for efficient data processing and analysis.
Collaborate with cross-functional teams to understand data requirements.
Develop and optimize data ingestion, storage, and transformation processes.
Build scalable data pipelines for seamless data processing and analysis.
Monitor and troubleshoot data processing and performance issues.
Stay updated with emerging big data technologies.
Requirements:
Bachelor's/Master's degree in CS, Engineering, or related field
7+ years of experience as a Big Data Engineer
Strong proficiency in Hadoop, Hive, and Spark
Extensive experience with AWS or Azure
Solid understanding of data ingestion, storage, and transformation
Excellent problem-solving and communication skills
Proactive and self-motivated
This is a 6+ month onsite contract position in Phoenix, AZ, with the possibility of extension. Compensation is $60/hr on C2C basis.
Please submit your updated resume highlighting relevant experience.
Thank you for your interest!
Thanks & Regards
Mohammed ZAIN
Job Type: Contract
Pay: $50.00 - $60.00 per hour
Experience level:
7 years
Ability to commute/relocate:
Phoenix, AZ: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
To improve your chances of getting Shortlisted, make sure to tailor your Profile to the Job Requirements, Highlight your Relevant Skills and Experience, and then upload your Resume.
Experience:
AWS (Amazon Web Services): 7 years (Required)
Azure: 7 years (Required)
Big data: 7 years (Required)
Hadoop: 7 years (Required)
Apache Hive: 7 years (Required)
Spark: 7 years (Required)
Work Location: One location",$55.00 /hr (est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$1 to $5 million (USD)
Sconcept,#N/A,Remote,Azure Data Engineer,"** W2 LONG TERM CONTRACT BASED....**(NO C2C)
JOB TYPE:- Long Term Contract(6 months)Extendable
JOB LOCATION:-USA
About Sconcept LLC:-Software Concepts Consulting was established in 2008. With our “hands-on” approach we are very successful in delivering high performance, secure IT solutions and IT services. With headquarters in Carrollton, Texas we are specialized in providing business solutions to the Fortune 500 clients. Our clients include Software, Banking, Financial, Healthcare, Retail, Consumer goods and Insurance sectors. We deliver candidates based on client requirements.
Required Skills:-
Must have worked on several (5+) projects building solutions for Data Pipelines, Data warehousing, Data Modelling on Azure Data Platform (AWS, Google Cloud Platform - good to have)
Expertise in design and development of Data pipelines and ETL using ADF, Databricks to move data from relational/structured/unstructured data from source to data lake to data warehouse, on Azure
Creating Pipelines and integrations of multiple data sources
Data processing and transformation using Databricks (Scala/Python)
Expertise in optimizing cost and performance of data pipelines
Experience working with Data Lake, Synapse on Azure
Designing Data Lake storage layer (Landing/staging, raw, trusted/curated zones)
Designing data storage DB schema, data models and processing on Azure Synapse DW
Experience with Synapse Spark Pool
Experience in Data Modelling
Building Data Models from structured and semi-structured data (Data Model, Common Data Model etc.)
Performance optimization of data model for high performance analytical and reporting workloads
Expertise with SQL Server and extensive experience with SQL programming
Programming and optimizing DB objects Views, Stored Procs, Functions
Good experience in defining data access policy and security for azure data services
Experience with Oracle EDW
Knowledge/Experience migrating from Oracle EDW to any cloud DW
Experience integrating AI skills/ML models with Data and reporting solutions
Experience in building API layer for downstream consumption
Job Type: Contract
Pay: $45.00 - $50.00 per hour
Benefits:
Health insurance
Compensation package:
Bonus pay
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$47.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"MARVEL TECHNOLOGIES INC
3.7",3.7,"Cincinnati, OH",Data Engineer,"Job Description
The data engineer designs and builds platforms tools and solutions that help the bank manage secure and generate value from its data. The person in this role creates scalable and reusable solutions for gathering collecting storing processing and serving data on both small and very large i.e. Big Data scales. These solutions can include on-premise and cloud-based data platforms and solutions in any of the following domains ETL business intelligence analytics persistence relational NoSQL data lakes search messaging data warehousing stream processing and machine learning.
Responsible and accountable for risk by openly exchanging ideas and opinions elevating concerns and personally following policies and procedures as defined. Accountable for always doing the right thing for customers and colleagues and ensures that actions and behaviors drive a positive customer experience. While operating within the Bank's risk appetite achieves results by consistently identifying assessing managing monitoring and reporting risks of all types.
ESSENTIAL DUTIES AND RESPONSIBILITIES:
Responsible for design Development and Support of data solutions APIs tools and processes to enable rapid delivery of business capabilities.
Work closely with IT application teams Enterprise architecture infrastructure information security and LOB stakeholders to translate business and technical strategies into data-driven solutions for the Bank.
Act as a technical Expert addressing problems related to system and application design performance integration security etc.
Conduct research and Development based on current trends and technologies related to the banking industry data engineering and architecture data security and related topics.
Work with developers to Build CI/CD pipelines Self-service Build tools and automated deployment processes.
Evaluate software products and Provide documented recommendations as needed.
Provide Support and troubleshooting for data platforms. Must be willing to Provide escalated on-call Support for complicated and/or critical incidents.
Participate in the planning process for hardware and software.
Plan and work on internal projects as needed including legacy system replacement Monitoring and analytics improvements tool Development and technical documentation.
Provide technical guidance and mentoring for other team members.
Manage and prioritize multiple assignments.
MINIMUM KNOWLEDGE SKILLS AND ABILITIES REQUIRED:
Bachelor's degree in Computer Science/Information Systems or equivalent combination of education and experience.
Must be able to communicate ideas both verbally and in writing to management business and IT sponsors and technical resources in language that is appropriate for each group.
Fundamental understanding of distributed computing principles
Knowledge of application and data security concepts best practices and common vulnerabilities.
Conceptual understanding of one or more of the following disciplines preferred big data technologies and distributions metadata management products commercial ETL tools Bi and reporting tools messaging systems data warehousing Java language and run time environment major version control systems continuous integration/delivery tools infrastructure automation and virtualization tools major cloud or rest API design and development.
Job Types: Full-time, Contract
Pay: $60.00 - $62.00 per hour
Compensation package:
Hourly pay
Yearly pay
Experience level:
9 years
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",$61.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,#N/A,$5 to $25 million (USD)
META FORCE IT LLC,#N/A,"Cincinnati, OH",Data Engineer,"Job Description
The data engineer designs and builds platforms tools and solutions that help the bank manage secure and generate value from its data. The person in this role creates scalable and reusable solutions for gathering collecting storing processing and serving data on both small and very large i.e. Big Data scales. These solutions can include on-premise and cloud-based data platforms and solutions in any of the following domains ETL business intelligence analytics persistence relational NoSQL data lakes search messaging data warehousing stream processing and machine learning.
Responsible and accountable for risk by openly exchanging ideas and opinions elevating concerns and personally following policies and procedures as defined. Accountable for always doing the right thing for customers and colleagues and ensures that actions and behaviors drive a positive customer experience. While operating within the Bank's risk appetite achieves results by consistently identifying assessing managing monitoring and reporting risks of all types.
ESSENTIAL DUTIES AND RESPONSIBILITIES:
Responsible for design Development and Support of data solutions APIs tools and processes to enable rapid delivery of business capabilities.
Work closely with IT application teams Enterprise architecture infrastructure information security and LOB stakeholders to translate business and technical strategies into data-driven solutions for the Bank.
Act as a technical Expert addressing problems related to system and application design performance integration security etc.
Conduct research and Development based on current trends and technologies related to the banking industry data engineering and architecture data security and related topics.
Work with developers to Build CI/CD pipelines Self-service Build tools and automated deployment processes.
Evaluate software products and Provide documented recommendations as needed.
Provide Support and troubleshooting for data platforms. Must be willing to Provide escalated on-call Support for complicated and/or critical incidents.
Participate in the planning process for hardware and software.
Plan and work on internal projects as needed including legacy system replacement Monitoring and analytics improvements tool Development and technical documentation.
Provide technical guidance and mentoring for other team members.
Manage and prioritize multiple assignments.
MINIMUM KNOWLEDGE SKILLS AND ABILITIES REQUIRED:
Bachelor's degree in Computer Science/Information Systems or equivalent combination of education and experience.
Must be able to communicate ideas both verbally and in writing to management business and IT sponsors and technical resources in language that is appropriate for each group.
Fundamental understanding of distributed computing principles
Knowledge of application and data security concepts best practices and common vulnerabilities.
Conceptual understanding of one or more of the following disciplines preferred big data technologies and distributions metadata management products commercial ETL tools Bi and reporting tools messaging systems data warehousing Java language and run time environment major version control systems continuous integration/delivery tools infrastructure automation and virtualization tools major cloud or rest API design and development.
Job Type: Contract
Pay: $45.96 - $50.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Cincinnati, OH 45215: Reliably commute or planning to relocate before starting work (Required)
Experience:
data engineer: 10 years (Preferred)
Big data: 3 years (Preferred)
Work Location: Hybrid remote in Cincinnati, OH 45215",$47.98 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Myticas Consulting
3.9",3.9,Arizona,BHJOB15656_20058 - Data Engineer,"Myticas's direct client based out of Phoenix, AZ is currently seeking a Data Engineer for a 100% remote contract position.
Pay Rate: Up to $34/hr

Job Description:
A Data Engineer can confidently understand, write, troubleshoot, optimize, and peer review SQL code. They understand business requirements and create code and reporting that exceeds specifications through anticipating underlying needs. They also understand relational database design, table structure, data types, and data models. They are experienced with and confident in development in reporting visualizations in Excel, PowerBI, Tableau, QuickSight, SSRS, or other data products. They can create reporting dashboards and products that inform the business. They easily grasp and retain complex data concepts and are able to explain them to others. Looking for engineers who are driven to innovate, use cutting edge technologies to solve problems and think outside the box.
What You’ll Do:
Digital Transformation into AWS
Data Warehouse related work (SQL, ETL, Replication, Databases)
Understand, write, troubleshoot, optimize, and peer review SQL code.
Understand business requirements and create code and reporting that exceeds specifications through anticipating underlying needs.
Understand relational database design, table structure, data types, and data models. 6.Experienced with and confidence in development in reporting visualizations in Excel, PowerBI, Tableau, QuickSight, SSRS, or other data products.
Data Warehouse related work (SQL, ETL, Replication, Databases
Perform other duties as assigned or apparent.
MINIMUM EDUCATION AND RELATED WORK EXPERIENCE:
Bachelor's degree in a Computer Science field and 2 years' work experience
2 years of experience in most phases of IT systems deployments in one or more of the following areas: design and deployment of cloud services, data migration to AWS cloud, Digital Transformation into AWS, and Data Warehouse related work
INDCHI
INDREM",$34.00 /hr (est.),1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"ProGrad
4.2",4.2,Remote,Data Engineer,"We are looking for data engineers that are excited about building and maintaining data pipelines for a variety of interests. The pipelines will ingest data from a variety of sources ranging from internal change data streams to external integrations with various SaaS platforms through webhooks and events. The ideal candidate is self driven and looking to grow as a data engineer while having an outsized impact on the future of data at the company. A love for database architecture and optimisation is a must as is the ability to work across diverse teams.
Requirements/Qualifications:
Bachelor's or Master's degree in Data Science (specialization)
Software development experience coding in a general-purpose programming language such as C, C++, Java, JavaScript, Golang, or Python
Experience working with data structures or algorithms (i. e., data structures /algorithms class, coursework/projects, research, internships, or other practical experience in/outside of school or work (including open source hobby coding)
Proficiency in SQL, and NoSQL Databases
Experience working with large datasets
Experience writing and debugging complex SQL queries
A strong understanding of cloud database best practices and performance optimisation
Experience building and maintaining data pipelines, coupled with knowledge of good programming practices such as unit and integration testing
. A strong understanding of cloud database best practices and performance optimisation
Good to have skills:
. Experience building and maintaining data pipelines, coupled with knowledge of good programming practices such as unit and integration testing
Job Types: Full-time, Permanent
Salary: Up to $1,200,000.00 per year
Experience level:
1 year
Under 1 year
Schedule:
Day shift
Work Location: Remote
Speak with the employer
+91 9057202434",#N/A,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Vedainfo
4.1",4.1,"Peoria, IL",Data Engineer,"Job Description
· Create and develop optimal data pipeline which fetches data from raw and apply business transformation and load derived.
· Build the AWS Fargate components required for optimal extraction, transformation, and loading of data-to-data lake.
· Develop data pipeline setup between base and derive values based on Caterpillar specification.
· Implement quality validation rules and follow process defined by stakeholder.
· Develop business events as defined by the solution team to generate CDC events for consumption layer (Kinesis)
· Create re-usable and configurable components wherever applicable.
· Create and execute unit, integration, and regression tests
· Ensure the code is developed satisfying all business rules, perform unit testing where applicable.
· Data pipelines development
· Python Development
· AWS cloud services
· Data quality validation rules
· Java (preferred)
· Implement CDC
· Transformation rules creation
· Message broker tools like Kinesis
· Azure DevOps – CI/CD pipelines
· Automated and Manual Testing experience
Job Type: Contract
Pay: Up to $60.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Peoria, IL 61602: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
This position is remote initially and you will be asked to work any of the client location once offer is made. Please confirm your current location.
Experience:
Data Engineer: 1 year (Preferred)
Python Development: 1 year (Preferred)
AWS: 1 year (Preferred)
Message broker tools: 1 year (Preferred)
Work Location: In person",$60.00 /hr (est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$1 to $5 million (USD)
shreetek,#N/A,"Charlotte, NC",Senior Data Engineer,"Job Title : Data Engineer
ONLY USC
On W2 only
Job Location : Newjersey OR Charlotte,NC
Collect, manage, and convert raw data accurately and reliably
Organize data systems for subgroup access and analyses
Configure and sustain data cloud structures
Able to work with structured and unstructured data.
Validate outputs of data pipelines
Degree in Data Engineering preferred.
Two Programmers –
Convert existing SAS code to python/pyspark code for model operation in the cloud.
Create and sustain policy analysis models in the cloud.
Troubleshoot user interfaces in the cloud.
Create and sustain intuitive user interfaces in the cloud.
A degree in Computer Science is preferred.
Overall across resources keep following stack as reference
Visualization : Tableau
Data Modeling/Science : Python / SAS
Engg : AWS cloud native services, security, data pipeline
Job Type: Contract
Salary: $50.00 - $60.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Charlotte, NC 28202: Reliably commute or planning to relocate before starting work (Required)
Work Location: One location",$55.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Deako
4.9",4.9,"Seattle, WA",Data Engineer II,"About Deako
Deako is delivering a revolutionary plug-n-play smart home platform with a laser focus on the untapped new home construction market. We make smart lighting so easy to upgrade that even those who would have never considered smart lighting are making it happen. We've built a company based on trust; where forming personal relationships is key to our success.
Software at Deako
The software team is a tight-knit team of smart, dedicated people. We are customer focused, we ship often, and are constantly asking “why.” We are passionate about the software we write. We work closely with other Deako teams. Developers at Deako are always looking for a better way and understand that no code is perfect.
Data at Deako
We believe that Data and Software go hand in hand. That's why the Data and Software teams work closely together to ensure we're collecting the right data, and getting it to the right people. Data is new at Deako, and we are working to make it a cornerstone of our company. We want every team at Deako to have a data-driven mindset.
Day to Day Expectations:
Participate in the code review process in our DBT repository
Build maintainable, testable SQL queries for various dashboards
Help maintain, contribute to and improve our data infrastructure and existing pipelines
Help maintain and improve our CI/CD pipelines
Responsible for obtaining, cleaning, and munging data and getting it into a form that our data analysts can access and analyze
Participate in meetings with stakeholders around data collection and cleaning

Must Have:
Demonstrable knowledge in SQL + Some Programming Language (Typescript, Python, Ect)
Demonstrates a high level autonomy and willingness to learn
1+ years of experience working as a Data Analyst/Engineer or Comparable field.
Experience with git, dbt
Nice to Have:
Experience in Snowflake, Fivetran, Hightouch
Experience with Gitlab CI
Experience with geospatial data
Stock Options
Hybrid work environment (Office + Remote)
Medical/Dental/Vision/Life/401K
Unlimited PTO
Free Snacks/Coffee/Drinks (Non-Alcoholic and Alcoholic)
Quarterly Company Parties","$115,000 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,2015,$5 to $25 million (USD)
"Fincons Group
4.0",4.0,Remote,Data Engineer – Databricks / Kafka / SQL,"Fincons Group is an IT business consulting company that has been designing the digital future of leading companies on international markets for 40 years. Fincons Group built its reputation on foreseeing and interpreting new business models and the rapid evolution of IT systems by building a complete range of services: from research and consulting to design and development, from system integration of leading vendor software solutions right up to application management, supporting clients step by step along their digital transformation journey. Fincons is a multinational with over 2,600 people and several offices worldwide (in Italy, Switzerland, Germany, France, the United Kingdom, Belgium and the USA), but above all a Family Company where the founders play a strategic role with commitment and passion, grounding the company in the same principles of a united and caring family.
We firmly believe in the value of cooperation and in the contribution that every idea and intuition can bring. We believe that everyone’s effort can make a difference. Transforming knowledge into a strategic resource is our company mission, and we pursue it with the help of our most strategic asset: our people.

Job Summary
Looking for an experienced data engineer with hands on experience in developing solutions based on Databricks. If you are a curious team-player with passion to learn and working for different clients to develop modern data streaming platforms, please reach out to us!

Work location: any US location might be considered
Employment type: freelance / contractor
Start date: about July 1th
Duration: about 6 months with possible renewal

Responsibilities and Duties
Design, develop and implement real time data ingestion pipelines from multiple sources using Databricks
Develop high-quality, scalable and re-usable frameworks for ingesting high volume and large data sets
Apply best practices to Integrate and ingest various big data formats like Protobuf, Parquet, AVRO and JSON
Work with event based / streaming technologies like Kafka, or Kinesis, to ingest and transform the data using KafkaStreams or KSQL
Working with other members of the project team to support delivery of additional project components (API interfaces, Search)
Work to load test the pipelines for quality and better performance using JMeter, or Gatling etc

Qualifications and skills
Bachelor’s degree and first experience designing, developing, deploying and/or supporting data pipelines using Databricks
Experience in designing and implementing Medallion architecture is preferred
Expertise in designing and deploying data applications on cloud solutions, such as Azure or AWS
Hands on experience in performance tuning and optimizing code running in Databricks environment
Proficient in programming languages like Pyspark and Python
Good understanding of SQL, T-SQL and/or PL/SQL
Demonstrated analytical and problem-solving skills particularly those that apply to a big data environment
Experience with agile development methodologies.
Ability to work effectively both independently and in a group or team environment

Authorization to Work
Applicants for employment in the US must be authorized to work in the US. Fincons Group will not sponsor applicants for work visas.
All qualified applicants will receive consideration for employment without regard to race, colour, religion, sex, sexual orientation, gender identity, national origin, age, disability, or protected veteran status, or any other legally protected basis, in accordance with applicable law.
The data will be processed and stored exclusively for the purposes of this or future selections, guaranteeing the rights referred to in art. 13 Legislative Decree 196/03 and EU regulation 679/2016 (GDPR)
2yrKJdtT13",#N/A,201 to 500 Employees,Company - Private,Financial Services,Accounting & Tax,#N/A,Unknown / Non-Applicable
Enterprise Minds,#N/A,Remote,Data Engineer,"Job Role: SQL Data Engineer
Location: Remote Opportunity - Need to work in PST hours
Visa: US Citizen Only
Duration: Long term
Roles and Responsibilities:
3+ years of experience with a complex build system in AWS using Starburst.
5+ years of experience with installation, maintenance, and administration of Oracle databases- PL/SQL and ANSI SQL
Oracle DBA + Oracle SQL to ANSI SQL conversion, performance/query tuning
3+ years of experience configuring, integrating, and securing multiple AWS database offerings (ex. RDS, Aurora, Casandra, and Dynamo)
2+ years of experience with supporting Linux systems engineering efforts in system design and evaluation, solution engineering, software development, or system administration.
Experience with Database Schema as Code tools
Ability to design and develop complex SQL queries and reports to retrieve required data.
Experience with database partitioning and microservices
Knowledge of Agile methodologies or the software development life cycle (SDLC)
US Citizenship is Mandatory.
Job Type: Full-time
Experience:
Data Engineer: 10 years (Preferred)
AWS using Starburst: 4 years (Preferred)
PL/SQL and ANSI SQL: 2 years (Preferred)
AWS RDS: 1 year (Preferred)
Work Location: Remote",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Stefanini, Inc
3.8",3.8,"Dearborn, MI",Data Engineer,"Stefanini Group is hiring!
Stefanini is looking for Data Engineer at Dearborn, MI (Hybrid)
For quick apply, please reach out to Rajat Baloria
Phone: (248) 728-2620
Email: Rajatsingh.Baloria@stefanini.com

Open to W2 candidates only!

Position Description:
Looking for a Software Engineer focused on delivering software leveraging Java and Python based on proven Lean/Agile methods. Knowledge of Big Data technologies like Hadoop and Spark is a plus.
The Software Engineer will work in a small, cross-functional, and co-located team. The Software Engineer will collaborate directly and continuously with business partners, product managers and designers, and will release early and often.
Position Responsibilities:
Work hands-on with the team and other stakeholders to deliver quality software products that meet our customer's requirements and needs.
Grow technical capabilities / expertise and provide guidance to other members on the team



Skills Required:
Exceptional software engineering knowledge; OO Design Principles
Basic understanding of Big Data and potential use cases
Strong desire to learn new skills and apply to solve business problems/opportunities “Spring Boot, Java, Angular, API, Micro Services, PCF, GCP (especially Cloud Run)”

Experience Required:
Overall 6 years of work experience in delivering customer facing products
Minimum 2 years of strong development experience in at least one of the following technologies: Java or Python on a Hadoop/Spark Platform

Education Required:
An associate's degree in Computer Science or similar technical discipline
Education Preferred:
Google Cloud Certification

Listed Salary Range may vary based on experience, qualifications, and local market, Also some positions may Include bonuses and other Incentives

About Stefanini Group
The Stefanini Group is a global provider of offshore, onshore and near shore outsourcing, IT digital consulting, systems integration, application and strategic staffing services to Fortune 1000 enterprises around the world. Our presence is in countries like Americas, Europe, Africa and Asia, and more than 400 clients across a broad spectrum of markets, including financial services, manufacturing, telecommunications, chemical services, technology, public sector, and utilities. Stefanini is a CMM level 5, IT consulting, company with global presence. We are CMM Level 5 company.",$82.00 /hr (est.),10000+ Employees,Subsidiary or Business Segment,Information Technology,Information Technology Support Services,1987,$1 to $5 billion (USD)
"DiamondPick
4.5",4.5,Remote,GCP Data Engineer,"Position: GCP Data Engineer (8+years)
Location: Remote
Type: Contract
Job Details:
· Relevant Industry Work Experience (6+ for Developer, preferred 9+ years)
· Experience extracting data from a variety of sources, and a desire to expand those skills (Excellent knowledge in SQL and Spark is mandatory)
· Strong knowledge of Google BigQuery and architecting data pipelines from on-prem to GCP.
· Experience building applications using Google Cloud Platform related frameworks such as DataProc and GCS at the minimum.
· Excellent Communication Skills to Understand and Pass on Requirements.
· Excellent Data Analysis skills. Must be comfortable with querying and analyzing large amount of data on Hadoop HDFS using Hive and Spark.
· Professional experience with a cloud hosting platform (GCP preferred)
· GCP certification is preferred.
Job Type: Contract
Salary: Up to $65.00 per hour
Experience level:
10 years
8 years
9 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
Google BigQuery: 4 years (Preferred)
DataProc: 2 years (Preferred)
Google Cloud Platform: 9 years (Required)
Spark: 8 years (Required)
SQL: 8 years (Required)
License/Certification:
Google Cloud Platform(GCP) Certification (Required)
Work Location: Remote",$65.00 /hr (est.),1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,#N/A,Unknown / Non-Applicable
Zllius Inc.,#N/A,"Chicago, IL",Data Engineer,"Hi,
Greetings from Zllius Inc!
We have an urgent opening for the role Data Engineer. If interested, revert me back with your Updated resume, along with contact details, work authorization.
Job Role: Data Engineer
Duration: Long-Term
Position Type: W2/ 1099
Visa: Any
Job Description:
Technical/Functional Skills
Successful candidates will have:
Hands on experience in Dremio and Scripting
Hands on Scripting experience in Python and Flask
Hands on experience in Integrating data from multiple source systems (PostgreSQL and CSV)
Experience with Apache Superset
Ability to write complex SQL queries
Experience in building data pipelines using PostgreSQL, Oracle database, Dremio, AWS Open Search
Experience with Docker Containerization of Code
Work as a part of a scrum team in Agile methodology
ETL, Data Pipelines, PostgreSQL, SQL Queries, Scripting
Job Types: Full-time, Contract
Salary: $111,076.11 - $133,769.08 per year
Schedule:
8 hour shift
Day shift
Monday to Friday
Work Location: On the road","$122,423 /yr (est.)",Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Amazee Global Ventures Inc
5.0",5.0,Remote,Data Engineer,"(W2 ONLY)
Job Title: Data Engineer
Duration : 12 months
Location: Remote
6-7 years of experience
Healthcare experience is must
Design, develop, and maintain Tableau dashboards and reports
Develop and maintain data models and ETL processes
Analyze data to identify trends and patterns
Create and maintain data dictionaries and data models
Develop and maintain data pipelines
Collaborate with stakeholders to understand data requirements
Develop and maintain data quality processes
Monitor data quality and performance
Troubleshoot data issues
Develop and maintain documentation
Requirements:
Bachelor’s degree in Computer Science, Information Systems, or related field
4+ years of experience in data engineering and analysis
Expertise in Tableau
Experience with SQL and data modeling
Knowledge of ETL processes
Knowledge of data warehousing
Excellent problem-solving and communication skills
Ability to work independently and in a team environment\
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Type: Full-time
Salary: $40.33 per hour
Work Location: Remote",$40.33 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,2019,Less than $1 million (USD)
"JPMorgan Chase Bank, N.A.
3.8",3.8,"Wilmington, DE",Data Engineer III,"As a Data Engineer III under Consumer and Community Banking within the ADE Data Governance team at JPMorgan Chase, you serve as a seasoned member of an agile team to design and deliver trusted data collection, storage, access, and analytics solutions in a secure, stable, and scalable way. You are responsible for developing, testing, and maintaining critical data pipelines and architectures across multiple technical areas within various business functions in support of the firm's business objectives.

Job responsibilities
Supports review of controls to ensure sufficient protection of enterprise data
Responsible for advising and making custom configuration changes in one to two tools to generate a product at the business or customer request
Updates logical or physical data models based on new use cases
Frequently uses SQL and understands NoSQL databases and their niche in the marketplace
Adds to team culture of diversity, equity, inclusion, and respect

Required qualifications, capabilities, and skills
Experience across the data lifecycle
Advanced at SQL (e.g., joins and aggregations)
Working understanding of NoSQL databases
Proficiency with JSON/Java, Erwin, Oracle, Snowflake, AWS
Significant experience with statistical data analysis and ability to determine appropriate tools and data patterns to perform analysis, including data modeling
Experience customizing changes in a tool to generate product
Executes data architecture solutions and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions and break down problems
Produces data architecture and design artifacts for complex applications while being accountable for ensuring design constraints are met by software code development
Gathers, analyzes, synthesizes, and develops visualizations and reporting from large, diverse data sets in service of continuous improvement of data frameworks, applications, and systems that proactively identifies hidden problems and patterns in data and uses these insights to drive improvements to coding hygiene and system architecture

Preferred qualifications, capabilities, and skills
Familiarity with modern technologies
Exposure to cloud technologies
Chase is a leading financial services firm, helping nearly half of America's households and small businesses achieve their financial goals through a broad range of financial products. Our mission is to create engaged, lifelong relationships and put our customers at the heart of everything we do. We also help small businesses, nonprofits and cities grow, delivering solutions to solve all their financial needs.

We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.
The health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the ""WELL Health-Safety Rating"" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.
As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm's current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm's vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.
We offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, discretionary incentive compensation which may be awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.
Equal Opportunity Employer/Disability/Veterans
Base Pay/Salary
Jersey City,NJ $97,850.00 - $150,000.00 / year","$123,925 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Banking & Lending,1799,$10+ billion (USD)
"Fresh Consulting
3.9",3.9,"Menlo Park, CA",Data Engineer,"Fresh Consulting is a design-led, software development and hardware engineering company, offering end-to-end digital services to help companies innovate. We bring together amazing UX designers, sophisticated developers, digital strategists, and top-notch engineers to help companies create fresh experiences that connect humans, systems, and machines. We’ve been growing fast and need someone to help us continue to manage the delivery of high-quality work in a fast-paced environment.
See more at freshconsulting.com Visit freshconsulting.com/portfolio to see our project work across several industries.
View and apply to all jobs - https://freshconsulting.applytojob.com/apply/ or visit freshconsulting.com/careers
Title: Data Engineer
Duration: 6 months with possible extension
Location: Onsite Menlo Park, CA
Benefits: Employee benefits at 100% including Medical, PTO, Holiday Pay, 401K Plan, and much more!
Hours: Minimum 40 Hours/Week
Role:
Design, construct, install, test, and maintain highly scalable data management systems.
Ensure systems meet business requirements and industry practices.
Design, implement, automate, and maintain large-scale enterprise data ETL processes.
Build high-performance algorithms, prototypes, predictive models, and proof of concepts.
Skills:
5+ years of work experience as a Data Engineer.
Ability to work as part of a team, as well as work independently or with minimal direction.
Excellent written, presentation, and verbal communication skills.
Collaborate with data architects, modelers, and IT team members on project goals.
Strong PC skills including knowledge of Microsoft SharePoint.
Process certification, such as Six Sigma, CBPP, BPM, ISO 20000, ITIL, and CMMI.
Education: BSCSE or related.
FRESH-
Work on engineering and research assignments with F500 companies and startups.
The relationships that we have created with our clients are one of a kind.
We help solve problems in many technologies focusing on R&D, product development, and manufacturing.
We work with the most cutting-edge and latest technologies from AR/VR to Autonomous technologies.
Closely working with our clients, we believe that long-term investments are extremely important to maintain the culture we together have created.
We’re a handpicked team of Engineers, digital strategists, designers, and developers united together in creating a fresh experience. Whether we are strategizing, designing, developing, or analyzing, our integrated team works as an extension of yours to improve your impact, your usability, and your customer conversion. In the process, we collaborate with you to get to know your business, understand your industry, and incorporate your big ideas into memorable experiences that keep your customers coming back for more.
Equal employment opportunity: All qualified persons will be considered for employment without regard to race, color, religion, sex, national origin, age, marital status, familial status, gender identity, sexual orientation, disability for which a reasonable accommodation can be made or any other status protected by law. Assistance will be gladly provided upon request for any applicant with sensory or non-sensory disabilities.
Fresh Consulting is a participating E-Verify company.
freshconsulting.com
Compensation offered will be determined by factors such as location, level, job-related knowledge, skills, and experience. Range $70/hr - $80/hr.
ylM5WIC1Wr",$75.00 /hr (est.),201 to 500 Employees,Company - Private,Management & Consulting,Business Consulting,2007,$25 to $100 million (USD)
"Stark Dev, LLC",#N/A,"Plano, TX",Data Engineer/Analyst / W2 /USC or GC (GC EAD) or H4 holders,"This position is open for United States Citizens or Green Card holders (GC EAD)/H4 EAD ONLY.
This is an on-site position in Plano/Dallas
CONTRACT W2
Top Skills Details
1) Experience working on a data migration project as a Data Analyst.
- This person will be working on their Permitting, Planning, and Inspection System Project. They are moving from a legacy permitting system, TRAKIT, to a completely new Salesforce application, Clariti. (TRAKIT and Clariti experience not required)
2) Experience doing data discovery, classification, verifying data, mapping rules
- On this project this team will be moving all of the historical data from the old application, TRAKIT, to the new application Clariti.
3) Proficient with SQL and writing SQL queries
\* Interpret data, analyze results using statistical techniques and provide ongoing reports
\* Develop and implement databases, data collection systems, data analytics and other strategies that optimize statistical efficiency and quality
\* Acquire data from primary or secondary data sources and maintain databases/data systems
\* Identify, analyze, and interpret trends or patterns in complex data sets
\* Filter and “clean” data by reviewing computer reports, printouts, and performance indicators to locate and correct code problems
\* Work with management to prioritize business and information needs
\* Locate and define new process improvement opportunities
Drug Test Required
false
Go To Work
false
Workplace Type
On-site
Experience Level
Expert Level
Job Type: Contract
Pay: $55.00 - $60.00 per hour
Schedule:
Monday to Friday
Application Question(s):
What is your visa status?
Work Location: In person",$57.50 /hr (est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Lightcast
4.4",4.4,"Moscow, ID",Senior Data Engineer,"A Senior Data Engineer will architect big data analytical frameworks, and translate complex functional and technical requirements into detailed architecture design and high-performing software. A Senior Data Engineer will select data solution software and define hardware requirements. The position will be responsible for developing and implementing standards and processes for integration projects and initiatives.

Major Responsibilities:
Architect software applications, and test and build automated tools.
Select data solution software and define hardware requirements
Develop standards and processes for integration projects and initiatives.
Lead the design/development of software applications, testing, and building tools
Ensure database changes are reviewed and approved to standards
Lead and communicate to leadership on solution design
Provide technical assistance to junior members and to colleagues across the company
Skills & Abilities:
Analytical mind, personality and aptitude for working with data
Ability to look at the numbers, trends, and data to derive conclusions based on findings
Work closely with management to prioritize business and information needs
Proven ability to work effectively to meet goals and deadlines with minimal supervision
Highly creative problem-solving skills and an ability to tailor efforts based on the importance of the issue being addressed.
A hands-on, detail-oriented mindset with the ability to look beyond the box
Expert knowledge of object-oriented design, data structures, and algorithms
Demonstrated experience with agile or other rapid application development methods, object-oriented design, coding, testing patterns with a variety of languages
Significant knowledge of data structures, algorithms, data modeling and disaster recovery of data systems
Education & Experience:
Requires 5 years of data/software engineering/science experience
Bachelor's degree in computer science or other technical disciplines (masters preferred)
About Lightcast:
Lightcast is a global leader in labor market insights with headquarters in Moscow (ID) and Boston (MA) and offices in the United Kingdom, Europe, and India. We work with partners across six continents to help drive economic prosperity and mobility by providing the insights needed to build and develop our people, our institutions and companies, and our communities.

Lightcast is proud to be an equal opportunity workplace and is committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. Lightcast has always been, and always will be, committed to diversity, equity and inclusion. We seek dynamic professionals from all backgrounds to join our teams, and we encourage our employees to bring their authentic, original, and best selves to work.

#LI-SS1","$118,643 /yr (est.)",1 to 50 Employees,Company - Private,Government & Public Administration,State & Regional Agencies,2011,Unknown / Non-Applicable
"Wallero
4.4",4.4,"Seattle, WA",Data Engineer,"Hi,
Pretty much interested in your resume, we have an urgent opening for Data Engineer role with one of our clients based in Seattle WA.
Position: Data Engineer
Location: Seattle WA - Remote (100%)
Description:
Good Experience in reporting space including PowerBI, ADF, and possibly standing up a data warehouse/data mart.
The main scope centering is around reporting.
Experience with SQL, and agile framework including requirement gathering and story/feature writing skills.
Need someone strong who is comfortable leading and guiding teams in that capacity.
Migrating data from third party to Azure Cloud
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Seattle, WA 98101: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",$75.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2015,$1 to $5 million (USD)
NAVA TECH LLC,#N/A,Remote,Azure Data Engineer,"Azure Data Engineer
Remote
Long Term
Note: Need 10+ years of experience
Sound skills and hands on experience with Azure Data Lake, Azure Data Factory, SQL Data Warehouse Azure Blob, Azure Storage Explorer
Proficient in creating Data Factory pipelines for on-cloud ETL processing; copy activity, custom Azure development etc.
Knowledge of Azure Data Catalog, Event Grid, Service Bus, SQL and Synapse
Experience using Azure Databricks platform.
Experience with Python programming.
Experience using pandas and numpy for data engineering and data cleansing;
Ability to troubleshoot the job scheduler and compute clusters.
Experience with databricks CLI and secrets module.
Experience with Azure Blob storage access configuration.
p l e a s e s h a r e r e s u m e s t o m o h a m m e d (a t) n a v a t e c h (d o t) u s O R 4 4 3 5 3 7 9 8 3 5
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$62.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
NLB Technology Services,#N/A,"Dallas, TX",CRM data Analytics Engineer,"MUST HAVE:
o Tableau CRMA & Einstein Discovery Certification
o Experience building Dashboards using CRMA (aka Tableau CRM and Einstein Analytics)
o Design, build, and support datasets, data recipes, dataflows, and dashboards using Salesforce and non-Salesforce data sources
o Troubleshoot functional, data, and performance issues with CRMA Dashboards
o Skilled in SAQL, JSON
o Develop and deploy AI/ML solutions using Einstein Discovery and deploy AI/ML Models built outside Salesforce
o Experience working with large data (1 B+ rows) and supporting large user base (10,000+)
o Mentor and coordinate the development efforts of other CRMA DevelopersAT&T
Job Type: Contract
Salary: $65.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Vision insurance
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
CRM ANalytics: 8 years (Required)
einstein: 1 year (Required)
Salesforce: 1 year (Required)
Work Location: One location",$70.00 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Serenity Healthcare
2.6",2.6,"Lehi, UT",Junior Data Engineer,"Junior Data Engineer
Serenity Healthcare is hiring a Junior Data Engineer for our Lehi, UT headquarters. (Remote availability for residents of Utah or Colorado) While previous ETL experience is preferred, we are open to exceptional entry-level talent for this role.
We intend to provide on the job training in data-skills: SQL, BI (PowerBI), ETL (SSIS), Warehousing (SQL Stored Procedures), Exploratory Data Analysis, etc. It’s our intention to train you in Microsoft’s new tool: PowerApps.
Desired skill sets:
Must be a quick learner
SSIS experience strongly preferred
Skills used in the role:
SQL 20%
SSIS 40%
PowerApps 40%
Day-to-day work description:
The Junior Data Engineer will be responsible for keeping the data flowing, building new data pipelines, and creating business applications using MS-PowerApps. You’ll need to be comfortable with SQL, SQL Server, and SSIS. You’ll be reading API documentation to establish new ETL flows, as well as automating report delivery.
Job Fit:
Capable of “Deep Work”
Problem Solver
Reliable and consistent
Attention to detail
What We Offer to You:
Competitive pay (DOE), including additional target compensation
Opportunity to work and grow your career in a fast-paced environment
Medical, Dental, Vision Insurance (90% coverage for you and codependents)
Life Insurance
Flexible spending account
Paid time off
Vision insurance
401k
Open and friendly, professional office environment
Who We Are:
We have helped thousands of patients take back their lives from mental illness with specialized clinical expertise and the foremost cutting-edge technology available in mental health today. Serenity’s approach to treating mental illnesses is to offer holistic options and treat the whole person by providing an atmosphere of positivity, support, and healing in an outpatient setting.
We believe people should live their best lives, and mental health is a substantial segment of total well-being. We bring the same passion we have for improving our patient’s lives to providing a work experience that will help you do your best work, enjoy the time you invest at work, and succeed in life outside of work. We take our people and culture seriously and make it a priority to invest in both.
Serenity Mental Health Centers is an equal opportunity employer. This position is contingent on successfully completing a criminal background check and drug screen upon hire.","$85,462 /yr (est.)",201 to 500 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2017,Unknown / Non-Applicable
"Infinity Quest
3.9",3.9,"Cleveland, OH",Data Center Engineer,"Required Skillsets:
1. Datacenter Management
2. Hardware Rack & Stack Cabling
3. Knowledge and operational of Diesel Generators, Cooling & Chillers
4. Inventory Asset management
5. DC Monitoring DC availability
6. Handling of CISCO UCS HW and VX RAIL, POWER FLEX
Activities:
1. Operate and Manage both routine and emergency service on a variety of state-of-the-art critical systems such as:
a. Medium voltage switchgear,
b. diesel generators,
c. UPS systems
d. Power distribution equipment, chillers, cooling towers, computer room air handlers,
e. Fire detection / suppression; building monitoring systems (BMS), building Automated Systems (BAS) ; etc
2. Supervise the on-site management of sub-contractors and vendors, ensuring that all work is performed according to established practices and procedures.
3. Manage local client relationship and act as the point of contact for the company at this site.
4. Establish performance benchmarks, conduct analyses and prepare reports on all aspects of the critical facility operations and maintenance.
5. Work with IT managers and other business leaders to coordinate projects, manage capacity and optimize plant safety, performance, reliability and efficiency.
6. Create, utilize and administer MOPs , SOPs, and Preventative Maintenance Procedures for all work on critical data center facility equipment.
7. Schedule work activities, within specified change control / management protocol.
8. Maintain a constant state of readiness in support of the mission goal of 99.999% uptime
9. Racking, Stacking of Infrastructure
a. Cabling Management
b. Patching, Network port swapping
c. Hardware reboots
d. Vendor coordination
e. Project Coordination
· Inventory asset management, Physical access management
Monitoring of Datacenter availability & update respective teams accordingly
Job Type: Contract
Salary: From $80.00 per year
Schedule:
8 hour shift
Ability to commute/relocate:
Cleveland, OH: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data center: 10 years (Preferred)
CISCO UCS: 10 years (Preferred)
MOPS: 10 years (Preferred)
SOP: 10 years (Preferred)
Cabling Management: 10 years (Preferred)
Network protocols: 10 years (Preferred)
Hardware reb: 10 years (Preferred)
VENDOR COORDINATION: 10 years (Preferred)
Project coordination: 10 years (Preferred)
Security clearance:
Confidential (Preferred)
Speak with the employer
+91 8838059965",#N/A,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,Unknown / Non-Applicable
SecurePro,#N/A,"Arlington, VA",Data Engineer,"Collect, manage, and convert raw data accurately and reliably
Organize data systems for subgroup access and analyses
Configure and sustain data cloud structures
Able to work with structured and unstructured data.
Validate outputs of data pipelines
Convert existing SAS code to python/pyspark code for model operation in the cloud
Create and sustain policy analysis models in the cloud
Troubleshoot user interfaces in the cloud
Create and sustain intuitive user interfaces in the cloud
Degree in Data Engineering preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $100.00 per hour
Experience level:
3 years
Schedule:
8 hour shift
Application Question(s):
Are you a US citizen?
Must have DOD secret Clearance?
Work Location: Remote",$90.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Augeo Affinity Marketing, Inc.
3.7",3.7,Minnesota,Data Analytics Engineer,"Hello, we're Augeo-architects of enterprise engagement and loyalty platforms, delivering compelling experiences and fostering meaningful connections with employees, customers, members and channel partners across industries. We serve thousands of clients, including more than 70 of the top Fortune 500 companies, representing millions of end users. We are thought leaders and disruptors who think differently and creatively, built by entrepreneurs, operators and innovators.
At Augeo, we're passionate about providing an inclusive workplace that values diversity. Everyone is welcome, and our employees are comfortable bringing their authentic whole selves to work. Be you.
We work hard, we play hard-and most importantly, we care to our CORE about our teams and each other. We over-communicate around everything, especially while we are all connected virtually.
Summary/Objective
A data analytics engineer is responsible for creating and implementing personalized user recommendations. This involves using data and analytics to understand user behavior and preferences, and developing algorithms and models to recommend personalized content, products, and services to each user.
The main responsibilities may include:
Collaborating with designers, product managers, and other stakeholders to understand user needs and develop personalized solutions.
Designing and implementing personalization algorithms and models using machine learning, data mining, and other relevant techniques.
Analyzing user data and behavior to identify patterns and develop insights that can be used to improve the personalized user experience.
Keeping up to date with the latest trends and technologies in personalization and machine learning and applying these insights to enhance the personalization capabilities of the platform.
The ideal candidate for this role should have a strong background in computer science, software engineering, data science, or a related field. They should have experience with the AWS cloud platform, preferably the AWS Personalize service, and be familiar with machine learning and data mining techniques. Additionally, strong analytical and problem-solving skills are essential, as well as excellent communication and collaboration abilities to work in a team.
Requirements
Coding proficiency in at least one modern programming language (Python, R, Java, etc)
Relational database experience in SQL, AWS RDS
Experience with ML Services in AWS (Personalize, SageMaker) or equivalent a plus.
Strong critical thinking, communication, and problem solving skills and a quick learner.
Experience with cloud-based platforms (particularly AWS)
Experience working in multi-developer environment, using version control (i.e. Git)
Write Extract-Transform-Load (ETL) jobs to calculate business metrics
Own the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions
Monitor and troubleshoot operational or data issues in the data pipelines
Strong verbal and written communication skills.
Ability to operate independently and in teams.
Experience working within an Agile environment and JIRA preferred
Education and Experience
Bachelor’s Degree in Computer Science or related area
1-3 years of relevant work experience in analytics, data engineering, business intelligence or related field.
Experience using SQL queries, experience in writing and optimizing SQL queries in a business environment with large-scale, complex datasets
Knowledge of data warehouse technical architecture, infrastructure components, ETL and reporting/analytic tools and environment
Benefits of joining our team
Medical, Dental & Vision Insurance
Employer-sponsored Long-term disability and Life Insurance
Paid Time Off and flexible work schedule
401(k) Plan
Fun and casual work environment
Career growth opportunities
Rewards & Incentives",#N/A,201 to 500 Employees,Company - Private,Media & Communication,Advertising & Public Relations,1999,$100 to $500 million (USD)
Cloudburst Technologies,#N/A,Remote,Data Collection Engineer,"About Us
Cloudburst is a seed-funded New York-based remote-first company helping customers with detection, prevention, and investigation into cryptocurrency market manipulation and fraud.
Cloudburst provides regulators, financial institutions, trading platforms, and others with access to a real-time, machine-readable crypto market monitoring tool, enabling advanced levels of diligence and customer/market protection. The Cloudburst team has multiple years of experience working together, plus it has worked directly with international law enforcement agencies and policymakers on cybercrime, terrorism, and high-level financial fraud.
What We’re Looking For
The ideal candidate
is an empathetic teammate who loves to help others, to bring order to chaos, and to document their path for others to follow
focuses primarily on web scraping, API integration, and collection of raw data
is capable of becoming fluent in Layer 7 protocols, blockchains, and emerging protocols
has high moral and ethical standards
is proficient in production-level Python
has worked in a threat intel or security environment
What You Will Do
Your primary responsibility will be to become expert at collecting open source data from the web and from other online protocols.
You will be comfortable building tools to bring in data from random files, varying degrees of quality of web sites, and new communications tools. You will have the opportunity to creatively apply your knowledge of software automation towards scaling data collection.
You will understand popular chat sites and how fraud actors use them. You will understand emerging technologies and will actively evaluate them for potential opportunities to collect data.
Why Cloudburst?
At Cloudburst we want to minimize our software engineers working on pixel-pushing and to maximize time spent understanding our domain and building products to help our customers investigate and prevent fraud.
We care about developing and sponsoring our engineers internally. The department adheres to these principles:
Rapid deployment of innovative new techniques for signal generation and attribution
KISS architecture that doesn’t get in developers’ or R&D’s way; we want code to be easy to understand, less abstract, easy to test, fast to deploy, and reducible to automation or simple manual processes
Building a diverse, unconventional team that cross-trains and grows together without ego or sacrifice to work-life balance
What You Will Be Using
Python
Cloud-based infrastructure
3rd party vendor integrations
Novel techniques for data collection using home-grown or discovered tools and frameworks
Hiring Process
Initial email
Initial phone screen with the hiring manager
Take-home test OR link to previous code
Panel interview
Call with CEO
Offer
Investors
Strategic Cyber Ventures
Coinbase Ventures
Bloccelerate
More Info:
https://www.crunchbase.com/organization/cloudburst-technologies-0a3e
https://www.prnewswire.com/news-releases/cloudburst-technologies-raises-3m-in-seed-funding-led-by-strategic-cyber-ventures-joined-by-coinbase-ventures-and-bloccelerate-301817742.html
https://burst.cloud/
https://open.spotify.com/episode/2cv1Is77s8jGLtHPPYRLPU
Job Type: Full-time
Pay: $115,000.00 - $150,000.00 per year
Benefits:
Dental insurance
Flexible schedule
Health insurance
Paid time off
Vision insurance
Compensation package:
Stock options
Experience level:
3 years
Schedule:
Choose your own hours
No nights
No weekends
Work Location: Remote","$132,500 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Dun & Bradstreet
3.9",3.9,Remote,Data Engineer I (R-14521),"Why We Work at Dun & Bradstreet
Dun & Bradstreet unlocks the power of data through analytics, creating a better tomorrow. Each day, we are finding new ways to strengthen our award-winning culture and accelerate creativity, innovation and growth. Our 6,000+ global team members are passionate about what we do. We are dedicated to helping clients turn uncertainty into confidence, risk into opportunity and potential into prosperity. Bold and diverse thinkers are always welcome. Come join us!

The Data Engineer I will work with the senior data engineers to build and maintain applications that are responsible for the ingestion of data into the Dun and Bradstreet Contact Pipeline.

The Data Engineer I will work on analyzing performance/throughput blockers of the applications and will work with other team members to remove the bottlenecks in the applications.

The Data Engineer I will also work with creating metrics from the ingestion applications to help determine the areas that need work in terms of performance and/or throughput.
Responsibilities:
Create and maintain applications in python.
Take ownership of existing applications for further development/improvements
Work closely with related groups to ensure business continuity
Perform analysis on code bases to increase performance.
Work as part of the team to code review and test other members’ code changes.
Work as a member of one or more agile teams, using lean principles and SCRUM methodology

Requirements:
Bachelor’s degree (preferable in computer science, mathematics, data science, or a related field)
Experience with Python for application development (2-5 years)
Experience with SQL for data analysis and querying (2-5 years)
Ability to work independently to deliver critical projects on time
Ability to work closely with others to problem solve
Experience with hosted environments, AWS, Azure, or other cloud service providers preferred
Benefits We Offer
Generous paid time off in your first year, increasing with tenure.
Up to 16 weeks 100% paid parental leave after one year of employment.
Paid sick time to care for yourself or family members.
Education assistance and extensive training resources.
Do Good Program: Paid volunteer days & donation matching.
Competitive 401k & Employee Stock Purchase Plan with company matching.
Health & wellness benefits, including discounted Gympass membership rates.
Medical, dental & vision insurance for you, spouse/partner & dependents.
Learn more about our benefits: http://bit.ly/41Yyc3d.

Pay Transparency
Dun & Bradstreet is an equal employment opportunity employer and believes in honesty and transparency in the employment hiring process, including pay transparency. Accordingly, listed on this posting is a good faith reasonable estimate of the salary range and other compensation in the job posting, as of the date of this posting. Actual compensation decisions for base salary and other compensation will be dependent upon a wide range of factors including but not limited to: an individual’s skill sets, experience, qualification, training, education, location, and any other legally permissible factors. Successful applicants will also be eligible for D&B’s generous benefit package, outlined above.

All Dun & Bradstreet job postings can be found at https://www.dnb.com/about-us/careers-and-people/joblistings.html. Official communication from Dun & Bradstreet will come from an email address ending in @dnb.com.

Equal Employment Opportunity (EEO): Dun & Bradstreet is an Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, creed, sex, age, national origin, citizenship status, disability status, sexual orientation, gender identity or expression, pregnancy, genetic information, protected military and veteran status, ancestry, marital status, medical condition (cancer and genetic characteristics) or any other characteristic protected by law. View the EEO is the Law poster here and its supplement here. View the pay transparency policy here.

Global Recruitment Privacy Notice",#N/A,5001 to 10000 Employees,Company - Public,Information Technology,Information Technology Support Services,1841,$1 to $5 million (USD)
"BigLynx Computer Software
4.9",4.9,"Redmond, WA",Databricks Data Engineer,"BigLynx, Inc is an American multinational technology corporation headquartered in Seattle, Washington, with operations in the United States, Canada, and India. The company began in 2016, as a product development company specializing in AI/ML Data Engineering in the Retail vertical space with its products warehouse & fast. Post Pandemic in 2022, BigLynx added a business division of boutique technology consulting, specializing inData Engineering, Full Stack , and Microsoft Dynamics helping clients build the next generation data platform and big data pipelines.
Data pipeline development: Design, develop, and maintain scalable and efficient data pipelines using Databricks to ingest, transform, and load data from various sources. This includes data extraction, data cleansing, data transformation, and data loading processes.
Data modeling and schema design: Design and implement data models, database schemas, and data structures on Databricks. Optimize data models for performance, scalability, and ease of use.
ETL processes: Develop and maintain ETL (Extract, Transform, Load) processes using Databricks to transform and cleanse data. Implement efficient data integration and transformation logic using languages such as Python, SQL, or Scala.
Data integration: Integrate data from multiple systems and sources, ensuring data consistency, accuracy, and quality. Develop and maintain data connectors, APIs, and data ingestion processes.
Performance optimization: Identify and address performance bottlenecks in data pipelines and data models. Optimize query performance, data loading, and data processing capabilities on Databricks.
Data governance and security: Implement data governance practices, data privacy measures, and security controls on Databricks. Ensure compliance with data governance policies and regulations.
Monitoring and troubleshooting: Monitor the health and performance of Databricks data infrastructure, data pipelines, and data processing jobs. Troubleshoot issues and provide timely resolutions.
Collaboration and teamwork: Collaborate with cross-functional teams, including data scientists, data analysts, and business stakeholders, to understand data requirements, provide data engineering expertise, and support their data-related needs.
Qualifications:
Databricks expertise: Strong knowledge and hands-on experience with the Databricks platform, including Databricks notebooks, Databricks runtime, and Databricks clusters.
Data engineering skills: Proficiency in data engineering principles, ETL processes, data modeling, and data integration techniques. Experience with programming languages such as Python, SQL, or Scala.
Big data technologies: Experience with big data technologies, such as Apache Spark, Apache Hadoop, or related frameworks. Familiarity with distributed computing and data processing concepts.
Cloud platforms: Experience working with cloud platforms, preferably Azure Databricks, AWS Databricks, or Google Cloud Databricks. Knowledge of cloud storage, compute, and networking services.
Database and data warehouse concepts: Understanding of relational databases, data warehousing concepts, and SQL. Familiarity with data warehousing best practices and dimensional modeling.
Performance optimization: Strong skills in optimizing Spark jobs and queries on Databricks. Ability to identify and resolve performance bottlenecks.
Problem-solving skills: Strong analytical and problem-solving abilities to tackle complex data engineering challenges and troubleshoot issues.
Collaboration and communication: Excellent collaboration and communication skills to work effectively with cross-functional teams and stakeholders, translating business requirements into technical solutions and providing technical guidance.
Education: A bachelor's or master's degree in computer science, data engineering, or a related field is typically required. Relevant certifications, such as Databricks Certified Developer or similar, are h
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Experience level:
7 years
Schedule:
Monday to Friday
Work Location: In person",$75.00 /hr (est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Maven Workforce
4.1",4.1,"Alpharetta, GA",Data Engineer,"Must Have skills:
Building Data Pipeline exp is a must
ETL Tools – SSIS, Alteryx
Data modelling
SQL Server development
Microsoft SQL Stack
Responsibilities:
Data acquisition and ingestion - Identify data sources and build pipelines using various ETL tools such as but not limited to, SSIS, and Alteryx. Data sources including but not limited to SQL Server, SAP, Teradata, Hadoop\Hive, PostgreSQL, Oracle and flat files.
Identifying ways to improve data reliability, efficiency and quality by various data solution techniques.
Expertise in Data project management with JIRA stories. Work as a liaison between business and IT to ensure successful and timely completion of the projects.
Assist in defining the data architecture framework, standards and principles, including modeling, metadata, security and reference data.
Create and optimize data models to support various business applications.
Reviewing modifications of existing data systems for cross-compatibility.
Automate and support workflows to ensure timely delivery.
Must Have:
5+ years of SQL Server development experience.
3+ years ALTERYX Admin /User management experience and advance workflow management.
5+ years data modeling experience.
5+ years of ETL experience.
5+ years of experience in working on more than one database technologies Microsoft SQL server, Teradata.
2+ years big data experience, Hadoop, Hive, Spark
Expert knowledge of data warehousing.
DESIRED SKILLS:
BI Lifecycle management
Working understanding of Microsoft VBA, HTML, Python
Data model development using ERWIN
Job Type: Contract
Salary: $50.00 per hour
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Building Data Pipeline: 8 years (Required)
ETL Tools – SSIS, Alteryx: 7 years (Required)
Data modelling: 6 years (Required)
SQL Server development: 8 years (Required)
Microsoft SQL Stack: 8 years (Required)
Work Location: One location
Speak with the employer
+91 7328456015",$50.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2008,Unknown / Non-Applicable
"VedaInfo Inc
4.1",4.1,"Phoenix, AZ",Big Data Engineer,"Hi,
I hope this note finds you well
I am reaching out to you regarding your career opportunity with us for “Big Data Engineer” for a “6+ Months Contract” @Phoenix, AZ
Please find the below requirement details.
Job Title: Big Data Engineer
Location: Phoenix, AZ (Onsite)
Duration: 6+ months
Rate: $60/hr C2C
Responsibilities:
Design, implement, and maintain big data systems handling large volumes of data.
Utilize Hadoop, Hive, and Spark for efficient data processing and analysis.
Collaborate with cross-functional teams to understand data requirements.
Develop and optimize data ingestion, storage, and transformation processes.
Build scalable data pipelines for seamless data processing and analysis.
Monitor and troubleshoot data processing and performance issues.
Stay updated with emerging big data technologies.
Requirements:
Bachelor's/Master's degree in CS, Engineering, or related field
7+ years of experience as a Big Data Engineer
Strong proficiency in Hadoop, Hive, and Spark
Extensive experience with AWS or Azure
Solid understanding of data ingestion, storage, and transformation
Excellent problem-solving and communication skills
Proactive and self-motivated
This is a 6+ month onsite contract position in Phoenix, AZ, with the possibility of extension. Compensation is $60/hr on C2C basis.
Please submit your updated resume highlighting relevant experience.
Thank you for your interest!
Thanks & Regards
Mohammed ZAIN
Job Type: Contract
Pay: $50.00 - $60.00 per hour
Experience level:
7 years
Ability to commute/relocate:
Phoenix, AZ: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
To improve your chances of getting Shortlisted, make sure to tailor your Profile to the Job Requirements, Highlight your Relevant Skills and Experience, and then upload your Resume.
Experience:
AWS (Amazon Web Services): 7 years (Required)
Azure: 7 years (Required)
Big data: 7 years (Required)
Hadoop: 7 years (Required)
Apache Hive: 7 years (Required)
Spark: 7 years (Required)
Work Location: One location",$55.00 /hr (est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$1 to $5 million (USD)
"MARVEL TECHNOLOGIES INC
3.7",3.7,"Cincinnati, OH",Data Engineer,"Job Description
The data engineer designs and builds platforms tools and solutions that help the bank manage secure and generate value from its data. The person in this role creates scalable and reusable solutions for gathering collecting storing processing and serving data on both small and very large i.e. Big Data scales. These solutions can include on-premise and cloud-based data platforms and solutions in any of the following domains ETL business intelligence analytics persistence relational NoSQL data lakes search messaging data warehousing stream processing and machine learning.
Responsible and accountable for risk by openly exchanging ideas and opinions elevating concerns and personally following policies and procedures as defined. Accountable for always doing the right thing for customers and colleagues and ensures that actions and behaviors drive a positive customer experience. While operating within the Bank's risk appetite achieves results by consistently identifying assessing managing monitoring and reporting risks of all types.
ESSENTIAL DUTIES AND RESPONSIBILITIES:
Responsible for design Development and Support of data solutions APIs tools and processes to enable rapid delivery of business capabilities.
Work closely with IT application teams Enterprise architecture infrastructure information security and LOB stakeholders to translate business and technical strategies into data-driven solutions for the Bank.
Act as a technical Expert addressing problems related to system and application design performance integration security etc.
Conduct research and Development based on current trends and technologies related to the banking industry data engineering and architecture data security and related topics.
Work with developers to Build CI/CD pipelines Self-service Build tools and automated deployment processes.
Evaluate software products and Provide documented recommendations as needed.
Provide Support and troubleshooting for data platforms. Must be willing to Provide escalated on-call Support for complicated and/or critical incidents.
Participate in the planning process for hardware and software.
Plan and work on internal projects as needed including legacy system replacement Monitoring and analytics improvements tool Development and technical documentation.
Provide technical guidance and mentoring for other team members.
Manage and prioritize multiple assignments.
MINIMUM KNOWLEDGE SKILLS AND ABILITIES REQUIRED:
Bachelor's degree in Computer Science/Information Systems or equivalent combination of education and experience.
Must be able to communicate ideas both verbally and in writing to management business and IT sponsors and technical resources in language that is appropriate for each group.
Fundamental understanding of distributed computing principles
Knowledge of application and data security concepts best practices and common vulnerabilities.
Conceptual understanding of one or more of the following disciplines preferred big data technologies and distributions metadata management products commercial ETL tools Bi and reporting tools messaging systems data warehousing Java language and run time environment major version control systems continuous integration/delivery tools infrastructure automation and virtualization tools major cloud or rest API design and development.
Job Types: Full-time, Contract
Pay: $60.00 - $62.00 per hour
Compensation package:
Hourly pay
Yearly pay
Experience level:
9 years
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",$61.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,#N/A,$5 to $25 million (USD)
"Vedainfo
4.1",4.1,"Peoria, IL",Data Engineer,"Job Description
· Create and develop optimal data pipeline which fetches data from raw and apply business transformation and load derived.
· Build the AWS Fargate components required for optimal extraction, transformation, and loading of data-to-data lake.
· Develop data pipeline setup between base and derive values based on Caterpillar specification.
· Implement quality validation rules and follow process defined by stakeholder.
· Develop business events as defined by the solution team to generate CDC events for consumption layer (Kinesis)
· Create re-usable and configurable components wherever applicable.
· Create and execute unit, integration, and regression tests
· Ensure the code is developed satisfying all business rules, perform unit testing where applicable.
· Data pipelines development
· Python Development
· AWS cloud services
· Data quality validation rules
· Java (preferred)
· Implement CDC
· Transformation rules creation
· Message broker tools like Kinesis
· Azure DevOps – CI/CD pipelines
· Automated and Manual Testing experience
Job Type: Contract
Pay: Up to $60.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Peoria, IL 61602: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
This position is remote initially and you will be asked to work any of the client location once offer is made. Please confirm your current location.
Experience:
Data Engineer: 1 year (Preferred)
Python Development: 1 year (Preferred)
AWS: 1 year (Preferred)
Message broker tools: 1 year (Preferred)
Work Location: In person",$60.00 /hr (est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$1 to $5 million (USD)
META FORCE IT LLC,#N/A,"Cincinnati, OH",Data Engineer,"Job Description
The data engineer designs and builds platforms tools and solutions that help the bank manage secure and generate value from its data. The person in this role creates scalable and reusable solutions for gathering collecting storing processing and serving data on both small and very large i.e. Big Data scales. These solutions can include on-premise and cloud-based data platforms and solutions in any of the following domains ETL business intelligence analytics persistence relational NoSQL data lakes search messaging data warehousing stream processing and machine learning.
Responsible and accountable for risk by openly exchanging ideas and opinions elevating concerns and personally following policies and procedures as defined. Accountable for always doing the right thing for customers and colleagues and ensures that actions and behaviors drive a positive customer experience. While operating within the Bank's risk appetite achieves results by consistently identifying assessing managing monitoring and reporting risks of all types.
ESSENTIAL DUTIES AND RESPONSIBILITIES:
Responsible for design Development and Support of data solutions APIs tools and processes to enable rapid delivery of business capabilities.
Work closely with IT application teams Enterprise architecture infrastructure information security and LOB stakeholders to translate business and technical strategies into data-driven solutions for the Bank.
Act as a technical Expert addressing problems related to system and application design performance integration security etc.
Conduct research and Development based on current trends and technologies related to the banking industry data engineering and architecture data security and related topics.
Work with developers to Build CI/CD pipelines Self-service Build tools and automated deployment processes.
Evaluate software products and Provide documented recommendations as needed.
Provide Support and troubleshooting for data platforms. Must be willing to Provide escalated on-call Support for complicated and/or critical incidents.
Participate in the planning process for hardware and software.
Plan and work on internal projects as needed including legacy system replacement Monitoring and analytics improvements tool Development and technical documentation.
Provide technical guidance and mentoring for other team members.
Manage and prioritize multiple assignments.
MINIMUM KNOWLEDGE SKILLS AND ABILITIES REQUIRED:
Bachelor's degree in Computer Science/Information Systems or equivalent combination of education and experience.
Must be able to communicate ideas both verbally and in writing to management business and IT sponsors and technical resources in language that is appropriate for each group.
Fundamental understanding of distributed computing principles
Knowledge of application and data security concepts best practices and common vulnerabilities.
Conceptual understanding of one or more of the following disciplines preferred big data technologies and distributions metadata management products commercial ETL tools Bi and reporting tools messaging systems data warehousing Java language and run time environment major version control systems continuous integration/delivery tools infrastructure automation and virtualization tools major cloud or rest API design and development.
Job Type: Contract
Pay: $45.96 - $50.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Cincinnati, OH 45215: Reliably commute or planning to relocate before starting work (Required)
Experience:
data engineer: 10 years (Preferred)
Big data: 3 years (Preferred)
Work Location: Hybrid remote in Cincinnati, OH 45215",$47.98 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Myticas Consulting
3.9",3.9,Arizona,BHJOB15656_20058 - Data Engineer,"Myticas's direct client based out of Phoenix, AZ is currently seeking a Data Engineer for a 100% remote contract position.
Pay Rate: Up to $34/hr

Job Description:
A Data Engineer can confidently understand, write, troubleshoot, optimize, and peer review SQL code. They understand business requirements and create code and reporting that exceeds specifications through anticipating underlying needs. They also understand relational database design, table structure, data types, and data models. They are experienced with and confident in development in reporting visualizations in Excel, PowerBI, Tableau, QuickSight, SSRS, or other data products. They can create reporting dashboards and products that inform the business. They easily grasp and retain complex data concepts and are able to explain them to others. Looking for engineers who are driven to innovate, use cutting edge technologies to solve problems and think outside the box.
What You’ll Do:
Digital Transformation into AWS
Data Warehouse related work (SQL, ETL, Replication, Databases)
Understand, write, troubleshoot, optimize, and peer review SQL code.
Understand business requirements and create code and reporting that exceeds specifications through anticipating underlying needs.
Understand relational database design, table structure, data types, and data models. 6.Experienced with and confidence in development in reporting visualizations in Excel, PowerBI, Tableau, QuickSight, SSRS, or other data products.
Data Warehouse related work (SQL, ETL, Replication, Databases
Perform other duties as assigned or apparent.
MINIMUM EDUCATION AND RELATED WORK EXPERIENCE:
Bachelor's degree in a Computer Science field and 2 years' work experience
2 years of experience in most phases of IT systems deployments in one or more of the following areas: design and deployment of cloud services, data migration to AWS cloud, Digital Transformation into AWS, and Data Warehouse related work
INDCHI
INDREM",$34.00 /hr (est.),1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
Sconcept,#N/A,Remote,Azure Data Engineer,"** W2 LONG TERM CONTRACT BASED....**(NO C2C)
JOB TYPE:- Long Term Contract(6 months)Extendable
JOB LOCATION:-USA
About Sconcept LLC:-Software Concepts Consulting was established in 2008. With our “hands-on” approach we are very successful in delivering high performance, secure IT solutions and IT services. With headquarters in Carrollton, Texas we are specialized in providing business solutions to the Fortune 500 clients. Our clients include Software, Banking, Financial, Healthcare, Retail, Consumer goods and Insurance sectors. We deliver candidates based on client requirements.
Required Skills:-
Must have worked on several (5+) projects building solutions for Data Pipelines, Data warehousing, Data Modelling on Azure Data Platform (AWS, Google Cloud Platform - good to have)
Expertise in design and development of Data pipelines and ETL using ADF, Databricks to move data from relational/structured/unstructured data from source to data lake to data warehouse, on Azure
Creating Pipelines and integrations of multiple data sources
Data processing and transformation using Databricks (Scala/Python)
Expertise in optimizing cost and performance of data pipelines
Experience working with Data Lake, Synapse on Azure
Designing Data Lake storage layer (Landing/staging, raw, trusted/curated zones)
Designing data storage DB schema, data models and processing on Azure Synapse DW
Experience with Synapse Spark Pool
Experience in Data Modelling
Building Data Models from structured and semi-structured data (Data Model, Common Data Model etc.)
Performance optimization of data model for high performance analytical and reporting workloads
Expertise with SQL Server and extensive experience with SQL programming
Programming and optimizing DB objects Views, Stored Procs, Functions
Good experience in defining data access policy and security for azure data services
Experience with Oracle EDW
Knowledge/Experience migrating from Oracle EDW to any cloud DW
Experience integrating AI skills/ML models with Data and reporting solutions
Experience in building API layer for downstream consumption
Job Type: Contract
Pay: $45.00 - $50.00 per hour
Benefits:
Health insurance
Compensation package:
Bonus pay
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$47.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"ProGrad
4.2",4.2,Remote,Data Engineer,"We are looking for data engineers that are excited about building and maintaining data pipelines for a variety of interests. The pipelines will ingest data from a variety of sources ranging from internal change data streams to external integrations with various SaaS platforms through webhooks and events. The ideal candidate is self driven and looking to grow as a data engineer while having an outsized impact on the future of data at the company. A love for database architecture and optimisation is a must as is the ability to work across diverse teams.
Requirements/Qualifications:
Bachelor's or Master's degree in Data Science (specialization)
Software development experience coding in a general-purpose programming language such as C, C++, Java, JavaScript, Golang, or Python
Experience working with data structures or algorithms (i. e., data structures /algorithms class, coursework/projects, research, internships, or other practical experience in/outside of school or work (including open source hobby coding)
Proficiency in SQL, and NoSQL Databases
Experience working with large datasets
Experience writing and debugging complex SQL queries
A strong understanding of cloud database best practices and performance optimisation
Experience building and maintaining data pipelines, coupled with knowledge of good programming practices such as unit and integration testing
. A strong understanding of cloud database best practices and performance optimisation
Good to have skills:
. Experience building and maintaining data pipelines, coupled with knowledge of good programming practices such as unit and integration testing
Job Types: Full-time, Permanent
Salary: Up to $1,200,000.00 per year
Experience level:
1 year
Under 1 year
Schedule:
Day shift
Work Location: Remote
Speak with the employer
+91 9057202434",#N/A,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Enterprise Minds,#N/A,Remote,Data Engineer,"Job Role: SQL Data Engineer
Location: Remote Opportunity - Need to work in PST hours
Visa: US Citizen Only
Duration: Long term
Roles and Responsibilities:
3+ years of experience with a complex build system in AWS using Starburst.
5+ years of experience with installation, maintenance, and administration of Oracle databases- PL/SQL and ANSI SQL
Oracle DBA + Oracle SQL to ANSI SQL conversion, performance/query tuning
3+ years of experience configuring, integrating, and securing multiple AWS database offerings (ex. RDS, Aurora, Casandra, and Dynamo)
2+ years of experience with supporting Linux systems engineering efforts in system design and evaluation, solution engineering, software development, or system administration.
Experience with Database Schema as Code tools
Ability to design and develop complex SQL queries and reports to retrieve required data.
Experience with database partitioning and microservices
Knowledge of Agile methodologies or the software development life cycle (SDLC)
US Citizenship is Mandatory.
Job Type: Full-time
Experience:
Data Engineer: 10 years (Preferred)
AWS using Starburst: 4 years (Preferred)
PL/SQL and ANSI SQL: 2 years (Preferred)
AWS RDS: 1 year (Preferred)
Work Location: Remote",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Stefanini, Inc
3.8",3.8,"Dearborn, MI",Data Engineer,"Stefanini Group is hiring!
Stefanini is looking for Data Engineer at Dearborn, MI (Hybrid)
For quick apply, please reach out to Rajat Baloria
Phone: (248) 728-2620
Email: Rajatsingh.Baloria@stefanini.com

Open to W2 candidates only!

Position Description:
Looking for a Software Engineer focused on delivering software leveraging Java and Python based on proven Lean/Agile methods. Knowledge of Big Data technologies like Hadoop and Spark is a plus.
The Software Engineer will work in a small, cross-functional, and co-located team. The Software Engineer will collaborate directly and continuously with business partners, product managers and designers, and will release early and often.
Position Responsibilities:
Work hands-on with the team and other stakeholders to deliver quality software products that meet our customer's requirements and needs.
Grow technical capabilities / expertise and provide guidance to other members on the team



Skills Required:
Exceptional software engineering knowledge; OO Design Principles
Basic understanding of Big Data and potential use cases
Strong desire to learn new skills and apply to solve business problems/opportunities “Spring Boot, Java, Angular, API, Micro Services, PCF, GCP (especially Cloud Run)”

Experience Required:
Overall 6 years of work experience in delivering customer facing products
Minimum 2 years of strong development experience in at least one of the following technologies: Java or Python on a Hadoop/Spark Platform

Education Required:
An associate's degree in Computer Science or similar technical discipline
Education Preferred:
Google Cloud Certification

Listed Salary Range may vary based on experience, qualifications, and local market, Also some positions may Include bonuses and other Incentives

About Stefanini Group
The Stefanini Group is a global provider of offshore, onshore and near shore outsourcing, IT digital consulting, systems integration, application and strategic staffing services to Fortune 1000 enterprises around the world. Our presence is in countries like Americas, Europe, Africa and Asia, and more than 400 clients across a broad spectrum of markets, including financial services, manufacturing, telecommunications, chemical services, technology, public sector, and utilities. Stefanini is a CMM level 5, IT consulting, company with global presence. We are CMM Level 5 company.",$82.00 /hr (est.),10000+ Employees,Subsidiary or Business Segment,Information Technology,Information Technology Support Services,1987,$1 to $5 billion (USD)
"Maven Workforce
4.1",4.1,"Alpharetta, GA",Data Engineer,"Must Have skills:
Building Data Pipeline exp is a must
ETL Tools – SSIS, Alteryx
Data modelling
SQL Server development
Microsoft SQL Stack
Responsibilities:
Data acquisition and ingestion - Identify data sources and build pipelines using various ETL tools such as but not limited to, SSIS, and Alteryx. Data sources including but not limited to SQL Server, SAP, Teradata, Hadoop\Hive, PostgreSQL, Oracle and flat files.
Identifying ways to improve data reliability, efficiency and quality by various data solution techniques.
Expertise in Data project management with JIRA stories. Work as a liaison between business and IT to ensure successful and timely completion of the projects.
Assist in defining the data architecture framework, standards and principles, including modeling, metadata, security and reference data.
Create and optimize data models to support various business applications.
Reviewing modifications of existing data systems for cross-compatibility.
Automate and support workflows to ensure timely delivery.
Must Have:
5+ years of SQL Server development experience.
3+ years ALTERYX Admin /User management experience and advance workflow management.
5+ years data modeling experience.
5+ years of ETL experience.
5+ years of experience in working on more than one database technologies Microsoft SQL server, Teradata.
2+ years big data experience, Hadoop, Hive, Spark
Expert knowledge of data warehousing.
DESIRED SKILLS:
BI Lifecycle management
Working understanding of Microsoft VBA, HTML, Python
Data model development using ERWIN
Job Type: Contract
Salary: $50.00 per hour
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Building Data Pipeline: 8 years (Required)
ETL Tools – SSIS, Alteryx: 7 years (Required)
Data modelling: 6 years (Required)
SQL Server development: 8 years (Required)
Microsoft SQL Stack: 8 years (Required)
Work Location: One location
Speak with the employer
+91 7328456015",$50.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2008,Unknown / Non-Applicable
Zllius Inc.,#N/A,"Canton, MI",Data Engineer AUG76,"Hi,
Greetings from Zllius Inc!
We have an urgent opening for the roleData Engineer. If interested, revert me back with your Updated resume, along with contact details, work authorization.
Job Role: Data Engineer
Location: Hartford, CT ( Initially Remote )
Duration: Long-term
Position: W2/1099
Visa: Any (Except H1B)
Job Ref: AUG76
Job Description:
Good experience on designing and developing data pipelines for data ingestion and transformation using Spark.
Distributed computing experience using Pyspark.
Good understanding of spark framework and spark architecture.
Experience working in Cloud based big data infrastructure.
Excellent in trouble shooting the performance and data skew issues.
Must have good understanding of spark run time metrics and tune applications based on metrics.
Deep knowledge in partitioning, bucketing concepts of data ingestion.
Good understanding of AWS services like Glue, Athena, S3, Lambda, Cloud formation.
Preferred working knowledge on the implementation of datalake ETL using AWS glue, Databricks etc.
Experience with data modelling techniques for cloud data stores and on prem databases like Teradata, Teradata Vantage (TDV) etc.
Preferred working experience in ETL development in Teradata vantage and data migration from on prem to Teradata vantage.
Proficiency in SQL, relational and non-relational databases, query optimization and data modelling.
Experience with source code control systems like Gitlab.
Experience with large scale distributed relational and NoSQL database systems.
Experience : 9+years
Thanks & Regards:
Zllius Inc.
844 495 5487
Job Types: Full-time, Contract
Schedule:
8 hour shift
Work Location: In person","$86,440 /yr (est.)",Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Fresh Consulting
3.9",3.9,"Menlo Park, CA",Data Engineer,"Fresh Consulting is a design-led, software development and hardware engineering company, offering end-to-end digital services to help companies innovate. We bring together amazing UX designers, sophisticated developers, digital strategists, and top-notch engineers to help companies create fresh experiences that connect humans, systems, and machines. We’ve been growing fast and need someone to help us continue to manage the delivery of high-quality work in a fast-paced environment.
See more at freshconsulting.com Visit freshconsulting.com/portfolio to see our project work across several industries.
View and apply to all jobs - https://freshconsulting.applytojob.com/apply/ or visit freshconsulting.com/careers
Title: Data Engineer
Duration: 6 months with possible extension
Location: Onsite Menlo Park, CA
Benefits: Employee benefits at 100% including Medical, PTO, Holiday Pay, 401K Plan, and much more!
Hours: Minimum 40 Hours/Week
Role:
Design, construct, install, test, and maintain highly scalable data management systems.
Ensure systems meet business requirements and industry practices.
Design, implement, automate, and maintain large-scale enterprise data ETL processes.
Build high-performance algorithms, prototypes, predictive models, and proof of concepts.
Skills:
5+ years of work experience as a Data Engineer.
Ability to work as part of a team, as well as work independently or with minimal direction.
Excellent written, presentation, and verbal communication skills.
Collaborate with data architects, modelers, and IT team members on project goals.
Strong PC skills including knowledge of Microsoft SharePoint.
Process certification, such as Six Sigma, CBPP, BPM, ISO 20000, ITIL, and CMMI.
Education: BSCSE or related.
FRESH-
Work on engineering and research assignments with F500 companies and startups.
The relationships that we have created with our clients are one of a kind.
We help solve problems in many technologies focusing on R&D, product development, and manufacturing.
We work with the most cutting-edge and latest technologies from AR/VR to Autonomous technologies.
Closely working with our clients, we believe that long-term investments are extremely important to maintain the culture we together have created.
We’re a handpicked team of Engineers, digital strategists, designers, and developers united together in creating a fresh experience. Whether we are strategizing, designing, developing, or analyzing, our integrated team works as an extension of yours to improve your impact, your usability, and your customer conversion. In the process, we collaborate with you to get to know your business, understand your industry, and incorporate your big ideas into memorable experiences that keep your customers coming back for more.
Equal employment opportunity: All qualified persons will be considered for employment without regard to race, color, religion, sex, national origin, age, marital status, familial status, gender identity, sexual orientation, disability for which a reasonable accommodation can be made or any other status protected by law. Assistance will be gladly provided upon request for any applicant with sensory or non-sensory disabilities.
Fresh Consulting is a participating E-Verify company.
freshconsulting.com
Compensation offered will be determined by factors such as location, level, job-related knowledge, skills, and experience. Range $70/hr - $80/hr.
ylM5WIC1Wr",$75.00 /hr (est.),201 to 500 Employees,Company - Private,Management & Consulting,Business Consulting,2007,$25 to $100 million (USD)
"Wallero
4.4",4.4,"Seattle, WA",Data Engineer,"Hi,
Pretty much interested in your resume, we have an urgent opening for Data Engineer role with one of our clients based in Seattle WA.
Position: Data Engineer
Location: Seattle WA - Remote (100%)
Description:
Good Experience in reporting space including PowerBI, ADF, and possibly standing up a data warehouse/data mart.
The main scope centering is around reporting.
Experience with SQL, and agile framework including requirement gathering and story/feature writing skills.
Need someone strong who is comfortable leading and guiding teams in that capacity.
Migrating data from third party to Azure Cloud
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Seattle, WA 98101: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",$75.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2015,$1 to $5 million (USD)
"Deako
4.9",4.9,"Seattle, WA",Data Engineer II,"About Deako
Deako is delivering a revolutionary plug-n-play smart home platform with a laser focus on the untapped new home construction market. We make smart lighting so easy to upgrade that even those who would have never considered smart lighting are making it happen. We've built a company based on trust; where forming personal relationships is key to our success.
Software at Deako
The software team is a tight-knit team of smart, dedicated people. We are customer focused, we ship often, and are constantly asking “why.” We are passionate about the software we write. We work closely with other Deako teams. Developers at Deako are always looking for a better way and understand that no code is perfect.
Data at Deako
We believe that Data and Software go hand in hand. That's why the Data and Software teams work closely together to ensure we're collecting the right data, and getting it to the right people. Data is new at Deako, and we are working to make it a cornerstone of our company. We want every team at Deako to have a data-driven mindset.
Day to Day Expectations:
Participate in the code review process in our DBT repository
Build maintainable, testable SQL queries for various dashboards
Help maintain, contribute to and improve our data infrastructure and existing pipelines
Help maintain and improve our CI/CD pipelines
Responsible for obtaining, cleaning, and munging data and getting it into a form that our data analysts can access and analyze
Participate in meetings with stakeholders around data collection and cleaning

Must Have:
Demonstrable knowledge in SQL + Some Programming Language (Typescript, Python, Ect)
Demonstrates a high level autonomy and willingness to learn
1+ years of experience working as a Data Analyst/Engineer or Comparable field.
Experience with git, dbt
Nice to Have:
Experience in Snowflake, Fivetran, Hightouch
Experience with Gitlab CI
Experience with geospatial data
Stock Options
Hybrid work environment (Office + Remote)
Medical/Dental/Vision/Life/401K
Unlimited PTO
Free Snacks/Coffee/Drinks (Non-Alcoholic and Alcoholic)
Quarterly Company Parties","$115,000 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,2015,$5 to $25 million (USD)
shreetek,#N/A,"Charlotte, NC",Senior Data Engineer,"Job Title : Data Engineer
ONLY USC
On W2 only
Job Location : Newjersey OR Charlotte,NC
Collect, manage, and convert raw data accurately and reliably
Organize data systems for subgroup access and analyses
Configure and sustain data cloud structures
Able to work with structured and unstructured data.
Validate outputs of data pipelines
Degree in Data Engineering preferred.
Two Programmers –
Convert existing SAS code to python/pyspark code for model operation in the cloud.
Create and sustain policy analysis models in the cloud.
Troubleshoot user interfaces in the cloud.
Create and sustain intuitive user interfaces in the cloud.
A degree in Computer Science is preferred.
Overall across resources keep following stack as reference
Visualization : Tableau
Data Modeling/Science : Python / SAS
Engg : AWS cloud native services, security, data pipeline
Job Type: Contract
Salary: $50.00 - $60.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Charlotte, NC 28202: Reliably commute or planning to relocate before starting work (Required)
Work Location: One location",$55.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Orange County's Credit Union
4.0",4.0,"Santa Ana, CA",Data Engineer,"Great Opportunity At Orange County's Credit Union
Must reside in the state of CA, AZ, NV or TX.
Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk.
Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture.
Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential.
Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY!
More about Orange County's Credit Union:
Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace.
As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service.
Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference!
Overview:
Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization.
Essential Functions:
Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making.
Translate business requirements to technical solutions by applying technical knowledge and strong business acumen.
Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services.
Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform.
Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc).
Technical Must Haves for this Role:
5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment.
3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases).
3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.).
2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores.
2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS).
Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.

The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.

We perform thorough background checks and credit checks. EOE.","$92,145 /yr (est.)",201 to 500 Employees,Nonprofit Organization,Financial Services,Banking & Lending,1938,$25 to $100 million (USD)
"Fincons Group
4.0",4.0,Remote,Data Engineer – Databricks / Kafka / SQL,"Fincons Group is an IT business consulting company that has been designing the digital future of leading companies on international markets for 40 years. Fincons Group built its reputation on foreseeing and interpreting new business models and the rapid evolution of IT systems by building a complete range of services: from research and consulting to design and development, from system integration of leading vendor software solutions right up to application management, supporting clients step by step along their digital transformation journey. Fincons is a multinational with over 2,600 people and several offices worldwide (in Italy, Switzerland, Germany, France, the United Kingdom, Belgium and the USA), but above all a Family Company where the founders play a strategic role with commitment and passion, grounding the company in the same principles of a united and caring family.
We firmly believe in the value of cooperation and in the contribution that every idea and intuition can bring. We believe that everyone’s effort can make a difference. Transforming knowledge into a strategic resource is our company mission, and we pursue it with the help of our most strategic asset: our people.

Job Summary
Looking for an experienced data engineer with hands on experience in developing solutions based on Databricks. If you are a curious team-player with passion to learn and working for different clients to develop modern data streaming platforms, please reach out to us!

Work location: any US location might be considered
Employment type: freelance / contractor
Start date: about July 1th
Duration: about 6 months with possible renewal

Responsibilities and Duties
Design, develop and implement real time data ingestion pipelines from multiple sources using Databricks
Develop high-quality, scalable and re-usable frameworks for ingesting high volume and large data sets
Apply best practices to Integrate and ingest various big data formats like Protobuf, Parquet, AVRO and JSON
Work with event based / streaming technologies like Kafka, or Kinesis, to ingest and transform the data using KafkaStreams or KSQL
Working with other members of the project team to support delivery of additional project components (API interfaces, Search)
Work to load test the pipelines for quality and better performance using JMeter, or Gatling etc

Qualifications and skills
Bachelor’s degree and first experience designing, developing, deploying and/or supporting data pipelines using Databricks
Experience in designing and implementing Medallion architecture is preferred
Expertise in designing and deploying data applications on cloud solutions, such as Azure or AWS
Hands on experience in performance tuning and optimizing code running in Databricks environment
Proficient in programming languages like Pyspark and Python
Good understanding of SQL, T-SQL and/or PL/SQL
Demonstrated analytical and problem-solving skills particularly those that apply to a big data environment
Experience with agile development methodologies.
Ability to work effectively both independently and in a group or team environment

Authorization to Work
Applicants for employment in the US must be authorized to work in the US. Fincons Group will not sponsor applicants for work visas.
All qualified applicants will receive consideration for employment without regard to race, colour, religion, sex, sexual orientation, gender identity, national origin, age, disability, or protected veteran status, or any other legally protected basis, in accordance with applicable law.
The data will be processed and stored exclusively for the purposes of this or future selections, guaranteeing the rights referred to in art. 13 Legislative Decree 196/03 and EU regulation 679/2016 (GDPR)
2yrKJdtT13",#N/A,201 to 500 Employees,Company - Private,Financial Services,Accounting & Tax,#N/A,Unknown / Non-Applicable
SecurePro,#N/A,"Arlington, VA",Data Engineer,"Collect, manage, and convert raw data accurately and reliably
Organize data systems for subgroup access and analyses
Configure and sustain data cloud structures
Able to work with structured and unstructured data.
Validate outputs of data pipelines
Convert existing SAS code to python/pyspark code for model operation in the cloud
Create and sustain policy analysis models in the cloud
Troubleshoot user interfaces in the cloud
Create and sustain intuitive user interfaces in the cloud
Degree in Data Engineering preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $100.00 per hour
Experience level:
3 years
Schedule:
8 hour shift
Application Question(s):
Are you a US citizen?
Must have DOD secret Clearance?
Work Location: Remote",$90.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Amazee Global Ventures Inc
5.0",5.0,Remote,Data Engineer,"(W2 ONLY)
Job Title: Data Engineer
Duration : 12 months
Location: Remote
6-7 years of experience
Healthcare experience is must
Design, develop, and maintain Tableau dashboards and reports
Develop and maintain data models and ETL processes
Analyze data to identify trends and patterns
Create and maintain data dictionaries and data models
Develop and maintain data pipelines
Collaborate with stakeholders to understand data requirements
Develop and maintain data quality processes
Monitor data quality and performance
Troubleshoot data issues
Develop and maintain documentation
Requirements:
Bachelor’s degree in Computer Science, Information Systems, or related field
4+ years of experience in data engineering and analysis
Expertise in Tableau
Experience with SQL and data modeling
Knowledge of ETL processes
Knowledge of data warehousing
Excellent problem-solving and communication skills
Ability to work independently and in a team environment\
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Type: Full-time
Salary: $40.33 per hour
Work Location: Remote",$40.33 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,2019,Less than $1 million (USD)
NLB Technology Services,#N/A,"Dallas, TX",CRM data Analytics Engineer,"MUST HAVE:
o Tableau CRMA & Einstein Discovery Certification
o Experience building Dashboards using CRMA (aka Tableau CRM and Einstein Analytics)
o Design, build, and support datasets, data recipes, dataflows, and dashboards using Salesforce and non-Salesforce data sources
o Troubleshoot functional, data, and performance issues with CRMA Dashboards
o Skilled in SAQL, JSON
o Develop and deploy AI/ML solutions using Einstein Discovery and deploy AI/ML Models built outside Salesforce
o Experience working with large data (1 B+ rows) and supporting large user base (10,000+)
o Mentor and coordinate the development efforts of other CRMA DevelopersAT&T
Job Type: Contract
Salary: $65.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Vision insurance
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
CRM ANalytics: 8 years (Required)
einstein: 1 year (Required)
Salesforce: 1 year (Required)
Work Location: One location",$70.00 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"E-Business International INC
3.6",3.6,"Alexandria, VA",AWS Data Engineer,"Role: AWS Data Engineers
Location: Alexandria, VA
Full-Time/Permanent
AWS Engineers
We are seeking TWO (2) AWS Engineers to join our growing team. The qualified applicants will become part of the Enterprise Data Analytics Services (EDAS) program for a large federal agency in Alexandria, VA. Hybrid work options are available.
The AWS Engineer is responsible for the development and support of cloud-based solutions. Solutions include moving assets from on-premise to cloud environment, evaluation, and optimization. AWS Engineer will collaborate with the team effectively to ensure the team's success.
Responsibilities/Duties:
Work on automating the migration process in AWS from development to production.
Create CFT deploying/updating cloud resources.
Advise the engineering and software engineering team as they migrate from on-premise to cloud infrastructure.
Optimize cloud workloads for cost, scalability, availability, governance, compliance, etc.
Guide and/or provide hands-on support to administer production, staging, and deployment environments.
Partner with multi-disciplinary teams to understand requirements and plan architecture and solutions.
Apply insight and expertise across AWS services.
Apply knowledge of scripting and automation using tools like PowerShell, Python, Bash, Ruby, Perl, etc.
Experience and Qualifications:
Good working knowledge of AWS Services, including Compute, Storage Networking, Database, Management tools, Security, Identity, and Compliance.
3 years’ experience in AWS (EC2, Glue, S3, Redshift, Python programming).
Experience implementing and debugging workloads in AWS.
Good knowledge of creating CFTs.
Experience building infrastructure inside AWS via code. Familiarity with tools such as Terraform or CloudFormation.
Ability to proactively communicate, interact, and solve critical customer problems.
Ability to think and act strategically and proactively.
3 years of experience in software development tools and methodologies.
3 years of experience in infrastructure architecture, database architecture, and networking.
3 years of experience architecting/deploying operating solutions built on AWS.
Education/Certifications:
Bachelor's Degree in Computer Science, Information Technology, Engineering, or associated discipline.
AWS certification is a plus!
Clearance Requirements:
Ability to attain and maintain a Public Trust.
Job Type: Full-time
Salary: From $100,000.00 per year
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Alexandria, VA 20598: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Expected Salary Range (Must)?
Visa Status/ Work Authorization (Must)?
Experience:
AWS: 4 years (Required)
Work Location: One location","$100,000 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,1992,Unknown / Non-Applicable
NAVA TECH LLC,#N/A,Remote,Azure Data Engineer,"Azure Data Engineer
Remote
Long Term
Note: Need 10+ years of experience
Sound skills and hands on experience with Azure Data Lake, Azure Data Factory, SQL Data Warehouse Azure Blob, Azure Storage Explorer
Proficient in creating Data Factory pipelines for on-cloud ETL processing; copy activity, custom Azure development etc.
Knowledge of Azure Data Catalog, Event Grid, Service Bus, SQL and Synapse
Experience using Azure Databricks platform.
Experience with Python programming.
Experience using pandas and numpy for data engineering and data cleansing;
Ability to troubleshoot the job scheduler and compute clusters.
Experience with databricks CLI and secrets module.
Experience with Azure Blob storage access configuration.
p l e a s e s h a r e r e s u m e s t o m o h a m m e d (a t) n a v a t e c h (d o t) u s O R 4 4 3 5 3 7 9 8 3 5
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$62.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"DiamondPick
4.5",4.5,Remote,GCP Data Engineer,"Position: GCP Data Engineer (8+years)
Location: Remote
Type: Contract
Job Details:
· Relevant Industry Work Experience (6+ for Developer, preferred 9+ years)
· Experience extracting data from a variety of sources, and a desire to expand those skills (Excellent knowledge in SQL and Spark is mandatory)
· Strong knowledge of Google BigQuery and architecting data pipelines from on-prem to GCP.
· Experience building applications using Google Cloud Platform related frameworks such as DataProc and GCS at the minimum.
· Excellent Communication Skills to Understand and Pass on Requirements.
· Excellent Data Analysis skills. Must be comfortable with querying and analyzing large amount of data on Hadoop HDFS using Hive and Spark.
· Professional experience with a cloud hosting platform (GCP preferred)
· GCP certification is preferred.
Job Type: Contract
Salary: Up to $65.00 per hour
Experience level:
10 years
8 years
9 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
Google BigQuery: 4 years (Preferred)
DataProc: 2 years (Preferred)
Google Cloud Platform: 9 years (Required)
Spark: 8 years (Required)
SQL: 8 years (Required)
License/Certification:
Google Cloud Platform(GCP) Certification (Required)
Work Location: Remote",$65.00 /hr (est.),1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,#N/A,Unknown / Non-Applicable
"Spectrum Communications & Consulting Inc.
4.3",4.3,"Chicago, IL",Data Engineer,"What do incubators and Spectrum have in common? Well, they’re great for growth, and even better for stability. As an innovative software development and digital marketing company pioneering the field of artificial intelligence, we can offer our newest Data Engineer the best of both words – a high energy, forward-thinking start-up culture, inside of a well-established, profitable, and stable structure . if you’re interested in getting your hands dirty and inciting change into a larger organization with a vision to change the world uses data today, then please read on.
Responsibilities
Beyond working with state of the art technology you will have many different fantastic projects to work on as a Data Engineer at Spectrum. Here are just a few different responsibilities you can expect off the bat:
Design, build and maintain both new and existing data infrastructures using tools including (but not limited to) Microsoft SQL Server, Azure Tables, Azure Blobs, and Azure Cosmos
Determine the right database system for a provided data set
Drafting database/table schemas
Properly index tables when applicable to ensure query performance
Create data pipelines for transforming new datasets into consistent, reliable data systems that are usable across multiple platforms
Identify optimizations and improvements in existing data pipelines
Create solutions for data validation to ensure data integrity, accuracy, and consistency
Ensure data is readily available for analysis and development usage
Shifting existing big data pipelines
Come up with data validation solutions for both initial data imports as well as results
Audit the automation process to make sure there are no gaps, redundancies, or faults in the data pipeline that would cause damage to data integrity
Planning query efficient data store schemas with respect to the data requirements of our internal data warehouse
Communicate with data scientists and other software engineers to build proper data pipelines for efficient and cost-effective querying
Develop, construct, test, and maintain architectures with the help and feedback of software engineers
Determine and implement proper data schemas that align with Spectrum’s and Spectrum’s client’s business objectives
Recommend and implement ways to optimize data reliability, efficiency, and quality
Consult and suggest different technologies for our team to test and utilize together
Incorporate the business objectives of key stakeholders in the architecture of our data warehouse
Research new and interesting data acquisition opportunities
Some Characteristics That Define You
We understand that as a Data Engineer for Spectrum, you have many different professional goals and personal interests. As such here are just a few different things that typically define our team members on the Data Science team:
Self-Starter. Building a data warehouse is no simple task. You will need to come in with a self-starter attitude to not only make this data engineering role what you want it to be, but make a stellar and efficient data warehouse while you’re at it.
Analytical. In order to solve problems and build innovative new digital marketing campaigns, it is essential that you know how to take an idea and analyze it from all of its angles.
Developer. In the ever changing world of artificial intelligence, it’s not enough just to build models. You bring to the table an eye for data patterns along with a knack for relational database engineering.
Patient. As a data engineer, you know that you work with extremely large data sets on a daily basis. As such we are looking for someone who is not only meticulous, but patient enough to sit and sift through that data in a thorough way.
Creative. Beyond just analyzing data sets, you are an explorer and a puzzle solver. Pulling insights out of your data and understanding how those insights can better shape our tools is something that you live to do.
Student. More so than most industries, the field of data science is always changing and evolving. As such, you are always looking to learn new things and gain new skills.
Required Skills and Experience
On top of the many intangible skills you bring to the table, there are many skills that can help improve the efficiencies and success of your work at Spectrum. Here are a few of those required skills and experience that you will come in with as a Data Engineer on our team:
A bachelor’s degree/pursuing a bachelor’s degree in computer science, mathematics, statistics, information systems, or a related field
Experience with statistical modeling
1-3 years working experience with Python and/or SAS languages
1-3 years working experience with SQL databases and database querying languages
1-3 years working experience with C#/.NET
Familiarity with Microsoft Azure
Familiarity with Graph database structures
Experience with both RDBMS and TDMS
Experience with data mining and data cleaning
Experience with data visualization and reporting techniques
Written and verbal expression
Benefits
As a Data Engineer at Spectrum there are a ton of fantastic perks and benefits that come along with your work. Here are just a few of the benefits you can expect when joining the Spectrum family:
Comprehensive medical & dental insurance
Retirement planning & company matching
Generous PTO, including sick days & holidays
Coworking space access for remote and hybrid work options
Year-round gym memberships
Paid continuing education
Hybrid work from Home , meetings a few times a month to strategize new projects
About Us
Our mission at Spectrum has always been the same— growth. Whether that be helping our customers find quality new business, developing and challenging our team members, or evolving our products and services with advancements in technology and best practices, we have always been looking towards bettering ourselves for the future. Now as we continue to grow, we find ourselves as not only the nation’s leading digital marketing and software provider for the home services industry, but an innovator and ground-shaker for the world of artificial intelligence as well. From marketing automation software powered by AI, to top notch digital marketing services via those same AI insights, we love what we do and are excited to continue to innovate for the future together.
INDSPCI
Responsibilities
Design, build and maintain both new and existing data infrastructures using tools including (but not limited to) Microsoft SQL Server, Azure Tables, Azure Blobs, and Azure Cosmos
Determine the right database system for a provided data set
Drafting database/table schemas
Properly index tables when applicable to ensure query performance
Create data pipelines for transforming new datasets into consistent, reliable data systems that are usable across multiple platforms
Identify optimizations and improvements in existing data pipelines
Create solutions for data validation to ensure data integrity, accuracy, and consistency
Ensure data is readily available for analysis and development usage
Shifting existing big data pipelines
Come up with data validation solutions for both initial data imports as well as results
Audit the automation process to make sure there are no gaps, redundancies, or faults in the data pipeline that would cause damage to data integrity
Beyond working with state of the art technology you will have many different fantastic projects to work on as a Data Engineer at Spectrum. Here are just a few different responsibilities you can expect off the bat:
Planning query efficient data store schemas with respect to the data requirements of our internal data warehouse
Communicate with data scientists and other software engineers to build proper data pipelines for efficient and cost-effective querying
Develop, construct, test, and maintain architectures with the help and feedback of software engineers
Determine and implement proper data schemas that align with Spectrum’s and Spectrum’s client’s business objectives
Recommend and implement ways to optimize data reliability, efficiency, and quality
Consult and suggest different technologies for our team to test and utilize together
Incorporate the business objectives of key stakeholders in the architecture of our data warehouse
Research new and interesting data acquisition opportunities
Some Characteristics That Define You
We understand that as a Data Engineer for Spectrum, you have many different professional goals and personal interests. As such here are just a few different things that typically define our team members on the Data Science team:
Self-Starter. Building a data warehouse is no simple task. You will need to come in with a self-starter attitude to not only make this data engineering role what you want it to be, but make a stellar and efficient data warehouse while you’re at it.
Analytical. In order to solve problems and build innovative new digital marketing campaigns, it is essential that you know how to take an idea and analyze it from all of its angles.
Developer. In the ever changing world of artificial intelligence, it’s not enough just to build models. You bring to the table an eye for data patterns along with a knack for relational database engineering.
Patient. As a data engineer, you know that you work with extremely large data sets on a daily basis. As such we are looking for someone who is not only meticulous, but patient enough to sit and sift through that data in a thorough way.
Creative. Beyond just analyzing data sets, you are an explorer and a puzzle solver. Pulling insights out of your data and understanding how those insights can better shape our tools is something that you live to do.
Student. More so than most industries, the field of data science is always changing and evolving. As such, you are always looking to learn new things and gain new skills.
Required Skills and Experience
On top of the many intangible skills you bring to the table, there are many skills that can help improve the efficiencies and success of your work at Spectrum. Here are a few of those required skills and experience that you will come in with as a Data Engineer on our team:
A bachelor’s degree/pursuing a bachelor’s degree in computer science, mathematics, statistics, information systems, or a related field
Experience with statistical modeling
1-3 years working experience with Python and/or SAS languages
1-3 years working experience with SQL databases and database querying languages
1-3 years working experience with C#/.NET
Familiarity with Microsoft Azure
Familiarity with Graph database structures
Experience with both RDBMS and TDMS
Experience with data mining and data cleaning
Experience with data visualization and reporting techniques
Written and verbal expression
Benefits
As a Data Engineer at Spectrum there are a ton of fantastic perks and benefits that come along with your work. Here are just a few of the benefits you can expect when joining the Spectrum family:
Comprehensive medical & dental insurance
Retirement planning & company matching
Generous PTO, including sick days & holidays
Coworking space access for remote and hybrid work options
Year-round gym memberships
Paid continuing education
Hybrid work from Home , meetings a few times a month to strategize new projects
About Us
Our mission at Spectrum has always been the same— growth. Whether that be helping our customers find quality new business, developing and challenging our team members, or evolving our products and services with advancements in technology and best practices, we have always been looking towards bettering ourselves for the future. Now as we continue to grow, we find ourselves as not only the nation’s leading digital marketing and software provider for the home services industry, but an innovator and ground-shaker for the world of artificial intelligence as well. From marketing automation software powered by AI, to top notch digital marketing services via those same AI insights, we love what we do and are excited to continue to innovate for the future together.
INDSPCI
Hours
40","$92,696 /yr (est.)",51 to 200 Employees,Company - Private,Media & Communication,Advertising & Public Relations,1992,$5 to $25 million (USD)
"JPMorgan Chase Bank, N.A.
3.8",3.8,"Wilmington, DE",Data Engineer III,"As a Data Engineer III under Consumer and Community Banking within the ADE Data Governance team at JPMorgan Chase, you serve as a seasoned member of an agile team to design and deliver trusted data collection, storage, access, and analytics solutions in a secure, stable, and scalable way. You are responsible for developing, testing, and maintaining critical data pipelines and architectures across multiple technical areas within various business functions in support of the firm's business objectives.

Job responsibilities
Supports review of controls to ensure sufficient protection of enterprise data
Responsible for advising and making custom configuration changes in one to two tools to generate a product at the business or customer request
Updates logical or physical data models based on new use cases
Frequently uses SQL and understands NoSQL databases and their niche in the marketplace
Adds to team culture of diversity, equity, inclusion, and respect

Required qualifications, capabilities, and skills
Experience across the data lifecycle
Advanced at SQL (e.g., joins and aggregations)
Working understanding of NoSQL databases
Proficiency with JSON/Java, Erwin, Oracle, Snowflake, AWS
Significant experience with statistical data analysis and ability to determine appropriate tools and data patterns to perform analysis, including data modeling
Experience customizing changes in a tool to generate product
Executes data architecture solutions and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions and break down problems
Produces data architecture and design artifacts for complex applications while being accountable for ensuring design constraints are met by software code development
Gathers, analyzes, synthesizes, and develops visualizations and reporting from large, diverse data sets in service of continuous improvement of data frameworks, applications, and systems that proactively identifies hidden problems and patterns in data and uses these insights to drive improvements to coding hygiene and system architecture

Preferred qualifications, capabilities, and skills
Familiarity with modern technologies
Exposure to cloud technologies
Chase is a leading financial services firm, helping nearly half of America's households and small businesses achieve their financial goals through a broad range of financial products. Our mission is to create engaged, lifelong relationships and put our customers at the heart of everything we do. We also help small businesses, nonprofits and cities grow, delivering solutions to solve all their financial needs.

We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.
The health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the ""WELL Health-Safety Rating"" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.
As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm's current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm's vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.
We offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, discretionary incentive compensation which may be awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.
Equal Opportunity Employer/Disability/Veterans
Base Pay/Salary
Jersey City,NJ $97,850.00 - $150,000.00 / year","$123,925 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Banking & Lending,1799,$10+ billion (USD)
i-Link Solutions Inc,#N/A,"Boston, MA",Data Engineer,"i-Link Solutions is seeking a Data Engineer to join our team in Hanscom, MA. The Data Engineer will be responsible for designing, developing, and maintaining the data infrastructure that supports the Kessel Run program. The ideal candidate will have experience with AWS, open-source tools, and data engineering best practices
Responsibilities
Design, develop, and maintain the data infrastructure that supports the Kessel Run program
Work with stakeholders to understand business requirements and translate them into technical solutions
Build and maintain data pipelines that ingest, transform, and load data into data warehouses and data lakes
Develop and deploy data models and algorithms to support data-driven decision making
Work with data scientists and analysts to develop and deploy data products
Ensure the security and compliance of data assets
Qualifications
Bachelor's degree in Computer Science, Information Technology, or a related field
10+ years of experience in data engineering
Experience with AWS, open-source tools, and data engineering best practices
Strong programming skills in Python, Java, Scala, or SQL
Experience with data modeling and data warehousing
Experience with data visualization and reporting
Strong analytical and problem-solving skills
Excellent communication and teamwork skills
US Citizenship
Secret Clearance
Kessel Run Experience Desirable
Job Types: Full-time, Permanent
Pay: $89,571.07 - $140,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Referral program
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Boston, MA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineering: 10 years (Required)
AWS: 3 years (Required)
Data warehouse: 4 years (Required)
Work Location: In person","$114,786 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Northeastern University
4.3",4.3,"Boston, MA",Data Engineer,"About the Opportunity
Do you love learning the shapes of datasets, and molding them into something new? The Digital Scholarship Group (DSG) in the Northeastern University Library is excited to open a search for a Data Engineer. Working within a warm and collaborative environment dedicated to social justice, the Data Engineer gathers, organizes, manipulates, transforms, and documents a variety of humanities research data. The Data Engineer works with colleagues across the university to create sustainable platforms and data for community-led digital scholarship.
The Data Engineer position is situated within the DSG, which is part of the Northeastern University Library. The Library is a vital partner in learning, teaching, and community-engaged research for a diverse R1 university. Northeastern is committed to intensive research and experiential learning for students at all levels.
The Digital Scholarship Group is based on Northeastern’s Boston campus. This position is eligible for a hybrid work arrangement. Specific arrangements can be negotiated at the time of hire.
Responsibilities
The Data Engineer has responsibility for helping DSG work with data in a wide range of formats, across multiple projects and often in unforeseen contexts. The Data Engineer develops data dictionaries, mappings between data standards, transformation routines, and other curatorial systems. This position also manages projects and engages in high-level needs analysis and project planning. The DSG is committed to digital approaches that consider the pedagogical, research, social, and ethical implications of data and its design and use.
Working closely with other DSG and library staff, faculty collaborators, and students, the Data Engineer contributes to grant-funded and internal projects including the Boston Research Center; the Civil Rights and Restorative Justice project; the Digital Archive of Indigenous Language Persistence; the TEI Archiving, Publishing, and Access Service; Digital Humanities Quarterly; and the Women Writers Project. They will also take the lead on building DSG’s policies and practices in working with external data platforms such as Wikidata and partner project APIs. To support this work, expertise with tools like regular expressions and OpenRefine, and facility with data including RDF, JSON, XML, various API responses, and other formats will be important.
We warmly invite people with various skills and levels of expertise to apply to this position. Candidates who meet some, but not all, of the qualifications listed below are strongly encouraged to apply. We seek colleagues who are committed to building an inclusive and diverse working environment and who have been and remain underrepresented or marginalized in the field of librarianship – including but not limited to people of color, LGBTQ+ people, individuals with disabilities and applicants from lower-income and first-generation library or academic backgrounds. We expect this position to be an ongoing learning experience and are committed to supporting professional development.
Qualifications
We realize that this is a lengthy list of activities and qualifications. There are multiple paths toward success in this position, and each may look somewhat different depending on the successful candidate’s interests and experience.
Bachelor’s degree required; Master’s degree or similar training in data science, information science, information design, or other relevant discipline preferred
Minimum of 2 years of experience working or studying in a data-intensive environment, preferably in an academic or non-profit research setting
Experience working with quantitative and qualitative datasets, especially with historical and cultural heritage data
Experience working with structured data formats (for instance, XML, RDF, JSON, CSV, relational databases) and with data conversion, data enhancement, and data analysis
Ability to write code to assist in carrying out these kinds of data-related work (for instance, using R, Python, SQL, SPARQL, XSLT, Perl, and/or regular expressions)
Ability to work on multiple concurrent projects and adapt to the evolving landscape of digital humanities
Collaborative problem-solving skills, and the ability to research and recommend solutions as part of a participatory design process
Commitment to thoughtful, adaptive engagement with the needs of community collaborators
Strong oral and written skills, ability to communicate across expertise levels and prepare project documentation
Desire and aptitude to grow skills (especially in technical areas) and learn new things
The following skills are desirable but are not all essential for applicants to possess at the outset; we can provide training:
Knowledge of metadata standards relevant to research data, such as the Data Documentation Initiative
Experience creating, manipulating, and querying linked open data
Experience in open-source development practices and workflows, preferably within an academic or non-profit environment
Experience working with databases, data management systems, and APIs
Experience with developing and leading workshops
Experience communicating complex ideas about data and how it is used to many audiences
Salary Range:
$82,725 - $93,000
About the Digital Scholarship Group
A recognized leader in the field, the Digital Scholarship Group supports digital modes of research, publication, and collaboration through applied research, systems and tools development, and consultative services. The DSG offers a friendly and closely collaborative work environment, and actively fosters the professional and intellectual development of all of our colleagues and collaborators, including training opportunities and mentorship.
Our team engages with faculty in the digital humanities and quantitative social sciences from across the university to develop digital research and teaching projects, organize events, plan grant-funded initiatives and provide training and mentorship. We also work in close partnership with Northeastern’s Archives and Special Collections, the NULab for Maps, Texts, and Networks, and with cultural heritage partners in Boston including the Massachusetts Historical Society and the Boston Public Library.
We develop tools and platforms for working with digital artifacts and data, for querying and publishing them. We also provide workshops, mentorship opportunities, and pedagogical frameworks to the Northeastern community. Some of our major projects include the Boston Research Center, the Civil Rights and Restorative Justice Project, and the Digital Archive of Indigenous Language Persistence, as well as a number of digital archiving projects from the Library’s Archives and Special Collections. In all of our projects, we are attentive to inclusive and anti-racist approaches to data modeling, platform development, and collaborative working processes.
About the Library
The Northeastern University Library supports the mission of the University by working in partnership with the University community to develop and disseminate new scholarship. The Library fosters intellectual and professional growth, enriches the research, teaching, and learning environment, and promotes the effective use of knowledge by managing and delivering information resources and services to library users.
Northeastern University is an equal opportunity employer, seeking to recruit and support a broadly diverse community of faculty and staff. Northeastern values and celebrates diversity in all its forms and strives to foster an inclusive culture built on respect that affirms inter-group relations and builds cohesion.
All qualified applicants are encouraged to apply and will receive consideration for employment without regard to race, religion, color, national origin, age, sex, sexual orientation, disability status, or any other characteristic protected by applicable law.
To learn more about Northeastern University’s commitment and support of diversity and inclusion, please see www.northeastern.edu/diversity.
About Northeastern
Founded in 1898, Northeastern is a global research university and the recognized leader in experience-driven lifelong learning. Our world-renowned experiential approach empowers our students, faculty, alumni, and partners to create impact far beyond the confines of discipline, degree, and campus.
Our locations—in Boston; Charlotte, North Carolina; London; Portland, Maine; San Francisco Bay area; Seattle; Silicon Valley; Toronto; Vancouver; and the Massachusetts communities of Burlington and Nahant—are nodes in our growing global university system. Through this network, we expand opportunities for flexible, student-centered learning and collaborative, solutions-focused research.
Northeastern’s comprehensive array of undergraduate and graduate programs— in a variety of on-campus and online formats—lead to degrees through the doctorate in nine colleges and schools. Among these, we offer more than 195 multi-discipline majors and degrees designed to prepare students for purposeful lives and careers.
The position will remain open until filled but application review will begin after June 16.
Position Type
Information Technology
Additional Information
Northeastern University considers factors such as candidate work experience, education and skills when extending an offer.
Northeastern has a comprehensive benefits package for benefit eligible employees. This includes medical, vision, dental, paid time off, tuition assistance, wellness & life, retirement- as well as commuting & transportation. Visit
https://hr.northeastern.edu/benefits/
for more information.
Northeastern University is an equal opportunity employer, seeking to recruit and support a broadly diverse community of faculty and staff. Northeastern values and celebrates diversity in all its forms and strives to foster an inclusive culture built on respect that affirms inter-group relations and builds cohesion.
All qualified applicants are encouraged to apply and will receive consideration for employment without regard to race, religion, color, national origin, age, sex, sexual orientation, disability status, or any other characteristic protected by applicable law.
To learn more about Northeastern University’s commitment and support of diversity and inclusion, please see
www.northeastern.edu/diversity
.","$87,863 /yr (est.)",1001 to 5000 Employees,College / University,Education,Colleges & Universities,1898,$100 to $500 million (USD)
"Lightcast
4.4",4.4,"Moscow, ID",Senior Data Engineer,"A Senior Data Engineer will architect big data analytical frameworks, and translate complex functional and technical requirements into detailed architecture design and high-performing software. A Senior Data Engineer will select data solution software and define hardware requirements. The position will be responsible for developing and implementing standards and processes for integration projects and initiatives.

Major Responsibilities:
Architect software applications, and test and build automated tools.
Select data solution software and define hardware requirements
Develop standards and processes for integration projects and initiatives.
Lead the design/development of software applications, testing, and building tools
Ensure database changes are reviewed and approved to standards
Lead and communicate to leadership on solution design
Provide technical assistance to junior members and to colleagues across the company
Skills & Abilities:
Analytical mind, personality and aptitude for working with data
Ability to look at the numbers, trends, and data to derive conclusions based on findings
Work closely with management to prioritize business and information needs
Proven ability to work effectively to meet goals and deadlines with minimal supervision
Highly creative problem-solving skills and an ability to tailor efforts based on the importance of the issue being addressed.
A hands-on, detail-oriented mindset with the ability to look beyond the box
Expert knowledge of object-oriented design, data structures, and algorithms
Demonstrated experience with agile or other rapid application development methods, object-oriented design, coding, testing patterns with a variety of languages
Significant knowledge of data structures, algorithms, data modeling and disaster recovery of data systems
Education & Experience:
Requires 5 years of data/software engineering/science experience
Bachelor's degree in computer science or other technical disciplines (masters preferred)
About Lightcast:
Lightcast is a global leader in labor market insights with headquarters in Moscow (ID) and Boston (MA) and offices in the United Kingdom, Europe, and India. We work with partners across six continents to help drive economic prosperity and mobility by providing the insights needed to build and develop our people, our institutions and companies, and our communities.

Lightcast is proud to be an equal opportunity workplace and is committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. Lightcast has always been, and always will be, committed to diversity, equity and inclusion. We seek dynamic professionals from all backgrounds to join our teams, and we encourage our employees to bring their authentic, original, and best selves to work.

#LI-SS1","$118,643 /yr (est.)",1 to 50 Employees,Company - Private,Government & Public Administration,State & Regional Agencies,2011,Unknown / Non-Applicable
"Guardsman Group
3.0",3.0,United States,Data Engineer,"A Little About Us
The Guardsman Group stands on over 40 years of experience, unmatched technical capabilities and the unwavering belief in the right of safety for all. Each and every day, our matchless range of services puts Guardsman in the lives of people in every corner of Jamaica and throughout the Caribbean. We've pioneered technologies and perfected procedures to give our customers the best solutions for their homes and businesses. As we enter another decade, we continue to be the industry leader. Today, Guardsman consists of 13 companies and over seven thousand talented staffers who are proud to call themselves a Guardsman.
The Role
This role is within the Business Performance, Analytics and Intelligence (BPAI) Unit of the Guardsman Group and is directly responsible for the shaping, building and implementation of solutions that satisfy the business intelligence needs across the Group.
What You'll Be Doing
Create and maintain optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build analytictools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics
Collaborate with other members of the BPAI unit to build and improve on the availability, integrity, accuracyand reliability of data pipelines
Work closely with all levels of management, IT department and other members of the BPAI Unit to achieve task objectives
A Little Bit About You
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases
Experience building and optimizing 'big data' data pipelines, architectures and data sets
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Strong analytic skills related to working with unstructured datasets
Build processes supporting data transformation, data structures, metadata, dependency and workload management
Minimum Qualifications
Bachelor's Degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline",#N/A,5001 to 10000 Employees,Company - Private,Management & Consulting,Security & Protective,#N/A,Unknown / Non-Applicable
"Contact Government Services, LLC
4.7",4.7,"Washington, DC",Data Engineer,"CGS is seeking a passionate and driven Data Engineer to support a rapidly growing Data Analytics and Business Intelligence platform focused on providing solutions that empower our Federal customers with the tools and capabilities needed to turn data into actionable insights. The ideal candidate is a critical thinker and perpetual learner; excited to gain exposure and build skillsets across a range of technologies while solving some of our clients’ toughest challenges. As a Data Engineer, you will be integral to data operations for the development and integration of multiple data types across a range of data sets and sources.

You will be responsible for the day-to-day operations of systems that depend on data, ensuring data is properly processed and securely transferred to its appropriate location, in a timely manner. Processing data will include managing, manipulating, storing and parsing data in a data pipeline for variety of target sources. You will also support maintenance of applications and tools that reside on these systems such as upgrades, patches, configuration changes, etc. The work is performed in a multidisciplinary team environment using agile methodologies. The candidate we seek must be highly motivated and enthusiastic about implementing new technologies and learning about new data in a small team environment where deadlines are important.
Responsibilities:
Complete development efforts across data pipeline to store, manage, store, and provision to data consumers
Being an active and collaborating member of an Agile/Scrum team and following all Agile/Scrum best practices
Write code to ensure the performance and reliability of data extraction and processing
Support continuous process automation for data ingest
Achieve technical excellence by advocating for and adhering to lean-agile engineering principles and practices such as API-first design, simple design, continuous integration, version control, and automated testing
Work with program management and engineers to implement and document complex and evolving requirements
Help cultivate an environment that promotes customer service excellence, innovation, collaboration, and teamwork
Collaborate with others as part of a cross-functional team that includes user experience researchers and designers, product managers, engineers, and other functional specialists
Required Skills:
Must be a US Citizen
Must be able to obtain a Public Trust Clearance
7+ years of IT experience including experience in design, management, and solutioning of large, complex data sets and models.
Experience with developing data pipelines from many sources from structured and unstructured data sets in a variety of formats
Proficiency developing ETL processes, and performing test and validation steps
Proficiency to manipulate data (Python, R, SQL, SAS)
Strong knowledge of big data analysis and storage tools and technologies
Strong understanding of the agile principles and ability to apply them
Strong understanding of the CI/CD pipelines and ability to apply them
Experience with relational database, such as, PostgreSQL
Work comfortably in version control systems, such as, Git Repositories
Contact Government Services strives to simplify and enhance government bureaucracy through the optimization of human, technical, and financial resources. We combine cutting edge technology with world class personnel to deliver customized solutions that fit specific needs and solve the most challenging, dynamic problems. Our team emphasizes technical expertise and an empathy to public service to provide unique top tier support for our government engagements.
SOME BENEFITS OF THIS POSITION INCLUDE:
Health, Dental, & Vision insurance
Life Insurance
401k
Flexible Spending Account (Health, Dependent Care, and Commuter)
Paid Time Off & Observance of State/Federal Holidays
For more information about CGS please visit: https://www.cgsfederal.com
1YGuQTHTUE","$103,515 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"VedaInfo Inc
4.1",4.1,"Phoenix, AZ",Big Data Engineer,"Hi,
I hope this note finds you well
I am reaching out to you regarding your career opportunity with us for “Big Data Engineer” for a “6+ Months Contract” @Phoenix, AZ
Please find the below requirement details.
Job Title: Big Data Engineer
Location: Phoenix, AZ (Onsite)
Duration: 6+ months
Rate: $60/hr C2C
Responsibilities:
Design, implement, and maintain big data systems handling large volumes of data.
Utilize Hadoop, Hive, and Spark for efficient data processing and analysis.
Collaborate with cross-functional teams to understand data requirements.
Develop and optimize data ingestion, storage, and transformation processes.
Build scalable data pipelines for seamless data processing and analysis.
Monitor and troubleshoot data processing and performance issues.
Stay updated with emerging big data technologies.
Requirements:
Bachelor's/Master's degree in CS, Engineering, or related field
7+ years of experience as a Big Data Engineer
Strong proficiency in Hadoop, Hive, and Spark
Extensive experience with AWS or Azure
Solid understanding of data ingestion, storage, and transformation
Excellent problem-solving and communication skills
Proactive and self-motivated
This is a 6+ month onsite contract position in Phoenix, AZ, with the possibility of extension. Compensation is $60/hr on C2C basis.
Please submit your updated resume highlighting relevant experience.
Thank you for your interest!
Thanks & Regards
Mohammed ZAIN
Job Type: Contract
Pay: $50.00 - $60.00 per hour
Experience level:
7 years
Ability to commute/relocate:
Phoenix, AZ: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
To improve your chances of getting Shortlisted, make sure to tailor your Profile to the Job Requirements, Highlight your Relevant Skills and Experience, and then upload your Resume.
Experience:
AWS (Amazon Web Services): 7 years (Required)
Azure: 7 years (Required)
Big data: 7 years (Required)
Hadoop: 7 years (Required)
Apache Hive: 7 years (Required)
Spark: 7 years (Required)
Work Location: One location",$55.00 /hr (est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$1 to $5 million (USD)
"MARVEL TECHNOLOGIES INC
3.7",3.7,"Cincinnati, OH",Data Engineer,"Job Description
The data engineer designs and builds platforms tools and solutions that help the bank manage secure and generate value from its data. The person in this role creates scalable and reusable solutions for gathering collecting storing processing and serving data on both small and very large i.e. Big Data scales. These solutions can include on-premise and cloud-based data platforms and solutions in any of the following domains ETL business intelligence analytics persistence relational NoSQL data lakes search messaging data warehousing stream processing and machine learning.
Responsible and accountable for risk by openly exchanging ideas and opinions elevating concerns and personally following policies and procedures as defined. Accountable for always doing the right thing for customers and colleagues and ensures that actions and behaviors drive a positive customer experience. While operating within the Bank's risk appetite achieves results by consistently identifying assessing managing monitoring and reporting risks of all types.
ESSENTIAL DUTIES AND RESPONSIBILITIES:
Responsible for design Development and Support of data solutions APIs tools and processes to enable rapid delivery of business capabilities.
Work closely with IT application teams Enterprise architecture infrastructure information security and LOB stakeholders to translate business and technical strategies into data-driven solutions for the Bank.
Act as a technical Expert addressing problems related to system and application design performance integration security etc.
Conduct research and Development based on current trends and technologies related to the banking industry data engineering and architecture data security and related topics.
Work with developers to Build CI/CD pipelines Self-service Build tools and automated deployment processes.
Evaluate software products and Provide documented recommendations as needed.
Provide Support and troubleshooting for data platforms. Must be willing to Provide escalated on-call Support for complicated and/or critical incidents.
Participate in the planning process for hardware and software.
Plan and work on internal projects as needed including legacy system replacement Monitoring and analytics improvements tool Development and technical documentation.
Provide technical guidance and mentoring for other team members.
Manage and prioritize multiple assignments.
MINIMUM KNOWLEDGE SKILLS AND ABILITIES REQUIRED:
Bachelor's degree in Computer Science/Information Systems or equivalent combination of education and experience.
Must be able to communicate ideas both verbally and in writing to management business and IT sponsors and technical resources in language that is appropriate for each group.
Fundamental understanding of distributed computing principles
Knowledge of application and data security concepts best practices and common vulnerabilities.
Conceptual understanding of one or more of the following disciplines preferred big data technologies and distributions metadata management products commercial ETL tools Bi and reporting tools messaging systems data warehousing Java language and run time environment major version control systems continuous integration/delivery tools infrastructure automation and virtualization tools major cloud or rest API design and development.
Job Types: Full-time, Contract
Pay: $60.00 - $62.00 per hour
Compensation package:
Hourly pay
Yearly pay
Experience level:
9 years
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",$61.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,#N/A,$5 to $25 million (USD)
"Vedainfo
4.1",4.1,"Peoria, IL",Data Engineer,"Job Description
· Create and develop optimal data pipeline which fetches data from raw and apply business transformation and load derived.
· Build the AWS Fargate components required for optimal extraction, transformation, and loading of data-to-data lake.
· Develop data pipeline setup between base and derive values based on Caterpillar specification.
· Implement quality validation rules and follow process defined by stakeholder.
· Develop business events as defined by the solution team to generate CDC events for consumption layer (Kinesis)
· Create re-usable and configurable components wherever applicable.
· Create and execute unit, integration, and regression tests
· Ensure the code is developed satisfying all business rules, perform unit testing where applicable.
· Data pipelines development
· Python Development
· AWS cloud services
· Data quality validation rules
· Java (preferred)
· Implement CDC
· Transformation rules creation
· Message broker tools like Kinesis
· Azure DevOps – CI/CD pipelines
· Automated and Manual Testing experience
Job Type: Contract
Pay: Up to $60.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Peoria, IL 61602: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
This position is remote initially and you will be asked to work any of the client location once offer is made. Please confirm your current location.
Experience:
Data Engineer: 1 year (Preferred)
Python Development: 1 year (Preferred)
AWS: 1 year (Preferred)
Message broker tools: 1 year (Preferred)
Work Location: In person",$60.00 /hr (est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$1 to $5 million (USD)
META FORCE IT LLC,#N/A,"Cincinnati, OH",Data Engineer,"Job Description
The data engineer designs and builds platforms tools and solutions that help the bank manage secure and generate value from its data. The person in this role creates scalable and reusable solutions for gathering collecting storing processing and serving data on both small and very large i.e. Big Data scales. These solutions can include on-premise and cloud-based data platforms and solutions in any of the following domains ETL business intelligence analytics persistence relational NoSQL data lakes search messaging data warehousing stream processing and machine learning.
Responsible and accountable for risk by openly exchanging ideas and opinions elevating concerns and personally following policies and procedures as defined. Accountable for always doing the right thing for customers and colleagues and ensures that actions and behaviors drive a positive customer experience. While operating within the Bank's risk appetite achieves results by consistently identifying assessing managing monitoring and reporting risks of all types.
ESSENTIAL DUTIES AND RESPONSIBILITIES:
Responsible for design Development and Support of data solutions APIs tools and processes to enable rapid delivery of business capabilities.
Work closely with IT application teams Enterprise architecture infrastructure information security and LOB stakeholders to translate business and technical strategies into data-driven solutions for the Bank.
Act as a technical Expert addressing problems related to system and application design performance integration security etc.
Conduct research and Development based on current trends and technologies related to the banking industry data engineering and architecture data security and related topics.
Work with developers to Build CI/CD pipelines Self-service Build tools and automated deployment processes.
Evaluate software products and Provide documented recommendations as needed.
Provide Support and troubleshooting for data platforms. Must be willing to Provide escalated on-call Support for complicated and/or critical incidents.
Participate in the planning process for hardware and software.
Plan and work on internal projects as needed including legacy system replacement Monitoring and analytics improvements tool Development and technical documentation.
Provide technical guidance and mentoring for other team members.
Manage and prioritize multiple assignments.
MINIMUM KNOWLEDGE SKILLS AND ABILITIES REQUIRED:
Bachelor's degree in Computer Science/Information Systems or equivalent combination of education and experience.
Must be able to communicate ideas both verbally and in writing to management business and IT sponsors and technical resources in language that is appropriate for each group.
Fundamental understanding of distributed computing principles
Knowledge of application and data security concepts best practices and common vulnerabilities.
Conceptual understanding of one or more of the following disciplines preferred big data technologies and distributions metadata management products commercial ETL tools Bi and reporting tools messaging systems data warehousing Java language and run time environment major version control systems continuous integration/delivery tools infrastructure automation and virtualization tools major cloud or rest API design and development.
Job Type: Contract
Pay: $45.96 - $50.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Cincinnati, OH 45215: Reliably commute or planning to relocate before starting work (Required)
Experience:
data engineer: 10 years (Preferred)
Big data: 3 years (Preferred)
Work Location: Hybrid remote in Cincinnati, OH 45215",$47.98 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Myticas Consulting
3.9",3.9,Arizona,BHJOB15656_20058 - Data Engineer,"Myticas's direct client based out of Phoenix, AZ is currently seeking a Data Engineer for a 100% remote contract position.
Pay Rate: Up to $34/hr

Job Description:
A Data Engineer can confidently understand, write, troubleshoot, optimize, and peer review SQL code. They understand business requirements and create code and reporting that exceeds specifications through anticipating underlying needs. They also understand relational database design, table structure, data types, and data models. They are experienced with and confident in development in reporting visualizations in Excel, PowerBI, Tableau, QuickSight, SSRS, or other data products. They can create reporting dashboards and products that inform the business. They easily grasp and retain complex data concepts and are able to explain them to others. Looking for engineers who are driven to innovate, use cutting edge technologies to solve problems and think outside the box.
What You’ll Do:
Digital Transformation into AWS
Data Warehouse related work (SQL, ETL, Replication, Databases)
Understand, write, troubleshoot, optimize, and peer review SQL code.
Understand business requirements and create code and reporting that exceeds specifications through anticipating underlying needs.
Understand relational database design, table structure, data types, and data models. 6.Experienced with and confidence in development in reporting visualizations in Excel, PowerBI, Tableau, QuickSight, SSRS, or other data products.
Data Warehouse related work (SQL, ETL, Replication, Databases
Perform other duties as assigned or apparent.
MINIMUM EDUCATION AND RELATED WORK EXPERIENCE:
Bachelor's degree in a Computer Science field and 2 years' work experience
2 years of experience in most phases of IT systems deployments in one or more of the following areas: design and deployment of cloud services, data migration to AWS cloud, Digital Transformation into AWS, and Data Warehouse related work
INDCHI
INDREM",$34.00 /hr (est.),1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
Sconcept,#N/A,Remote,Azure Data Engineer,"** W2 LONG TERM CONTRACT BASED....**(NO C2C)
JOB TYPE:- Long Term Contract(6 months)Extendable
JOB LOCATION:-USA
About Sconcept LLC:-Software Concepts Consulting was established in 2008. With our “hands-on” approach we are very successful in delivering high performance, secure IT solutions and IT services. With headquarters in Carrollton, Texas we are specialized in providing business solutions to the Fortune 500 clients. Our clients include Software, Banking, Financial, Healthcare, Retail, Consumer goods and Insurance sectors. We deliver candidates based on client requirements.
Required Skills:-
Must have worked on several (5+) projects building solutions for Data Pipelines, Data warehousing, Data Modelling on Azure Data Platform (AWS, Google Cloud Platform - good to have)
Expertise in design and development of Data pipelines and ETL using ADF, Databricks to move data from relational/structured/unstructured data from source to data lake to data warehouse, on Azure
Creating Pipelines and integrations of multiple data sources
Data processing and transformation using Databricks (Scala/Python)
Expertise in optimizing cost and performance of data pipelines
Experience working with Data Lake, Synapse on Azure
Designing Data Lake storage layer (Landing/staging, raw, trusted/curated zones)
Designing data storage DB schema, data models and processing on Azure Synapse DW
Experience with Synapse Spark Pool
Experience in Data Modelling
Building Data Models from structured and semi-structured data (Data Model, Common Data Model etc.)
Performance optimization of data model for high performance analytical and reporting workloads
Expertise with SQL Server and extensive experience with SQL programming
Programming and optimizing DB objects Views, Stored Procs, Functions
Good experience in defining data access policy and security for azure data services
Experience with Oracle EDW
Knowledge/Experience migrating from Oracle EDW to any cloud DW
Experience integrating AI skills/ML models with Data and reporting solutions
Experience in building API layer for downstream consumption
Job Type: Contract
Pay: $45.00 - $50.00 per hour
Benefits:
Health insurance
Compensation package:
Bonus pay
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$47.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"ProGrad
4.2",4.2,Remote,Data Engineer,"We are looking for data engineers that are excited about building and maintaining data pipelines for a variety of interests. The pipelines will ingest data from a variety of sources ranging from internal change data streams to external integrations with various SaaS platforms through webhooks and events. The ideal candidate is self driven and looking to grow as a data engineer while having an outsized impact on the future of data at the company. A love for database architecture and optimisation is a must as is the ability to work across diverse teams.
Requirements/Qualifications:
Bachelor's or Master's degree in Data Science (specialization)
Software development experience coding in a general-purpose programming language such as C, C++, Java, JavaScript, Golang, or Python
Experience working with data structures or algorithms (i. e., data structures /algorithms class, coursework/projects, research, internships, or other practical experience in/outside of school or work (including open source hobby coding)
Proficiency in SQL, and NoSQL Databases
Experience working with large datasets
Experience writing and debugging complex SQL queries
A strong understanding of cloud database best practices and performance optimisation
Experience building and maintaining data pipelines, coupled with knowledge of good programming practices such as unit and integration testing
. A strong understanding of cloud database best practices and performance optimisation
Good to have skills:
. Experience building and maintaining data pipelines, coupled with knowledge of good programming practices such as unit and integration testing
Job Types: Full-time, Permanent
Salary: Up to $1,200,000.00 per year
Experience level:
1 year
Under 1 year
Schedule:
Day shift
Work Location: Remote
Speak with the employer
+91 9057202434",#N/A,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Enterprise Minds,#N/A,Remote,Data Engineer,"Job Role: SQL Data Engineer
Location: Remote Opportunity - Need to work in PST hours
Visa: US Citizen Only
Duration: Long term
Roles and Responsibilities:
3+ years of experience with a complex build system in AWS using Starburst.
5+ years of experience with installation, maintenance, and administration of Oracle databases- PL/SQL and ANSI SQL
Oracle DBA + Oracle SQL to ANSI SQL conversion, performance/query tuning
3+ years of experience configuring, integrating, and securing multiple AWS database offerings (ex. RDS, Aurora, Casandra, and Dynamo)
2+ years of experience with supporting Linux systems engineering efforts in system design and evaluation, solution engineering, software development, or system administration.
Experience with Database Schema as Code tools
Ability to design and develop complex SQL queries and reports to retrieve required data.
Experience with database partitioning and microservices
Knowledge of Agile methodologies or the software development life cycle (SDLC)
US Citizenship is Mandatory.
Job Type: Full-time
Experience:
Data Engineer: 10 years (Preferred)
AWS using Starburst: 4 years (Preferred)
PL/SQL and ANSI SQL: 2 years (Preferred)
AWS RDS: 1 year (Preferred)
Work Location: Remote",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Stefanini, Inc
3.8",3.8,"Dearborn, MI",Data Engineer,"Stefanini Group is hiring!
Stefanini is looking for Data Engineer at Dearborn, MI (Hybrid)
For quick apply, please reach out to Rajat Baloria
Phone: (248) 728-2620
Email: Rajatsingh.Baloria@stefanini.com

Open to W2 candidates only!

Position Description:
Looking for a Software Engineer focused on delivering software leveraging Java and Python based on proven Lean/Agile methods. Knowledge of Big Data technologies like Hadoop and Spark is a plus.
The Software Engineer will work in a small, cross-functional, and co-located team. The Software Engineer will collaborate directly and continuously with business partners, product managers and designers, and will release early and often.
Position Responsibilities:
Work hands-on with the team and other stakeholders to deliver quality software products that meet our customer's requirements and needs.
Grow technical capabilities / expertise and provide guidance to other members on the team



Skills Required:
Exceptional software engineering knowledge; OO Design Principles
Basic understanding of Big Data and potential use cases
Strong desire to learn new skills and apply to solve business problems/opportunities “Spring Boot, Java, Angular, API, Micro Services, PCF, GCP (especially Cloud Run)”

Experience Required:
Overall 6 years of work experience in delivering customer facing products
Minimum 2 years of strong development experience in at least one of the following technologies: Java or Python on a Hadoop/Spark Platform

Education Required:
An associate's degree in Computer Science or similar technical discipline
Education Preferred:
Google Cloud Certification

Listed Salary Range may vary based on experience, qualifications, and local market, Also some positions may Include bonuses and other Incentives

About Stefanini Group
The Stefanini Group is a global provider of offshore, onshore and near shore outsourcing, IT digital consulting, systems integration, application and strategic staffing services to Fortune 1000 enterprises around the world. Our presence is in countries like Americas, Europe, Africa and Asia, and more than 400 clients across a broad spectrum of markets, including financial services, manufacturing, telecommunications, chemical services, technology, public sector, and utilities. Stefanini is a CMM level 5, IT consulting, company with global presence. We are CMM Level 5 company.",$82.00 /hr (est.),10000+ Employees,Subsidiary or Business Segment,Information Technology,Information Technology Support Services,1987,$1 to $5 billion (USD)
"Maven Workforce
4.1",4.1,"Alpharetta, GA",Data Engineer,"Must Have skills:
Building Data Pipeline exp is a must
ETL Tools – SSIS, Alteryx
Data modelling
SQL Server development
Microsoft SQL Stack
Responsibilities:
Data acquisition and ingestion - Identify data sources and build pipelines using various ETL tools such as but not limited to, SSIS, and Alteryx. Data sources including but not limited to SQL Server, SAP, Teradata, Hadoop\Hive, PostgreSQL, Oracle and flat files.
Identifying ways to improve data reliability, efficiency and quality by various data solution techniques.
Expertise in Data project management with JIRA stories. Work as a liaison between business and IT to ensure successful and timely completion of the projects.
Assist in defining the data architecture framework, standards and principles, including modeling, metadata, security and reference data.
Create and optimize data models to support various business applications.
Reviewing modifications of existing data systems for cross-compatibility.
Automate and support workflows to ensure timely delivery.
Must Have:
5+ years of SQL Server development experience.
3+ years ALTERYX Admin /User management experience and advance workflow management.
5+ years data modeling experience.
5+ years of ETL experience.
5+ years of experience in working on more than one database technologies Microsoft SQL server, Teradata.
2+ years big data experience, Hadoop, Hive, Spark
Expert knowledge of data warehousing.
DESIRED SKILLS:
BI Lifecycle management
Working understanding of Microsoft VBA, HTML, Python
Data model development using ERWIN
Job Type: Contract
Salary: $50.00 per hour
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Building Data Pipeline: 8 years (Required)
ETL Tools – SSIS, Alteryx: 7 years (Required)
Data modelling: 6 years (Required)
SQL Server development: 8 years (Required)
Microsoft SQL Stack: 8 years (Required)
Work Location: One location
Speak with the employer
+91 7328456015",$50.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2008,Unknown / Non-Applicable
Zllius Inc.,#N/A,"Canton, MI",Data Engineer AUG76,"Hi,
Greetings from Zllius Inc!
We have an urgent opening for the roleData Engineer. If interested, revert me back with your Updated resume, along with contact details, work authorization.
Job Role: Data Engineer
Location: Hartford, CT ( Initially Remote )
Duration: Long-term
Position: W2/1099
Visa: Any (Except H1B)
Job Ref: AUG76
Job Description:
Good experience on designing and developing data pipelines for data ingestion and transformation using Spark.
Distributed computing experience using Pyspark.
Good understanding of spark framework and spark architecture.
Experience working in Cloud based big data infrastructure.
Excellent in trouble shooting the performance and data skew issues.
Must have good understanding of spark run time metrics and tune applications based on metrics.
Deep knowledge in partitioning, bucketing concepts of data ingestion.
Good understanding of AWS services like Glue, Athena, S3, Lambda, Cloud formation.
Preferred working knowledge on the implementation of datalake ETL using AWS glue, Databricks etc.
Experience with data modelling techniques for cloud data stores and on prem databases like Teradata, Teradata Vantage (TDV) etc.
Preferred working experience in ETL development in Teradata vantage and data migration from on prem to Teradata vantage.
Proficiency in SQL, relational and non-relational databases, query optimization and data modelling.
Experience with source code control systems like Gitlab.
Experience with large scale distributed relational and NoSQL database systems.
Experience : 9+years
Thanks & Regards:
Zllius Inc.
844 495 5487
Job Types: Full-time, Contract
Schedule:
8 hour shift
Work Location: In person","$86,440 /yr (est.)",Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Fresh Consulting
3.9",3.9,"Menlo Park, CA",Data Engineer,"Fresh Consulting is a design-led, software development and hardware engineering company, offering end-to-end digital services to help companies innovate. We bring together amazing UX designers, sophisticated developers, digital strategists, and top-notch engineers to help companies create fresh experiences that connect humans, systems, and machines. We’ve been growing fast and need someone to help us continue to manage the delivery of high-quality work in a fast-paced environment.
See more at freshconsulting.com Visit freshconsulting.com/portfolio to see our project work across several industries.
View and apply to all jobs - https://freshconsulting.applytojob.com/apply/ or visit freshconsulting.com/careers
Title: Data Engineer
Duration: 6 months with possible extension
Location: Onsite Menlo Park, CA
Benefits: Employee benefits at 100% including Medical, PTO, Holiday Pay, 401K Plan, and much more!
Hours: Minimum 40 Hours/Week
Role:
Design, construct, install, test, and maintain highly scalable data management systems.
Ensure systems meet business requirements and industry practices.
Design, implement, automate, and maintain large-scale enterprise data ETL processes.
Build high-performance algorithms, prototypes, predictive models, and proof of concepts.
Skills:
5+ years of work experience as a Data Engineer.
Ability to work as part of a team, as well as work independently or with minimal direction.
Excellent written, presentation, and verbal communication skills.
Collaborate with data architects, modelers, and IT team members on project goals.
Strong PC skills including knowledge of Microsoft SharePoint.
Process certification, such as Six Sigma, CBPP, BPM, ISO 20000, ITIL, and CMMI.
Education: BSCSE or related.
FRESH-
Work on engineering and research assignments with F500 companies and startups.
The relationships that we have created with our clients are one of a kind.
We help solve problems in many technologies focusing on R&D, product development, and manufacturing.
We work with the most cutting-edge and latest technologies from AR/VR to Autonomous technologies.
Closely working with our clients, we believe that long-term investments are extremely important to maintain the culture we together have created.
We’re a handpicked team of Engineers, digital strategists, designers, and developers united together in creating a fresh experience. Whether we are strategizing, designing, developing, or analyzing, our integrated team works as an extension of yours to improve your impact, your usability, and your customer conversion. In the process, we collaborate with you to get to know your business, understand your industry, and incorporate your big ideas into memorable experiences that keep your customers coming back for more.
Equal employment opportunity: All qualified persons will be considered for employment without regard to race, color, religion, sex, national origin, age, marital status, familial status, gender identity, sexual orientation, disability for which a reasonable accommodation can be made or any other status protected by law. Assistance will be gladly provided upon request for any applicant with sensory or non-sensory disabilities.
Fresh Consulting is a participating E-Verify company.
freshconsulting.com
Compensation offered will be determined by factors such as location, level, job-related knowledge, skills, and experience. Range $70/hr - $80/hr.
ylM5WIC1Wr",$75.00 /hr (est.),201 to 500 Employees,Company - Private,Management & Consulting,Business Consulting,2007,$25 to $100 million (USD)
"Wallero
4.4",4.4,"Seattle, WA",Data Engineer,"Hi,
Pretty much interested in your resume, we have an urgent opening for Data Engineer role with one of our clients based in Seattle WA.
Position: Data Engineer
Location: Seattle WA - Remote (100%)
Description:
Good Experience in reporting space including PowerBI, ADF, and possibly standing up a data warehouse/data mart.
The main scope centering is around reporting.
Experience with SQL, and agile framework including requirement gathering and story/feature writing skills.
Need someone strong who is comfortable leading and guiding teams in that capacity.
Migrating data from third party to Azure Cloud
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Seattle, WA 98101: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",$75.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2015,$1 to $5 million (USD)
"Deako
4.9",4.9,"Seattle, WA",Data Engineer II,"About Deako
Deako is delivering a revolutionary plug-n-play smart home platform with a laser focus on the untapped new home construction market. We make smart lighting so easy to upgrade that even those who would have never considered smart lighting are making it happen. We've built a company based on trust; where forming personal relationships is key to our success.
Software at Deako
The software team is a tight-knit team of smart, dedicated people. We are customer focused, we ship often, and are constantly asking “why.” We are passionate about the software we write. We work closely with other Deako teams. Developers at Deako are always looking for a better way and understand that no code is perfect.
Data at Deako
We believe that Data and Software go hand in hand. That's why the Data and Software teams work closely together to ensure we're collecting the right data, and getting it to the right people. Data is new at Deako, and we are working to make it a cornerstone of our company. We want every team at Deako to have a data-driven mindset.
Day to Day Expectations:
Participate in the code review process in our DBT repository
Build maintainable, testable SQL queries for various dashboards
Help maintain, contribute to and improve our data infrastructure and existing pipelines
Help maintain and improve our CI/CD pipelines
Responsible for obtaining, cleaning, and munging data and getting it into a form that our data analysts can access and analyze
Participate in meetings with stakeholders around data collection and cleaning

Must Have:
Demonstrable knowledge in SQL + Some Programming Language (Typescript, Python, Ect)
Demonstrates a high level autonomy and willingness to learn
1+ years of experience working as a Data Analyst/Engineer or Comparable field.
Experience with git, dbt
Nice to Have:
Experience in Snowflake, Fivetran, Hightouch
Experience with Gitlab CI
Experience with geospatial data
Stock Options
Hybrid work environment (Office + Remote)
Medical/Dental/Vision/Life/401K
Unlimited PTO
Free Snacks/Coffee/Drinks (Non-Alcoholic and Alcoholic)
Quarterly Company Parties","$115,000 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,2015,$5 to $25 million (USD)
shreetek,#N/A,"Charlotte, NC",Senior Data Engineer,"Job Title : Data Engineer
ONLY USC
On W2 only
Job Location : Newjersey OR Charlotte,NC
Collect, manage, and convert raw data accurately and reliably
Organize data systems for subgroup access and analyses
Configure and sustain data cloud structures
Able to work with structured and unstructured data.
Validate outputs of data pipelines
Degree in Data Engineering preferred.
Two Programmers –
Convert existing SAS code to python/pyspark code for model operation in the cloud.
Create and sustain policy analysis models in the cloud.
Troubleshoot user interfaces in the cloud.
Create and sustain intuitive user interfaces in the cloud.
A degree in Computer Science is preferred.
Overall across resources keep following stack as reference
Visualization : Tableau
Data Modeling/Science : Python / SAS
Engg : AWS cloud native services, security, data pipeline
Job Type: Contract
Salary: $50.00 - $60.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Charlotte, NC 28202: Reliably commute or planning to relocate before starting work (Required)
Work Location: One location",$55.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Orange County's Credit Union
4.0",4.0,"Santa Ana, CA",Data Engineer,"Great Opportunity At Orange County's Credit Union
Must reside in the state of CA, AZ, NV or TX.
Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk.
Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture.
Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential.
Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY!
More about Orange County's Credit Union:
Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace.
As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service.
Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference!
Overview:
Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization.
Essential Functions:
Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making.
Translate business requirements to technical solutions by applying technical knowledge and strong business acumen.
Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services.
Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform.
Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc).
Technical Must Haves for this Role:
5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment.
3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases).
3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.).
2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores.
2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS).
Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.

The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.

We perform thorough background checks and credit checks. EOE.","$92,145 /yr (est.)",201 to 500 Employees,Nonprofit Organization,Financial Services,Banking & Lending,1938,$25 to $100 million (USD)
"Fincons Group
4.0",4.0,Remote,Data Engineer – Databricks / Kafka / SQL,"Fincons Group is an IT business consulting company that has been designing the digital future of leading companies on international markets for 40 years. Fincons Group built its reputation on foreseeing and interpreting new business models and the rapid evolution of IT systems by building a complete range of services: from research and consulting to design and development, from system integration of leading vendor software solutions right up to application management, supporting clients step by step along their digital transformation journey. Fincons is a multinational with over 2,600 people and several offices worldwide (in Italy, Switzerland, Germany, France, the United Kingdom, Belgium and the USA), but above all a Family Company where the founders play a strategic role with commitment and passion, grounding the company in the same principles of a united and caring family.
We firmly believe in the value of cooperation and in the contribution that every idea and intuition can bring. We believe that everyone’s effort can make a difference. Transforming knowledge into a strategic resource is our company mission, and we pursue it with the help of our most strategic asset: our people.

Job Summary
Looking for an experienced data engineer with hands on experience in developing solutions based on Databricks. If you are a curious team-player with passion to learn and working for different clients to develop modern data streaming platforms, please reach out to us!

Work location: any US location might be considered
Employment type: freelance / contractor
Start date: about July 1th
Duration: about 6 months with possible renewal

Responsibilities and Duties
Design, develop and implement real time data ingestion pipelines from multiple sources using Databricks
Develop high-quality, scalable and re-usable frameworks for ingesting high volume and large data sets
Apply best practices to Integrate and ingest various big data formats like Protobuf, Parquet, AVRO and JSON
Work with event based / streaming technologies like Kafka, or Kinesis, to ingest and transform the data using KafkaStreams or KSQL
Working with other members of the project team to support delivery of additional project components (API interfaces, Search)
Work to load test the pipelines for quality and better performance using JMeter, or Gatling etc

Qualifications and skills
Bachelor’s degree and first experience designing, developing, deploying and/or supporting data pipelines using Databricks
Experience in designing and implementing Medallion architecture is preferred
Expertise in designing and deploying data applications on cloud solutions, such as Azure or AWS
Hands on experience in performance tuning and optimizing code running in Databricks environment
Proficient in programming languages like Pyspark and Python
Good understanding of SQL, T-SQL and/or PL/SQL
Demonstrated analytical and problem-solving skills particularly those that apply to a big data environment
Experience with agile development methodologies.
Ability to work effectively both independently and in a group or team environment

Authorization to Work
Applicants for employment in the US must be authorized to work in the US. Fincons Group will not sponsor applicants for work visas.
All qualified applicants will receive consideration for employment without regard to race, colour, religion, sex, sexual orientation, gender identity, national origin, age, disability, or protected veteran status, or any other legally protected basis, in accordance with applicable law.
The data will be processed and stored exclusively for the purposes of this or future selections, guaranteeing the rights referred to in art. 13 Legislative Decree 196/03 and EU regulation 679/2016 (GDPR)
2yrKJdtT13",#N/A,201 to 500 Employees,Company - Private,Financial Services,Accounting & Tax,#N/A,Unknown / Non-Applicable
SecurePro,#N/A,"Arlington, VA",Data Engineer,"Collect, manage, and convert raw data accurately and reliably
Organize data systems for subgroup access and analyses
Configure and sustain data cloud structures
Able to work with structured and unstructured data.
Validate outputs of data pipelines
Convert existing SAS code to python/pyspark code for model operation in the cloud
Create and sustain policy analysis models in the cloud
Troubleshoot user interfaces in the cloud
Create and sustain intuitive user interfaces in the cloud
Degree in Data Engineering preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $100.00 per hour
Experience level:
3 years
Schedule:
8 hour shift
Application Question(s):
Are you a US citizen?
Must have DOD secret Clearance?
Work Location: Remote",$90.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Amazee Global Ventures Inc
5.0",5.0,Remote,Data Engineer,"(W2 ONLY)
Job Title: Data Engineer
Duration : 12 months
Location: Remote
6-7 years of experience
Healthcare experience is must
Design, develop, and maintain Tableau dashboards and reports
Develop and maintain data models and ETL processes
Analyze data to identify trends and patterns
Create and maintain data dictionaries and data models
Develop and maintain data pipelines
Collaborate with stakeholders to understand data requirements
Develop and maintain data quality processes
Monitor data quality and performance
Troubleshoot data issues
Develop and maintain documentation
Requirements:
Bachelor’s degree in Computer Science, Information Systems, or related field
4+ years of experience in data engineering and analysis
Expertise in Tableau
Experience with SQL and data modeling
Knowledge of ETL processes
Knowledge of data warehousing
Excellent problem-solving and communication skills
Ability to work independently and in a team environment\
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Type: Full-time
Salary: $40.33 per hour
Work Location: Remote",$40.33 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,2019,Less than $1 million (USD)
NLB Technology Services,#N/A,"Dallas, TX",CRM data Analytics Engineer,"MUST HAVE:
o Tableau CRMA & Einstein Discovery Certification
o Experience building Dashboards using CRMA (aka Tableau CRM and Einstein Analytics)
o Design, build, and support datasets, data recipes, dataflows, and dashboards using Salesforce and non-Salesforce data sources
o Troubleshoot functional, data, and performance issues with CRMA Dashboards
o Skilled in SAQL, JSON
o Develop and deploy AI/ML solutions using Einstein Discovery and deploy AI/ML Models built outside Salesforce
o Experience working with large data (1 B+ rows) and supporting large user base (10,000+)
o Mentor and coordinate the development efforts of other CRMA DevelopersAT&T
Job Type: Contract
Salary: $65.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Vision insurance
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
CRM ANalytics: 8 years (Required)
einstein: 1 year (Required)
Salesforce: 1 year (Required)
Work Location: One location",$70.00 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"E-Business International INC
3.6",3.6,"Alexandria, VA",AWS Data Engineer,"Role: AWS Data Engineers
Location: Alexandria, VA
Full-Time/Permanent
AWS Engineers
We are seeking TWO (2) AWS Engineers to join our growing team. The qualified applicants will become part of the Enterprise Data Analytics Services (EDAS) program for a large federal agency in Alexandria, VA. Hybrid work options are available.
The AWS Engineer is responsible for the development and support of cloud-based solutions. Solutions include moving assets from on-premise to cloud environment, evaluation, and optimization. AWS Engineer will collaborate with the team effectively to ensure the team's success.
Responsibilities/Duties:
Work on automating the migration process in AWS from development to production.
Create CFT deploying/updating cloud resources.
Advise the engineering and software engineering team as they migrate from on-premise to cloud infrastructure.
Optimize cloud workloads for cost, scalability, availability, governance, compliance, etc.
Guide and/or provide hands-on support to administer production, staging, and deployment environments.
Partner with multi-disciplinary teams to understand requirements and plan architecture and solutions.
Apply insight and expertise across AWS services.
Apply knowledge of scripting and automation using tools like PowerShell, Python, Bash, Ruby, Perl, etc.
Experience and Qualifications:
Good working knowledge of AWS Services, including Compute, Storage Networking, Database, Management tools, Security, Identity, and Compliance.
3 years’ experience in AWS (EC2, Glue, S3, Redshift, Python programming).
Experience implementing and debugging workloads in AWS.
Good knowledge of creating CFTs.
Experience building infrastructure inside AWS via code. Familiarity with tools such as Terraform or CloudFormation.
Ability to proactively communicate, interact, and solve critical customer problems.
Ability to think and act strategically and proactively.
3 years of experience in software development tools and methodologies.
3 years of experience in infrastructure architecture, database architecture, and networking.
3 years of experience architecting/deploying operating solutions built on AWS.
Education/Certifications:
Bachelor's Degree in Computer Science, Information Technology, Engineering, or associated discipline.
AWS certification is a plus!
Clearance Requirements:
Ability to attain and maintain a Public Trust.
Job Type: Full-time
Salary: From $100,000.00 per year
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Alexandria, VA 20598: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Expected Salary Range (Must)?
Visa Status/ Work Authorization (Must)?
Experience:
AWS: 4 years (Required)
Work Location: One location","$100,000 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,1992,Unknown / Non-Applicable
NAVA TECH LLC,#N/A,Remote,Azure Data Engineer,"Azure Data Engineer
Remote
Long Term
Note: Need 10+ years of experience
Sound skills and hands on experience with Azure Data Lake, Azure Data Factory, SQL Data Warehouse Azure Blob, Azure Storage Explorer
Proficient in creating Data Factory pipelines for on-cloud ETL processing; copy activity, custom Azure development etc.
Knowledge of Azure Data Catalog, Event Grid, Service Bus, SQL and Synapse
Experience using Azure Databricks platform.
Experience with Python programming.
Experience using pandas and numpy for data engineering and data cleansing;
Ability to troubleshoot the job scheduler and compute clusters.
Experience with databricks CLI and secrets module.
Experience with Azure Blob storage access configuration.
p l e a s e s h a r e r e s u m e s t o m o h a m m e d (a t) n a v a t e c h (d o t) u s O R 4 4 3 5 3 7 9 8 3 5
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$62.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"DiamondPick
4.5",4.5,Remote,GCP Data Engineer,"Position: GCP Data Engineer (8+years)
Location: Remote
Type: Contract
Job Details:
· Relevant Industry Work Experience (6+ for Developer, preferred 9+ years)
· Experience extracting data from a variety of sources, and a desire to expand those skills (Excellent knowledge in SQL and Spark is mandatory)
· Strong knowledge of Google BigQuery and architecting data pipelines from on-prem to GCP.
· Experience building applications using Google Cloud Platform related frameworks such as DataProc and GCS at the minimum.
· Excellent Communication Skills to Understand and Pass on Requirements.
· Excellent Data Analysis skills. Must be comfortable with querying and analyzing large amount of data on Hadoop HDFS using Hive and Spark.
· Professional experience with a cloud hosting platform (GCP preferred)
· GCP certification is preferred.
Job Type: Contract
Salary: Up to $65.00 per hour
Experience level:
10 years
8 years
9 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
Google BigQuery: 4 years (Preferred)
DataProc: 2 years (Preferred)
Google Cloud Platform: 9 years (Required)
Spark: 8 years (Required)
SQL: 8 years (Required)
License/Certification:
Google Cloud Platform(GCP) Certification (Required)
Work Location: Remote",$65.00 /hr (est.),1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,#N/A,Unknown / Non-Applicable
"Spectrum Communications & Consulting Inc.
4.3",4.3,"Chicago, IL",Data Engineer,"What do incubators and Spectrum have in common? Well, they’re great for growth, and even better for stability. As an innovative software development and digital marketing company pioneering the field of artificial intelligence, we can offer our newest Data Engineer the best of both words – a high energy, forward-thinking start-up culture, inside of a well-established, profitable, and stable structure . if you’re interested in getting your hands dirty and inciting change into a larger organization with a vision to change the world uses data today, then please read on.
Responsibilities
Beyond working with state of the art technology you will have many different fantastic projects to work on as a Data Engineer at Spectrum. Here are just a few different responsibilities you can expect off the bat:
Design, build and maintain both new and existing data infrastructures using tools including (but not limited to) Microsoft SQL Server, Azure Tables, Azure Blobs, and Azure Cosmos
Determine the right database system for a provided data set
Drafting database/table schemas
Properly index tables when applicable to ensure query performance
Create data pipelines for transforming new datasets into consistent, reliable data systems that are usable across multiple platforms
Identify optimizations and improvements in existing data pipelines
Create solutions for data validation to ensure data integrity, accuracy, and consistency
Ensure data is readily available for analysis and development usage
Shifting existing big data pipelines
Come up with data validation solutions for both initial data imports as well as results
Audit the automation process to make sure there are no gaps, redundancies, or faults in the data pipeline that would cause damage to data integrity
Planning query efficient data store schemas with respect to the data requirements of our internal data warehouse
Communicate with data scientists and other software engineers to build proper data pipelines for efficient and cost-effective querying
Develop, construct, test, and maintain architectures with the help and feedback of software engineers
Determine and implement proper data schemas that align with Spectrum’s and Spectrum’s client’s business objectives
Recommend and implement ways to optimize data reliability, efficiency, and quality
Consult and suggest different technologies for our team to test and utilize together
Incorporate the business objectives of key stakeholders in the architecture of our data warehouse
Research new and interesting data acquisition opportunities
Some Characteristics That Define You
We understand that as a Data Engineer for Spectrum, you have many different professional goals and personal interests. As such here are just a few different things that typically define our team members on the Data Science team:
Self-Starter. Building a data warehouse is no simple task. You will need to come in with a self-starter attitude to not only make this data engineering role what you want it to be, but make a stellar and efficient data warehouse while you’re at it.
Analytical. In order to solve problems and build innovative new digital marketing campaigns, it is essential that you know how to take an idea and analyze it from all of its angles.
Developer. In the ever changing world of artificial intelligence, it’s not enough just to build models. You bring to the table an eye for data patterns along with a knack for relational database engineering.
Patient. As a data engineer, you know that you work with extremely large data sets on a daily basis. As such we are looking for someone who is not only meticulous, but patient enough to sit and sift through that data in a thorough way.
Creative. Beyond just analyzing data sets, you are an explorer and a puzzle solver. Pulling insights out of your data and understanding how those insights can better shape our tools is something that you live to do.
Student. More so than most industries, the field of data science is always changing and evolving. As such, you are always looking to learn new things and gain new skills.
Required Skills and Experience
On top of the many intangible skills you bring to the table, there are many skills that can help improve the efficiencies and success of your work at Spectrum. Here are a few of those required skills and experience that you will come in with as a Data Engineer on our team:
A bachelor’s degree/pursuing a bachelor’s degree in computer science, mathematics, statistics, information systems, or a related field
Experience with statistical modeling
1-3 years working experience with Python and/or SAS languages
1-3 years working experience with SQL databases and database querying languages
1-3 years working experience with C#/.NET
Familiarity with Microsoft Azure
Familiarity with Graph database structures
Experience with both RDBMS and TDMS
Experience with data mining and data cleaning
Experience with data visualization and reporting techniques
Written and verbal expression
Benefits
As a Data Engineer at Spectrum there are a ton of fantastic perks and benefits that come along with your work. Here are just a few of the benefits you can expect when joining the Spectrum family:
Comprehensive medical & dental insurance
Retirement planning & company matching
Generous PTO, including sick days & holidays
Coworking space access for remote and hybrid work options
Year-round gym memberships
Paid continuing education
Hybrid work from Home , meetings a few times a month to strategize new projects
About Us
Our mission at Spectrum has always been the same— growth. Whether that be helping our customers find quality new business, developing and challenging our team members, or evolving our products and services with advancements in technology and best practices, we have always been looking towards bettering ourselves for the future. Now as we continue to grow, we find ourselves as not only the nation’s leading digital marketing and software provider for the home services industry, but an innovator and ground-shaker for the world of artificial intelligence as well. From marketing automation software powered by AI, to top notch digital marketing services via those same AI insights, we love what we do and are excited to continue to innovate for the future together.
INDSPCI
Responsibilities
Design, build and maintain both new and existing data infrastructures using tools including (but not limited to) Microsoft SQL Server, Azure Tables, Azure Blobs, and Azure Cosmos
Determine the right database system for a provided data set
Drafting database/table schemas
Properly index tables when applicable to ensure query performance
Create data pipelines for transforming new datasets into consistent, reliable data systems that are usable across multiple platforms
Identify optimizations and improvements in existing data pipelines
Create solutions for data validation to ensure data integrity, accuracy, and consistency
Ensure data is readily available for analysis and development usage
Shifting existing big data pipelines
Come up with data validation solutions for both initial data imports as well as results
Audit the automation process to make sure there are no gaps, redundancies, or faults in the data pipeline that would cause damage to data integrity
Beyond working with state of the art technology you will have many different fantastic projects to work on as a Data Engineer at Spectrum. Here are just a few different responsibilities you can expect off the bat:
Planning query efficient data store schemas with respect to the data requirements of our internal data warehouse
Communicate with data scientists and other software engineers to build proper data pipelines for efficient and cost-effective querying
Develop, construct, test, and maintain architectures with the help and feedback of software engineers
Determine and implement proper data schemas that align with Spectrum’s and Spectrum’s client’s business objectives
Recommend and implement ways to optimize data reliability, efficiency, and quality
Consult and suggest different technologies for our team to test and utilize together
Incorporate the business objectives of key stakeholders in the architecture of our data warehouse
Research new and interesting data acquisition opportunities
Some Characteristics That Define You
We understand that as a Data Engineer for Spectrum, you have many different professional goals and personal interests. As such here are just a few different things that typically define our team members on the Data Science team:
Self-Starter. Building a data warehouse is no simple task. You will need to come in with a self-starter attitude to not only make this data engineering role what you want it to be, but make a stellar and efficient data warehouse while you’re at it.
Analytical. In order to solve problems and build innovative new digital marketing campaigns, it is essential that you know how to take an idea and analyze it from all of its angles.
Developer. In the ever changing world of artificial intelligence, it’s not enough just to build models. You bring to the table an eye for data patterns along with a knack for relational database engineering.
Patient. As a data engineer, you know that you work with extremely large data sets on a daily basis. As such we are looking for someone who is not only meticulous, but patient enough to sit and sift through that data in a thorough way.
Creative. Beyond just analyzing data sets, you are an explorer and a puzzle solver. Pulling insights out of your data and understanding how those insights can better shape our tools is something that you live to do.
Student. More so than most industries, the field of data science is always changing and evolving. As such, you are always looking to learn new things and gain new skills.
Required Skills and Experience
On top of the many intangible skills you bring to the table, there are many skills that can help improve the efficiencies and success of your work at Spectrum. Here are a few of those required skills and experience that you will come in with as a Data Engineer on our team:
A bachelor’s degree/pursuing a bachelor’s degree in computer science, mathematics, statistics, information systems, or a related field
Experience with statistical modeling
1-3 years working experience with Python and/or SAS languages
1-3 years working experience with SQL databases and database querying languages
1-3 years working experience with C#/.NET
Familiarity with Microsoft Azure
Familiarity with Graph database structures
Experience with both RDBMS and TDMS
Experience with data mining and data cleaning
Experience with data visualization and reporting techniques
Written and verbal expression
Benefits
As a Data Engineer at Spectrum there are a ton of fantastic perks and benefits that come along with your work. Here are just a few of the benefits you can expect when joining the Spectrum family:
Comprehensive medical & dental insurance
Retirement planning & company matching
Generous PTO, including sick days & holidays
Coworking space access for remote and hybrid work options
Year-round gym memberships
Paid continuing education
Hybrid work from Home , meetings a few times a month to strategize new projects
About Us
Our mission at Spectrum has always been the same— growth. Whether that be helping our customers find quality new business, developing and challenging our team members, or evolving our products and services with advancements in technology and best practices, we have always been looking towards bettering ourselves for the future. Now as we continue to grow, we find ourselves as not only the nation’s leading digital marketing and software provider for the home services industry, but an innovator and ground-shaker for the world of artificial intelligence as well. From marketing automation software powered by AI, to top notch digital marketing services via those same AI insights, we love what we do and are excited to continue to innovate for the future together.
INDSPCI
Hours
40","$92,696 /yr (est.)",51 to 200 Employees,Company - Private,Media & Communication,Advertising & Public Relations,1992,$5 to $25 million (USD)
"JPMorgan Chase Bank, N.A.
3.8",3.8,"Wilmington, DE",Data Engineer III,"As a Data Engineer III under Consumer and Community Banking within the ADE Data Governance team at JPMorgan Chase, you serve as a seasoned member of an agile team to design and deliver trusted data collection, storage, access, and analytics solutions in a secure, stable, and scalable way. You are responsible for developing, testing, and maintaining critical data pipelines and architectures across multiple technical areas within various business functions in support of the firm's business objectives.

Job responsibilities
Supports review of controls to ensure sufficient protection of enterprise data
Responsible for advising and making custom configuration changes in one to two tools to generate a product at the business or customer request
Updates logical or physical data models based on new use cases
Frequently uses SQL and understands NoSQL databases and their niche in the marketplace
Adds to team culture of diversity, equity, inclusion, and respect

Required qualifications, capabilities, and skills
Experience across the data lifecycle
Advanced at SQL (e.g., joins and aggregations)
Working understanding of NoSQL databases
Proficiency with JSON/Java, Erwin, Oracle, Snowflake, AWS
Significant experience with statistical data analysis and ability to determine appropriate tools and data patterns to perform analysis, including data modeling
Experience customizing changes in a tool to generate product
Executes data architecture solutions and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions and break down problems
Produces data architecture and design artifacts for complex applications while being accountable for ensuring design constraints are met by software code development
Gathers, analyzes, synthesizes, and develops visualizations and reporting from large, diverse data sets in service of continuous improvement of data frameworks, applications, and systems that proactively identifies hidden problems and patterns in data and uses these insights to drive improvements to coding hygiene and system architecture

Preferred qualifications, capabilities, and skills
Familiarity with modern technologies
Exposure to cloud technologies
Chase is a leading financial services firm, helping nearly half of America's households and small businesses achieve their financial goals through a broad range of financial products. Our mission is to create engaged, lifelong relationships and put our customers at the heart of everything we do. We also help small businesses, nonprofits and cities grow, delivering solutions to solve all their financial needs.

We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.
The health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the ""WELL Health-Safety Rating"" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.
As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm's current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm's vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.
We offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, discretionary incentive compensation which may be awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.
Equal Opportunity Employer/Disability/Veterans
Base Pay/Salary
Jersey City,NJ $97,850.00 - $150,000.00 / year","$123,925 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Banking & Lending,1799,$10+ billion (USD)
i-Link Solutions Inc,#N/A,"Boston, MA",Data Engineer,"i-Link Solutions is seeking a Data Engineer to join our team in Hanscom, MA. The Data Engineer will be responsible for designing, developing, and maintaining the data infrastructure that supports the Kessel Run program. The ideal candidate will have experience with AWS, open-source tools, and data engineering best practices
Responsibilities
Design, develop, and maintain the data infrastructure that supports the Kessel Run program
Work with stakeholders to understand business requirements and translate them into technical solutions
Build and maintain data pipelines that ingest, transform, and load data into data warehouses and data lakes
Develop and deploy data models and algorithms to support data-driven decision making
Work with data scientists and analysts to develop and deploy data products
Ensure the security and compliance of data assets
Qualifications
Bachelor's degree in Computer Science, Information Technology, or a related field
10+ years of experience in data engineering
Experience with AWS, open-source tools, and data engineering best practices
Strong programming skills in Python, Java, Scala, or SQL
Experience with data modeling and data warehousing
Experience with data visualization and reporting
Strong analytical and problem-solving skills
Excellent communication and teamwork skills
US Citizenship
Secret Clearance
Kessel Run Experience Desirable
Job Types: Full-time, Permanent
Pay: $89,571.07 - $140,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Referral program
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Boston, MA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineering: 10 years (Required)
AWS: 3 years (Required)
Data warehouse: 4 years (Required)
Work Location: In person","$114,786 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Northeastern University
4.3",4.3,"Boston, MA",Data Engineer,"About the Opportunity
Do you love learning the shapes of datasets, and molding them into something new? The Digital Scholarship Group (DSG) in the Northeastern University Library is excited to open a search for a Data Engineer. Working within a warm and collaborative environment dedicated to social justice, the Data Engineer gathers, organizes, manipulates, transforms, and documents a variety of humanities research data. The Data Engineer works with colleagues across the university to create sustainable platforms and data for community-led digital scholarship.
The Data Engineer position is situated within the DSG, which is part of the Northeastern University Library. The Library is a vital partner in learning, teaching, and community-engaged research for a diverse R1 university. Northeastern is committed to intensive research and experiential learning for students at all levels.
The Digital Scholarship Group is based on Northeastern’s Boston campus. This position is eligible for a hybrid work arrangement. Specific arrangements can be negotiated at the time of hire.
Responsibilities
The Data Engineer has responsibility for helping DSG work with data in a wide range of formats, across multiple projects and often in unforeseen contexts. The Data Engineer develops data dictionaries, mappings between data standards, transformation routines, and other curatorial systems. This position also manages projects and engages in high-level needs analysis and project planning. The DSG is committed to digital approaches that consider the pedagogical, research, social, and ethical implications of data and its design and use.
Working closely with other DSG and library staff, faculty collaborators, and students, the Data Engineer contributes to grant-funded and internal projects including the Boston Research Center; the Civil Rights and Restorative Justice project; the Digital Archive of Indigenous Language Persistence; the TEI Archiving, Publishing, and Access Service; Digital Humanities Quarterly; and the Women Writers Project. They will also take the lead on building DSG’s policies and practices in working with external data platforms such as Wikidata and partner project APIs. To support this work, expertise with tools like regular expressions and OpenRefine, and facility with data including RDF, JSON, XML, various API responses, and other formats will be important.
We warmly invite people with various skills and levels of expertise to apply to this position. Candidates who meet some, but not all, of the qualifications listed below are strongly encouraged to apply. We seek colleagues who are committed to building an inclusive and diverse working environment and who have been and remain underrepresented or marginalized in the field of librarianship – including but not limited to people of color, LGBTQ+ people, individuals with disabilities and applicants from lower-income and first-generation library or academic backgrounds. We expect this position to be an ongoing learning experience and are committed to supporting professional development.
Qualifications
We realize that this is a lengthy list of activities and qualifications. There are multiple paths toward success in this position, and each may look somewhat different depending on the successful candidate’s interests and experience.
Bachelor’s degree required; Master’s degree or similar training in data science, information science, information design, or other relevant discipline preferred
Minimum of 2 years of experience working or studying in a data-intensive environment, preferably in an academic or non-profit research setting
Experience working with quantitative and qualitative datasets, especially with historical and cultural heritage data
Experience working with structured data formats (for instance, XML, RDF, JSON, CSV, relational databases) and with data conversion, data enhancement, and data analysis
Ability to write code to assist in carrying out these kinds of data-related work (for instance, using R, Python, SQL, SPARQL, XSLT, Perl, and/or regular expressions)
Ability to work on multiple concurrent projects and adapt to the evolving landscape of digital humanities
Collaborative problem-solving skills, and the ability to research and recommend solutions as part of a participatory design process
Commitment to thoughtful, adaptive engagement with the needs of community collaborators
Strong oral and written skills, ability to communicate across expertise levels and prepare project documentation
Desire and aptitude to grow skills (especially in technical areas) and learn new things
The following skills are desirable but are not all essential for applicants to possess at the outset; we can provide training:
Knowledge of metadata standards relevant to research data, such as the Data Documentation Initiative
Experience creating, manipulating, and querying linked open data
Experience in open-source development practices and workflows, preferably within an academic or non-profit environment
Experience working with databases, data management systems, and APIs
Experience with developing and leading workshops
Experience communicating complex ideas about data and how it is used to many audiences
Salary Range:
$82,725 - $93,000
About the Digital Scholarship Group
A recognized leader in the field, the Digital Scholarship Group supports digital modes of research, publication, and collaboration through applied research, systems and tools development, and consultative services. The DSG offers a friendly and closely collaborative work environment, and actively fosters the professional and intellectual development of all of our colleagues and collaborators, including training opportunities and mentorship.
Our team engages with faculty in the digital humanities and quantitative social sciences from across the university to develop digital research and teaching projects, organize events, plan grant-funded initiatives and provide training and mentorship. We also work in close partnership with Northeastern’s Archives and Special Collections, the NULab for Maps, Texts, and Networks, and with cultural heritage partners in Boston including the Massachusetts Historical Society and the Boston Public Library.
We develop tools and platforms for working with digital artifacts and data, for querying and publishing them. We also provide workshops, mentorship opportunities, and pedagogical frameworks to the Northeastern community. Some of our major projects include the Boston Research Center, the Civil Rights and Restorative Justice Project, and the Digital Archive of Indigenous Language Persistence, as well as a number of digital archiving projects from the Library’s Archives and Special Collections. In all of our projects, we are attentive to inclusive and anti-racist approaches to data modeling, platform development, and collaborative working processes.
About the Library
The Northeastern University Library supports the mission of the University by working in partnership with the University community to develop and disseminate new scholarship. The Library fosters intellectual and professional growth, enriches the research, teaching, and learning environment, and promotes the effective use of knowledge by managing and delivering information resources and services to library users.
Northeastern University is an equal opportunity employer, seeking to recruit and support a broadly diverse community of faculty and staff. Northeastern values and celebrates diversity in all its forms and strives to foster an inclusive culture built on respect that affirms inter-group relations and builds cohesion.
All qualified applicants are encouraged to apply and will receive consideration for employment without regard to race, religion, color, national origin, age, sex, sexual orientation, disability status, or any other characteristic protected by applicable law.
To learn more about Northeastern University’s commitment and support of diversity and inclusion, please see www.northeastern.edu/diversity.
About Northeastern
Founded in 1898, Northeastern is a global research university and the recognized leader in experience-driven lifelong learning. Our world-renowned experiential approach empowers our students, faculty, alumni, and partners to create impact far beyond the confines of discipline, degree, and campus.
Our locations—in Boston; Charlotte, North Carolina; London; Portland, Maine; San Francisco Bay area; Seattle; Silicon Valley; Toronto; Vancouver; and the Massachusetts communities of Burlington and Nahant—are nodes in our growing global university system. Through this network, we expand opportunities for flexible, student-centered learning and collaborative, solutions-focused research.
Northeastern’s comprehensive array of undergraduate and graduate programs— in a variety of on-campus and online formats—lead to degrees through the doctorate in nine colleges and schools. Among these, we offer more than 195 multi-discipline majors and degrees designed to prepare students for purposeful lives and careers.
The position will remain open until filled but application review will begin after June 16.
Position Type
Information Technology
Additional Information
Northeastern University considers factors such as candidate work experience, education and skills when extending an offer.
Northeastern has a comprehensive benefits package for benefit eligible employees. This includes medical, vision, dental, paid time off, tuition assistance, wellness & life, retirement- as well as commuting & transportation. Visit
https://hr.northeastern.edu/benefits/
for more information.
Northeastern University is an equal opportunity employer, seeking to recruit and support a broadly diverse community of faculty and staff. Northeastern values and celebrates diversity in all its forms and strives to foster an inclusive culture built on respect that affirms inter-group relations and builds cohesion.
All qualified applicants are encouraged to apply and will receive consideration for employment without regard to race, religion, color, national origin, age, sex, sexual orientation, disability status, or any other characteristic protected by applicable law.
To learn more about Northeastern University’s commitment and support of diversity and inclusion, please see
www.northeastern.edu/diversity
.","$87,863 /yr (est.)",1001 to 5000 Employees,College / University,Education,Colleges & Universities,1898,$100 to $500 million (USD)
"Lightcast
4.4",4.4,"Moscow, ID",Senior Data Engineer,"A Senior Data Engineer will architect big data analytical frameworks, and translate complex functional and technical requirements into detailed architecture design and high-performing software. A Senior Data Engineer will select data solution software and define hardware requirements. The position will be responsible for developing and implementing standards and processes for integration projects and initiatives.

Major Responsibilities:
Architect software applications, and test and build automated tools.
Select data solution software and define hardware requirements
Develop standards and processes for integration projects and initiatives.
Lead the design/development of software applications, testing, and building tools
Ensure database changes are reviewed and approved to standards
Lead and communicate to leadership on solution design
Provide technical assistance to junior members and to colleagues across the company
Skills & Abilities:
Analytical mind, personality and aptitude for working with data
Ability to look at the numbers, trends, and data to derive conclusions based on findings
Work closely with management to prioritize business and information needs
Proven ability to work effectively to meet goals and deadlines with minimal supervision
Highly creative problem-solving skills and an ability to tailor efforts based on the importance of the issue being addressed.
A hands-on, detail-oriented mindset with the ability to look beyond the box
Expert knowledge of object-oriented design, data structures, and algorithms
Demonstrated experience with agile or other rapid application development methods, object-oriented design, coding, testing patterns with a variety of languages
Significant knowledge of data structures, algorithms, data modeling and disaster recovery of data systems
Education & Experience:
Requires 5 years of data/software engineering/science experience
Bachelor's degree in computer science or other technical disciplines (masters preferred)
About Lightcast:
Lightcast is a global leader in labor market insights with headquarters in Moscow (ID) and Boston (MA) and offices in the United Kingdom, Europe, and India. We work with partners across six continents to help drive economic prosperity and mobility by providing the insights needed to build and develop our people, our institutions and companies, and our communities.

Lightcast is proud to be an equal opportunity workplace and is committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. Lightcast has always been, and always will be, committed to diversity, equity and inclusion. We seek dynamic professionals from all backgrounds to join our teams, and we encourage our employees to bring their authentic, original, and best selves to work.

#LI-SS1","$118,643 /yr (est.)",1 to 50 Employees,Company - Private,Government & Public Administration,State & Regional Agencies,2011,Unknown / Non-Applicable
"Guardsman Group
3.0",3.0,United States,Data Engineer,"A Little About Us
The Guardsman Group stands on over 40 years of experience, unmatched technical capabilities and the unwavering belief in the right of safety for all. Each and every day, our matchless range of services puts Guardsman in the lives of people in every corner of Jamaica and throughout the Caribbean. We've pioneered technologies and perfected procedures to give our customers the best solutions for their homes and businesses. As we enter another decade, we continue to be the industry leader. Today, Guardsman consists of 13 companies and over seven thousand talented staffers who are proud to call themselves a Guardsman.
The Role
This role is within the Business Performance, Analytics and Intelligence (BPAI) Unit of the Guardsman Group and is directly responsible for the shaping, building and implementation of solutions that satisfy the business intelligence needs across the Group.
What You'll Be Doing
Create and maintain optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build analytictools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics
Collaborate with other members of the BPAI unit to build and improve on the availability, integrity, accuracyand reliability of data pipelines
Work closely with all levels of management, IT department and other members of the BPAI Unit to achieve task objectives
A Little Bit About You
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases
Experience building and optimizing 'big data' data pipelines, architectures and data sets
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Strong analytic skills related to working with unstructured datasets
Build processes supporting data transformation, data structures, metadata, dependency and workload management
Minimum Qualifications
Bachelor's Degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline",#N/A,5001 to 10000 Employees,Company - Private,Management & Consulting,Security & Protective,#N/A,Unknown / Non-Applicable
"Contact Government Services, LLC
4.7",4.7,"Washington, DC",Data Engineer,"CGS is seeking a passionate and driven Data Engineer to support a rapidly growing Data Analytics and Business Intelligence platform focused on providing solutions that empower our Federal customers with the tools and capabilities needed to turn data into actionable insights. The ideal candidate is a critical thinker and perpetual learner; excited to gain exposure and build skillsets across a range of technologies while solving some of our clients’ toughest challenges. As a Data Engineer, you will be integral to data operations for the development and integration of multiple data types across a range of data sets and sources.

You will be responsible for the day-to-day operations of systems that depend on data, ensuring data is properly processed and securely transferred to its appropriate location, in a timely manner. Processing data will include managing, manipulating, storing and parsing data in a data pipeline for variety of target sources. You will also support maintenance of applications and tools that reside on these systems such as upgrades, patches, configuration changes, etc. The work is performed in a multidisciplinary team environment using agile methodologies. The candidate we seek must be highly motivated and enthusiastic about implementing new technologies and learning about new data in a small team environment where deadlines are important.
Responsibilities:
Complete development efforts across data pipeline to store, manage, store, and provision to data consumers
Being an active and collaborating member of an Agile/Scrum team and following all Agile/Scrum best practices
Write code to ensure the performance and reliability of data extraction and processing
Support continuous process automation for data ingest
Achieve technical excellence by advocating for and adhering to lean-agile engineering principles and practices such as API-first design, simple design, continuous integration, version control, and automated testing
Work with program management and engineers to implement and document complex and evolving requirements
Help cultivate an environment that promotes customer service excellence, innovation, collaboration, and teamwork
Collaborate with others as part of a cross-functional team that includes user experience researchers and designers, product managers, engineers, and other functional specialists
Required Skills:
Must be a US Citizen
Must be able to obtain a Public Trust Clearance
7+ years of IT experience including experience in design, management, and solutioning of large, complex data sets and models.
Experience with developing data pipelines from many sources from structured and unstructured data sets in a variety of formats
Proficiency developing ETL processes, and performing test and validation steps
Proficiency to manipulate data (Python, R, SQL, SAS)
Strong knowledge of big data analysis and storage tools and technologies
Strong understanding of the agile principles and ability to apply them
Strong understanding of the CI/CD pipelines and ability to apply them
Experience with relational database, such as, PostgreSQL
Work comfortably in version control systems, such as, Git Repositories
Contact Government Services strives to simplify and enhance government bureaucracy through the optimization of human, technical, and financial resources. We combine cutting edge technology with world class personnel to deliver customized solutions that fit specific needs and solve the most challenging, dynamic problems. Our team emphasizes technical expertise and an empathy to public service to provide unique top tier support for our government engagements.
SOME BENEFITS OF THIS POSITION INCLUDE:
Health, Dental, & Vision insurance
Life Insurance
401k
Flexible Spending Account (Health, Dependent Care, and Commuter)
Paid Time Off & Observance of State/Federal Holidays
For more information about CGS please visit: https://www.cgsfederal.com
1YGuQTHTUE","$103,515 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"VedaInfo Inc
4.1",4.1,"Phoenix, AZ",Big Data Engineer,"Hi,
I hope this note finds you well
I am reaching out to you regarding your career opportunity with us for “Big Data Engineer” for a “6+ Months Contract” @Phoenix, AZ
Please find the below requirement details.
Job Title: Big Data Engineer
Location: Phoenix, AZ (Onsite)
Duration: 6+ months
Rate: $60/hr C2C
Responsibilities:
Design, implement, and maintain big data systems handling large volumes of data.
Utilize Hadoop, Hive, and Spark for efficient data processing and analysis.
Collaborate with cross-functional teams to understand data requirements.
Develop and optimize data ingestion, storage, and transformation processes.
Build scalable data pipelines for seamless data processing and analysis.
Monitor and troubleshoot data processing and performance issues.
Stay updated with emerging big data technologies.
Requirements:
Bachelor's/Master's degree in CS, Engineering, or related field
7+ years of experience as a Big Data Engineer
Strong proficiency in Hadoop, Hive, and Spark
Extensive experience with AWS or Azure
Solid understanding of data ingestion, storage, and transformation
Excellent problem-solving and communication skills
Proactive and self-motivated
This is a 6+ month onsite contract position in Phoenix, AZ, with the possibility of extension. Compensation is $60/hr on C2C basis.
Please submit your updated resume highlighting relevant experience.
Thank you for your interest!
Thanks & Regards
Mohammed ZAIN
Job Type: Contract
Pay: $50.00 - $60.00 per hour
Experience level:
7 years
Ability to commute/relocate:
Phoenix, AZ: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
To improve your chances of getting Shortlisted, make sure to tailor your Profile to the Job Requirements, Highlight your Relevant Skills and Experience, and then upload your Resume.
Experience:
AWS (Amazon Web Services): 7 years (Required)
Azure: 7 years (Required)
Big data: 7 years (Required)
Hadoop: 7 years (Required)
Apache Hive: 7 years (Required)
Spark: 7 years (Required)
Work Location: One location",$55.00 /hr (est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$1 to $5 million (USD)
"MARVEL TECHNOLOGIES INC
3.7",3.7,"Cincinnati, OH",Data Engineer,"Job Description
The data engineer designs and builds platforms tools and solutions that help the bank manage secure and generate value from its data. The person in this role creates scalable and reusable solutions for gathering collecting storing processing and serving data on both small and very large i.e. Big Data scales. These solutions can include on-premise and cloud-based data platforms and solutions in any of the following domains ETL business intelligence analytics persistence relational NoSQL data lakes search messaging data warehousing stream processing and machine learning.
Responsible and accountable for risk by openly exchanging ideas and opinions elevating concerns and personally following policies and procedures as defined. Accountable for always doing the right thing for customers and colleagues and ensures that actions and behaviors drive a positive customer experience. While operating within the Bank's risk appetite achieves results by consistently identifying assessing managing monitoring and reporting risks of all types.
ESSENTIAL DUTIES AND RESPONSIBILITIES:
Responsible for design Development and Support of data solutions APIs tools and processes to enable rapid delivery of business capabilities.
Work closely with IT application teams Enterprise architecture infrastructure information security and LOB stakeholders to translate business and technical strategies into data-driven solutions for the Bank.
Act as a technical Expert addressing problems related to system and application design performance integration security etc.
Conduct research and Development based on current trends and technologies related to the banking industry data engineering and architecture data security and related topics.
Work with developers to Build CI/CD pipelines Self-service Build tools and automated deployment processes.
Evaluate software products and Provide documented recommendations as needed.
Provide Support and troubleshooting for data platforms. Must be willing to Provide escalated on-call Support for complicated and/or critical incidents.
Participate in the planning process for hardware and software.
Plan and work on internal projects as needed including legacy system replacement Monitoring and analytics improvements tool Development and technical documentation.
Provide technical guidance and mentoring for other team members.
Manage and prioritize multiple assignments.
MINIMUM KNOWLEDGE SKILLS AND ABILITIES REQUIRED:
Bachelor's degree in Computer Science/Information Systems or equivalent combination of education and experience.
Must be able to communicate ideas both verbally and in writing to management business and IT sponsors and technical resources in language that is appropriate for each group.
Fundamental understanding of distributed computing principles
Knowledge of application and data security concepts best practices and common vulnerabilities.
Conceptual understanding of one or more of the following disciplines preferred big data technologies and distributions metadata management products commercial ETL tools Bi and reporting tools messaging systems data warehousing Java language and run time environment major version control systems continuous integration/delivery tools infrastructure automation and virtualization tools major cloud or rest API design and development.
Job Types: Full-time, Contract
Pay: $60.00 - $62.00 per hour
Compensation package:
Hourly pay
Yearly pay
Experience level:
9 years
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",$61.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,#N/A,$5 to $25 million (USD)
"Vedainfo
4.1",4.1,"Peoria, IL",Data Engineer,"Job Description
· Create and develop optimal data pipeline which fetches data from raw and apply business transformation and load derived.
· Build the AWS Fargate components required for optimal extraction, transformation, and loading of data-to-data lake.
· Develop data pipeline setup between base and derive values based on Caterpillar specification.
· Implement quality validation rules and follow process defined by stakeholder.
· Develop business events as defined by the solution team to generate CDC events for consumption layer (Kinesis)
· Create re-usable and configurable components wherever applicable.
· Create and execute unit, integration, and regression tests
· Ensure the code is developed satisfying all business rules, perform unit testing where applicable.
· Data pipelines development
· Python Development
· AWS cloud services
· Data quality validation rules
· Java (preferred)
· Implement CDC
· Transformation rules creation
· Message broker tools like Kinesis
· Azure DevOps – CI/CD pipelines
· Automated and Manual Testing experience
Job Type: Contract
Pay: Up to $60.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Peoria, IL 61602: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
This position is remote initially and you will be asked to work any of the client location once offer is made. Please confirm your current location.
Experience:
Data Engineer: 1 year (Preferred)
Python Development: 1 year (Preferred)
AWS: 1 year (Preferred)
Message broker tools: 1 year (Preferred)
Work Location: In person",$60.00 /hr (est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$1 to $5 million (USD)
META FORCE IT LLC,#N/A,"Cincinnati, OH",Data Engineer,"Job Description
The data engineer designs and builds platforms tools and solutions that help the bank manage secure and generate value from its data. The person in this role creates scalable and reusable solutions for gathering collecting storing processing and serving data on both small and very large i.e. Big Data scales. These solutions can include on-premise and cloud-based data platforms and solutions in any of the following domains ETL business intelligence analytics persistence relational NoSQL data lakes search messaging data warehousing stream processing and machine learning.
Responsible and accountable for risk by openly exchanging ideas and opinions elevating concerns and personally following policies and procedures as defined. Accountable for always doing the right thing for customers and colleagues and ensures that actions and behaviors drive a positive customer experience. While operating within the Bank's risk appetite achieves results by consistently identifying assessing managing monitoring and reporting risks of all types.
ESSENTIAL DUTIES AND RESPONSIBILITIES:
Responsible for design Development and Support of data solutions APIs tools and processes to enable rapid delivery of business capabilities.
Work closely with IT application teams Enterprise architecture infrastructure information security and LOB stakeholders to translate business and technical strategies into data-driven solutions for the Bank.
Act as a technical Expert addressing problems related to system and application design performance integration security etc.
Conduct research and Development based on current trends and technologies related to the banking industry data engineering and architecture data security and related topics.
Work with developers to Build CI/CD pipelines Self-service Build tools and automated deployment processes.
Evaluate software products and Provide documented recommendations as needed.
Provide Support and troubleshooting for data platforms. Must be willing to Provide escalated on-call Support for complicated and/or critical incidents.
Participate in the planning process for hardware and software.
Plan and work on internal projects as needed including legacy system replacement Monitoring and analytics improvements tool Development and technical documentation.
Provide technical guidance and mentoring for other team members.
Manage and prioritize multiple assignments.
MINIMUM KNOWLEDGE SKILLS AND ABILITIES REQUIRED:
Bachelor's degree in Computer Science/Information Systems or equivalent combination of education and experience.
Must be able to communicate ideas both verbally and in writing to management business and IT sponsors and technical resources in language that is appropriate for each group.
Fundamental understanding of distributed computing principles
Knowledge of application and data security concepts best practices and common vulnerabilities.
Conceptual understanding of one or more of the following disciplines preferred big data technologies and distributions metadata management products commercial ETL tools Bi and reporting tools messaging systems data warehousing Java language and run time environment major version control systems continuous integration/delivery tools infrastructure automation and virtualization tools major cloud or rest API design and development.
Job Type: Contract
Pay: $45.96 - $50.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Cincinnati, OH 45215: Reliably commute or planning to relocate before starting work (Required)
Experience:
data engineer: 10 years (Preferred)
Big data: 3 years (Preferred)
Work Location: Hybrid remote in Cincinnati, OH 45215",$47.98 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Myticas Consulting
3.9",3.9,Arizona,BHJOB15656_20058 - Data Engineer,"Myticas's direct client based out of Phoenix, AZ is currently seeking a Data Engineer for a 100% remote contract position.
Pay Rate: Up to $34/hr

Job Description:
A Data Engineer can confidently understand, write, troubleshoot, optimize, and peer review SQL code. They understand business requirements and create code and reporting that exceeds specifications through anticipating underlying needs. They also understand relational database design, table structure, data types, and data models. They are experienced with and confident in development in reporting visualizations in Excel, PowerBI, Tableau, QuickSight, SSRS, or other data products. They can create reporting dashboards and products that inform the business. They easily grasp and retain complex data concepts and are able to explain them to others. Looking for engineers who are driven to innovate, use cutting edge technologies to solve problems and think outside the box.
What You’ll Do:
Digital Transformation into AWS
Data Warehouse related work (SQL, ETL, Replication, Databases)
Understand, write, troubleshoot, optimize, and peer review SQL code.
Understand business requirements and create code and reporting that exceeds specifications through anticipating underlying needs.
Understand relational database design, table structure, data types, and data models. 6.Experienced with and confidence in development in reporting visualizations in Excel, PowerBI, Tableau, QuickSight, SSRS, or other data products.
Data Warehouse related work (SQL, ETL, Replication, Databases
Perform other duties as assigned or apparent.
MINIMUM EDUCATION AND RELATED WORK EXPERIENCE:
Bachelor's degree in a Computer Science field and 2 years' work experience
2 years of experience in most phases of IT systems deployments in one or more of the following areas: design and deployment of cloud services, data migration to AWS cloud, Digital Transformation into AWS, and Data Warehouse related work
INDCHI
INDREM",$34.00 /hr (est.),1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
Sconcept,#N/A,Remote,Azure Data Engineer,"** W2 LONG TERM CONTRACT BASED....**(NO C2C)
JOB TYPE:- Long Term Contract(6 months)Extendable
JOB LOCATION:-USA
About Sconcept LLC:-Software Concepts Consulting was established in 2008. With our “hands-on” approach we are very successful in delivering high performance, secure IT solutions and IT services. With headquarters in Carrollton, Texas we are specialized in providing business solutions to the Fortune 500 clients. Our clients include Software, Banking, Financial, Healthcare, Retail, Consumer goods and Insurance sectors. We deliver candidates based on client requirements.
Required Skills:-
Must have worked on several (5+) projects building solutions for Data Pipelines, Data warehousing, Data Modelling on Azure Data Platform (AWS, Google Cloud Platform - good to have)
Expertise in design and development of Data pipelines and ETL using ADF, Databricks to move data from relational/structured/unstructured data from source to data lake to data warehouse, on Azure
Creating Pipelines and integrations of multiple data sources
Data processing and transformation using Databricks (Scala/Python)
Expertise in optimizing cost and performance of data pipelines
Experience working with Data Lake, Synapse on Azure
Designing Data Lake storage layer (Landing/staging, raw, trusted/curated zones)
Designing data storage DB schema, data models and processing on Azure Synapse DW
Experience with Synapse Spark Pool
Experience in Data Modelling
Building Data Models from structured and semi-structured data (Data Model, Common Data Model etc.)
Performance optimization of data model for high performance analytical and reporting workloads
Expertise with SQL Server and extensive experience with SQL programming
Programming and optimizing DB objects Views, Stored Procs, Functions
Good experience in defining data access policy and security for azure data services
Experience with Oracle EDW
Knowledge/Experience migrating from Oracle EDW to any cloud DW
Experience integrating AI skills/ML models with Data and reporting solutions
Experience in building API layer for downstream consumption
Job Type: Contract
Pay: $45.00 - $50.00 per hour
Benefits:
Health insurance
Compensation package:
Bonus pay
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$47.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"ProGrad
4.2",4.2,Remote,Data Engineer,"We are looking for data engineers that are excited about building and maintaining data pipelines for a variety of interests. The pipelines will ingest data from a variety of sources ranging from internal change data streams to external integrations with various SaaS platforms through webhooks and events. The ideal candidate is self driven and looking to grow as a data engineer while having an outsized impact on the future of data at the company. A love for database architecture and optimisation is a must as is the ability to work across diverse teams.
Requirements/Qualifications:
Bachelor's or Master's degree in Data Science (specialization)
Software development experience coding in a general-purpose programming language such as C, C++, Java, JavaScript, Golang, or Python
Experience working with data structures or algorithms (i. e., data structures /algorithms class, coursework/projects, research, internships, or other practical experience in/outside of school or work (including open source hobby coding)
Proficiency in SQL, and NoSQL Databases
Experience working with large datasets
Experience writing and debugging complex SQL queries
A strong understanding of cloud database best practices and performance optimisation
Experience building and maintaining data pipelines, coupled with knowledge of good programming practices such as unit and integration testing
. A strong understanding of cloud database best practices and performance optimisation
Good to have skills:
. Experience building and maintaining data pipelines, coupled with knowledge of good programming practices such as unit and integration testing
Job Types: Full-time, Permanent
Salary: Up to $1,200,000.00 per year
Experience level:
1 year
Under 1 year
Schedule:
Day shift
Work Location: Remote
Speak with the employer
+91 9057202434",#N/A,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Enterprise Minds,#N/A,Remote,Data Engineer,"Job Role: SQL Data Engineer
Location: Remote Opportunity - Need to work in PST hours
Visa: US Citizen Only
Duration: Long term
Roles and Responsibilities:
3+ years of experience with a complex build system in AWS using Starburst.
5+ years of experience with installation, maintenance, and administration of Oracle databases- PL/SQL and ANSI SQL
Oracle DBA + Oracle SQL to ANSI SQL conversion, performance/query tuning
3+ years of experience configuring, integrating, and securing multiple AWS database offerings (ex. RDS, Aurora, Casandra, and Dynamo)
2+ years of experience with supporting Linux systems engineering efforts in system design and evaluation, solution engineering, software development, or system administration.
Experience with Database Schema as Code tools
Ability to design and develop complex SQL queries and reports to retrieve required data.
Experience with database partitioning and microservices
Knowledge of Agile methodologies or the software development life cycle (SDLC)
US Citizenship is Mandatory.
Job Type: Full-time
Experience:
Data Engineer: 10 years (Preferred)
AWS using Starburst: 4 years (Preferred)
PL/SQL and ANSI SQL: 2 years (Preferred)
AWS RDS: 1 year (Preferred)
Work Location: Remote",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Stefanini, Inc
3.8",3.8,"Dearborn, MI",Data Engineer,"Stefanini Group is hiring!
Stefanini is looking for Data Engineer at Dearborn, MI (Hybrid)
For quick apply, please reach out to Rajat Baloria
Phone: (248) 728-2620
Email: Rajatsingh.Baloria@stefanini.com

Open to W2 candidates only!

Position Description:
Looking for a Software Engineer focused on delivering software leveraging Java and Python based on proven Lean/Agile methods. Knowledge of Big Data technologies like Hadoop and Spark is a plus.
The Software Engineer will work in a small, cross-functional, and co-located team. The Software Engineer will collaborate directly and continuously with business partners, product managers and designers, and will release early and often.
Position Responsibilities:
Work hands-on with the team and other stakeholders to deliver quality software products that meet our customer's requirements and needs.
Grow technical capabilities / expertise and provide guidance to other members on the team



Skills Required:
Exceptional software engineering knowledge; OO Design Principles
Basic understanding of Big Data and potential use cases
Strong desire to learn new skills and apply to solve business problems/opportunities “Spring Boot, Java, Angular, API, Micro Services, PCF, GCP (especially Cloud Run)”

Experience Required:
Overall 6 years of work experience in delivering customer facing products
Minimum 2 years of strong development experience in at least one of the following technologies: Java or Python on a Hadoop/Spark Platform

Education Required:
An associate's degree in Computer Science or similar technical discipline
Education Preferred:
Google Cloud Certification

Listed Salary Range may vary based on experience, qualifications, and local market, Also some positions may Include bonuses and other Incentives

About Stefanini Group
The Stefanini Group is a global provider of offshore, onshore and near shore outsourcing, IT digital consulting, systems integration, application and strategic staffing services to Fortune 1000 enterprises around the world. Our presence is in countries like Americas, Europe, Africa and Asia, and more than 400 clients across a broad spectrum of markets, including financial services, manufacturing, telecommunications, chemical services, technology, public sector, and utilities. Stefanini is a CMM level 5, IT consulting, company with global presence. We are CMM Level 5 company.",$82.00 /hr (est.),10000+ Employees,Subsidiary or Business Segment,Information Technology,Information Technology Support Services,1987,$1 to $5 billion (USD)
"Maven Workforce
4.1",4.1,"Alpharetta, GA",Data Engineer,"Must Have skills:
Building Data Pipeline exp is a must
ETL Tools – SSIS, Alteryx
Data modelling
SQL Server development
Microsoft SQL Stack
Responsibilities:
Data acquisition and ingestion - Identify data sources and build pipelines using various ETL tools such as but not limited to, SSIS, and Alteryx. Data sources including but not limited to SQL Server, SAP, Teradata, Hadoop\Hive, PostgreSQL, Oracle and flat files.
Identifying ways to improve data reliability, efficiency and quality by various data solution techniques.
Expertise in Data project management with JIRA stories. Work as a liaison between business and IT to ensure successful and timely completion of the projects.
Assist in defining the data architecture framework, standards and principles, including modeling, metadata, security and reference data.
Create and optimize data models to support various business applications.
Reviewing modifications of existing data systems for cross-compatibility.
Automate and support workflows to ensure timely delivery.
Must Have:
5+ years of SQL Server development experience.
3+ years ALTERYX Admin /User management experience and advance workflow management.
5+ years data modeling experience.
5+ years of ETL experience.
5+ years of experience in working on more than one database technologies Microsoft SQL server, Teradata.
2+ years big data experience, Hadoop, Hive, Spark
Expert knowledge of data warehousing.
DESIRED SKILLS:
BI Lifecycle management
Working understanding of Microsoft VBA, HTML, Python
Data model development using ERWIN
Job Type: Contract
Salary: $50.00 per hour
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Building Data Pipeline: 8 years (Required)
ETL Tools – SSIS, Alteryx: 7 years (Required)
Data modelling: 6 years (Required)
SQL Server development: 8 years (Required)
Microsoft SQL Stack: 8 years (Required)
Work Location: One location
Speak with the employer
+91 7328456015",$50.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2008,Unknown / Non-Applicable
Zllius Inc.,#N/A,"Canton, MI",Data Engineer AUG76,"Hi,
Greetings from Zllius Inc!
We have an urgent opening for the roleData Engineer. If interested, revert me back with your Updated resume, along with contact details, work authorization.
Job Role: Data Engineer
Location: Hartford, CT ( Initially Remote )
Duration: Long-term
Position: W2/1099
Visa: Any (Except H1B)
Job Ref: AUG76
Job Description:
Good experience on designing and developing data pipelines for data ingestion and transformation using Spark.
Distributed computing experience using Pyspark.
Good understanding of spark framework and spark architecture.
Experience working in Cloud based big data infrastructure.
Excellent in trouble shooting the performance and data skew issues.
Must have good understanding of spark run time metrics and tune applications based on metrics.
Deep knowledge in partitioning, bucketing concepts of data ingestion.
Good understanding of AWS services like Glue, Athena, S3, Lambda, Cloud formation.
Preferred working knowledge on the implementation of datalake ETL using AWS glue, Databricks etc.
Experience with data modelling techniques for cloud data stores and on prem databases like Teradata, Teradata Vantage (TDV) etc.
Preferred working experience in ETL development in Teradata vantage and data migration from on prem to Teradata vantage.
Proficiency in SQL, relational and non-relational databases, query optimization and data modelling.
Experience with source code control systems like Gitlab.
Experience with large scale distributed relational and NoSQL database systems.
Experience : 9+years
Thanks & Regards:
Zllius Inc.
844 495 5487
Job Types: Full-time, Contract
Schedule:
8 hour shift
Work Location: In person","$86,440 /yr (est.)",Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Fresh Consulting
3.9",3.9,"Menlo Park, CA",Data Engineer,"Fresh Consulting is a design-led, software development and hardware engineering company, offering end-to-end digital services to help companies innovate. We bring together amazing UX designers, sophisticated developers, digital strategists, and top-notch engineers to help companies create fresh experiences that connect humans, systems, and machines. We’ve been growing fast and need someone to help us continue to manage the delivery of high-quality work in a fast-paced environment.
See more at freshconsulting.com Visit freshconsulting.com/portfolio to see our project work across several industries.
View and apply to all jobs - https://freshconsulting.applytojob.com/apply/ or visit freshconsulting.com/careers
Title: Data Engineer
Duration: 6 months with possible extension
Location: Onsite Menlo Park, CA
Benefits: Employee benefits at 100% including Medical, PTO, Holiday Pay, 401K Plan, and much more!
Hours: Minimum 40 Hours/Week
Role:
Design, construct, install, test, and maintain highly scalable data management systems.
Ensure systems meet business requirements and industry practices.
Design, implement, automate, and maintain large-scale enterprise data ETL processes.
Build high-performance algorithms, prototypes, predictive models, and proof of concepts.
Skills:
5+ years of work experience as a Data Engineer.
Ability to work as part of a team, as well as work independently or with minimal direction.
Excellent written, presentation, and verbal communication skills.
Collaborate with data architects, modelers, and IT team members on project goals.
Strong PC skills including knowledge of Microsoft SharePoint.
Process certification, such as Six Sigma, CBPP, BPM, ISO 20000, ITIL, and CMMI.
Education: BSCSE or related.
FRESH-
Work on engineering and research assignments with F500 companies and startups.
The relationships that we have created with our clients are one of a kind.
We help solve problems in many technologies focusing on R&D, product development, and manufacturing.
We work with the most cutting-edge and latest technologies from AR/VR to Autonomous technologies.
Closely working with our clients, we believe that long-term investments are extremely important to maintain the culture we together have created.
We’re a handpicked team of Engineers, digital strategists, designers, and developers united together in creating a fresh experience. Whether we are strategizing, designing, developing, or analyzing, our integrated team works as an extension of yours to improve your impact, your usability, and your customer conversion. In the process, we collaborate with you to get to know your business, understand your industry, and incorporate your big ideas into memorable experiences that keep your customers coming back for more.
Equal employment opportunity: All qualified persons will be considered for employment without regard to race, color, religion, sex, national origin, age, marital status, familial status, gender identity, sexual orientation, disability for which a reasonable accommodation can be made or any other status protected by law. Assistance will be gladly provided upon request for any applicant with sensory or non-sensory disabilities.
Fresh Consulting is a participating E-Verify company.
freshconsulting.com
Compensation offered will be determined by factors such as location, level, job-related knowledge, skills, and experience. Range $70/hr - $80/hr.
ylM5WIC1Wr",$75.00 /hr (est.),201 to 500 Employees,Company - Private,Management & Consulting,Business Consulting,2007,$25 to $100 million (USD)
"Wallero
4.4",4.4,"Seattle, WA",Data Engineer,"Hi,
Pretty much interested in your resume, we have an urgent opening for Data Engineer role with one of our clients based in Seattle WA.
Position: Data Engineer
Location: Seattle WA - Remote (100%)
Description:
Good Experience in reporting space including PowerBI, ADF, and possibly standing up a data warehouse/data mart.
The main scope centering is around reporting.
Experience with SQL, and agile framework including requirement gathering and story/feature writing skills.
Need someone strong who is comfortable leading and guiding teams in that capacity.
Migrating data from third party to Azure Cloud
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Seattle, WA 98101: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",$75.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2015,$1 to $5 million (USD)
"Deako
4.9",4.9,"Seattle, WA",Data Engineer II,"About Deako
Deako is delivering a revolutionary plug-n-play smart home platform with a laser focus on the untapped new home construction market. We make smart lighting so easy to upgrade that even those who would have never considered smart lighting are making it happen. We've built a company based on trust; where forming personal relationships is key to our success.
Software at Deako
The software team is a tight-knit team of smart, dedicated people. We are customer focused, we ship often, and are constantly asking “why.” We are passionate about the software we write. We work closely with other Deako teams. Developers at Deako are always looking for a better way and understand that no code is perfect.
Data at Deako
We believe that Data and Software go hand in hand. That's why the Data and Software teams work closely together to ensure we're collecting the right data, and getting it to the right people. Data is new at Deako, and we are working to make it a cornerstone of our company. We want every team at Deako to have a data-driven mindset.
Day to Day Expectations:
Participate in the code review process in our DBT repository
Build maintainable, testable SQL queries for various dashboards
Help maintain, contribute to and improve our data infrastructure and existing pipelines
Help maintain and improve our CI/CD pipelines
Responsible for obtaining, cleaning, and munging data and getting it into a form that our data analysts can access and analyze
Participate in meetings with stakeholders around data collection and cleaning

Must Have:
Demonstrable knowledge in SQL + Some Programming Language (Typescript, Python, Ect)
Demonstrates a high level autonomy and willingness to learn
1+ years of experience working as a Data Analyst/Engineer or Comparable field.
Experience with git, dbt
Nice to Have:
Experience in Snowflake, Fivetran, Hightouch
Experience with Gitlab CI
Experience with geospatial data
Stock Options
Hybrid work environment (Office + Remote)
Medical/Dental/Vision/Life/401K
Unlimited PTO
Free Snacks/Coffee/Drinks (Non-Alcoholic and Alcoholic)
Quarterly Company Parties","$115,000 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,2015,$5 to $25 million (USD)
shreetek,#N/A,"Charlotte, NC",Senior Data Engineer,"Job Title : Data Engineer
ONLY USC
On W2 only
Job Location : Newjersey OR Charlotte,NC
Collect, manage, and convert raw data accurately and reliably
Organize data systems for subgroup access and analyses
Configure and sustain data cloud structures
Able to work with structured and unstructured data.
Validate outputs of data pipelines
Degree in Data Engineering preferred.
Two Programmers –
Convert existing SAS code to python/pyspark code for model operation in the cloud.
Create and sustain policy analysis models in the cloud.
Troubleshoot user interfaces in the cloud.
Create and sustain intuitive user interfaces in the cloud.
A degree in Computer Science is preferred.
Overall across resources keep following stack as reference
Visualization : Tableau
Data Modeling/Science : Python / SAS
Engg : AWS cloud native services, security, data pipeline
Job Type: Contract
Salary: $50.00 - $60.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Charlotte, NC 28202: Reliably commute or planning to relocate before starting work (Required)
Work Location: One location",$55.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Orange County's Credit Union
4.0",4.0,"Santa Ana, CA",Data Engineer,"Great Opportunity At Orange County's Credit Union
Must reside in the state of CA, AZ, NV or TX.
Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk.
Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture.
Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential.
Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY!
More about Orange County's Credit Union:
Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace.
As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service.
Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference!
Overview:
Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization.
Essential Functions:
Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making.
Translate business requirements to technical solutions by applying technical knowledge and strong business acumen.
Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services.
Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform.
Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc).
Technical Must Haves for this Role:
5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment.
3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases).
3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.).
2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores.
2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS).
Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.

The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.

We perform thorough background checks and credit checks. EOE.","$92,145 /yr (est.)",201 to 500 Employees,Nonprofit Organization,Financial Services,Banking & Lending,1938,$25 to $100 million (USD)
"Fincons Group
4.0",4.0,Remote,Data Engineer – Databricks / Kafka / SQL,"Fincons Group is an IT business consulting company that has been designing the digital future of leading companies on international markets for 40 years. Fincons Group built its reputation on foreseeing and interpreting new business models and the rapid evolution of IT systems by building a complete range of services: from research and consulting to design and development, from system integration of leading vendor software solutions right up to application management, supporting clients step by step along their digital transformation journey. Fincons is a multinational with over 2,600 people and several offices worldwide (in Italy, Switzerland, Germany, France, the United Kingdom, Belgium and the USA), but above all a Family Company where the founders play a strategic role with commitment and passion, grounding the company in the same principles of a united and caring family.
We firmly believe in the value of cooperation and in the contribution that every idea and intuition can bring. We believe that everyone’s effort can make a difference. Transforming knowledge into a strategic resource is our company mission, and we pursue it with the help of our most strategic asset: our people.

Job Summary
Looking for an experienced data engineer with hands on experience in developing solutions based on Databricks. If you are a curious team-player with passion to learn and working for different clients to develop modern data streaming platforms, please reach out to us!

Work location: any US location might be considered
Employment type: freelance / contractor
Start date: about July 1th
Duration: about 6 months with possible renewal

Responsibilities and Duties
Design, develop and implement real time data ingestion pipelines from multiple sources using Databricks
Develop high-quality, scalable and re-usable frameworks for ingesting high volume and large data sets
Apply best practices to Integrate and ingest various big data formats like Protobuf, Parquet, AVRO and JSON
Work with event based / streaming technologies like Kafka, or Kinesis, to ingest and transform the data using KafkaStreams or KSQL
Working with other members of the project team to support delivery of additional project components (API interfaces, Search)
Work to load test the pipelines for quality and better performance using JMeter, or Gatling etc

Qualifications and skills
Bachelor’s degree and first experience designing, developing, deploying and/or supporting data pipelines using Databricks
Experience in designing and implementing Medallion architecture is preferred
Expertise in designing and deploying data applications on cloud solutions, such as Azure or AWS
Hands on experience in performance tuning and optimizing code running in Databricks environment
Proficient in programming languages like Pyspark and Python
Good understanding of SQL, T-SQL and/or PL/SQL
Demonstrated analytical and problem-solving skills particularly those that apply to a big data environment
Experience with agile development methodologies.
Ability to work effectively both independently and in a group or team environment

Authorization to Work
Applicants for employment in the US must be authorized to work in the US. Fincons Group will not sponsor applicants for work visas.
All qualified applicants will receive consideration for employment without regard to race, colour, religion, sex, sexual orientation, gender identity, national origin, age, disability, or protected veteran status, or any other legally protected basis, in accordance with applicable law.
The data will be processed and stored exclusively for the purposes of this or future selections, guaranteeing the rights referred to in art. 13 Legislative Decree 196/03 and EU regulation 679/2016 (GDPR)
2yrKJdtT13",#N/A,201 to 500 Employees,Company - Private,Financial Services,Accounting & Tax,#N/A,Unknown / Non-Applicable
SecurePro,#N/A,"Arlington, VA",Data Engineer,"Collect, manage, and convert raw data accurately and reliably
Organize data systems for subgroup access and analyses
Configure and sustain data cloud structures
Able to work with structured and unstructured data.
Validate outputs of data pipelines
Convert existing SAS code to python/pyspark code for model operation in the cloud
Create and sustain policy analysis models in the cloud
Troubleshoot user interfaces in the cloud
Create and sustain intuitive user interfaces in the cloud
Degree in Data Engineering preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $100.00 per hour
Experience level:
3 years
Schedule:
8 hour shift
Application Question(s):
Are you a US citizen?
Must have DOD secret Clearance?
Work Location: Remote",$90.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Amazee Global Ventures Inc
5.0",5.0,Remote,Data Engineer,"(W2 ONLY)
Job Title: Data Engineer
Duration : 12 months
Location: Remote
6-7 years of experience
Healthcare experience is must
Design, develop, and maintain Tableau dashboards and reports
Develop and maintain data models and ETL processes
Analyze data to identify trends and patterns
Create and maintain data dictionaries and data models
Develop and maintain data pipelines
Collaborate with stakeholders to understand data requirements
Develop and maintain data quality processes
Monitor data quality and performance
Troubleshoot data issues
Develop and maintain documentation
Requirements:
Bachelor’s degree in Computer Science, Information Systems, or related field
4+ years of experience in data engineering and analysis
Expertise in Tableau
Experience with SQL and data modeling
Knowledge of ETL processes
Knowledge of data warehousing
Excellent problem-solving and communication skills
Ability to work independently and in a team environment\
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Type: Full-time
Salary: $40.33 per hour
Work Location: Remote",$40.33 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,2019,Less than $1 million (USD)
NLB Technology Services,#N/A,"Dallas, TX",CRM data Analytics Engineer,"MUST HAVE:
o Tableau CRMA & Einstein Discovery Certification
o Experience building Dashboards using CRMA (aka Tableau CRM and Einstein Analytics)
o Design, build, and support datasets, data recipes, dataflows, and dashboards using Salesforce and non-Salesforce data sources
o Troubleshoot functional, data, and performance issues with CRMA Dashboards
o Skilled in SAQL, JSON
o Develop and deploy AI/ML solutions using Einstein Discovery and deploy AI/ML Models built outside Salesforce
o Experience working with large data (1 B+ rows) and supporting large user base (10,000+)
o Mentor and coordinate the development efforts of other CRMA DevelopersAT&T
Job Type: Contract
Salary: $65.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Vision insurance
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
CRM ANalytics: 8 years (Required)
einstein: 1 year (Required)
Salesforce: 1 year (Required)
Work Location: One location",$70.00 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"E-Business International INC
3.6",3.6,"Alexandria, VA",AWS Data Engineer,"Role: AWS Data Engineers
Location: Alexandria, VA
Full-Time/Permanent
AWS Engineers
We are seeking TWO (2) AWS Engineers to join our growing team. The qualified applicants will become part of the Enterprise Data Analytics Services (EDAS) program for a large federal agency in Alexandria, VA. Hybrid work options are available.
The AWS Engineer is responsible for the development and support of cloud-based solutions. Solutions include moving assets from on-premise to cloud environment, evaluation, and optimization. AWS Engineer will collaborate with the team effectively to ensure the team's success.
Responsibilities/Duties:
Work on automating the migration process in AWS from development to production.
Create CFT deploying/updating cloud resources.
Advise the engineering and software engineering team as they migrate from on-premise to cloud infrastructure.
Optimize cloud workloads for cost, scalability, availability, governance, compliance, etc.
Guide and/or provide hands-on support to administer production, staging, and deployment environments.
Partner with multi-disciplinary teams to understand requirements and plan architecture and solutions.
Apply insight and expertise across AWS services.
Apply knowledge of scripting and automation using tools like PowerShell, Python, Bash, Ruby, Perl, etc.
Experience and Qualifications:
Good working knowledge of AWS Services, including Compute, Storage Networking, Database, Management tools, Security, Identity, and Compliance.
3 years’ experience in AWS (EC2, Glue, S3, Redshift, Python programming).
Experience implementing and debugging workloads in AWS.
Good knowledge of creating CFTs.
Experience building infrastructure inside AWS via code. Familiarity with tools such as Terraform or CloudFormation.
Ability to proactively communicate, interact, and solve critical customer problems.
Ability to think and act strategically and proactively.
3 years of experience in software development tools and methodologies.
3 years of experience in infrastructure architecture, database architecture, and networking.
3 years of experience architecting/deploying operating solutions built on AWS.
Education/Certifications:
Bachelor's Degree in Computer Science, Information Technology, Engineering, or associated discipline.
AWS certification is a plus!
Clearance Requirements:
Ability to attain and maintain a Public Trust.
Job Type: Full-time
Salary: From $100,000.00 per year
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Alexandria, VA 20598: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Expected Salary Range (Must)?
Visa Status/ Work Authorization (Must)?
Experience:
AWS: 4 years (Required)
Work Location: One location","$100,000 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,1992,Unknown / Non-Applicable
NAVA TECH LLC,#N/A,Remote,Azure Data Engineer,"Azure Data Engineer
Remote
Long Term
Note: Need 10+ years of experience
Sound skills and hands on experience with Azure Data Lake, Azure Data Factory, SQL Data Warehouse Azure Blob, Azure Storage Explorer
Proficient in creating Data Factory pipelines for on-cloud ETL processing; copy activity, custom Azure development etc.
Knowledge of Azure Data Catalog, Event Grid, Service Bus, SQL and Synapse
Experience using Azure Databricks platform.
Experience with Python programming.
Experience using pandas and numpy for data engineering and data cleansing;
Ability to troubleshoot the job scheduler and compute clusters.
Experience with databricks CLI and secrets module.
Experience with Azure Blob storage access configuration.
p l e a s e s h a r e r e s u m e s t o m o h a m m e d (a t) n a v a t e c h (d o t) u s O R 4 4 3 5 3 7 9 8 3 5
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$62.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"DiamondPick
4.5",4.5,Remote,GCP Data Engineer,"Position: GCP Data Engineer (8+years)
Location: Remote
Type: Contract
Job Details:
· Relevant Industry Work Experience (6+ for Developer, preferred 9+ years)
· Experience extracting data from a variety of sources, and a desire to expand those skills (Excellent knowledge in SQL and Spark is mandatory)
· Strong knowledge of Google BigQuery and architecting data pipelines from on-prem to GCP.
· Experience building applications using Google Cloud Platform related frameworks such as DataProc and GCS at the minimum.
· Excellent Communication Skills to Understand and Pass on Requirements.
· Excellent Data Analysis skills. Must be comfortable with querying and analyzing large amount of data on Hadoop HDFS using Hive and Spark.
· Professional experience with a cloud hosting platform (GCP preferred)
· GCP certification is preferred.
Job Type: Contract
Salary: Up to $65.00 per hour
Experience level:
10 years
8 years
9 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
Google BigQuery: 4 years (Preferred)
DataProc: 2 years (Preferred)
Google Cloud Platform: 9 years (Required)
Spark: 8 years (Required)
SQL: 8 years (Required)
License/Certification:
Google Cloud Platform(GCP) Certification (Required)
Work Location: Remote",$65.00 /hr (est.),1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,#N/A,Unknown / Non-Applicable
"Spectrum Communications & Consulting Inc.
4.3",4.3,"Chicago, IL",Data Engineer,"What do incubators and Spectrum have in common? Well, they’re great for growth, and even better for stability. As an innovative software development and digital marketing company pioneering the field of artificial intelligence, we can offer our newest Data Engineer the best of both words – a high energy, forward-thinking start-up culture, inside of a well-established, profitable, and stable structure . if you’re interested in getting your hands dirty and inciting change into a larger organization with a vision to change the world uses data today, then please read on.
Responsibilities
Beyond working with state of the art technology you will have many different fantastic projects to work on as a Data Engineer at Spectrum. Here are just a few different responsibilities you can expect off the bat:
Design, build and maintain both new and existing data infrastructures using tools including (but not limited to) Microsoft SQL Server, Azure Tables, Azure Blobs, and Azure Cosmos
Determine the right database system for a provided data set
Drafting database/table schemas
Properly index tables when applicable to ensure query performance
Create data pipelines for transforming new datasets into consistent, reliable data systems that are usable across multiple platforms
Identify optimizations and improvements in existing data pipelines
Create solutions for data validation to ensure data integrity, accuracy, and consistency
Ensure data is readily available for analysis and development usage
Shifting existing big data pipelines
Come up with data validation solutions for both initial data imports as well as results
Audit the automation process to make sure there are no gaps, redundancies, or faults in the data pipeline that would cause damage to data integrity
Planning query efficient data store schemas with respect to the data requirements of our internal data warehouse
Communicate with data scientists and other software engineers to build proper data pipelines for efficient and cost-effective querying
Develop, construct, test, and maintain architectures with the help and feedback of software engineers
Determine and implement proper data schemas that align with Spectrum’s and Spectrum’s client’s business objectives
Recommend and implement ways to optimize data reliability, efficiency, and quality
Consult and suggest different technologies for our team to test and utilize together
Incorporate the business objectives of key stakeholders in the architecture of our data warehouse
Research new and interesting data acquisition opportunities
Some Characteristics That Define You
We understand that as a Data Engineer for Spectrum, you have many different professional goals and personal interests. As such here are just a few different things that typically define our team members on the Data Science team:
Self-Starter. Building a data warehouse is no simple task. You will need to come in with a self-starter attitude to not only make this data engineering role what you want it to be, but make a stellar and efficient data warehouse while you’re at it.
Analytical. In order to solve problems and build innovative new digital marketing campaigns, it is essential that you know how to take an idea and analyze it from all of its angles.
Developer. In the ever changing world of artificial intelligence, it’s not enough just to build models. You bring to the table an eye for data patterns along with a knack for relational database engineering.
Patient. As a data engineer, you know that you work with extremely large data sets on a daily basis. As such we are looking for someone who is not only meticulous, but patient enough to sit and sift through that data in a thorough way.
Creative. Beyond just analyzing data sets, you are an explorer and a puzzle solver. Pulling insights out of your data and understanding how those insights can better shape our tools is something that you live to do.
Student. More so than most industries, the field of data science is always changing and evolving. As such, you are always looking to learn new things and gain new skills.
Required Skills and Experience
On top of the many intangible skills you bring to the table, there are many skills that can help improve the efficiencies and success of your work at Spectrum. Here are a few of those required skills and experience that you will come in with as a Data Engineer on our team:
A bachelor’s degree/pursuing a bachelor’s degree in computer science, mathematics, statistics, information systems, or a related field
Experience with statistical modeling
1-3 years working experience with Python and/or SAS languages
1-3 years working experience with SQL databases and database querying languages
1-3 years working experience with C#/.NET
Familiarity with Microsoft Azure
Familiarity with Graph database structures
Experience with both RDBMS and TDMS
Experience with data mining and data cleaning
Experience with data visualization and reporting techniques
Written and verbal expression
Benefits
As a Data Engineer at Spectrum there are a ton of fantastic perks and benefits that come along with your work. Here are just a few of the benefits you can expect when joining the Spectrum family:
Comprehensive medical & dental insurance
Retirement planning & company matching
Generous PTO, including sick days & holidays
Coworking space access for remote and hybrid work options
Year-round gym memberships
Paid continuing education
Hybrid work from Home , meetings a few times a month to strategize new projects
About Us
Our mission at Spectrum has always been the same— growth. Whether that be helping our customers find quality new business, developing and challenging our team members, or evolving our products and services with advancements in technology and best practices, we have always been looking towards bettering ourselves for the future. Now as we continue to grow, we find ourselves as not only the nation’s leading digital marketing and software provider for the home services industry, but an innovator and ground-shaker for the world of artificial intelligence as well. From marketing automation software powered by AI, to top notch digital marketing services via those same AI insights, we love what we do and are excited to continue to innovate for the future together.
INDSPCI
Responsibilities
Design, build and maintain both new and existing data infrastructures using tools including (but not limited to) Microsoft SQL Server, Azure Tables, Azure Blobs, and Azure Cosmos
Determine the right database system for a provided data set
Drafting database/table schemas
Properly index tables when applicable to ensure query performance
Create data pipelines for transforming new datasets into consistent, reliable data systems that are usable across multiple platforms
Identify optimizations and improvements in existing data pipelines
Create solutions for data validation to ensure data integrity, accuracy, and consistency
Ensure data is readily available for analysis and development usage
Shifting existing big data pipelines
Come up with data validation solutions for both initial data imports as well as results
Audit the automation process to make sure there are no gaps, redundancies, or faults in the data pipeline that would cause damage to data integrity
Beyond working with state of the art technology you will have many different fantastic projects to work on as a Data Engineer at Spectrum. Here are just a few different responsibilities you can expect off the bat:
Planning query efficient data store schemas with respect to the data requirements of our internal data warehouse
Communicate with data scientists and other software engineers to build proper data pipelines for efficient and cost-effective querying
Develop, construct, test, and maintain architectures with the help and feedback of software engineers
Determine and implement proper data schemas that align with Spectrum’s and Spectrum’s client’s business objectives
Recommend and implement ways to optimize data reliability, efficiency, and quality
Consult and suggest different technologies for our team to test and utilize together
Incorporate the business objectives of key stakeholders in the architecture of our data warehouse
Research new and interesting data acquisition opportunities
Some Characteristics That Define You
We understand that as a Data Engineer for Spectrum, you have many different professional goals and personal interests. As such here are just a few different things that typically define our team members on the Data Science team:
Self-Starter. Building a data warehouse is no simple task. You will need to come in with a self-starter attitude to not only make this data engineering role what you want it to be, but make a stellar and efficient data warehouse while you’re at it.
Analytical. In order to solve problems and build innovative new digital marketing campaigns, it is essential that you know how to take an idea and analyze it from all of its angles.
Developer. In the ever changing world of artificial intelligence, it’s not enough just to build models. You bring to the table an eye for data patterns along with a knack for relational database engineering.
Patient. As a data engineer, you know that you work with extremely large data sets on a daily basis. As such we are looking for someone who is not only meticulous, but patient enough to sit and sift through that data in a thorough way.
Creative. Beyond just analyzing data sets, you are an explorer and a puzzle solver. Pulling insights out of your data and understanding how those insights can better shape our tools is something that you live to do.
Student. More so than most industries, the field of data science is always changing and evolving. As such, you are always looking to learn new things and gain new skills.
Required Skills and Experience
On top of the many intangible skills you bring to the table, there are many skills that can help improve the efficiencies and success of your work at Spectrum. Here are a few of those required skills and experience that you will come in with as a Data Engineer on our team:
A bachelor’s degree/pursuing a bachelor’s degree in computer science, mathematics, statistics, information systems, or a related field
Experience with statistical modeling
1-3 years working experience with Python and/or SAS languages
1-3 years working experience with SQL databases and database querying languages
1-3 years working experience with C#/.NET
Familiarity with Microsoft Azure
Familiarity with Graph database structures
Experience with both RDBMS and TDMS
Experience with data mining and data cleaning
Experience with data visualization and reporting techniques
Written and verbal expression
Benefits
As a Data Engineer at Spectrum there are a ton of fantastic perks and benefits that come along with your work. Here are just a few of the benefits you can expect when joining the Spectrum family:
Comprehensive medical & dental insurance
Retirement planning & company matching
Generous PTO, including sick days & holidays
Coworking space access for remote and hybrid work options
Year-round gym memberships
Paid continuing education
Hybrid work from Home , meetings a few times a month to strategize new projects
About Us
Our mission at Spectrum has always been the same— growth. Whether that be helping our customers find quality new business, developing and challenging our team members, or evolving our products and services with advancements in technology and best practices, we have always been looking towards bettering ourselves for the future. Now as we continue to grow, we find ourselves as not only the nation’s leading digital marketing and software provider for the home services industry, but an innovator and ground-shaker for the world of artificial intelligence as well. From marketing automation software powered by AI, to top notch digital marketing services via those same AI insights, we love what we do and are excited to continue to innovate for the future together.
INDSPCI
Hours
40","$92,696 /yr (est.)",51 to 200 Employees,Company - Private,Media & Communication,Advertising & Public Relations,1992,$5 to $25 million (USD)
"JPMorgan Chase Bank, N.A.
3.8",3.8,"Wilmington, DE",Data Engineer III,"As a Data Engineer III under Consumer and Community Banking within the ADE Data Governance team at JPMorgan Chase, you serve as a seasoned member of an agile team to design and deliver trusted data collection, storage, access, and analytics solutions in a secure, stable, and scalable way. You are responsible for developing, testing, and maintaining critical data pipelines and architectures across multiple technical areas within various business functions in support of the firm's business objectives.

Job responsibilities
Supports review of controls to ensure sufficient protection of enterprise data
Responsible for advising and making custom configuration changes in one to two tools to generate a product at the business or customer request
Updates logical or physical data models based on new use cases
Frequently uses SQL and understands NoSQL databases and their niche in the marketplace
Adds to team culture of diversity, equity, inclusion, and respect

Required qualifications, capabilities, and skills
Experience across the data lifecycle
Advanced at SQL (e.g., joins and aggregations)
Working understanding of NoSQL databases
Proficiency with JSON/Java, Erwin, Oracle, Snowflake, AWS
Significant experience with statistical data analysis and ability to determine appropriate tools and data patterns to perform analysis, including data modeling
Experience customizing changes in a tool to generate product
Executes data architecture solutions and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions and break down problems
Produces data architecture and design artifacts for complex applications while being accountable for ensuring design constraints are met by software code development
Gathers, analyzes, synthesizes, and develops visualizations and reporting from large, diverse data sets in service of continuous improvement of data frameworks, applications, and systems that proactively identifies hidden problems and patterns in data and uses these insights to drive improvements to coding hygiene and system architecture

Preferred qualifications, capabilities, and skills
Familiarity with modern technologies
Exposure to cloud technologies
Chase is a leading financial services firm, helping nearly half of America's households and small businesses achieve their financial goals through a broad range of financial products. Our mission is to create engaged, lifelong relationships and put our customers at the heart of everything we do. We also help small businesses, nonprofits and cities grow, delivering solutions to solve all their financial needs.

We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.
The health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the ""WELL Health-Safety Rating"" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.
As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm's current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm's vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.
We offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, discretionary incentive compensation which may be awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.
Equal Opportunity Employer/Disability/Veterans
Base Pay/Salary
Jersey City,NJ $97,850.00 - $150,000.00 / year","$123,925 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Banking & Lending,1799,$10+ billion (USD)
i-Link Solutions Inc,#N/A,"Boston, MA",Data Engineer,"i-Link Solutions is seeking a Data Engineer to join our team in Hanscom, MA. The Data Engineer will be responsible for designing, developing, and maintaining the data infrastructure that supports the Kessel Run program. The ideal candidate will have experience with AWS, open-source tools, and data engineering best practices
Responsibilities
Design, develop, and maintain the data infrastructure that supports the Kessel Run program
Work with stakeholders to understand business requirements and translate them into technical solutions
Build and maintain data pipelines that ingest, transform, and load data into data warehouses and data lakes
Develop and deploy data models and algorithms to support data-driven decision making
Work with data scientists and analysts to develop and deploy data products
Ensure the security and compliance of data assets
Qualifications
Bachelor's degree in Computer Science, Information Technology, or a related field
10+ years of experience in data engineering
Experience with AWS, open-source tools, and data engineering best practices
Strong programming skills in Python, Java, Scala, or SQL
Experience with data modeling and data warehousing
Experience with data visualization and reporting
Strong analytical and problem-solving skills
Excellent communication and teamwork skills
US Citizenship
Secret Clearance
Kessel Run Experience Desirable
Job Types: Full-time, Permanent
Pay: $89,571.07 - $140,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Referral program
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Boston, MA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineering: 10 years (Required)
AWS: 3 years (Required)
Data warehouse: 4 years (Required)
Work Location: In person","$114,786 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Northeastern University
4.3",4.3,"Boston, MA",Data Engineer,"About the Opportunity
Do you love learning the shapes of datasets, and molding them into something new? The Digital Scholarship Group (DSG) in the Northeastern University Library is excited to open a search for a Data Engineer. Working within a warm and collaborative environment dedicated to social justice, the Data Engineer gathers, organizes, manipulates, transforms, and documents a variety of humanities research data. The Data Engineer works with colleagues across the university to create sustainable platforms and data for community-led digital scholarship.
The Data Engineer position is situated within the DSG, which is part of the Northeastern University Library. The Library is a vital partner in learning, teaching, and community-engaged research for a diverse R1 university. Northeastern is committed to intensive research and experiential learning for students at all levels.
The Digital Scholarship Group is based on Northeastern’s Boston campus. This position is eligible for a hybrid work arrangement. Specific arrangements can be negotiated at the time of hire.
Responsibilities
The Data Engineer has responsibility for helping DSG work with data in a wide range of formats, across multiple projects and often in unforeseen contexts. The Data Engineer develops data dictionaries, mappings between data standards, transformation routines, and other curatorial systems. This position also manages projects and engages in high-level needs analysis and project planning. The DSG is committed to digital approaches that consider the pedagogical, research, social, and ethical implications of data and its design and use.
Working closely with other DSG and library staff, faculty collaborators, and students, the Data Engineer contributes to grant-funded and internal projects including the Boston Research Center; the Civil Rights and Restorative Justice project; the Digital Archive of Indigenous Language Persistence; the TEI Archiving, Publishing, and Access Service; Digital Humanities Quarterly; and the Women Writers Project. They will also take the lead on building DSG’s policies and practices in working with external data platforms such as Wikidata and partner project APIs. To support this work, expertise with tools like regular expressions and OpenRefine, and facility with data including RDF, JSON, XML, various API responses, and other formats will be important.
We warmly invite people with various skills and levels of expertise to apply to this position. Candidates who meet some, but not all, of the qualifications listed below are strongly encouraged to apply. We seek colleagues who are committed to building an inclusive and diverse working environment and who have been and remain underrepresented or marginalized in the field of librarianship – including but not limited to people of color, LGBTQ+ people, individuals with disabilities and applicants from lower-income and first-generation library or academic backgrounds. We expect this position to be an ongoing learning experience and are committed to supporting professional development.
Qualifications
We realize that this is a lengthy list of activities and qualifications. There are multiple paths toward success in this position, and each may look somewhat different depending on the successful candidate’s interests and experience.
Bachelor’s degree required; Master’s degree or similar training in data science, information science, information design, or other relevant discipline preferred
Minimum of 2 years of experience working or studying in a data-intensive environment, preferably in an academic or non-profit research setting
Experience working with quantitative and qualitative datasets, especially with historical and cultural heritage data
Experience working with structured data formats (for instance, XML, RDF, JSON, CSV, relational databases) and with data conversion, data enhancement, and data analysis
Ability to write code to assist in carrying out these kinds of data-related work (for instance, using R, Python, SQL, SPARQL, XSLT, Perl, and/or regular expressions)
Ability to work on multiple concurrent projects and adapt to the evolving landscape of digital humanities
Collaborative problem-solving skills, and the ability to research and recommend solutions as part of a participatory design process
Commitment to thoughtful, adaptive engagement with the needs of community collaborators
Strong oral and written skills, ability to communicate across expertise levels and prepare project documentation
Desire and aptitude to grow skills (especially in technical areas) and learn new things
The following skills are desirable but are not all essential for applicants to possess at the outset; we can provide training:
Knowledge of metadata standards relevant to research data, such as the Data Documentation Initiative
Experience creating, manipulating, and querying linked open data
Experience in open-source development practices and workflows, preferably within an academic or non-profit environment
Experience working with databases, data management systems, and APIs
Experience with developing and leading workshops
Experience communicating complex ideas about data and how it is used to many audiences
Salary Range:
$82,725 - $93,000
About the Digital Scholarship Group
A recognized leader in the field, the Digital Scholarship Group supports digital modes of research, publication, and collaboration through applied research, systems and tools development, and consultative services. The DSG offers a friendly and closely collaborative work environment, and actively fosters the professional and intellectual development of all of our colleagues and collaborators, including training opportunities and mentorship.
Our team engages with faculty in the digital humanities and quantitative social sciences from across the university to develop digital research and teaching projects, organize events, plan grant-funded initiatives and provide training and mentorship. We also work in close partnership with Northeastern’s Archives and Special Collections, the NULab for Maps, Texts, and Networks, and with cultural heritage partners in Boston including the Massachusetts Historical Society and the Boston Public Library.
We develop tools and platforms for working with digital artifacts and data, for querying and publishing them. We also provide workshops, mentorship opportunities, and pedagogical frameworks to the Northeastern community. Some of our major projects include the Boston Research Center, the Civil Rights and Restorative Justice Project, and the Digital Archive of Indigenous Language Persistence, as well as a number of digital archiving projects from the Library’s Archives and Special Collections. In all of our projects, we are attentive to inclusive and anti-racist approaches to data modeling, platform development, and collaborative working processes.
About the Library
The Northeastern University Library supports the mission of the University by working in partnership with the University community to develop and disseminate new scholarship. The Library fosters intellectual and professional growth, enriches the research, teaching, and learning environment, and promotes the effective use of knowledge by managing and delivering information resources and services to library users.
Northeastern University is an equal opportunity employer, seeking to recruit and support a broadly diverse community of faculty and staff. Northeastern values and celebrates diversity in all its forms and strives to foster an inclusive culture built on respect that affirms inter-group relations and builds cohesion.
All qualified applicants are encouraged to apply and will receive consideration for employment without regard to race, religion, color, national origin, age, sex, sexual orientation, disability status, or any other characteristic protected by applicable law.
To learn more about Northeastern University’s commitment and support of diversity and inclusion, please see www.northeastern.edu/diversity.
About Northeastern
Founded in 1898, Northeastern is a global research university and the recognized leader in experience-driven lifelong learning. Our world-renowned experiential approach empowers our students, faculty, alumni, and partners to create impact far beyond the confines of discipline, degree, and campus.
Our locations—in Boston; Charlotte, North Carolina; London; Portland, Maine; San Francisco Bay area; Seattle; Silicon Valley; Toronto; Vancouver; and the Massachusetts communities of Burlington and Nahant—are nodes in our growing global university system. Through this network, we expand opportunities for flexible, student-centered learning and collaborative, solutions-focused research.
Northeastern’s comprehensive array of undergraduate and graduate programs— in a variety of on-campus and online formats—lead to degrees through the doctorate in nine colleges and schools. Among these, we offer more than 195 multi-discipline majors and degrees designed to prepare students for purposeful lives and careers.
The position will remain open until filled but application review will begin after June 16.
Position Type
Information Technology
Additional Information
Northeastern University considers factors such as candidate work experience, education and skills when extending an offer.
Northeastern has a comprehensive benefits package for benefit eligible employees. This includes medical, vision, dental, paid time off, tuition assistance, wellness & life, retirement- as well as commuting & transportation. Visit
https://hr.northeastern.edu/benefits/
for more information.
Northeastern University is an equal opportunity employer, seeking to recruit and support a broadly diverse community of faculty and staff. Northeastern values and celebrates diversity in all its forms and strives to foster an inclusive culture built on respect that affirms inter-group relations and builds cohesion.
All qualified applicants are encouraged to apply and will receive consideration for employment without regard to race, religion, color, national origin, age, sex, sexual orientation, disability status, or any other characteristic protected by applicable law.
To learn more about Northeastern University’s commitment and support of diversity and inclusion, please see
www.northeastern.edu/diversity
.","$87,863 /yr (est.)",1001 to 5000 Employees,College / University,Education,Colleges & Universities,1898,$100 to $500 million (USD)
"Lightcast
4.4",4.4,"Moscow, ID",Senior Data Engineer,"A Senior Data Engineer will architect big data analytical frameworks, and translate complex functional and technical requirements into detailed architecture design and high-performing software. A Senior Data Engineer will select data solution software and define hardware requirements. The position will be responsible for developing and implementing standards and processes for integration projects and initiatives.

Major Responsibilities:
Architect software applications, and test and build automated tools.
Select data solution software and define hardware requirements
Develop standards and processes for integration projects and initiatives.
Lead the design/development of software applications, testing, and building tools
Ensure database changes are reviewed and approved to standards
Lead and communicate to leadership on solution design
Provide technical assistance to junior members and to colleagues across the company
Skills & Abilities:
Analytical mind, personality and aptitude for working with data
Ability to look at the numbers, trends, and data to derive conclusions based on findings
Work closely with management to prioritize business and information needs
Proven ability to work effectively to meet goals and deadlines with minimal supervision
Highly creative problem-solving skills and an ability to tailor efforts based on the importance of the issue being addressed.
A hands-on, detail-oriented mindset with the ability to look beyond the box
Expert knowledge of object-oriented design, data structures, and algorithms
Demonstrated experience with agile or other rapid application development methods, object-oriented design, coding, testing patterns with a variety of languages
Significant knowledge of data structures, algorithms, data modeling and disaster recovery of data systems
Education & Experience:
Requires 5 years of data/software engineering/science experience
Bachelor's degree in computer science or other technical disciplines (masters preferred)
About Lightcast:
Lightcast is a global leader in labor market insights with headquarters in Moscow (ID) and Boston (MA) and offices in the United Kingdom, Europe, and India. We work with partners across six continents to help drive economic prosperity and mobility by providing the insights needed to build and develop our people, our institutions and companies, and our communities.

Lightcast is proud to be an equal opportunity workplace and is committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. Lightcast has always been, and always will be, committed to diversity, equity and inclusion. We seek dynamic professionals from all backgrounds to join our teams, and we encourage our employees to bring their authentic, original, and best selves to work.

#LI-SS1","$118,643 /yr (est.)",1 to 50 Employees,Company - Private,Government & Public Administration,State & Regional Agencies,2011,Unknown / Non-Applicable
"Guardsman Group
3.0",3.0,United States,Data Engineer,"A Little About Us
The Guardsman Group stands on over 40 years of experience, unmatched technical capabilities and the unwavering belief in the right of safety for all. Each and every day, our matchless range of services puts Guardsman in the lives of people in every corner of Jamaica and throughout the Caribbean. We've pioneered technologies and perfected procedures to give our customers the best solutions for their homes and businesses. As we enter another decade, we continue to be the industry leader. Today, Guardsman consists of 13 companies and over seven thousand talented staffers who are proud to call themselves a Guardsman.
The Role
This role is within the Business Performance, Analytics and Intelligence (BPAI) Unit of the Guardsman Group and is directly responsible for the shaping, building and implementation of solutions that satisfy the business intelligence needs across the Group.
What You'll Be Doing
Create and maintain optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build analytictools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics
Collaborate with other members of the BPAI unit to build and improve on the availability, integrity, accuracyand reliability of data pipelines
Work closely with all levels of management, IT department and other members of the BPAI Unit to achieve task objectives
A Little Bit About You
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases
Experience building and optimizing 'big data' data pipelines, architectures and data sets
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Strong analytic skills related to working with unstructured datasets
Build processes supporting data transformation, data structures, metadata, dependency and workload management
Minimum Qualifications
Bachelor's Degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline",#N/A,5001 to 10000 Employees,Company - Private,Management & Consulting,Security & Protective,#N/A,Unknown / Non-Applicable
"Contact Government Services, LLC
4.7",4.7,"Washington, DC",Data Engineer,"CGS is seeking a passionate and driven Data Engineer to support a rapidly growing Data Analytics and Business Intelligence platform focused on providing solutions that empower our Federal customers with the tools and capabilities needed to turn data into actionable insights. The ideal candidate is a critical thinker and perpetual learner; excited to gain exposure and build skillsets across a range of technologies while solving some of our clients’ toughest challenges. As a Data Engineer, you will be integral to data operations for the development and integration of multiple data types across a range of data sets and sources.

You will be responsible for the day-to-day operations of systems that depend on data, ensuring data is properly processed and securely transferred to its appropriate location, in a timely manner. Processing data will include managing, manipulating, storing and parsing data in a data pipeline for variety of target sources. You will also support maintenance of applications and tools that reside on these systems such as upgrades, patches, configuration changes, etc. The work is performed in a multidisciplinary team environment using agile methodologies. The candidate we seek must be highly motivated and enthusiastic about implementing new technologies and learning about new data in a small team environment where deadlines are important.
Responsibilities:
Complete development efforts across data pipeline to store, manage, store, and provision to data consumers
Being an active and collaborating member of an Agile/Scrum team and following all Agile/Scrum best practices
Write code to ensure the performance and reliability of data extraction and processing
Support continuous process automation for data ingest
Achieve technical excellence by advocating for and adhering to lean-agile engineering principles and practices such as API-first design, simple design, continuous integration, version control, and automated testing
Work with program management and engineers to implement and document complex and evolving requirements
Help cultivate an environment that promotes customer service excellence, innovation, collaboration, and teamwork
Collaborate with others as part of a cross-functional team that includes user experience researchers and designers, product managers, engineers, and other functional specialists
Required Skills:
Must be a US Citizen
Must be able to obtain a Public Trust Clearance
7+ years of IT experience including experience in design, management, and solutioning of large, complex data sets and models.
Experience with developing data pipelines from many sources from structured and unstructured data sets in a variety of formats
Proficiency developing ETL processes, and performing test and validation steps
Proficiency to manipulate data (Python, R, SQL, SAS)
Strong knowledge of big data analysis and storage tools and technologies
Strong understanding of the agile principles and ability to apply them
Strong understanding of the CI/CD pipelines and ability to apply them
Experience with relational database, such as, PostgreSQL
Work comfortably in version control systems, such as, Git Repositories
Contact Government Services strives to simplify and enhance government bureaucracy through the optimization of human, technical, and financial resources. We combine cutting edge technology with world class personnel to deliver customized solutions that fit specific needs and solve the most challenging, dynamic problems. Our team emphasizes technical expertise and an empathy to public service to provide unique top tier support for our government engagements.
SOME BENEFITS OF THIS POSITION INCLUDE:
Health, Dental, & Vision insurance
Life Insurance
401k
Flexible Spending Account (Health, Dependent Care, and Commuter)
Paid Time Off & Observance of State/Federal Holidays
For more information about CGS please visit: https://www.cgsfederal.com
1YGuQTHTUE","$103,515 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"VedaInfo Inc
4.1",4.1,"Phoenix, AZ",Big Data Engineer,"Hi,
I hope this note finds you well
I am reaching out to you regarding your career opportunity with us for “Big Data Engineer” for a “6+ Months Contract” @Phoenix, AZ
Please find the below requirement details.
Job Title: Big Data Engineer
Location: Phoenix, AZ (Onsite)
Duration: 6+ months
Rate: $60/hr C2C
Responsibilities:
Design, implement, and maintain big data systems handling large volumes of data.
Utilize Hadoop, Hive, and Spark for efficient data processing and analysis.
Collaborate with cross-functional teams to understand data requirements.
Develop and optimize data ingestion, storage, and transformation processes.
Build scalable data pipelines for seamless data processing and analysis.
Monitor and troubleshoot data processing and performance issues.
Stay updated with emerging big data technologies.
Requirements:
Bachelor's/Master's degree in CS, Engineering, or related field
7+ years of experience as a Big Data Engineer
Strong proficiency in Hadoop, Hive, and Spark
Extensive experience with AWS or Azure
Solid understanding of data ingestion, storage, and transformation
Excellent problem-solving and communication skills
Proactive and self-motivated
This is a 6+ month onsite contract position in Phoenix, AZ, with the possibility of extension. Compensation is $60/hr on C2C basis.
Please submit your updated resume highlighting relevant experience.
Thank you for your interest!
Thanks & Regards
Mohammed ZAIN
Job Type: Contract
Pay: $50.00 - $60.00 per hour
Experience level:
7 years
Ability to commute/relocate:
Phoenix, AZ: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
To improve your chances of getting Shortlisted, make sure to tailor your Profile to the Job Requirements, Highlight your Relevant Skills and Experience, and then upload your Resume.
Experience:
AWS (Amazon Web Services): 7 years (Required)
Azure: 7 years (Required)
Big data: 7 years (Required)
Hadoop: 7 years (Required)
Apache Hive: 7 years (Required)
Spark: 7 years (Required)
Work Location: One location",$55.00 /hr (est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$1 to $5 million (USD)
"MARVEL TECHNOLOGIES INC
3.7",3.7,"Cincinnati, OH",Data Engineer,"Job Description
The data engineer designs and builds platforms tools and solutions that help the bank manage secure and generate value from its data. The person in this role creates scalable and reusable solutions for gathering collecting storing processing and serving data on both small and very large i.e. Big Data scales. These solutions can include on-premise and cloud-based data platforms and solutions in any of the following domains ETL business intelligence analytics persistence relational NoSQL data lakes search messaging data warehousing stream processing and machine learning.
Responsible and accountable for risk by openly exchanging ideas and opinions elevating concerns and personally following policies and procedures as defined. Accountable for always doing the right thing for customers and colleagues and ensures that actions and behaviors drive a positive customer experience. While operating within the Bank's risk appetite achieves results by consistently identifying assessing managing monitoring and reporting risks of all types.
ESSENTIAL DUTIES AND RESPONSIBILITIES:
Responsible for design Development and Support of data solutions APIs tools and processes to enable rapid delivery of business capabilities.
Work closely with IT application teams Enterprise architecture infrastructure information security and LOB stakeholders to translate business and technical strategies into data-driven solutions for the Bank.
Act as a technical Expert addressing problems related to system and application design performance integration security etc.
Conduct research and Development based on current trends and technologies related to the banking industry data engineering and architecture data security and related topics.
Work with developers to Build CI/CD pipelines Self-service Build tools and automated deployment processes.
Evaluate software products and Provide documented recommendations as needed.
Provide Support and troubleshooting for data platforms. Must be willing to Provide escalated on-call Support for complicated and/or critical incidents.
Participate in the planning process for hardware and software.
Plan and work on internal projects as needed including legacy system replacement Monitoring and analytics improvements tool Development and technical documentation.
Provide technical guidance and mentoring for other team members.
Manage and prioritize multiple assignments.
MINIMUM KNOWLEDGE SKILLS AND ABILITIES REQUIRED:
Bachelor's degree in Computer Science/Information Systems or equivalent combination of education and experience.
Must be able to communicate ideas both verbally and in writing to management business and IT sponsors and technical resources in language that is appropriate for each group.
Fundamental understanding of distributed computing principles
Knowledge of application and data security concepts best practices and common vulnerabilities.
Conceptual understanding of one or more of the following disciplines preferred big data technologies and distributions metadata management products commercial ETL tools Bi and reporting tools messaging systems data warehousing Java language and run time environment major version control systems continuous integration/delivery tools infrastructure automation and virtualization tools major cloud or rest API design and development.
Job Types: Full-time, Contract
Pay: $60.00 - $62.00 per hour
Compensation package:
Hourly pay
Yearly pay
Experience level:
9 years
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",$61.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,#N/A,$5 to $25 million (USD)
"Vedainfo
4.1",4.1,"Peoria, IL",Data Engineer,"Job Description
· Create and develop optimal data pipeline which fetches data from raw and apply business transformation and load derived.
· Build the AWS Fargate components required for optimal extraction, transformation, and loading of data-to-data lake.
· Develop data pipeline setup between base and derive values based on Caterpillar specification.
· Implement quality validation rules and follow process defined by stakeholder.
· Develop business events as defined by the solution team to generate CDC events for consumption layer (Kinesis)
· Create re-usable and configurable components wherever applicable.
· Create and execute unit, integration, and regression tests
· Ensure the code is developed satisfying all business rules, perform unit testing where applicable.
· Data pipelines development
· Python Development
· AWS cloud services
· Data quality validation rules
· Java (preferred)
· Implement CDC
· Transformation rules creation
· Message broker tools like Kinesis
· Azure DevOps – CI/CD pipelines
· Automated and Manual Testing experience
Job Type: Contract
Pay: Up to $60.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Peoria, IL 61602: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
This position is remote initially and you will be asked to work any of the client location once offer is made. Please confirm your current location.
Experience:
Data Engineer: 1 year (Preferred)
Python Development: 1 year (Preferred)
AWS: 1 year (Preferred)
Message broker tools: 1 year (Preferred)
Work Location: In person",$60.00 /hr (est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$1 to $5 million (USD)
META FORCE IT LLC,#N/A,"Cincinnati, OH",Data Engineer,"Job Description
The data engineer designs and builds platforms tools and solutions that help the bank manage secure and generate value from its data. The person in this role creates scalable and reusable solutions for gathering collecting storing processing and serving data on both small and very large i.e. Big Data scales. These solutions can include on-premise and cloud-based data platforms and solutions in any of the following domains ETL business intelligence analytics persistence relational NoSQL data lakes search messaging data warehousing stream processing and machine learning.
Responsible and accountable for risk by openly exchanging ideas and opinions elevating concerns and personally following policies and procedures as defined. Accountable for always doing the right thing for customers and colleagues and ensures that actions and behaviors drive a positive customer experience. While operating within the Bank's risk appetite achieves results by consistently identifying assessing managing monitoring and reporting risks of all types.
ESSENTIAL DUTIES AND RESPONSIBILITIES:
Responsible for design Development and Support of data solutions APIs tools and processes to enable rapid delivery of business capabilities.
Work closely with IT application teams Enterprise architecture infrastructure information security and LOB stakeholders to translate business and technical strategies into data-driven solutions for the Bank.
Act as a technical Expert addressing problems related to system and application design performance integration security etc.
Conduct research and Development based on current trends and technologies related to the banking industry data engineering and architecture data security and related topics.
Work with developers to Build CI/CD pipelines Self-service Build tools and automated deployment processes.
Evaluate software products and Provide documented recommendations as needed.
Provide Support and troubleshooting for data platforms. Must be willing to Provide escalated on-call Support for complicated and/or critical incidents.
Participate in the planning process for hardware and software.
Plan and work on internal projects as needed including legacy system replacement Monitoring and analytics improvements tool Development and technical documentation.
Provide technical guidance and mentoring for other team members.
Manage and prioritize multiple assignments.
MINIMUM KNOWLEDGE SKILLS AND ABILITIES REQUIRED:
Bachelor's degree in Computer Science/Information Systems or equivalent combination of education and experience.
Must be able to communicate ideas both verbally and in writing to management business and IT sponsors and technical resources in language that is appropriate for each group.
Fundamental understanding of distributed computing principles
Knowledge of application and data security concepts best practices and common vulnerabilities.
Conceptual understanding of one or more of the following disciplines preferred big data technologies and distributions metadata management products commercial ETL tools Bi and reporting tools messaging systems data warehousing Java language and run time environment major version control systems continuous integration/delivery tools infrastructure automation and virtualization tools major cloud or rest API design and development.
Job Type: Contract
Pay: $45.96 - $50.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Cincinnati, OH 45215: Reliably commute or planning to relocate before starting work (Required)
Experience:
data engineer: 10 years (Preferred)
Big data: 3 years (Preferred)
Work Location: Hybrid remote in Cincinnati, OH 45215",$47.98 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Myticas Consulting
3.9",3.9,Arizona,BHJOB15656_20058 - Data Engineer,"Myticas's direct client based out of Phoenix, AZ is currently seeking a Data Engineer for a 100% remote contract position.
Pay Rate: Up to $34/hr

Job Description:
A Data Engineer can confidently understand, write, troubleshoot, optimize, and peer review SQL code. They understand business requirements and create code and reporting that exceeds specifications through anticipating underlying needs. They also understand relational database design, table structure, data types, and data models. They are experienced with and confident in development in reporting visualizations in Excel, PowerBI, Tableau, QuickSight, SSRS, or other data products. They can create reporting dashboards and products that inform the business. They easily grasp and retain complex data concepts and are able to explain them to others. Looking for engineers who are driven to innovate, use cutting edge technologies to solve problems and think outside the box.
What You’ll Do:
Digital Transformation into AWS
Data Warehouse related work (SQL, ETL, Replication, Databases)
Understand, write, troubleshoot, optimize, and peer review SQL code.
Understand business requirements and create code and reporting that exceeds specifications through anticipating underlying needs.
Understand relational database design, table structure, data types, and data models. 6.Experienced with and confidence in development in reporting visualizations in Excel, PowerBI, Tableau, QuickSight, SSRS, or other data products.
Data Warehouse related work (SQL, ETL, Replication, Databases
Perform other duties as assigned or apparent.
MINIMUM EDUCATION AND RELATED WORK EXPERIENCE:
Bachelor's degree in a Computer Science field and 2 years' work experience
2 years of experience in most phases of IT systems deployments in one or more of the following areas: design and deployment of cloud services, data migration to AWS cloud, Digital Transformation into AWS, and Data Warehouse related work
INDCHI
INDREM",$34.00 /hr (est.),1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
Sconcept,#N/A,Remote,Azure Data Engineer,"** W2 LONG TERM CONTRACT BASED....**(NO C2C)
JOB TYPE:- Long Term Contract(6 months)Extendable
JOB LOCATION:-USA
About Sconcept LLC:-Software Concepts Consulting was established in 2008. With our “hands-on” approach we are very successful in delivering high performance, secure IT solutions and IT services. With headquarters in Carrollton, Texas we are specialized in providing business solutions to the Fortune 500 clients. Our clients include Software, Banking, Financial, Healthcare, Retail, Consumer goods and Insurance sectors. We deliver candidates based on client requirements.
Required Skills:-
Must have worked on several (5+) projects building solutions for Data Pipelines, Data warehousing, Data Modelling on Azure Data Platform (AWS, Google Cloud Platform - good to have)
Expertise in design and development of Data pipelines and ETL using ADF, Databricks to move data from relational/structured/unstructured data from source to data lake to data warehouse, on Azure
Creating Pipelines and integrations of multiple data sources
Data processing and transformation using Databricks (Scala/Python)
Expertise in optimizing cost and performance of data pipelines
Experience working with Data Lake, Synapse on Azure
Designing Data Lake storage layer (Landing/staging, raw, trusted/curated zones)
Designing data storage DB schema, data models and processing on Azure Synapse DW
Experience with Synapse Spark Pool
Experience in Data Modelling
Building Data Models from structured and semi-structured data (Data Model, Common Data Model etc.)
Performance optimization of data model for high performance analytical and reporting workloads
Expertise with SQL Server and extensive experience with SQL programming
Programming and optimizing DB objects Views, Stored Procs, Functions
Good experience in defining data access policy and security for azure data services
Experience with Oracle EDW
Knowledge/Experience migrating from Oracle EDW to any cloud DW
Experience integrating AI skills/ML models with Data and reporting solutions
Experience in building API layer for downstream consumption
Job Type: Contract
Pay: $45.00 - $50.00 per hour
Benefits:
Health insurance
Compensation package:
Bonus pay
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$47.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"ProGrad
4.2",4.2,Remote,Data Engineer,"We are looking for data engineers that are excited about building and maintaining data pipelines for a variety of interests. The pipelines will ingest data from a variety of sources ranging from internal change data streams to external integrations with various SaaS platforms through webhooks and events. The ideal candidate is self driven and looking to grow as a data engineer while having an outsized impact on the future of data at the company. A love for database architecture and optimisation is a must as is the ability to work across diverse teams.
Requirements/Qualifications:
Bachelor's or Master's degree in Data Science (specialization)
Software development experience coding in a general-purpose programming language such as C, C++, Java, JavaScript, Golang, or Python
Experience working with data structures or algorithms (i. e., data structures /algorithms class, coursework/projects, research, internships, or other practical experience in/outside of school or work (including open source hobby coding)
Proficiency in SQL, and NoSQL Databases
Experience working with large datasets
Experience writing and debugging complex SQL queries
A strong understanding of cloud database best practices and performance optimisation
Experience building and maintaining data pipelines, coupled with knowledge of good programming practices such as unit and integration testing
. A strong understanding of cloud database best practices and performance optimisation
Good to have skills:
. Experience building and maintaining data pipelines, coupled with knowledge of good programming practices such as unit and integration testing
Job Types: Full-time, Permanent
Salary: Up to $1,200,000.00 per year
Experience level:
1 year
Under 1 year
Schedule:
Day shift
Work Location: Remote
Speak with the employer
+91 9057202434",#N/A,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Enterprise Minds,#N/A,Remote,Data Engineer,"Job Role: SQL Data Engineer
Location: Remote Opportunity - Need to work in PST hours
Visa: US Citizen Only
Duration: Long term
Roles and Responsibilities:
3+ years of experience with a complex build system in AWS using Starburst.
5+ years of experience with installation, maintenance, and administration of Oracle databases- PL/SQL and ANSI SQL
Oracle DBA + Oracle SQL to ANSI SQL conversion, performance/query tuning
3+ years of experience configuring, integrating, and securing multiple AWS database offerings (ex. RDS, Aurora, Casandra, and Dynamo)
2+ years of experience with supporting Linux systems engineering efforts in system design and evaluation, solution engineering, software development, or system administration.
Experience with Database Schema as Code tools
Ability to design and develop complex SQL queries and reports to retrieve required data.
Experience with database partitioning and microservices
Knowledge of Agile methodologies or the software development life cycle (SDLC)
US Citizenship is Mandatory.
Job Type: Full-time
Experience:
Data Engineer: 10 years (Preferred)
AWS using Starburst: 4 years (Preferred)
PL/SQL and ANSI SQL: 2 years (Preferred)
AWS RDS: 1 year (Preferred)
Work Location: Remote",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Stefanini, Inc
3.8",3.8,"Dearborn, MI",Data Engineer,"Stefanini Group is hiring!
Stefanini is looking for Data Engineer at Dearborn, MI (Hybrid)
For quick apply, please reach out to Rajat Baloria
Phone: (248) 728-2620
Email: Rajatsingh.Baloria@stefanini.com

Open to W2 candidates only!

Position Description:
Looking for a Software Engineer focused on delivering software leveraging Java and Python based on proven Lean/Agile methods. Knowledge of Big Data technologies like Hadoop and Spark is a plus.
The Software Engineer will work in a small, cross-functional, and co-located team. The Software Engineer will collaborate directly and continuously with business partners, product managers and designers, and will release early and often.
Position Responsibilities:
Work hands-on with the team and other stakeholders to deliver quality software products that meet our customer's requirements and needs.
Grow technical capabilities / expertise and provide guidance to other members on the team



Skills Required:
Exceptional software engineering knowledge; OO Design Principles
Basic understanding of Big Data and potential use cases
Strong desire to learn new skills and apply to solve business problems/opportunities “Spring Boot, Java, Angular, API, Micro Services, PCF, GCP (especially Cloud Run)”

Experience Required:
Overall 6 years of work experience in delivering customer facing products
Minimum 2 years of strong development experience in at least one of the following technologies: Java or Python on a Hadoop/Spark Platform

Education Required:
An associate's degree in Computer Science or similar technical discipline
Education Preferred:
Google Cloud Certification

Listed Salary Range may vary based on experience, qualifications, and local market, Also some positions may Include bonuses and other Incentives

About Stefanini Group
The Stefanini Group is a global provider of offshore, onshore and near shore outsourcing, IT digital consulting, systems integration, application and strategic staffing services to Fortune 1000 enterprises around the world. Our presence is in countries like Americas, Europe, Africa and Asia, and more than 400 clients across a broad spectrum of markets, including financial services, manufacturing, telecommunications, chemical services, technology, public sector, and utilities. Stefanini is a CMM level 5, IT consulting, company with global presence. We are CMM Level 5 company.",$82.00 /hr (est.),10000+ Employees,Subsidiary or Business Segment,Information Technology,Information Technology Support Services,1987,$1 to $5 billion (USD)
"Maven Workforce
4.1",4.1,"Alpharetta, GA",Data Engineer,"Must Have skills:
Building Data Pipeline exp is a must
ETL Tools – SSIS, Alteryx
Data modelling
SQL Server development
Microsoft SQL Stack
Responsibilities:
Data acquisition and ingestion - Identify data sources and build pipelines using various ETL tools such as but not limited to, SSIS, and Alteryx. Data sources including but not limited to SQL Server, SAP, Teradata, Hadoop\Hive, PostgreSQL, Oracle and flat files.
Identifying ways to improve data reliability, efficiency and quality by various data solution techniques.
Expertise in Data project management with JIRA stories. Work as a liaison between business and IT to ensure successful and timely completion of the projects.
Assist in defining the data architecture framework, standards and principles, including modeling, metadata, security and reference data.
Create and optimize data models to support various business applications.
Reviewing modifications of existing data systems for cross-compatibility.
Automate and support workflows to ensure timely delivery.
Must Have:
5+ years of SQL Server development experience.
3+ years ALTERYX Admin /User management experience and advance workflow management.
5+ years data modeling experience.
5+ years of ETL experience.
5+ years of experience in working on more than one database technologies Microsoft SQL server, Teradata.
2+ years big data experience, Hadoop, Hive, Spark
Expert knowledge of data warehousing.
DESIRED SKILLS:
BI Lifecycle management
Working understanding of Microsoft VBA, HTML, Python
Data model development using ERWIN
Job Type: Contract
Salary: $50.00 per hour
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Building Data Pipeline: 8 years (Required)
ETL Tools – SSIS, Alteryx: 7 years (Required)
Data modelling: 6 years (Required)
SQL Server development: 8 years (Required)
Microsoft SQL Stack: 8 years (Required)
Work Location: One location
Speak with the employer
+91 7328456015",$50.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2008,Unknown / Non-Applicable
Zllius Inc.,#N/A,"Canton, MI",Data Engineer AUG76,"Hi,
Greetings from Zllius Inc!
We have an urgent opening for the roleData Engineer. If interested, revert me back with your Updated resume, along with contact details, work authorization.
Job Role: Data Engineer
Location: Hartford, CT ( Initially Remote )
Duration: Long-term
Position: W2/1099
Visa: Any (Except H1B)
Job Ref: AUG76
Job Description:
Good experience on designing and developing data pipelines for data ingestion and transformation using Spark.
Distributed computing experience using Pyspark.
Good understanding of spark framework and spark architecture.
Experience working in Cloud based big data infrastructure.
Excellent in trouble shooting the performance and data skew issues.
Must have good understanding of spark run time metrics and tune applications based on metrics.
Deep knowledge in partitioning, bucketing concepts of data ingestion.
Good understanding of AWS services like Glue, Athena, S3, Lambda, Cloud formation.
Preferred working knowledge on the implementation of datalake ETL using AWS glue, Databricks etc.
Experience with data modelling techniques for cloud data stores and on prem databases like Teradata, Teradata Vantage (TDV) etc.
Preferred working experience in ETL development in Teradata vantage and data migration from on prem to Teradata vantage.
Proficiency in SQL, relational and non-relational databases, query optimization and data modelling.
Experience with source code control systems like Gitlab.
Experience with large scale distributed relational and NoSQL database systems.
Experience : 9+years
Thanks & Regards:
Zllius Inc.
844 495 5487
Job Types: Full-time, Contract
Schedule:
8 hour shift
Work Location: In person","$86,440 /yr (est.)",Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Fresh Consulting
3.9",3.9,"Menlo Park, CA",Data Engineer,"Fresh Consulting is a design-led, software development and hardware engineering company, offering end-to-end digital services to help companies innovate. We bring together amazing UX designers, sophisticated developers, digital strategists, and top-notch engineers to help companies create fresh experiences that connect humans, systems, and machines. We’ve been growing fast and need someone to help us continue to manage the delivery of high-quality work in a fast-paced environment.
See more at freshconsulting.com Visit freshconsulting.com/portfolio to see our project work across several industries.
View and apply to all jobs - https://freshconsulting.applytojob.com/apply/ or visit freshconsulting.com/careers
Title: Data Engineer
Duration: 6 months with possible extension
Location: Onsite Menlo Park, CA
Benefits: Employee benefits at 100% including Medical, PTO, Holiday Pay, 401K Plan, and much more!
Hours: Minimum 40 Hours/Week
Role:
Design, construct, install, test, and maintain highly scalable data management systems.
Ensure systems meet business requirements and industry practices.
Design, implement, automate, and maintain large-scale enterprise data ETL processes.
Build high-performance algorithms, prototypes, predictive models, and proof of concepts.
Skills:
5+ years of work experience as a Data Engineer.
Ability to work as part of a team, as well as work independently or with minimal direction.
Excellent written, presentation, and verbal communication skills.
Collaborate with data architects, modelers, and IT team members on project goals.
Strong PC skills including knowledge of Microsoft SharePoint.
Process certification, such as Six Sigma, CBPP, BPM, ISO 20000, ITIL, and CMMI.
Education: BSCSE or related.
FRESH-
Work on engineering and research assignments with F500 companies and startups.
The relationships that we have created with our clients are one of a kind.
We help solve problems in many technologies focusing on R&D, product development, and manufacturing.
We work with the most cutting-edge and latest technologies from AR/VR to Autonomous technologies.
Closely working with our clients, we believe that long-term investments are extremely important to maintain the culture we together have created.
We’re a handpicked team of Engineers, digital strategists, designers, and developers united together in creating a fresh experience. Whether we are strategizing, designing, developing, or analyzing, our integrated team works as an extension of yours to improve your impact, your usability, and your customer conversion. In the process, we collaborate with you to get to know your business, understand your industry, and incorporate your big ideas into memorable experiences that keep your customers coming back for more.
Equal employment opportunity: All qualified persons will be considered for employment without regard to race, color, religion, sex, national origin, age, marital status, familial status, gender identity, sexual orientation, disability for which a reasonable accommodation can be made or any other status protected by law. Assistance will be gladly provided upon request for any applicant with sensory or non-sensory disabilities.
Fresh Consulting is a participating E-Verify company.
freshconsulting.com
Compensation offered will be determined by factors such as location, level, job-related knowledge, skills, and experience. Range $70/hr - $80/hr.
ylM5WIC1Wr",$75.00 /hr (est.),201 to 500 Employees,Company - Private,Management & Consulting,Business Consulting,2007,$25 to $100 million (USD)
"Wallero
4.4",4.4,"Seattle, WA",Data Engineer,"Hi,
Pretty much interested in your resume, we have an urgent opening for Data Engineer role with one of our clients based in Seattle WA.
Position: Data Engineer
Location: Seattle WA - Remote (100%)
Description:
Good Experience in reporting space including PowerBI, ADF, and possibly standing up a data warehouse/data mart.
The main scope centering is around reporting.
Experience with SQL, and agile framework including requirement gathering and story/feature writing skills.
Need someone strong who is comfortable leading and guiding teams in that capacity.
Migrating data from third party to Azure Cloud
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Seattle, WA 98101: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",$75.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2015,$1 to $5 million (USD)
"Deako
4.9",4.9,"Seattle, WA",Data Engineer II,"About Deako
Deako is delivering a revolutionary plug-n-play smart home platform with a laser focus on the untapped new home construction market. We make smart lighting so easy to upgrade that even those who would have never considered smart lighting are making it happen. We've built a company based on trust; where forming personal relationships is key to our success.
Software at Deako
The software team is a tight-knit team of smart, dedicated people. We are customer focused, we ship often, and are constantly asking “why.” We are passionate about the software we write. We work closely with other Deako teams. Developers at Deako are always looking for a better way and understand that no code is perfect.
Data at Deako
We believe that Data and Software go hand in hand. That's why the Data and Software teams work closely together to ensure we're collecting the right data, and getting it to the right people. Data is new at Deako, and we are working to make it a cornerstone of our company. We want every team at Deako to have a data-driven mindset.
Day to Day Expectations:
Participate in the code review process in our DBT repository
Build maintainable, testable SQL queries for various dashboards
Help maintain, contribute to and improve our data infrastructure and existing pipelines
Help maintain and improve our CI/CD pipelines
Responsible for obtaining, cleaning, and munging data and getting it into a form that our data analysts can access and analyze
Participate in meetings with stakeholders around data collection and cleaning

Must Have:
Demonstrable knowledge in SQL + Some Programming Language (Typescript, Python, Ect)
Demonstrates a high level autonomy and willingness to learn
1+ years of experience working as a Data Analyst/Engineer or Comparable field.
Experience with git, dbt
Nice to Have:
Experience in Snowflake, Fivetran, Hightouch
Experience with Gitlab CI
Experience with geospatial data
Stock Options
Hybrid work environment (Office + Remote)
Medical/Dental/Vision/Life/401K
Unlimited PTO
Free Snacks/Coffee/Drinks (Non-Alcoholic and Alcoholic)
Quarterly Company Parties","$115,000 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,2015,$5 to $25 million (USD)
shreetek,#N/A,"Charlotte, NC",Senior Data Engineer,"Job Title : Data Engineer
ONLY USC
On W2 only
Job Location : Newjersey OR Charlotte,NC
Collect, manage, and convert raw data accurately and reliably
Organize data systems for subgroup access and analyses
Configure and sustain data cloud structures
Able to work with structured and unstructured data.
Validate outputs of data pipelines
Degree in Data Engineering preferred.
Two Programmers –
Convert existing SAS code to python/pyspark code for model operation in the cloud.
Create and sustain policy analysis models in the cloud.
Troubleshoot user interfaces in the cloud.
Create and sustain intuitive user interfaces in the cloud.
A degree in Computer Science is preferred.
Overall across resources keep following stack as reference
Visualization : Tableau
Data Modeling/Science : Python / SAS
Engg : AWS cloud native services, security, data pipeline
Job Type: Contract
Salary: $50.00 - $60.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Charlotte, NC 28202: Reliably commute or planning to relocate before starting work (Required)
Work Location: One location",$55.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Orange County's Credit Union
4.0",4.0,"Santa Ana, CA",Data Engineer,"Great Opportunity At Orange County's Credit Union
Must reside in the state of CA, AZ, NV or TX.
Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk.
Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture.
Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential.
Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY!
More about Orange County's Credit Union:
Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace.
As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service.
Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference!
Overview:
Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization.
Essential Functions:
Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making.
Translate business requirements to technical solutions by applying technical knowledge and strong business acumen.
Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services.
Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform.
Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc).
Technical Must Haves for this Role:
5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment.
3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases).
3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.).
2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores.
2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS).
Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.

The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.

We perform thorough background checks and credit checks. EOE.","$92,145 /yr (est.)",201 to 500 Employees,Nonprofit Organization,Financial Services,Banking & Lending,1938,$25 to $100 million (USD)
"Fincons Group
4.0",4.0,Remote,Data Engineer – Databricks / Kafka / SQL,"Fincons Group is an IT business consulting company that has been designing the digital future of leading companies on international markets for 40 years. Fincons Group built its reputation on foreseeing and interpreting new business models and the rapid evolution of IT systems by building a complete range of services: from research and consulting to design and development, from system integration of leading vendor software solutions right up to application management, supporting clients step by step along their digital transformation journey. Fincons is a multinational with over 2,600 people and several offices worldwide (in Italy, Switzerland, Germany, France, the United Kingdom, Belgium and the USA), but above all a Family Company where the founders play a strategic role with commitment and passion, grounding the company in the same principles of a united and caring family.
We firmly believe in the value of cooperation and in the contribution that every idea and intuition can bring. We believe that everyone’s effort can make a difference. Transforming knowledge into a strategic resource is our company mission, and we pursue it with the help of our most strategic asset: our people.

Job Summary
Looking for an experienced data engineer with hands on experience in developing solutions based on Databricks. If you are a curious team-player with passion to learn and working for different clients to develop modern data streaming platforms, please reach out to us!

Work location: any US location might be considered
Employment type: freelance / contractor
Start date: about July 1th
Duration: about 6 months with possible renewal

Responsibilities and Duties
Design, develop and implement real time data ingestion pipelines from multiple sources using Databricks
Develop high-quality, scalable and re-usable frameworks for ingesting high volume and large data sets
Apply best practices to Integrate and ingest various big data formats like Protobuf, Parquet, AVRO and JSON
Work with event based / streaming technologies like Kafka, or Kinesis, to ingest and transform the data using KafkaStreams or KSQL
Working with other members of the project team to support delivery of additional project components (API interfaces, Search)
Work to load test the pipelines for quality and better performance using JMeter, or Gatling etc

Qualifications and skills
Bachelor’s degree and first experience designing, developing, deploying and/or supporting data pipelines using Databricks
Experience in designing and implementing Medallion architecture is preferred
Expertise in designing and deploying data applications on cloud solutions, such as Azure or AWS
Hands on experience in performance tuning and optimizing code running in Databricks environment
Proficient in programming languages like Pyspark and Python
Good understanding of SQL, T-SQL and/or PL/SQL
Demonstrated analytical and problem-solving skills particularly those that apply to a big data environment
Experience with agile development methodologies.
Ability to work effectively both independently and in a group or team environment

Authorization to Work
Applicants for employment in the US must be authorized to work in the US. Fincons Group will not sponsor applicants for work visas.
All qualified applicants will receive consideration for employment without regard to race, colour, religion, sex, sexual orientation, gender identity, national origin, age, disability, or protected veteran status, or any other legally protected basis, in accordance with applicable law.
The data will be processed and stored exclusively for the purposes of this or future selections, guaranteeing the rights referred to in art. 13 Legislative Decree 196/03 and EU regulation 679/2016 (GDPR)
2yrKJdtT13",#N/A,201 to 500 Employees,Company - Private,Financial Services,Accounting & Tax,#N/A,Unknown / Non-Applicable
SecurePro,#N/A,"Arlington, VA",Data Engineer,"Collect, manage, and convert raw data accurately and reliably
Organize data systems for subgroup access and analyses
Configure and sustain data cloud structures
Able to work with structured and unstructured data.
Validate outputs of data pipelines
Convert existing SAS code to python/pyspark code for model operation in the cloud
Create and sustain policy analysis models in the cloud
Troubleshoot user interfaces in the cloud
Create and sustain intuitive user interfaces in the cloud
Degree in Data Engineering preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $100.00 per hour
Experience level:
3 years
Schedule:
8 hour shift
Application Question(s):
Are you a US citizen?
Must have DOD secret Clearance?
Work Location: Remote",$90.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Amazee Global Ventures Inc
5.0",5.0,Remote,Data Engineer,"(W2 ONLY)
Job Title: Data Engineer
Duration : 12 months
Location: Remote
6-7 years of experience
Healthcare experience is must
Design, develop, and maintain Tableau dashboards and reports
Develop and maintain data models and ETL processes
Analyze data to identify trends and patterns
Create and maintain data dictionaries and data models
Develop and maintain data pipelines
Collaborate with stakeholders to understand data requirements
Develop and maintain data quality processes
Monitor data quality and performance
Troubleshoot data issues
Develop and maintain documentation
Requirements:
Bachelor’s degree in Computer Science, Information Systems, or related field
4+ years of experience in data engineering and analysis
Expertise in Tableau
Experience with SQL and data modeling
Knowledge of ETL processes
Knowledge of data warehousing
Excellent problem-solving and communication skills
Ability to work independently and in a team environment\
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Type: Full-time
Salary: $40.33 per hour
Work Location: Remote",$40.33 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,2019,Less than $1 million (USD)
NLB Technology Services,#N/A,"Dallas, TX",CRM data Analytics Engineer,"MUST HAVE:
o Tableau CRMA & Einstein Discovery Certification
o Experience building Dashboards using CRMA (aka Tableau CRM and Einstein Analytics)
o Design, build, and support datasets, data recipes, dataflows, and dashboards using Salesforce and non-Salesforce data sources
o Troubleshoot functional, data, and performance issues with CRMA Dashboards
o Skilled in SAQL, JSON
o Develop and deploy AI/ML solutions using Einstein Discovery and deploy AI/ML Models built outside Salesforce
o Experience working with large data (1 B+ rows) and supporting large user base (10,000+)
o Mentor and coordinate the development efforts of other CRMA DevelopersAT&T
Job Type: Contract
Salary: $65.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Vision insurance
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
CRM ANalytics: 8 years (Required)
einstein: 1 year (Required)
Salesforce: 1 year (Required)
Work Location: One location",$70.00 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"E-Business International INC
3.6",3.6,"Alexandria, VA",AWS Data Engineer,"Role: AWS Data Engineers
Location: Alexandria, VA
Full-Time/Permanent
AWS Engineers
We are seeking TWO (2) AWS Engineers to join our growing team. The qualified applicants will become part of the Enterprise Data Analytics Services (EDAS) program for a large federal agency in Alexandria, VA. Hybrid work options are available.
The AWS Engineer is responsible for the development and support of cloud-based solutions. Solutions include moving assets from on-premise to cloud environment, evaluation, and optimization. AWS Engineer will collaborate with the team effectively to ensure the team's success.
Responsibilities/Duties:
Work on automating the migration process in AWS from development to production.
Create CFT deploying/updating cloud resources.
Advise the engineering and software engineering team as they migrate from on-premise to cloud infrastructure.
Optimize cloud workloads for cost, scalability, availability, governance, compliance, etc.
Guide and/or provide hands-on support to administer production, staging, and deployment environments.
Partner with multi-disciplinary teams to understand requirements and plan architecture and solutions.
Apply insight and expertise across AWS services.
Apply knowledge of scripting and automation using tools like PowerShell, Python, Bash, Ruby, Perl, etc.
Experience and Qualifications:
Good working knowledge of AWS Services, including Compute, Storage Networking, Database, Management tools, Security, Identity, and Compliance.
3 years’ experience in AWS (EC2, Glue, S3, Redshift, Python programming).
Experience implementing and debugging workloads in AWS.
Good knowledge of creating CFTs.
Experience building infrastructure inside AWS via code. Familiarity with tools such as Terraform or CloudFormation.
Ability to proactively communicate, interact, and solve critical customer problems.
Ability to think and act strategically and proactively.
3 years of experience in software development tools and methodologies.
3 years of experience in infrastructure architecture, database architecture, and networking.
3 years of experience architecting/deploying operating solutions built on AWS.
Education/Certifications:
Bachelor's Degree in Computer Science, Information Technology, Engineering, or associated discipline.
AWS certification is a plus!
Clearance Requirements:
Ability to attain and maintain a Public Trust.
Job Type: Full-time
Salary: From $100,000.00 per year
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Alexandria, VA 20598: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Expected Salary Range (Must)?
Visa Status/ Work Authorization (Must)?
Experience:
AWS: 4 years (Required)
Work Location: One location","$100,000 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,1992,Unknown / Non-Applicable
NAVA TECH LLC,#N/A,Remote,Azure Data Engineer,"Azure Data Engineer
Remote
Long Term
Note: Need 10+ years of experience
Sound skills and hands on experience with Azure Data Lake, Azure Data Factory, SQL Data Warehouse Azure Blob, Azure Storage Explorer
Proficient in creating Data Factory pipelines for on-cloud ETL processing; copy activity, custom Azure development etc.
Knowledge of Azure Data Catalog, Event Grid, Service Bus, SQL and Synapse
Experience using Azure Databricks platform.
Experience with Python programming.
Experience using pandas and numpy for data engineering and data cleansing;
Ability to troubleshoot the job scheduler and compute clusters.
Experience with databricks CLI and secrets module.
Experience with Azure Blob storage access configuration.
p l e a s e s h a r e r e s u m e s t o m o h a m m e d (a t) n a v a t e c h (d o t) u s O R 4 4 3 5 3 7 9 8 3 5
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$62.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"DiamondPick
4.5",4.5,Remote,GCP Data Engineer,"Position: GCP Data Engineer (8+years)
Location: Remote
Type: Contract
Job Details:
· Relevant Industry Work Experience (6+ for Developer, preferred 9+ years)
· Experience extracting data from a variety of sources, and a desire to expand those skills (Excellent knowledge in SQL and Spark is mandatory)
· Strong knowledge of Google BigQuery and architecting data pipelines from on-prem to GCP.
· Experience building applications using Google Cloud Platform related frameworks such as DataProc and GCS at the minimum.
· Excellent Communication Skills to Understand and Pass on Requirements.
· Excellent Data Analysis skills. Must be comfortable with querying and analyzing large amount of data on Hadoop HDFS using Hive and Spark.
· Professional experience with a cloud hosting platform (GCP preferred)
· GCP certification is preferred.
Job Type: Contract
Salary: Up to $65.00 per hour
Experience level:
10 years
8 years
9 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
Google BigQuery: 4 years (Preferred)
DataProc: 2 years (Preferred)
Google Cloud Platform: 9 years (Required)
Spark: 8 years (Required)
SQL: 8 years (Required)
License/Certification:
Google Cloud Platform(GCP) Certification (Required)
Work Location: Remote",$65.00 /hr (est.),1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,#N/A,Unknown / Non-Applicable
"Spectrum Communications & Consulting Inc.
4.3",4.3,"Chicago, IL",Data Engineer,"What do incubators and Spectrum have in common? Well, they’re great for growth, and even better for stability. As an innovative software development and digital marketing company pioneering the field of artificial intelligence, we can offer our newest Data Engineer the best of both words – a high energy, forward-thinking start-up culture, inside of a well-established, profitable, and stable structure . if you’re interested in getting your hands dirty and inciting change into a larger organization with a vision to change the world uses data today, then please read on.
Responsibilities
Beyond working with state of the art technology you will have many different fantastic projects to work on as a Data Engineer at Spectrum. Here are just a few different responsibilities you can expect off the bat:
Design, build and maintain both new and existing data infrastructures using tools including (but not limited to) Microsoft SQL Server, Azure Tables, Azure Blobs, and Azure Cosmos
Determine the right database system for a provided data set
Drafting database/table schemas
Properly index tables when applicable to ensure query performance
Create data pipelines for transforming new datasets into consistent, reliable data systems that are usable across multiple platforms
Identify optimizations and improvements in existing data pipelines
Create solutions for data validation to ensure data integrity, accuracy, and consistency
Ensure data is readily available for analysis and development usage
Shifting existing big data pipelines
Come up with data validation solutions for both initial data imports as well as results
Audit the automation process to make sure there are no gaps, redundancies, or faults in the data pipeline that would cause damage to data integrity
Planning query efficient data store schemas with respect to the data requirements of our internal data warehouse
Communicate with data scientists and other software engineers to build proper data pipelines for efficient and cost-effective querying
Develop, construct, test, and maintain architectures with the help and feedback of software engineers
Determine and implement proper data schemas that align with Spectrum’s and Spectrum’s client’s business objectives
Recommend and implement ways to optimize data reliability, efficiency, and quality
Consult and suggest different technologies for our team to test and utilize together
Incorporate the business objectives of key stakeholders in the architecture of our data warehouse
Research new and interesting data acquisition opportunities
Some Characteristics That Define You
We understand that as a Data Engineer for Spectrum, you have many different professional goals and personal interests. As such here are just a few different things that typically define our team members on the Data Science team:
Self-Starter. Building a data warehouse is no simple task. You will need to come in with a self-starter attitude to not only make this data engineering role what you want it to be, but make a stellar and efficient data warehouse while you’re at it.
Analytical. In order to solve problems and build innovative new digital marketing campaigns, it is essential that you know how to take an idea and analyze it from all of its angles.
Developer. In the ever changing world of artificial intelligence, it’s not enough just to build models. You bring to the table an eye for data patterns along with a knack for relational database engineering.
Patient. As a data engineer, you know that you work with extremely large data sets on a daily basis. As such we are looking for someone who is not only meticulous, but patient enough to sit and sift through that data in a thorough way.
Creative. Beyond just analyzing data sets, you are an explorer and a puzzle solver. Pulling insights out of your data and understanding how those insights can better shape our tools is something that you live to do.
Student. More so than most industries, the field of data science is always changing and evolving. As such, you are always looking to learn new things and gain new skills.
Required Skills and Experience
On top of the many intangible skills you bring to the table, there are many skills that can help improve the efficiencies and success of your work at Spectrum. Here are a few of those required skills and experience that you will come in with as a Data Engineer on our team:
A bachelor’s degree/pursuing a bachelor’s degree in computer science, mathematics, statistics, information systems, or a related field
Experience with statistical modeling
1-3 years working experience with Python and/or SAS languages
1-3 years working experience with SQL databases and database querying languages
1-3 years working experience with C#/.NET
Familiarity with Microsoft Azure
Familiarity with Graph database structures
Experience with both RDBMS and TDMS
Experience with data mining and data cleaning
Experience with data visualization and reporting techniques
Written and verbal expression
Benefits
As a Data Engineer at Spectrum there are a ton of fantastic perks and benefits that come along with your work. Here are just a few of the benefits you can expect when joining the Spectrum family:
Comprehensive medical & dental insurance
Retirement planning & company matching
Generous PTO, including sick days & holidays
Coworking space access for remote and hybrid work options
Year-round gym memberships
Paid continuing education
Hybrid work from Home , meetings a few times a month to strategize new projects
About Us
Our mission at Spectrum has always been the same— growth. Whether that be helping our customers find quality new business, developing and challenging our team members, or evolving our products and services with advancements in technology and best practices, we have always been looking towards bettering ourselves for the future. Now as we continue to grow, we find ourselves as not only the nation’s leading digital marketing and software provider for the home services industry, but an innovator and ground-shaker for the world of artificial intelligence as well. From marketing automation software powered by AI, to top notch digital marketing services via those same AI insights, we love what we do and are excited to continue to innovate for the future together.
INDSPCI
Responsibilities
Design, build and maintain both new and existing data infrastructures using tools including (but not limited to) Microsoft SQL Server, Azure Tables, Azure Blobs, and Azure Cosmos
Determine the right database system for a provided data set
Drafting database/table schemas
Properly index tables when applicable to ensure query performance
Create data pipelines for transforming new datasets into consistent, reliable data systems that are usable across multiple platforms
Identify optimizations and improvements in existing data pipelines
Create solutions for data validation to ensure data integrity, accuracy, and consistency
Ensure data is readily available for analysis and development usage
Shifting existing big data pipelines
Come up with data validation solutions for both initial data imports as well as results
Audit the automation process to make sure there are no gaps, redundancies, or faults in the data pipeline that would cause damage to data integrity
Beyond working with state of the art technology you will have many different fantastic projects to work on as a Data Engineer at Spectrum. Here are just a few different responsibilities you can expect off the bat:
Planning query efficient data store schemas with respect to the data requirements of our internal data warehouse
Communicate with data scientists and other software engineers to build proper data pipelines for efficient and cost-effective querying
Develop, construct, test, and maintain architectures with the help and feedback of software engineers
Determine and implement proper data schemas that align with Spectrum’s and Spectrum’s client’s business objectives
Recommend and implement ways to optimize data reliability, efficiency, and quality
Consult and suggest different technologies for our team to test and utilize together
Incorporate the business objectives of key stakeholders in the architecture of our data warehouse
Research new and interesting data acquisition opportunities
Some Characteristics That Define You
We understand that as a Data Engineer for Spectrum, you have many different professional goals and personal interests. As such here are just a few different things that typically define our team members on the Data Science team:
Self-Starter. Building a data warehouse is no simple task. You will need to come in with a self-starter attitude to not only make this data engineering role what you want it to be, but make a stellar and efficient data warehouse while you’re at it.
Analytical. In order to solve problems and build innovative new digital marketing campaigns, it is essential that you know how to take an idea and analyze it from all of its angles.
Developer. In the ever changing world of artificial intelligence, it’s not enough just to build models. You bring to the table an eye for data patterns along with a knack for relational database engineering.
Patient. As a data engineer, you know that you work with extremely large data sets on a daily basis. As such we are looking for someone who is not only meticulous, but patient enough to sit and sift through that data in a thorough way.
Creative. Beyond just analyzing data sets, you are an explorer and a puzzle solver. Pulling insights out of your data and understanding how those insights can better shape our tools is something that you live to do.
Student. More so than most industries, the field of data science is always changing and evolving. As such, you are always looking to learn new things and gain new skills.
Required Skills and Experience
On top of the many intangible skills you bring to the table, there are many skills that can help improve the efficiencies and success of your work at Spectrum. Here are a few of those required skills and experience that you will come in with as a Data Engineer on our team:
A bachelor’s degree/pursuing a bachelor’s degree in computer science, mathematics, statistics, information systems, or a related field
Experience with statistical modeling
1-3 years working experience with Python and/or SAS languages
1-3 years working experience with SQL databases and database querying languages
1-3 years working experience with C#/.NET
Familiarity with Microsoft Azure
Familiarity with Graph database structures
Experience with both RDBMS and TDMS
Experience with data mining and data cleaning
Experience with data visualization and reporting techniques
Written and verbal expression
Benefits
As a Data Engineer at Spectrum there are a ton of fantastic perks and benefits that come along with your work. Here are just a few of the benefits you can expect when joining the Spectrum family:
Comprehensive medical & dental insurance
Retirement planning & company matching
Generous PTO, including sick days & holidays
Coworking space access for remote and hybrid work options
Year-round gym memberships
Paid continuing education
Hybrid work from Home , meetings a few times a month to strategize new projects
About Us
Our mission at Spectrum has always been the same— growth. Whether that be helping our customers find quality new business, developing and challenging our team members, or evolving our products and services with advancements in technology and best practices, we have always been looking towards bettering ourselves for the future. Now as we continue to grow, we find ourselves as not only the nation’s leading digital marketing and software provider for the home services industry, but an innovator and ground-shaker for the world of artificial intelligence as well. From marketing automation software powered by AI, to top notch digital marketing services via those same AI insights, we love what we do and are excited to continue to innovate for the future together.
INDSPCI
Hours
40","$92,696 /yr (est.)",51 to 200 Employees,Company - Private,Media & Communication,Advertising & Public Relations,1992,$5 to $25 million (USD)
"JPMorgan Chase Bank, N.A.
3.8",3.8,"Wilmington, DE",Data Engineer III,"As a Data Engineer III under Consumer and Community Banking within the ADE Data Governance team at JPMorgan Chase, you serve as a seasoned member of an agile team to design and deliver trusted data collection, storage, access, and analytics solutions in a secure, stable, and scalable way. You are responsible for developing, testing, and maintaining critical data pipelines and architectures across multiple technical areas within various business functions in support of the firm's business objectives.

Job responsibilities
Supports review of controls to ensure sufficient protection of enterprise data
Responsible for advising and making custom configuration changes in one to two tools to generate a product at the business or customer request
Updates logical or physical data models based on new use cases
Frequently uses SQL and understands NoSQL databases and their niche in the marketplace
Adds to team culture of diversity, equity, inclusion, and respect

Required qualifications, capabilities, and skills
Experience across the data lifecycle
Advanced at SQL (e.g., joins and aggregations)
Working understanding of NoSQL databases
Proficiency with JSON/Java, Erwin, Oracle, Snowflake, AWS
Significant experience with statistical data analysis and ability to determine appropriate tools and data patterns to perform analysis, including data modeling
Experience customizing changes in a tool to generate product
Executes data architecture solutions and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions and break down problems
Produces data architecture and design artifacts for complex applications while being accountable for ensuring design constraints are met by software code development
Gathers, analyzes, synthesizes, and develops visualizations and reporting from large, diverse data sets in service of continuous improvement of data frameworks, applications, and systems that proactively identifies hidden problems and patterns in data and uses these insights to drive improvements to coding hygiene and system architecture

Preferred qualifications, capabilities, and skills
Familiarity with modern technologies
Exposure to cloud technologies
Chase is a leading financial services firm, helping nearly half of America's households and small businesses achieve their financial goals through a broad range of financial products. Our mission is to create engaged, lifelong relationships and put our customers at the heart of everything we do. We also help small businesses, nonprofits and cities grow, delivering solutions to solve all their financial needs.

We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.
The health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the ""WELL Health-Safety Rating"" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.
As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm's current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm's vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.
We offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, discretionary incentive compensation which may be awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.
Equal Opportunity Employer/Disability/Veterans
Base Pay/Salary
Jersey City,NJ $97,850.00 - $150,000.00 / year","$123,925 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Banking & Lending,1799,$10+ billion (USD)
i-Link Solutions Inc,#N/A,"Boston, MA",Data Engineer,"i-Link Solutions is seeking a Data Engineer to join our team in Hanscom, MA. The Data Engineer will be responsible for designing, developing, and maintaining the data infrastructure that supports the Kessel Run program. The ideal candidate will have experience with AWS, open-source tools, and data engineering best practices
Responsibilities
Design, develop, and maintain the data infrastructure that supports the Kessel Run program
Work with stakeholders to understand business requirements and translate them into technical solutions
Build and maintain data pipelines that ingest, transform, and load data into data warehouses and data lakes
Develop and deploy data models and algorithms to support data-driven decision making
Work with data scientists and analysts to develop and deploy data products
Ensure the security and compliance of data assets
Qualifications
Bachelor's degree in Computer Science, Information Technology, or a related field
10+ years of experience in data engineering
Experience with AWS, open-source tools, and data engineering best practices
Strong programming skills in Python, Java, Scala, or SQL
Experience with data modeling and data warehousing
Experience with data visualization and reporting
Strong analytical and problem-solving skills
Excellent communication and teamwork skills
US Citizenship
Secret Clearance
Kessel Run Experience Desirable
Job Types: Full-time, Permanent
Pay: $89,571.07 - $140,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Referral program
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Boston, MA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineering: 10 years (Required)
AWS: 3 years (Required)
Data warehouse: 4 years (Required)
Work Location: In person","$114,786 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Northeastern University
4.3",4.3,"Boston, MA",Data Engineer,"About the Opportunity
Do you love learning the shapes of datasets, and molding them into something new? The Digital Scholarship Group (DSG) in the Northeastern University Library is excited to open a search for a Data Engineer. Working within a warm and collaborative environment dedicated to social justice, the Data Engineer gathers, organizes, manipulates, transforms, and documents a variety of humanities research data. The Data Engineer works with colleagues across the university to create sustainable platforms and data for community-led digital scholarship.
The Data Engineer position is situated within the DSG, which is part of the Northeastern University Library. The Library is a vital partner in learning, teaching, and community-engaged research for a diverse R1 university. Northeastern is committed to intensive research and experiential learning for students at all levels.
The Digital Scholarship Group is based on Northeastern’s Boston campus. This position is eligible for a hybrid work arrangement. Specific arrangements can be negotiated at the time of hire.
Responsibilities
The Data Engineer has responsibility for helping DSG work with data in a wide range of formats, across multiple projects and often in unforeseen contexts. The Data Engineer develops data dictionaries, mappings between data standards, transformation routines, and other curatorial systems. This position also manages projects and engages in high-level needs analysis and project planning. The DSG is committed to digital approaches that consider the pedagogical, research, social, and ethical implications of data and its design and use.
Working closely with other DSG and library staff, faculty collaborators, and students, the Data Engineer contributes to grant-funded and internal projects including the Boston Research Center; the Civil Rights and Restorative Justice project; the Digital Archive of Indigenous Language Persistence; the TEI Archiving, Publishing, and Access Service; Digital Humanities Quarterly; and the Women Writers Project. They will also take the lead on building DSG’s policies and practices in working with external data platforms such as Wikidata and partner project APIs. To support this work, expertise with tools like regular expressions and OpenRefine, and facility with data including RDF, JSON, XML, various API responses, and other formats will be important.
We warmly invite people with various skills and levels of expertise to apply to this position. Candidates who meet some, but not all, of the qualifications listed below are strongly encouraged to apply. We seek colleagues who are committed to building an inclusive and diverse working environment and who have been and remain underrepresented or marginalized in the field of librarianship – including but not limited to people of color, LGBTQ+ people, individuals with disabilities and applicants from lower-income and first-generation library or academic backgrounds. We expect this position to be an ongoing learning experience and are committed to supporting professional development.
Qualifications
We realize that this is a lengthy list of activities and qualifications. There are multiple paths toward success in this position, and each may look somewhat different depending on the successful candidate’s interests and experience.
Bachelor’s degree required; Master’s degree or similar training in data science, information science, information design, or other relevant discipline preferred
Minimum of 2 years of experience working or studying in a data-intensive environment, preferably in an academic or non-profit research setting
Experience working with quantitative and qualitative datasets, especially with historical and cultural heritage data
Experience working with structured data formats (for instance, XML, RDF, JSON, CSV, relational databases) and with data conversion, data enhancement, and data analysis
Ability to write code to assist in carrying out these kinds of data-related work (for instance, using R, Python, SQL, SPARQL, XSLT, Perl, and/or regular expressions)
Ability to work on multiple concurrent projects and adapt to the evolving landscape of digital humanities
Collaborative problem-solving skills, and the ability to research and recommend solutions as part of a participatory design process
Commitment to thoughtful, adaptive engagement with the needs of community collaborators
Strong oral and written skills, ability to communicate across expertise levels and prepare project documentation
Desire and aptitude to grow skills (especially in technical areas) and learn new things
The following skills are desirable but are not all essential for applicants to possess at the outset; we can provide training:
Knowledge of metadata standards relevant to research data, such as the Data Documentation Initiative
Experience creating, manipulating, and querying linked open data
Experience in open-source development practices and workflows, preferably within an academic or non-profit environment
Experience working with databases, data management systems, and APIs
Experience with developing and leading workshops
Experience communicating complex ideas about data and how it is used to many audiences
Salary Range:
$82,725 - $93,000
About the Digital Scholarship Group
A recognized leader in the field, the Digital Scholarship Group supports digital modes of research, publication, and collaboration through applied research, systems and tools development, and consultative services. The DSG offers a friendly and closely collaborative work environment, and actively fosters the professional and intellectual development of all of our colleagues and collaborators, including training opportunities and mentorship.
Our team engages with faculty in the digital humanities and quantitative social sciences from across the university to develop digital research and teaching projects, organize events, plan grant-funded initiatives and provide training and mentorship. We also work in close partnership with Northeastern’s Archives and Special Collections, the NULab for Maps, Texts, and Networks, and with cultural heritage partners in Boston including the Massachusetts Historical Society and the Boston Public Library.
We develop tools and platforms for working with digital artifacts and data, for querying and publishing them. We also provide workshops, mentorship opportunities, and pedagogical frameworks to the Northeastern community. Some of our major projects include the Boston Research Center, the Civil Rights and Restorative Justice Project, and the Digital Archive of Indigenous Language Persistence, as well as a number of digital archiving projects from the Library’s Archives and Special Collections. In all of our projects, we are attentive to inclusive and anti-racist approaches to data modeling, platform development, and collaborative working processes.
About the Library
The Northeastern University Library supports the mission of the University by working in partnership with the University community to develop and disseminate new scholarship. The Library fosters intellectual and professional growth, enriches the research, teaching, and learning environment, and promotes the effective use of knowledge by managing and delivering information resources and services to library users.
Northeastern University is an equal opportunity employer, seeking to recruit and support a broadly diverse community of faculty and staff. Northeastern values and celebrates diversity in all its forms and strives to foster an inclusive culture built on respect that affirms inter-group relations and builds cohesion.
All qualified applicants are encouraged to apply and will receive consideration for employment without regard to race, religion, color, national origin, age, sex, sexual orientation, disability status, or any other characteristic protected by applicable law.
To learn more about Northeastern University’s commitment and support of diversity and inclusion, please see www.northeastern.edu/diversity.
About Northeastern
Founded in 1898, Northeastern is a global research university and the recognized leader in experience-driven lifelong learning. Our world-renowned experiential approach empowers our students, faculty, alumni, and partners to create impact far beyond the confines of discipline, degree, and campus.
Our locations—in Boston; Charlotte, North Carolina; London; Portland, Maine; San Francisco Bay area; Seattle; Silicon Valley; Toronto; Vancouver; and the Massachusetts communities of Burlington and Nahant—are nodes in our growing global university system. Through this network, we expand opportunities for flexible, student-centered learning and collaborative, solutions-focused research.
Northeastern’s comprehensive array of undergraduate and graduate programs— in a variety of on-campus and online formats—lead to degrees through the doctorate in nine colleges and schools. Among these, we offer more than 195 multi-discipline majors and degrees designed to prepare students for purposeful lives and careers.
The position will remain open until filled but application review will begin after June 16.
Position Type
Information Technology
Additional Information
Northeastern University considers factors such as candidate work experience, education and skills when extending an offer.
Northeastern has a comprehensive benefits package for benefit eligible employees. This includes medical, vision, dental, paid time off, tuition assistance, wellness & life, retirement- as well as commuting & transportation. Visit
https://hr.northeastern.edu/benefits/
for more information.
Northeastern University is an equal opportunity employer, seeking to recruit and support a broadly diverse community of faculty and staff. Northeastern values and celebrates diversity in all its forms and strives to foster an inclusive culture built on respect that affirms inter-group relations and builds cohesion.
All qualified applicants are encouraged to apply and will receive consideration for employment without regard to race, religion, color, national origin, age, sex, sexual orientation, disability status, or any other characteristic protected by applicable law.
To learn more about Northeastern University’s commitment and support of diversity and inclusion, please see
www.northeastern.edu/diversity
.","$87,863 /yr (est.)",1001 to 5000 Employees,College / University,Education,Colleges & Universities,1898,$100 to $500 million (USD)
"Lightcast
4.4",4.4,"Moscow, ID",Senior Data Engineer,"A Senior Data Engineer will architect big data analytical frameworks, and translate complex functional and technical requirements into detailed architecture design and high-performing software. A Senior Data Engineer will select data solution software and define hardware requirements. The position will be responsible for developing and implementing standards and processes for integration projects and initiatives.

Major Responsibilities:
Architect software applications, and test and build automated tools.
Select data solution software and define hardware requirements
Develop standards and processes for integration projects and initiatives.
Lead the design/development of software applications, testing, and building tools
Ensure database changes are reviewed and approved to standards
Lead and communicate to leadership on solution design
Provide technical assistance to junior members and to colleagues across the company
Skills & Abilities:
Analytical mind, personality and aptitude for working with data
Ability to look at the numbers, trends, and data to derive conclusions based on findings
Work closely with management to prioritize business and information needs
Proven ability to work effectively to meet goals and deadlines with minimal supervision
Highly creative problem-solving skills and an ability to tailor efforts based on the importance of the issue being addressed.
A hands-on, detail-oriented mindset with the ability to look beyond the box
Expert knowledge of object-oriented design, data structures, and algorithms
Demonstrated experience with agile or other rapid application development methods, object-oriented design, coding, testing patterns with a variety of languages
Significant knowledge of data structures, algorithms, data modeling and disaster recovery of data systems
Education & Experience:
Requires 5 years of data/software engineering/science experience
Bachelor's degree in computer science or other technical disciplines (masters preferred)
About Lightcast:
Lightcast is a global leader in labor market insights with headquarters in Moscow (ID) and Boston (MA) and offices in the United Kingdom, Europe, and India. We work with partners across six continents to help drive economic prosperity and mobility by providing the insights needed to build and develop our people, our institutions and companies, and our communities.

Lightcast is proud to be an equal opportunity workplace and is committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. Lightcast has always been, and always will be, committed to diversity, equity and inclusion. We seek dynamic professionals from all backgrounds to join our teams, and we encourage our employees to bring their authentic, original, and best selves to work.

#LI-SS1","$118,643 /yr (est.)",1 to 50 Employees,Company - Private,Government & Public Administration,State & Regional Agencies,2011,Unknown / Non-Applicable
"Guardsman Group
3.0",3.0,United States,Data Engineer,"A Little About Us
The Guardsman Group stands on over 40 years of experience, unmatched technical capabilities and the unwavering belief in the right of safety for all. Each and every day, our matchless range of services puts Guardsman in the lives of people in every corner of Jamaica and throughout the Caribbean. We've pioneered technologies and perfected procedures to give our customers the best solutions for their homes and businesses. As we enter another decade, we continue to be the industry leader. Today, Guardsman consists of 13 companies and over seven thousand talented staffers who are proud to call themselves a Guardsman.
The Role
This role is within the Business Performance, Analytics and Intelligence (BPAI) Unit of the Guardsman Group and is directly responsible for the shaping, building and implementation of solutions that satisfy the business intelligence needs across the Group.
What You'll Be Doing
Create and maintain optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build analytictools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics
Collaborate with other members of the BPAI unit to build and improve on the availability, integrity, accuracyand reliability of data pipelines
Work closely with all levels of management, IT department and other members of the BPAI Unit to achieve task objectives
A Little Bit About You
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases
Experience building and optimizing 'big data' data pipelines, architectures and data sets
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Strong analytic skills related to working with unstructured datasets
Build processes supporting data transformation, data structures, metadata, dependency and workload management
Minimum Qualifications
Bachelor's Degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline",#N/A,5001 to 10000 Employees,Company - Private,Management & Consulting,Security & Protective,#N/A,Unknown / Non-Applicable
"Contact Government Services, LLC
4.7",4.7,"Washington, DC",Data Engineer,"CGS is seeking a passionate and driven Data Engineer to support a rapidly growing Data Analytics and Business Intelligence platform focused on providing solutions that empower our Federal customers with the tools and capabilities needed to turn data into actionable insights. The ideal candidate is a critical thinker and perpetual learner; excited to gain exposure and build skillsets across a range of technologies while solving some of our clients’ toughest challenges. As a Data Engineer, you will be integral to data operations for the development and integration of multiple data types across a range of data sets and sources.

You will be responsible for the day-to-day operations of systems that depend on data, ensuring data is properly processed and securely transferred to its appropriate location, in a timely manner. Processing data will include managing, manipulating, storing and parsing data in a data pipeline for variety of target sources. You will also support maintenance of applications and tools that reside on these systems such as upgrades, patches, configuration changes, etc. The work is performed in a multidisciplinary team environment using agile methodologies. The candidate we seek must be highly motivated and enthusiastic about implementing new technologies and learning about new data in a small team environment where deadlines are important.
Responsibilities:
Complete development efforts across data pipeline to store, manage, store, and provision to data consumers
Being an active and collaborating member of an Agile/Scrum team and following all Agile/Scrum best practices
Write code to ensure the performance and reliability of data extraction and processing
Support continuous process automation for data ingest
Achieve technical excellence by advocating for and adhering to lean-agile engineering principles and practices such as API-first design, simple design, continuous integration, version control, and automated testing
Work with program management and engineers to implement and document complex and evolving requirements
Help cultivate an environment that promotes customer service excellence, innovation, collaboration, and teamwork
Collaborate with others as part of a cross-functional team that includes user experience researchers and designers, product managers, engineers, and other functional specialists
Required Skills:
Must be a US Citizen
Must be able to obtain a Public Trust Clearance
7+ years of IT experience including experience in design, management, and solutioning of large, complex data sets and models.
Experience with developing data pipelines from many sources from structured and unstructured data sets in a variety of formats
Proficiency developing ETL processes, and performing test and validation steps
Proficiency to manipulate data (Python, R, SQL, SAS)
Strong knowledge of big data analysis and storage tools and technologies
Strong understanding of the agile principles and ability to apply them
Strong understanding of the CI/CD pipelines and ability to apply them
Experience with relational database, such as, PostgreSQL
Work comfortably in version control systems, such as, Git Repositories
Contact Government Services strives to simplify and enhance government bureaucracy through the optimization of human, technical, and financial resources. We combine cutting edge technology with world class personnel to deliver customized solutions that fit specific needs and solve the most challenging, dynamic problems. Our team emphasizes technical expertise and an empathy to public service to provide unique top tier support for our government engagements.
SOME BENEFITS OF THIS POSITION INCLUDE:
Health, Dental, & Vision insurance
Life Insurance
401k
Flexible Spending Account (Health, Dependent Care, and Commuter)
Paid Time Off & Observance of State/Federal Holidays
For more information about CGS please visit: https://www.cgsfederal.com
1YGuQTHTUE","$103,515 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Sconcept,#N/A,Remote,Azure Data Engineer,"** W2 LONG TERM CONTRACT BASED....**(NO C2C)
JOB TYPE:- Long Term Contract(6 months)Extendable
JOB LOCATION:-USA
About Sconcept LLC:-Software Concepts Consulting was established in 2008. With our “hands-on” approach we are very successful in delivering high performance, secure IT solutions and IT services. With headquarters in Carrollton, Texas we are specialized in providing business solutions to the Fortune 500 clients. Our clients include Software, Banking, Financial, Healthcare, Retail, Consumer goods and Insurance sectors. We deliver candidates based on client requirements.
Required Skills:-
Must have worked on several (5+) projects building solutions for Data Pipelines, Data warehousing, Data Modelling on Azure Data Platform (AWS, Google Cloud Platform - good to have)
Expertise in design and development of Data pipelines and ETL using ADF, Databricks to move data from relational/structured/unstructured data from source to data lake to data warehouse, on Azure
Creating Pipelines and integrations of multiple data sources
Data processing and transformation using Databricks (Scala/Python)
Expertise in optimizing cost and performance of data pipelines
Experience working with Data Lake, Synapse on Azure
Designing Data Lake storage layer (Landing/staging, raw, trusted/curated zones)
Designing data storage DB schema, data models and processing on Azure Synapse DW
Experience with Synapse Spark Pool
Experience in Data Modelling
Building Data Models from structured and semi-structured data (Data Model, Common Data Model etc.)
Performance optimization of data model for high performance analytical and reporting workloads
Expertise with SQL Server and extensive experience with SQL programming
Programming and optimizing DB objects Views, Stored Procs, Functions
Good experience in defining data access policy and security for azure data services
Experience with Oracle EDW
Knowledge/Experience migrating from Oracle EDW to any cloud DW
Experience integrating AI skills/ML models with Data and reporting solutions
Experience in building API layer for downstream consumption
Job Type: Contract
Pay: $45.00 - $50.00 per hour
Benefits:
Health insurance
Compensation package:
Bonus pay
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$47.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"VedaInfo Inc
4.1",4.1,"Phoenix, AZ",Big Data Engineer,"Hi,
I hope this note finds you well
I am reaching out to you regarding your career opportunity with us for “Big Data Engineer” for a “6+ Months Contract” @Phoenix, AZ
Please find the below requirement details.
Job Title: Big Data Engineer
Location: Phoenix, AZ (Onsite)
Duration: 6+ months
Rate: $60/hr C2C
Responsibilities:
Design, implement, and maintain big data systems handling large volumes of data.
Utilize Hadoop, Hive, and Spark for efficient data processing and analysis.
Collaborate with cross-functional teams to understand data requirements.
Develop and optimize data ingestion, storage, and transformation processes.
Build scalable data pipelines for seamless data processing and analysis.
Monitor and troubleshoot data processing and performance issues.
Stay updated with emerging big data technologies.
Requirements:
Bachelor's/Master's degree in CS, Engineering, or related field
7+ years of experience as a Big Data Engineer
Strong proficiency in Hadoop, Hive, and Spark
Extensive experience with AWS or Azure
Solid understanding of data ingestion, storage, and transformation
Excellent problem-solving and communication skills
Proactive and self-motivated
This is a 6+ month onsite contract position in Phoenix, AZ, with the possibility of extension. Compensation is $60/hr on C2C basis.
Please submit your updated resume highlighting relevant experience.
Thank you for your interest!
Thanks & Regards
Mohammed ZAIN
Job Type: Contract
Pay: $50.00 - $60.00 per hour
Experience level:
7 years
Ability to commute/relocate:
Phoenix, AZ: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
To improve your chances of getting Shortlisted, make sure to tailor your Profile to the Job Requirements, Highlight your Relevant Skills and Experience, and then upload your Resume.
Experience:
AWS (Amazon Web Services): 7 years (Required)
Azure: 7 years (Required)
Big data: 7 years (Required)
Hadoop: 7 years (Required)
Apache Hive: 7 years (Required)
Spark: 7 years (Required)
Work Location: One location",$55.00 /hr (est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$1 to $5 million (USD)
"Myticas Consulting
3.9",3.9,Arizona,BHJOB15656_20058 - Data Engineer,"Myticas's direct client based out of Phoenix, AZ is currently seeking a Data Engineer for a 100% remote contract position.
Pay Rate: Up to $34/hr

Job Description:
A Data Engineer can confidently understand, write, troubleshoot, optimize, and peer review SQL code. They understand business requirements and create code and reporting that exceeds specifications through anticipating underlying needs. They also understand relational database design, table structure, data types, and data models. They are experienced with and confident in development in reporting visualizations in Excel, PowerBI, Tableau, QuickSight, SSRS, or other data products. They can create reporting dashboards and products that inform the business. They easily grasp and retain complex data concepts and are able to explain them to others. Looking for engineers who are driven to innovate, use cutting edge technologies to solve problems and think outside the box.
What You’ll Do:
Digital Transformation into AWS
Data Warehouse related work (SQL, ETL, Replication, Databases)
Understand, write, troubleshoot, optimize, and peer review SQL code.
Understand business requirements and create code and reporting that exceeds specifications through anticipating underlying needs.
Understand relational database design, table structure, data types, and data models. 6.Experienced with and confidence in development in reporting visualizations in Excel, PowerBI, Tableau, QuickSight, SSRS, or other data products.
Data Warehouse related work (SQL, ETL, Replication, Databases
Perform other duties as assigned or apparent.
MINIMUM EDUCATION AND RELATED WORK EXPERIENCE:
Bachelor's degree in a Computer Science field and 2 years' work experience
2 years of experience in most phases of IT systems deployments in one or more of the following areas: design and deployment of cloud services, data migration to AWS cloud, Digital Transformation into AWS, and Data Warehouse related work
INDCHI
INDREM",$34.00 /hr (est.),1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"MARVEL TECHNOLOGIES INC
3.7",3.7,"Cincinnati, OH",Data Engineer,"Job Description
The data engineer designs and builds platforms tools and solutions that help the bank manage secure and generate value from its data. The person in this role creates scalable and reusable solutions for gathering collecting storing processing and serving data on both small and very large i.e. Big Data scales. These solutions can include on-premise and cloud-based data platforms and solutions in any of the following domains ETL business intelligence analytics persistence relational NoSQL data lakes search messaging data warehousing stream processing and machine learning.
Responsible and accountable for risk by openly exchanging ideas and opinions elevating concerns and personally following policies and procedures as defined. Accountable for always doing the right thing for customers and colleagues and ensures that actions and behaviors drive a positive customer experience. While operating within the Bank's risk appetite achieves results by consistently identifying assessing managing monitoring and reporting risks of all types.
ESSENTIAL DUTIES AND RESPONSIBILITIES:
Responsible for design Development and Support of data solutions APIs tools and processes to enable rapid delivery of business capabilities.
Work closely with IT application teams Enterprise architecture infrastructure information security and LOB stakeholders to translate business and technical strategies into data-driven solutions for the Bank.
Act as a technical Expert addressing problems related to system and application design performance integration security etc.
Conduct research and Development based on current trends and technologies related to the banking industry data engineering and architecture data security and related topics.
Work with developers to Build CI/CD pipelines Self-service Build tools and automated deployment processes.
Evaluate software products and Provide documented recommendations as needed.
Provide Support and troubleshooting for data platforms. Must be willing to Provide escalated on-call Support for complicated and/or critical incidents.
Participate in the planning process for hardware and software.
Plan and work on internal projects as needed including legacy system replacement Monitoring and analytics improvements tool Development and technical documentation.
Provide technical guidance and mentoring for other team members.
Manage and prioritize multiple assignments.
MINIMUM KNOWLEDGE SKILLS AND ABILITIES REQUIRED:
Bachelor's degree in Computer Science/Information Systems or equivalent combination of education and experience.
Must be able to communicate ideas both verbally and in writing to management business and IT sponsors and technical resources in language that is appropriate for each group.
Fundamental understanding of distributed computing principles
Knowledge of application and data security concepts best practices and common vulnerabilities.
Conceptual understanding of one or more of the following disciplines preferred big data technologies and distributions metadata management products commercial ETL tools Bi and reporting tools messaging systems data warehousing Java language and run time environment major version control systems continuous integration/delivery tools infrastructure automation and virtualization tools major cloud or rest API design and development.
Job Types: Full-time, Contract
Pay: $60.00 - $62.00 per hour
Compensation package:
Hourly pay
Yearly pay
Experience level:
9 years
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",$61.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,#N/A,$5 to $25 million (USD)
"ProGrad
4.2",4.2,Remote,Data Engineer,"We are looking for data engineers that are excited about building and maintaining data pipelines for a variety of interests. The pipelines will ingest data from a variety of sources ranging from internal change data streams to external integrations with various SaaS platforms through webhooks and events. The ideal candidate is self driven and looking to grow as a data engineer while having an outsized impact on the future of data at the company. A love for database architecture and optimisation is a must as is the ability to work across diverse teams.
Requirements/Qualifications:
Bachelor's or Master's degree in Data Science (specialization)
Software development experience coding in a general-purpose programming language such as C, C++, Java, JavaScript, Golang, or Python
Experience working with data structures or algorithms (i. e., data structures /algorithms class, coursework/projects, research, internships, or other practical experience in/outside of school or work (including open source hobby coding)
Proficiency in SQL, and NoSQL Databases
Experience working with large datasets
Experience writing and debugging complex SQL queries
A strong understanding of cloud database best practices and performance optimisation
Experience building and maintaining data pipelines, coupled with knowledge of good programming practices such as unit and integration testing
. A strong understanding of cloud database best practices and performance optimisation
Good to have skills:
. Experience building and maintaining data pipelines, coupled with knowledge of good programming practices such as unit and integration testing
Job Types: Full-time, Permanent
Salary: Up to $1,200,000.00 per year
Experience level:
1 year
Under 1 year
Schedule:
Day shift
Work Location: Remote
Speak with the employer
+91 9057202434",#N/A,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
META FORCE IT LLC,#N/A,"Cincinnati, OH",Data Engineer,"Job Description
The data engineer designs and builds platforms tools and solutions that help the bank manage secure and generate value from its data. The person in this role creates scalable and reusable solutions for gathering collecting storing processing and serving data on both small and very large i.e. Big Data scales. These solutions can include on-premise and cloud-based data platforms and solutions in any of the following domains ETL business intelligence analytics persistence relational NoSQL data lakes search messaging data warehousing stream processing and machine learning.
Responsible and accountable for risk by openly exchanging ideas and opinions elevating concerns and personally following policies and procedures as defined. Accountable for always doing the right thing for customers and colleagues and ensures that actions and behaviors drive a positive customer experience. While operating within the Bank's risk appetite achieves results by consistently identifying assessing managing monitoring and reporting risks of all types.
ESSENTIAL DUTIES AND RESPONSIBILITIES:
Responsible for design Development and Support of data solutions APIs tools and processes to enable rapid delivery of business capabilities.
Work closely with IT application teams Enterprise architecture infrastructure information security and LOB stakeholders to translate business and technical strategies into data-driven solutions for the Bank.
Act as a technical Expert addressing problems related to system and application design performance integration security etc.
Conduct research and Development based on current trends and technologies related to the banking industry data engineering and architecture data security and related topics.
Work with developers to Build CI/CD pipelines Self-service Build tools and automated deployment processes.
Evaluate software products and Provide documented recommendations as needed.
Provide Support and troubleshooting for data platforms. Must be willing to Provide escalated on-call Support for complicated and/or critical incidents.
Participate in the planning process for hardware and software.
Plan and work on internal projects as needed including legacy system replacement Monitoring and analytics improvements tool Development and technical documentation.
Provide technical guidance and mentoring for other team members.
Manage and prioritize multiple assignments.
MINIMUM KNOWLEDGE SKILLS AND ABILITIES REQUIRED:
Bachelor's degree in Computer Science/Information Systems or equivalent combination of education and experience.
Must be able to communicate ideas both verbally and in writing to management business and IT sponsors and technical resources in language that is appropriate for each group.
Fundamental understanding of distributed computing principles
Knowledge of application and data security concepts best practices and common vulnerabilities.
Conceptual understanding of one or more of the following disciplines preferred big data technologies and distributions metadata management products commercial ETL tools Bi and reporting tools messaging systems data warehousing Java language and run time environment major version control systems continuous integration/delivery tools infrastructure automation and virtualization tools major cloud or rest API design and development.
Job Type: Contract
Pay: $45.96 - $50.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Cincinnati, OH 45215: Reliably commute or planning to relocate before starting work (Required)
Experience:
data engineer: 10 years (Preferred)
Big data: 3 years (Preferred)
Work Location: Hybrid remote in Cincinnati, OH 45215",$47.98 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Fincons Group
4.0",4.0,Remote,Data Engineer – Databricks / Kafka / SQL,"Fincons Group is an IT business consulting company that has been designing the digital future of leading companies on international markets for 40 years. Fincons Group built its reputation on foreseeing and interpreting new business models and the rapid evolution of IT systems by building a complete range of services: from research and consulting to design and development, from system integration of leading vendor software solutions right up to application management, supporting clients step by step along their digital transformation journey. Fincons is a multinational with over 2,600 people and several offices worldwide (in Italy, Switzerland, Germany, France, the United Kingdom, Belgium and the USA), but above all a Family Company where the founders play a strategic role with commitment and passion, grounding the company in the same principles of a united and caring family.
We firmly believe in the value of cooperation and in the contribution that every idea and intuition can bring. We believe that everyone’s effort can make a difference. Transforming knowledge into a strategic resource is our company mission, and we pursue it with the help of our most strategic asset: our people.

Job Summary
Looking for an experienced data engineer with hands on experience in developing solutions based on Databricks. If you are a curious team-player with passion to learn and working for different clients to develop modern data streaming platforms, please reach out to us!

Work location: any US location might be considered
Employment type: freelance / contractor
Start date: about July 1th
Duration: about 6 months with possible renewal

Responsibilities and Duties
Design, develop and implement real time data ingestion pipelines from multiple sources using Databricks
Develop high-quality, scalable and re-usable frameworks for ingesting high volume and large data sets
Apply best practices to Integrate and ingest various big data formats like Protobuf, Parquet, AVRO and JSON
Work with event based / streaming technologies like Kafka, or Kinesis, to ingest and transform the data using KafkaStreams or KSQL
Working with other members of the project team to support delivery of additional project components (API interfaces, Search)
Work to load test the pipelines for quality and better performance using JMeter, or Gatling etc

Qualifications and skills
Bachelor’s degree and first experience designing, developing, deploying and/or supporting data pipelines using Databricks
Experience in designing and implementing Medallion architecture is preferred
Expertise in designing and deploying data applications on cloud solutions, such as Azure or AWS
Hands on experience in performance tuning and optimizing code running in Databricks environment
Proficient in programming languages like Pyspark and Python
Good understanding of SQL, T-SQL and/or PL/SQL
Demonstrated analytical and problem-solving skills particularly those that apply to a big data environment
Experience with agile development methodologies.
Ability to work effectively both independently and in a group or team environment

Authorization to Work
Applicants for employment in the US must be authorized to work in the US. Fincons Group will not sponsor applicants for work visas.
All qualified applicants will receive consideration for employment without regard to race, colour, religion, sex, sexual orientation, gender identity, national origin, age, disability, or protected veteran status, or any other legally protected basis, in accordance with applicable law.
The data will be processed and stored exclusively for the purposes of this or future selections, guaranteeing the rights referred to in art. 13 Legislative Decree 196/03 and EU regulation 679/2016 (GDPR)
2yrKJdtT13",#N/A,201 to 500 Employees,Company - Private,Financial Services,Accounting & Tax,#N/A,Unknown / Non-Applicable
"Vedainfo
4.1",4.1,"Peoria, IL",Data Engineer,"Job Description
· Create and develop optimal data pipeline which fetches data from raw and apply business transformation and load derived.
· Build the AWS Fargate components required for optimal extraction, transformation, and loading of data-to-data lake.
· Develop data pipeline setup between base and derive values based on Caterpillar specification.
· Implement quality validation rules and follow process defined by stakeholder.
· Develop business events as defined by the solution team to generate CDC events for consumption layer (Kinesis)
· Create re-usable and configurable components wherever applicable.
· Create and execute unit, integration, and regression tests
· Ensure the code is developed satisfying all business rules, perform unit testing where applicable.
· Data pipelines development
· Python Development
· AWS cloud services
· Data quality validation rules
· Java (preferred)
· Implement CDC
· Transformation rules creation
· Message broker tools like Kinesis
· Azure DevOps – CI/CD pipelines
· Automated and Manual Testing experience
Job Type: Contract
Pay: Up to $60.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Peoria, IL 61602: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
This position is remote initially and you will be asked to work any of the client location once offer is made. Please confirm your current location.
Experience:
Data Engineer: 1 year (Preferred)
Python Development: 1 year (Preferred)
AWS: 1 year (Preferred)
Message broker tools: 1 year (Preferred)
Work Location: In person",$60.00 /hr (est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$1 to $5 million (USD)
Enterprise Minds,#N/A,Remote,Data Engineer,"Job Role: SQL Data Engineer
Location: Remote Opportunity - Need to work in PST hours
Visa: US Citizen Only
Duration: Long term
Roles and Responsibilities:
3+ years of experience with a complex build system in AWS using Starburst.
5+ years of experience with installation, maintenance, and administration of Oracle databases- PL/SQL and ANSI SQL
Oracle DBA + Oracle SQL to ANSI SQL conversion, performance/query tuning
3+ years of experience configuring, integrating, and securing multiple AWS database offerings (ex. RDS, Aurora, Casandra, and Dynamo)
2+ years of experience with supporting Linux systems engineering efforts in system design and evaluation, solution engineering, software development, or system administration.
Experience with Database Schema as Code tools
Ability to design and develop complex SQL queries and reports to retrieve required data.
Experience with database partitioning and microservices
Knowledge of Agile methodologies or the software development life cycle (SDLC)
US Citizenship is Mandatory.
Job Type: Full-time
Experience:
Data Engineer: 10 years (Preferred)
AWS using Starburst: 4 years (Preferred)
PL/SQL and ANSI SQL: 2 years (Preferred)
AWS RDS: 1 year (Preferred)
Work Location: Remote",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
shreetek,#N/A,"Charlotte, NC",Senior Data Engineer,"Job Title : Data Engineer
ONLY USC
On W2 only
Job Location : Newjersey OR Charlotte,NC
Collect, manage, and convert raw data accurately and reliably
Organize data systems for subgroup access and analyses
Configure and sustain data cloud structures
Able to work with structured and unstructured data.
Validate outputs of data pipelines
Degree in Data Engineering preferred.
Two Programmers –
Convert existing SAS code to python/pyspark code for model operation in the cloud.
Create and sustain policy analysis models in the cloud.
Troubleshoot user interfaces in the cloud.
Create and sustain intuitive user interfaces in the cloud.
A degree in Computer Science is preferred.
Overall across resources keep following stack as reference
Visualization : Tableau
Data Modeling/Science : Python / SAS
Engg : AWS cloud native services, security, data pipeline
Job Type: Contract
Salary: $50.00 - $60.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Charlotte, NC 28202: Reliably commute or planning to relocate before starting work (Required)
Work Location: One location",$55.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Zllius Inc.,#N/A,"Chicago, IL",Data Engineer,"Hi,
Greetings from Zllius Inc!
We have an urgent opening for the role Data Engineer. If interested, revert me back with your Updated resume, along with contact details, work authorization.
Job Role: Data Engineer
Duration: Long-Term
Position Type: W2/ 1099
Visa: Any
Job Description:
Technical/Functional Skills
Successful candidates will have:
Hands on experience in Dremio and Scripting
Hands on Scripting experience in Python and Flask
Hands on experience in Integrating data from multiple source systems (PostgreSQL and CSV)
Experience with Apache Superset
Ability to write complex SQL queries
Experience in building data pipelines using PostgreSQL, Oracle database, Dremio, AWS Open Search
Experience with Docker Containerization of Code
Work as a part of a scrum team in Agile methodology
ETL, Data Pipelines, PostgreSQL, SQL Queries, Scripting
Job Types: Full-time, Contract
Salary: $111,076.11 - $133,769.08 per year
Schedule:
8 hour shift
Day shift
Monday to Friday
Work Location: On the road","$122,423 /yr (est.)",Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
NLB Technology Services,#N/A,"Dallas, TX",CRM data Analytics Engineer,"MUST HAVE:
o Tableau CRMA & Einstein Discovery Certification
o Experience building Dashboards using CRMA (aka Tableau CRM and Einstein Analytics)
o Design, build, and support datasets, data recipes, dataflows, and dashboards using Salesforce and non-Salesforce data sources
o Troubleshoot functional, data, and performance issues with CRMA Dashboards
o Skilled in SAQL, JSON
o Develop and deploy AI/ML solutions using Einstein Discovery and deploy AI/ML Models built outside Salesforce
o Experience working with large data (1 B+ rows) and supporting large user base (10,000+)
o Mentor and coordinate the development efforts of other CRMA DevelopersAT&T
Job Type: Contract
Salary: $65.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Vision insurance
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
CRM ANalytics: 8 years (Required)
einstein: 1 year (Required)
Salesforce: 1 year (Required)
Work Location: One location",$70.00 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Lightcast
4.4",4.4,"Moscow, ID",Senior Data Engineer,"A Senior Data Engineer will architect big data analytical frameworks, and translate complex functional and technical requirements into detailed architecture design and high-performing software. A Senior Data Engineer will select data solution software and define hardware requirements. The position will be responsible for developing and implementing standards and processes for integration projects and initiatives.

Major Responsibilities:
Architect software applications, and test and build automated tools.
Select data solution software and define hardware requirements
Develop standards and processes for integration projects and initiatives.
Lead the design/development of software applications, testing, and building tools
Ensure database changes are reviewed and approved to standards
Lead and communicate to leadership on solution design
Provide technical assistance to junior members and to colleagues across the company
Skills & Abilities:
Analytical mind, personality and aptitude for working with data
Ability to look at the numbers, trends, and data to derive conclusions based on findings
Work closely with management to prioritize business and information needs
Proven ability to work effectively to meet goals and deadlines with minimal supervision
Highly creative problem-solving skills and an ability to tailor efforts based on the importance of the issue being addressed.
A hands-on, detail-oriented mindset with the ability to look beyond the box
Expert knowledge of object-oriented design, data structures, and algorithms
Demonstrated experience with agile or other rapid application development methods, object-oriented design, coding, testing patterns with a variety of languages
Significant knowledge of data structures, algorithms, data modeling and disaster recovery of data systems
Education & Experience:
Requires 5 years of data/software engineering/science experience
Bachelor's degree in computer science or other technical disciplines (masters preferred)
About Lightcast:
Lightcast is a global leader in labor market insights with headquarters in Moscow (ID) and Boston (MA) and offices in the United Kingdom, Europe, and India. We work with partners across six continents to help drive economic prosperity and mobility by providing the insights needed to build and develop our people, our institutions and companies, and our communities.

Lightcast is proud to be an equal opportunity workplace and is committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. Lightcast has always been, and always will be, committed to diversity, equity and inclusion. We seek dynamic professionals from all backgrounds to join our teams, and we encourage our employees to bring their authentic, original, and best selves to work.

#LI-SS1","$118,643 /yr (est.)",1 to 50 Employees,Company - Private,Government & Public Administration,State & Regional Agencies,2011,Unknown / Non-Applicable
"DiamondPick
4.5",4.5,Remote,GCP Data Engineer,"Position: GCP Data Engineer (8+years)
Location: Remote
Type: Contract
Job Details:
· Relevant Industry Work Experience (6+ for Developer, preferred 9+ years)
· Experience extracting data from a variety of sources, and a desire to expand those skills (Excellent knowledge in SQL and Spark is mandatory)
· Strong knowledge of Google BigQuery and architecting data pipelines from on-prem to GCP.
· Experience building applications using Google Cloud Platform related frameworks such as DataProc and GCS at the minimum.
· Excellent Communication Skills to Understand and Pass on Requirements.
· Excellent Data Analysis skills. Must be comfortable with querying and analyzing large amount of data on Hadoop HDFS using Hive and Spark.
· Professional experience with a cloud hosting platform (GCP preferred)
· GCP certification is preferred.
Job Type: Contract
Salary: Up to $65.00 per hour
Experience level:
10 years
8 years
9 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
Google BigQuery: 4 years (Preferred)
DataProc: 2 years (Preferred)
Google Cloud Platform: 9 years (Required)
Spark: 8 years (Required)
SQL: 8 years (Required)
License/Certification:
Google Cloud Platform(GCP) Certification (Required)
Work Location: Remote",$65.00 /hr (est.),1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,#N/A,Unknown / Non-Applicable
TekValue IT Solutions,#N/A,"Houston, TX",Data Engineer with Migrating,"Required Skills:
8-10 Required Experince on Data Engineer and Revalent Skills
Experience with NoSQL (MongoDB)
Experience with API'S
Good Knowledge on Data Migration like Oracle to Mongo db
Experience with Python Programming
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Houston, TX 77002: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 8 years (Required)
NoSQL: 6 years (Required)
MongoDB: 6 years (Required)
Migrating: 5 years (Required)
Work Location: One location
Speak with the employer
+91 7328323606",$72.50 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Infinity Quest
3.9",3.9,"Cleveland, OH",Data Center Engineer,"Required Skillsets:
1. Datacenter Management
2. Hardware Rack & Stack Cabling
3. Knowledge and operational of Diesel Generators, Cooling & Chillers
4. Inventory Asset management
5. DC Monitoring DC availability
6. Handling of CISCO UCS HW and VX RAIL, POWER FLEX
Activities:
1. Operate and Manage both routine and emergency service on a variety of state-of-the-art critical systems such as:
a. Medium voltage switchgear,
b. diesel generators,
c. UPS systems
d. Power distribution equipment, chillers, cooling towers, computer room air handlers,
e. Fire detection / suppression; building monitoring systems (BMS), building Automated Systems (BAS) ; etc
2. Supervise the on-site management of sub-contractors and vendors, ensuring that all work is performed according to established practices and procedures.
3. Manage local client relationship and act as the point of contact for the company at this site.
4. Establish performance benchmarks, conduct analyses and prepare reports on all aspects of the critical facility operations and maintenance.
5. Work with IT managers and other business leaders to coordinate projects, manage capacity and optimize plant safety, performance, reliability and efficiency.
6. Create, utilize and administer MOPs , SOPs, and Preventative Maintenance Procedures for all work on critical data center facility equipment.
7. Schedule work activities, within specified change control / management protocol.
8. Maintain a constant state of readiness in support of the mission goal of 99.999% uptime
9. Racking, Stacking of Infrastructure
a. Cabling Management
b. Patching, Network port swapping
c. Hardware reboots
d. Vendor coordination
e. Project Coordination
· Inventory asset management, Physical access management
Monitoring of Datacenter availability & update respective teams accordingly
Job Type: Contract
Salary: From $80.00 per year
Schedule:
8 hour shift
Ability to commute/relocate:
Cleveland, OH: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data center: 10 years (Preferred)
CISCO UCS: 10 years (Preferred)
MOPS: 10 years (Preferred)
SOP: 10 years (Preferred)
Cabling Management: 10 years (Preferred)
Network protocols: 10 years (Preferred)
Hardware reb: 10 years (Preferred)
VENDOR COORDINATION: 10 years (Preferred)
Project coordination: 10 years (Preferred)
Security clearance:
Confidential (Preferred)
Speak with the employer
+91 8838059965",#N/A,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,Unknown / Non-Applicable
"E-Business International INC
3.6",3.6,"Alexandria, VA",AWS Data Engineer,"Role: AWS Data Engineers
Location: Alexandria, VA
Full-Time/Permanent
AWS Engineers
We are seeking TWO (2) AWS Engineers to join our growing team. The qualified applicants will become part of the Enterprise Data Analytics Services (EDAS) program for a large federal agency in Alexandria, VA. Hybrid work options are available.
The AWS Engineer is responsible for the development and support of cloud-based solutions. Solutions include moving assets from on-premise to cloud environment, evaluation, and optimization. AWS Engineer will collaborate with the team effectively to ensure the team's success.
Responsibilities/Duties:
Work on automating the migration process in AWS from development to production.
Create CFT deploying/updating cloud resources.
Advise the engineering and software engineering team as they migrate from on-premise to cloud infrastructure.
Optimize cloud workloads for cost, scalability, availability, governance, compliance, etc.
Guide and/or provide hands-on support to administer production, staging, and deployment environments.
Partner with multi-disciplinary teams to understand requirements and plan architecture and solutions.
Apply insight and expertise across AWS services.
Apply knowledge of scripting and automation using tools like PowerShell, Python, Bash, Ruby, Perl, etc.
Experience and Qualifications:
Good working knowledge of AWS Services, including Compute, Storage Networking, Database, Management tools, Security, Identity, and Compliance.
3 years’ experience in AWS (EC2, Glue, S3, Redshift, Python programming).
Experience implementing and debugging workloads in AWS.
Good knowledge of creating CFTs.
Experience building infrastructure inside AWS via code. Familiarity with tools such as Terraform or CloudFormation.
Ability to proactively communicate, interact, and solve critical customer problems.
Ability to think and act strategically and proactively.
3 years of experience in software development tools and methodologies.
3 years of experience in infrastructure architecture, database architecture, and networking.
3 years of experience architecting/deploying operating solutions built on AWS.
Education/Certifications:
Bachelor's Degree in Computer Science, Information Technology, Engineering, or associated discipline.
AWS certification is a plus!
Clearance Requirements:
Ability to attain and maintain a Public Trust.
Job Type: Full-time
Salary: From $100,000.00 per year
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Alexandria, VA 20598: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Expected Salary Range (Must)?
Visa Status/ Work Authorization (Must)?
Experience:
AWS: 4 years (Required)
Work Location: One location","$100,000 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,1992,Unknown / Non-Applicable
"Amazee Global Ventures Inc
5.0",5.0,Remote,Data Engineer,"(W2 ONLY)
Job Title: Data Engineer
Duration : 12 months
Location: Remote
6-7 years of experience
Healthcare experience is must
Design, develop, and maintain Tableau dashboards and reports
Develop and maintain data models and ETL processes
Analyze data to identify trends and patterns
Create and maintain data dictionaries and data models
Develop and maintain data pipelines
Collaborate with stakeholders to understand data requirements
Develop and maintain data quality processes
Monitor data quality and performance
Troubleshoot data issues
Develop and maintain documentation
Requirements:
Bachelor’s degree in Computer Science, Information Systems, or related field
4+ years of experience in data engineering and analysis
Expertise in Tableau
Experience with SQL and data modeling
Knowledge of ETL processes
Knowledge of data warehousing
Excellent problem-solving and communication skills
Ability to work independently and in a team environment\
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Type: Full-time
Salary: $40.33 per hour
Work Location: Remote",$40.33 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,2019,Less than $1 million (USD)
"BigLynx Computer Software
4.9",4.9,"Redmond, WA",Databricks Data Engineer,"BigLynx, Inc is an American multinational technology corporation headquartered in Seattle, Washington, with operations in the United States, Canada, and India. The company began in 2016, as a product development company specializing in AI/ML Data Engineering in the Retail vertical space with its products warehouse & fast. Post Pandemic in 2022, BigLynx added a business division of boutique technology consulting, specializing inData Engineering, Full Stack , and Microsoft Dynamics helping clients build the next generation data platform and big data pipelines.
Data pipeline development: Design, develop, and maintain scalable and efficient data pipelines using Databricks to ingest, transform, and load data from various sources. This includes data extraction, data cleansing, data transformation, and data loading processes.
Data modeling and schema design: Design and implement data models, database schemas, and data structures on Databricks. Optimize data models for performance, scalability, and ease of use.
ETL processes: Develop and maintain ETL (Extract, Transform, Load) processes using Databricks to transform and cleanse data. Implement efficient data integration and transformation logic using languages such as Python, SQL, or Scala.
Data integration: Integrate data from multiple systems and sources, ensuring data consistency, accuracy, and quality. Develop and maintain data connectors, APIs, and data ingestion processes.
Performance optimization: Identify and address performance bottlenecks in data pipelines and data models. Optimize query performance, data loading, and data processing capabilities on Databricks.
Data governance and security: Implement data governance practices, data privacy measures, and security controls on Databricks. Ensure compliance with data governance policies and regulations.
Monitoring and troubleshooting: Monitor the health and performance of Databricks data infrastructure, data pipelines, and data processing jobs. Troubleshoot issues and provide timely resolutions.
Collaboration and teamwork: Collaborate with cross-functional teams, including data scientists, data analysts, and business stakeholders, to understand data requirements, provide data engineering expertise, and support their data-related needs.
Qualifications:
Databricks expertise: Strong knowledge and hands-on experience with the Databricks platform, including Databricks notebooks, Databricks runtime, and Databricks clusters.
Data engineering skills: Proficiency in data engineering principles, ETL processes, data modeling, and data integration techniques. Experience with programming languages such as Python, SQL, or Scala.
Big data technologies: Experience with big data technologies, such as Apache Spark, Apache Hadoop, or related frameworks. Familiarity with distributed computing and data processing concepts.
Cloud platforms: Experience working with cloud platforms, preferably Azure Databricks, AWS Databricks, or Google Cloud Databricks. Knowledge of cloud storage, compute, and networking services.
Database and data warehouse concepts: Understanding of relational databases, data warehousing concepts, and SQL. Familiarity with data warehousing best practices and dimensional modeling.
Performance optimization: Strong skills in optimizing Spark jobs and queries on Databricks. Ability to identify and resolve performance bottlenecks.
Problem-solving skills: Strong analytical and problem-solving abilities to tackle complex data engineering challenges and troubleshoot issues.
Collaboration and communication: Excellent collaboration and communication skills to work effectively with cross-functional teams and stakeholders, translating business requirements into technical solutions and providing technical guidance.
Education: A bachelor's or master's degree in computer science, data engineering, or a related field is typically required. Relevant certifications, such as Databricks Certified Developer or similar, are h
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Experience level:
7 years
Schedule:
Monday to Friday
Work Location: In person",$75.00 /hr (est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Wallero
4.4",4.4,"Seattle, WA",Data Engineer,"Hi,
Pretty much interested in your resume, we have an urgent opening for Data Engineer role with one of our clients based in Seattle WA.
Position: Data Engineer
Location: Seattle WA - Remote (100%)
Description:
Good Experience in reporting space including PowerBI, ADF, and possibly standing up a data warehouse/data mart.
The main scope centering is around reporting.
Experience with SQL, and agile framework including requirement gathering and story/feature writing skills.
Need someone strong who is comfortable leading and guiding teams in that capacity.
Migrating data from third party to Azure Cloud
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Seattle, WA 98101: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",$75.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2015,$1 to $5 million (USD)
"JPMorgan Chase Bank, N.A.
3.8",3.8,"Wilmington, DE",Data Engineer III,"As a Data Engineer III under Consumer and Community Banking within the ADE Data Governance team at JPMorgan Chase, you serve as a seasoned member of an agile team to design and deliver trusted data collection, storage, access, and analytics solutions in a secure, stable, and scalable way. You are responsible for developing, testing, and maintaining critical data pipelines and architectures across multiple technical areas within various business functions in support of the firm's business objectives.

Job responsibilities
Supports review of controls to ensure sufficient protection of enterprise data
Responsible for advising and making custom configuration changes in one to two tools to generate a product at the business or customer request
Updates logical or physical data models based on new use cases
Frequently uses SQL and understands NoSQL databases and their niche in the marketplace
Adds to team culture of diversity, equity, inclusion, and respect

Required qualifications, capabilities, and skills
Experience across the data lifecycle
Advanced at SQL (e.g., joins and aggregations)
Working understanding of NoSQL databases
Proficiency with JSON/Java, Erwin, Oracle, Snowflake, AWS
Significant experience with statistical data analysis and ability to determine appropriate tools and data patterns to perform analysis, including data modeling
Experience customizing changes in a tool to generate product
Executes data architecture solutions and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions and break down problems
Produces data architecture and design artifacts for complex applications while being accountable for ensuring design constraints are met by software code development
Gathers, analyzes, synthesizes, and develops visualizations and reporting from large, diverse data sets in service of continuous improvement of data frameworks, applications, and systems that proactively identifies hidden problems and patterns in data and uses these insights to drive improvements to coding hygiene and system architecture

Preferred qualifications, capabilities, and skills
Familiarity with modern technologies
Exposure to cloud technologies
Chase is a leading financial services firm, helping nearly half of America's households and small businesses achieve their financial goals through a broad range of financial products. Our mission is to create engaged, lifelong relationships and put our customers at the heart of everything we do. We also help small businesses, nonprofits and cities grow, delivering solutions to solve all their financial needs.

We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.
The health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the ""WELL Health-Safety Rating"" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.
As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm's current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm's vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.
We offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, discretionary incentive compensation which may be awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.
Equal Opportunity Employer/Disability/Veterans
Base Pay/Salary
Jersey City,NJ $97,850.00 - $150,000.00 / year","$123,925 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Banking & Lending,1799,$10+ billion (USD)
Cloudburst Technologies,#N/A,Remote,Data Collection Engineer,"About Us
Cloudburst is a seed-funded New York-based remote-first company helping customers with detection, prevention, and investigation into cryptocurrency market manipulation and fraud.
Cloudburst provides regulators, financial institutions, trading platforms, and others with access to a real-time, machine-readable crypto market monitoring tool, enabling advanced levels of diligence and customer/market protection. The Cloudburst team has multiple years of experience working together, plus it has worked directly with international law enforcement agencies and policymakers on cybercrime, terrorism, and high-level financial fraud.
What We’re Looking For
The ideal candidate
is an empathetic teammate who loves to help others, to bring order to chaos, and to document their path for others to follow
focuses primarily on web scraping, API integration, and collection of raw data
is capable of becoming fluent in Layer 7 protocols, blockchains, and emerging protocols
has high moral and ethical standards
is proficient in production-level Python
has worked in a threat intel or security environment
What You Will Do
Your primary responsibility will be to become expert at collecting open source data from the web and from other online protocols.
You will be comfortable building tools to bring in data from random files, varying degrees of quality of web sites, and new communications tools. You will have the opportunity to creatively apply your knowledge of software automation towards scaling data collection.
You will understand popular chat sites and how fraud actors use them. You will understand emerging technologies and will actively evaluate them for potential opportunities to collect data.
Why Cloudburst?
At Cloudburst we want to minimize our software engineers working on pixel-pushing and to maximize time spent understanding our domain and building products to help our customers investigate and prevent fraud.
We care about developing and sponsoring our engineers internally. The department adheres to these principles:
Rapid deployment of innovative new techniques for signal generation and attribution
KISS architecture that doesn’t get in developers’ or R&D’s way; we want code to be easy to understand, less abstract, easy to test, fast to deploy, and reducible to automation or simple manual processes
Building a diverse, unconventional team that cross-trains and grows together without ego or sacrifice to work-life balance
What You Will Be Using
Python
Cloud-based infrastructure
3rd party vendor integrations
Novel techniques for data collection using home-grown or discovered tools and frameworks
Hiring Process
Initial email
Initial phone screen with the hiring manager
Take-home test OR link to previous code
Panel interview
Call with CEO
Offer
Investors
Strategic Cyber Ventures
Coinbase Ventures
Bloccelerate
More Info:
https://www.crunchbase.com/organization/cloudburst-technologies-0a3e
https://www.prnewswire.com/news-releases/cloudburst-technologies-raises-3m-in-seed-funding-led-by-strategic-cyber-ventures-joined-by-coinbase-ventures-and-bloccelerate-301817742.html
https://burst.cloud/
https://open.spotify.com/episode/2cv1Is77s8jGLtHPPYRLPU
Job Type: Full-time
Pay: $115,000.00 - $150,000.00 per year
Benefits:
Dental insurance
Flexible schedule
Health insurance
Paid time off
Vision insurance
Compensation package:
Stock options
Experience level:
3 years
Schedule:
Choose your own hours
No nights
No weekends
Work Location: Remote","$132,500 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Stark Dev, LLC",#N/A,"Plano, TX",Data Engineer/Analyst / W2 /USC or GC (GC EAD) or H4 holders,"This position is open for United States Citizens or Green Card holders (GC EAD)/H4 EAD ONLY.
This is an on-site position in Plano/Dallas
CONTRACT W2
Top Skills Details
1) Experience working on a data migration project as a Data Analyst.
- This person will be working on their Permitting, Planning, and Inspection System Project. They are moving from a legacy permitting system, TRAKIT, to a completely new Salesforce application, Clariti. (TRAKIT and Clariti experience not required)
2) Experience doing data discovery, classification, verifying data, mapping rules
- On this project this team will be moving all of the historical data from the old application, TRAKIT, to the new application Clariti.
3) Proficient with SQL and writing SQL queries
\* Interpret data, analyze results using statistical techniques and provide ongoing reports
\* Develop and implement databases, data collection systems, data analytics and other strategies that optimize statistical efficiency and quality
\* Acquire data from primary or secondary data sources and maintain databases/data systems
\* Identify, analyze, and interpret trends or patterns in complex data sets
\* Filter and “clean” data by reviewing computer reports, printouts, and performance indicators to locate and correct code problems
\* Work with management to prioritize business and information needs
\* Locate and define new process improvement opportunities
Drug Test Required
false
Go To Work
false
Workplace Type
On-site
Experience Level
Expert Level
Job Type: Contract
Pay: $55.00 - $60.00 per hour
Schedule:
Monday to Friday
Application Question(s):
What is your visa status?
Work Location: In person",$57.50 /hr (est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Fresh Consulting
3.9",3.9,"Menlo Park, CA",Data Engineer,"Fresh Consulting is a design-led, software development and hardware engineering company, offering end-to-end digital services to help companies innovate. We bring together amazing UX designers, sophisticated developers, digital strategists, and top-notch engineers to help companies create fresh experiences that connect humans, systems, and machines. We’ve been growing fast and need someone to help us continue to manage the delivery of high-quality work in a fast-paced environment.
See more at freshconsulting.com Visit freshconsulting.com/portfolio to see our project work across several industries.
View and apply to all jobs - https://freshconsulting.applytojob.com/apply/ or visit freshconsulting.com/careers
Title: Data Engineer
Duration: 6 months with possible extension
Location: Onsite Menlo Park, CA
Benefits: Employee benefits at 100% including Medical, PTO, Holiday Pay, 401K Plan, and much more!
Hours: Minimum 40 Hours/Week
Role:
Design, construct, install, test, and maintain highly scalable data management systems.
Ensure systems meet business requirements and industry practices.
Design, implement, automate, and maintain large-scale enterprise data ETL processes.
Build high-performance algorithms, prototypes, predictive models, and proof of concepts.
Skills:
5+ years of work experience as a Data Engineer.
Ability to work as part of a team, as well as work independently or with minimal direction.
Excellent written, presentation, and verbal communication skills.
Collaborate with data architects, modelers, and IT team members on project goals.
Strong PC skills including knowledge of Microsoft SharePoint.
Process certification, such as Six Sigma, CBPP, BPM, ISO 20000, ITIL, and CMMI.
Education: BSCSE or related.
FRESH-
Work on engineering and research assignments with F500 companies and startups.
The relationships that we have created with our clients are one of a kind.
We help solve problems in many technologies focusing on R&D, product development, and manufacturing.
We work with the most cutting-edge and latest technologies from AR/VR to Autonomous technologies.
Closely working with our clients, we believe that long-term investments are extremely important to maintain the culture we together have created.
We’re a handpicked team of Engineers, digital strategists, designers, and developers united together in creating a fresh experience. Whether we are strategizing, designing, developing, or analyzing, our integrated team works as an extension of yours to improve your impact, your usability, and your customer conversion. In the process, we collaborate with you to get to know your business, understand your industry, and incorporate your big ideas into memorable experiences that keep your customers coming back for more.
Equal employment opportunity: All qualified persons will be considered for employment without regard to race, color, religion, sex, national origin, age, marital status, familial status, gender identity, sexual orientation, disability for which a reasonable accommodation can be made or any other status protected by law. Assistance will be gladly provided upon request for any applicant with sensory or non-sensory disabilities.
Fresh Consulting is a participating E-Verify company.
freshconsulting.com
Compensation offered will be determined by factors such as location, level, job-related knowledge, skills, and experience. Range $70/hr - $80/hr.
ylM5WIC1Wr",$75.00 /hr (est.),201 to 500 Employees,Company - Private,Management & Consulting,Business Consulting,2007,$25 to $100 million (USD)
NAVA TECH LLC,#N/A,Remote,Azure Data Engineer,"Azure Data Engineer
Remote
Long Term
Note: Need 10+ years of experience
Sound skills and hands on experience with Azure Data Lake, Azure Data Factory, SQL Data Warehouse Azure Blob, Azure Storage Explorer
Proficient in creating Data Factory pipelines for on-cloud ETL processing; copy activity, custom Azure development etc.
Knowledge of Azure Data Catalog, Event Grid, Service Bus, SQL and Synapse
Experience using Azure Databricks platform.
Experience with Python programming.
Experience using pandas and numpy for data engineering and data cleansing;
Ability to troubleshoot the job scheduler and compute clusters.
Experience with databricks CLI and secrets module.
Experience with Azure Blob storage access configuration.
p l e a s e s h a r e r e s u m e s t o m o h a m m e d (a t) n a v a t e c h (d o t) u s O R 4 4 3 5 3 7 9 8 3 5
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$62.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Maven Workforce
4.1",4.1,"Alpharetta, GA",Data Engineer,"Must Have skills:
Building Data Pipeline exp is a must
ETL Tools – SSIS, Alteryx
Data modelling
SQL Server development
Microsoft SQL Stack
Responsibilities:
Data acquisition and ingestion - Identify data sources and build pipelines using various ETL tools such as but not limited to, SSIS, and Alteryx. Data sources including but not limited to SQL Server, SAP, Teradata, Hadoop\Hive, PostgreSQL, Oracle and flat files.
Identifying ways to improve data reliability, efficiency and quality by various data solution techniques.
Expertise in Data project management with JIRA stories. Work as a liaison between business and IT to ensure successful and timely completion of the projects.
Assist in defining the data architecture framework, standards and principles, including modeling, metadata, security and reference data.
Create and optimize data models to support various business applications.
Reviewing modifications of existing data systems for cross-compatibility.
Automate and support workflows to ensure timely delivery.
Must Have:
5+ years of SQL Server development experience.
3+ years ALTERYX Admin /User management experience and advance workflow management.
5+ years data modeling experience.
5+ years of ETL experience.
5+ years of experience in working on more than one database technologies Microsoft SQL server, Teradata.
2+ years big data experience, Hadoop, Hive, Spark
Expert knowledge of data warehousing.
DESIRED SKILLS:
BI Lifecycle management
Working understanding of Microsoft VBA, HTML, Python
Data model development using ERWIN
Job Type: Contract
Salary: $50.00 per hour
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Building Data Pipeline: 8 years (Required)
ETL Tools – SSIS, Alteryx: 7 years (Required)
Data modelling: 6 years (Required)
SQL Server development: 8 years (Required)
Microsoft SQL Stack: 8 years (Required)
Work Location: One location
Speak with the employer
+91 7328456015",$50.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2008,Unknown / Non-Applicable
"Procore Technologies
4.5",4.5,Oregon,Staff Data Engineer,"Job Description

What if you could use your technology skills to develop a product that impacts the way communities' hospitals, homes, sports stadiums, and schools across the world are built? Construction impacts the lives of nearly everyone in the world, and yet it's also one of the world's least digitized industries, not to mention one of the most dangerous. That's why we're looking for a talented Staff Data Engineer to join Procore's journey to revolutionize a historically underserved industry.
As a Staff Data Engineer, you'll design and develop data products for Procore Data Platform data management area. You'll be part of the high-performance team of Data Engineers and will collaborate with platform engineers and product leaders.
This position will report to our Senior Manager of Data Engineering, and can be based remotely from any US location. We're looking for someone to join our team immediately.
What you'll do:
Lead the design and development of big data predictive analytics using object-oriented analysis, design and programming skills, and design patterns
Implement ETL workflows for data matching, data cleansing, data integration, and management
Maintain existing data pipelines and develop new data pipelines using big data technologies
Develop and maintain tables and data models in SQL, abstracting multiple sources and historical data across varied schemas to a format suitable for further analysis
Responsible for leading the effort to continuously improve the reliability, scalability, and stability of the enterprise data platform
Contribute to and lead the continuous improvement of the software development framework and processes by collaborating with Quality Assurance engineers
Deliver observable, reliable, and secure software, embracing the ""you build it, you run it"" mentality, focusing on automation and GitOps
Participate in daily standups, team meetings, sprint planning, and demo/retrospectives while working cross-functionality with other teams to drive the innovation of our products
Apply data governance framework, including the management of data, data compliance operating model, data policies, and standards
What we're looking for:
BS degree in Computer Science, a similar technical field of study, or equivalent practical experience; MS or Ph.D. degree in Computer Science or a related field is preferred
5+ years of experience in a Data Engineering position
Strong expertise with 3+ years of experience building enterprise techniques for large-scale distributed system design and data processing, including:
Building data pipelines with Databricks as the source
Building and maintaining data warehouses in support of BI tools (Snowflake, dbt, Tableau)
Building data pipeline framework for data workflow to process large data sets and Real-Time & Batch Data Pipeline development
Experience in processing structured and unstructured data into a form suitable for analysis and reporting with integration with a variety of data metrics providers ranging from advertising, web analytics, and consumer devices
Desire to be actively hands-on with code, using Java, Python (80%), and SQL, along with willingness and passion for mentoring junior engineers and performing code reviews
Possess familiarity with AWS-managed services for data (Glue, Athena, Data Pipeline, Flink, Spark) and Snowflake

Additional Information

Base Pay Range $147,200-$202,400. Eligible for Bonus Incentive Compensation. Eligible for Equity Compensation. Procore is committed to offering competitive, fair, and commensurate compensation, and has provided an estimated pay range for this role. Actual compensation will be based on a candidate’s job-related skills, experience, education or training, and location.
Perks & Benefits
At Procore, we invest in our employees and provide a full range of benefits and perks to help you grow and thrive. From generous paid time off and healthcare coverage to career enrichment and development programs, learn more details about what we offer and how we empower you to be your best.
About Us
Procore Technologies is building the software that builds the world. We provide cloud-based construction management software that helps clients more efficiently build skyscrapers, hospitals, retail centers, airports, housing complexes, and more. At Procore, we have worked hard to create and maintain a culture where you can own your work and are encouraged and given resources to try new ideas. Check us out on Glassdoor to see what others are saying about working at Procore.
We are an equal-opportunity employer and welcome builders of all backgrounds. We thrive in a diverse, dynamic, and inclusive environment. We do not tolerate discrimination against employees on the basis of age, color, disability, gender, gender identity or expression, marital status, national origin, political affiliation, race, religion, sexual orientation, veteran status, or any other classification protected by law.
If you'd like to stay in touch and be the first to hear about new roles at Procore, join our Talent Community.","$174,800 /yr (est.)",1001 to 5000 Employees,Company - Public,Information Technology,Enterprise Software & Network Solutions,2002,Unknown / Non-Applicable
"Deako
4.9",4.9,"Seattle, WA",Data Engineer II,"About Deako
Deako is delivering a revolutionary plug-n-play smart home platform with a laser focus on the untapped new home construction market. We make smart lighting so easy to upgrade that even those who would have never considered smart lighting are making it happen. We've built a company based on trust; where forming personal relationships is key to our success.
Software at Deako
The software team is a tight-knit team of smart, dedicated people. We are customer focused, we ship often, and are constantly asking “why.” We are passionate about the software we write. We work closely with other Deako teams. Developers at Deako are always looking for a better way and understand that no code is perfect.
Data at Deako
We believe that Data and Software go hand in hand. That's why the Data and Software teams work closely together to ensure we're collecting the right data, and getting it to the right people. Data is new at Deako, and we are working to make it a cornerstone of our company. We want every team at Deako to have a data-driven mindset.
Day to Day Expectations:
Participate in the code review process in our DBT repository
Build maintainable, testable SQL queries for various dashboards
Help maintain, contribute to and improve our data infrastructure and existing pipelines
Help maintain and improve our CI/CD pipelines
Responsible for obtaining, cleaning, and munging data and getting it into a form that our data analysts can access and analyze
Participate in meetings with stakeholders around data collection and cleaning

Must Have:
Demonstrable knowledge in SQL + Some Programming Language (Typescript, Python, Ect)
Demonstrates a high level autonomy and willingness to learn
1+ years of experience working as a Data Analyst/Engineer or Comparable field.
Experience with git, dbt
Nice to Have:
Experience in Snowflake, Fivetran, Hightouch
Experience with Gitlab CI
Experience with geospatial data
Stock Options
Hybrid work environment (Office + Remote)
Medical/Dental/Vision/Life/401K
Unlimited PTO
Free Snacks/Coffee/Drinks (Non-Alcoholic and Alcoholic)
Quarterly Company Parties","$115,000 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,2015,$5 to $25 million (USD)
"PRIMUS Global Services, Inc
4.1",4.1,"Austin, TX","Data Engineer – Snowflake, SQL – REMOTE WORK 43357","We have an immediate long-term opportunity with one of our key clients for a position of Senior Data Engineer, to work on a remote basis.

The ideal applicant must have extensive experience with Snowflake, SQL and Multiple ETL tools.

**ALL successful candidates for this position are required to work directly for PRIMUS. No agencies please**

For immediate consideration, please contact:

Tanya Khatri
PRIMUS Global Services
Direct: 972-200-4514
Phone No: 972-753-6500 Ext: 258
Email: jobs@primusglobal.com","$89,729 /yr (est.)",501 to 1000 Employees,Private Practice / Firm,Information Technology,Information Technology Support Services,2002,$5 to $25 million (USD)
"Orange County's Credit Union
4.0",4.0,"Santa Ana, CA",Data Engineer,"Great Opportunity At Orange County's Credit Union
Must reside in the state of CA, AZ, NV or TX.
Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk.
Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture.
Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential.
Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY!
More about Orange County's Credit Union:
Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace.
As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service.
Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference!
Overview:
Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization.
Essential Functions:
Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making.
Translate business requirements to technical solutions by applying technical knowledge and strong business acumen.
Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services.
Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform.
Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc).
Technical Must Haves for this Role:
5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment.
3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases).
3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.).
2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores.
2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS).
Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.

The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.

We perform thorough background checks and credit checks. EOE.","$92,145 /yr (est.)",201 to 500 Employees,Nonprofit Organization,Financial Services,Banking & Lending,1938,$25 to $100 million (USD)
Sconcept,#N/A,Remote,Azure Data Engineer,"** W2 LONG TERM CONTRACT BASED....**(NO C2C)
JOB TYPE:- Long Term Contract(6 months)Extendable
JOB LOCATION:-USA
About Sconcept LLC:-Software Concepts Consulting was established in 2008. With our “hands-on” approach we are very successful in delivering high performance, secure IT solutions and IT services. With headquarters in Carrollton, Texas we are specialized in providing business solutions to the Fortune 500 clients. Our clients include Software, Banking, Financial, Healthcare, Retail, Consumer goods and Insurance sectors. We deliver candidates based on client requirements.
Required Skills:-
Must have worked on several (5+) projects building solutions for Data Pipelines, Data warehousing, Data Modelling on Azure Data Platform (AWS, Google Cloud Platform - good to have)
Expertise in design and development of Data pipelines and ETL using ADF, Databricks to move data from relational/structured/unstructured data from source to data lake to data warehouse, on Azure
Creating Pipelines and integrations of multiple data sources
Data processing and transformation using Databricks (Scala/Python)
Expertise in optimizing cost and performance of data pipelines
Experience working with Data Lake, Synapse on Azure
Designing Data Lake storage layer (Landing/staging, raw, trusted/curated zones)
Designing data storage DB schema, data models and processing on Azure Synapse DW
Experience with Synapse Spark Pool
Experience in Data Modelling
Building Data Models from structured and semi-structured data (Data Model, Common Data Model etc.)
Performance optimization of data model for high performance analytical and reporting workloads
Expertise with SQL Server and extensive experience with SQL programming
Programming and optimizing DB objects Views, Stored Procs, Functions
Good experience in defining data access policy and security for azure data services
Experience with Oracle EDW
Knowledge/Experience migrating from Oracle EDW to any cloud DW
Experience integrating AI skills/ML models with Data and reporting solutions
Experience in building API layer for downstream consumption
Job Type: Contract
Pay: $45.00 - $50.00 per hour
Benefits:
Health insurance
Compensation package:
Bonus pay
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$47.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"VedaInfo Inc
4.1",4.1,"Phoenix, AZ",Big Data Engineer,"Hi,
I hope this note finds you well
I am reaching out to you regarding your career opportunity with us for “Big Data Engineer” for a “6+ Months Contract” @Phoenix, AZ
Please find the below requirement details.
Job Title: Big Data Engineer
Location: Phoenix, AZ (Onsite)
Duration: 6+ months
Rate: $60/hr C2C
Responsibilities:
Design, implement, and maintain big data systems handling large volumes of data.
Utilize Hadoop, Hive, and Spark for efficient data processing and analysis.
Collaborate with cross-functional teams to understand data requirements.
Develop and optimize data ingestion, storage, and transformation processes.
Build scalable data pipelines for seamless data processing and analysis.
Monitor and troubleshoot data processing and performance issues.
Stay updated with emerging big data technologies.
Requirements:
Bachelor's/Master's degree in CS, Engineering, or related field
7+ years of experience as a Big Data Engineer
Strong proficiency in Hadoop, Hive, and Spark
Extensive experience with AWS or Azure
Solid understanding of data ingestion, storage, and transformation
Excellent problem-solving and communication skills
Proactive and self-motivated
This is a 6+ month onsite contract position in Phoenix, AZ, with the possibility of extension. Compensation is $60/hr on C2C basis.
Please submit your updated resume highlighting relevant experience.
Thank you for your interest!
Thanks & Regards
Mohammed ZAIN
Job Type: Contract
Pay: $50.00 - $60.00 per hour
Experience level:
7 years
Ability to commute/relocate:
Phoenix, AZ: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
To improve your chances of getting Shortlisted, make sure to tailor your Profile to the Job Requirements, Highlight your Relevant Skills and Experience, and then upload your Resume.
Experience:
AWS (Amazon Web Services): 7 years (Required)
Azure: 7 years (Required)
Big data: 7 years (Required)
Hadoop: 7 years (Required)
Apache Hive: 7 years (Required)
Spark: 7 years (Required)
Work Location: One location",$55.00 /hr (est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$1 to $5 million (USD)
"Myticas Consulting
3.9",3.9,Arizona,BHJOB15656_20058 - Data Engineer,"Myticas's direct client based out of Phoenix, AZ is currently seeking a Data Engineer for a 100% remote contract position.
Pay Rate: Up to $34/hr

Job Description:
A Data Engineer can confidently understand, write, troubleshoot, optimize, and peer review SQL code. They understand business requirements and create code and reporting that exceeds specifications through anticipating underlying needs. They also understand relational database design, table structure, data types, and data models. They are experienced with and confident in development in reporting visualizations in Excel, PowerBI, Tableau, QuickSight, SSRS, or other data products. They can create reporting dashboards and products that inform the business. They easily grasp and retain complex data concepts and are able to explain them to others. Looking for engineers who are driven to innovate, use cutting edge technologies to solve problems and think outside the box.
What You’ll Do:
Digital Transformation into AWS
Data Warehouse related work (SQL, ETL, Replication, Databases)
Understand, write, troubleshoot, optimize, and peer review SQL code.
Understand business requirements and create code and reporting that exceeds specifications through anticipating underlying needs.
Understand relational database design, table structure, data types, and data models. 6.Experienced with and confidence in development in reporting visualizations in Excel, PowerBI, Tableau, QuickSight, SSRS, or other data products.
Data Warehouse related work (SQL, ETL, Replication, Databases
Perform other duties as assigned or apparent.
MINIMUM EDUCATION AND RELATED WORK EXPERIENCE:
Bachelor's degree in a Computer Science field and 2 years' work experience
2 years of experience in most phases of IT systems deployments in one or more of the following areas: design and deployment of cloud services, data migration to AWS cloud, Digital Transformation into AWS, and Data Warehouse related work
INDCHI
INDREM",$34.00 /hr (est.),1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"MARVEL TECHNOLOGIES INC
3.7",3.7,"Cincinnati, OH",Data Engineer,"Job Description
The data engineer designs and builds platforms tools and solutions that help the bank manage secure and generate value from its data. The person in this role creates scalable and reusable solutions for gathering collecting storing processing and serving data on both small and very large i.e. Big Data scales. These solutions can include on-premise and cloud-based data platforms and solutions in any of the following domains ETL business intelligence analytics persistence relational NoSQL data lakes search messaging data warehousing stream processing and machine learning.
Responsible and accountable for risk by openly exchanging ideas and opinions elevating concerns and personally following policies and procedures as defined. Accountable for always doing the right thing for customers and colleagues and ensures that actions and behaviors drive a positive customer experience. While operating within the Bank's risk appetite achieves results by consistently identifying assessing managing monitoring and reporting risks of all types.
ESSENTIAL DUTIES AND RESPONSIBILITIES:
Responsible for design Development and Support of data solutions APIs tools and processes to enable rapid delivery of business capabilities.
Work closely with IT application teams Enterprise architecture infrastructure information security and LOB stakeholders to translate business and technical strategies into data-driven solutions for the Bank.
Act as a technical Expert addressing problems related to system and application design performance integration security etc.
Conduct research and Development based on current trends and technologies related to the banking industry data engineering and architecture data security and related topics.
Work with developers to Build CI/CD pipelines Self-service Build tools and automated deployment processes.
Evaluate software products and Provide documented recommendations as needed.
Provide Support and troubleshooting for data platforms. Must be willing to Provide escalated on-call Support for complicated and/or critical incidents.
Participate in the planning process for hardware and software.
Plan and work on internal projects as needed including legacy system replacement Monitoring and analytics improvements tool Development and technical documentation.
Provide technical guidance and mentoring for other team members.
Manage and prioritize multiple assignments.
MINIMUM KNOWLEDGE SKILLS AND ABILITIES REQUIRED:
Bachelor's degree in Computer Science/Information Systems or equivalent combination of education and experience.
Must be able to communicate ideas both verbally and in writing to management business and IT sponsors and technical resources in language that is appropriate for each group.
Fundamental understanding of distributed computing principles
Knowledge of application and data security concepts best practices and common vulnerabilities.
Conceptual understanding of one or more of the following disciplines preferred big data technologies and distributions metadata management products commercial ETL tools Bi and reporting tools messaging systems data warehousing Java language and run time environment major version control systems continuous integration/delivery tools infrastructure automation and virtualization tools major cloud or rest API design and development.
Job Types: Full-time, Contract
Pay: $60.00 - $62.00 per hour
Compensation package:
Hourly pay
Yearly pay
Experience level:
9 years
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",$61.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,#N/A,$5 to $25 million (USD)
"ProGrad
4.2",4.2,Remote,Data Engineer,"We are looking for data engineers that are excited about building and maintaining data pipelines for a variety of interests. The pipelines will ingest data from a variety of sources ranging from internal change data streams to external integrations with various SaaS platforms through webhooks and events. The ideal candidate is self driven and looking to grow as a data engineer while having an outsized impact on the future of data at the company. A love for database architecture and optimisation is a must as is the ability to work across diverse teams.
Requirements/Qualifications:
Bachelor's or Master's degree in Data Science (specialization)
Software development experience coding in a general-purpose programming language such as C, C++, Java, JavaScript, Golang, or Python
Experience working with data structures or algorithms (i. e., data structures /algorithms class, coursework/projects, research, internships, or other practical experience in/outside of school or work (including open source hobby coding)
Proficiency in SQL, and NoSQL Databases
Experience working with large datasets
Experience writing and debugging complex SQL queries
A strong understanding of cloud database best practices and performance optimisation
Experience building and maintaining data pipelines, coupled with knowledge of good programming practices such as unit and integration testing
. A strong understanding of cloud database best practices and performance optimisation
Good to have skills:
. Experience building and maintaining data pipelines, coupled with knowledge of good programming practices such as unit and integration testing
Job Types: Full-time, Permanent
Salary: Up to $1,200,000.00 per year
Experience level:
1 year
Under 1 year
Schedule:
Day shift
Work Location: Remote
Speak with the employer
+91 9057202434",#N/A,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
META FORCE IT LLC,#N/A,"Cincinnati, OH",Data Engineer,"Job Description
The data engineer designs and builds platforms tools and solutions that help the bank manage secure and generate value from its data. The person in this role creates scalable and reusable solutions for gathering collecting storing processing and serving data on both small and very large i.e. Big Data scales. These solutions can include on-premise and cloud-based data platforms and solutions in any of the following domains ETL business intelligence analytics persistence relational NoSQL data lakes search messaging data warehousing stream processing and machine learning.
Responsible and accountable for risk by openly exchanging ideas and opinions elevating concerns and personally following policies and procedures as defined. Accountable for always doing the right thing for customers and colleagues and ensures that actions and behaviors drive a positive customer experience. While operating within the Bank's risk appetite achieves results by consistently identifying assessing managing monitoring and reporting risks of all types.
ESSENTIAL DUTIES AND RESPONSIBILITIES:
Responsible for design Development and Support of data solutions APIs tools and processes to enable rapid delivery of business capabilities.
Work closely with IT application teams Enterprise architecture infrastructure information security and LOB stakeholders to translate business and technical strategies into data-driven solutions for the Bank.
Act as a technical Expert addressing problems related to system and application design performance integration security etc.
Conduct research and Development based on current trends and technologies related to the banking industry data engineering and architecture data security and related topics.
Work with developers to Build CI/CD pipelines Self-service Build tools and automated deployment processes.
Evaluate software products and Provide documented recommendations as needed.
Provide Support and troubleshooting for data platforms. Must be willing to Provide escalated on-call Support for complicated and/or critical incidents.
Participate in the planning process for hardware and software.
Plan and work on internal projects as needed including legacy system replacement Monitoring and analytics improvements tool Development and technical documentation.
Provide technical guidance and mentoring for other team members.
Manage and prioritize multiple assignments.
MINIMUM KNOWLEDGE SKILLS AND ABILITIES REQUIRED:
Bachelor's degree in Computer Science/Information Systems or equivalent combination of education and experience.
Must be able to communicate ideas both verbally and in writing to management business and IT sponsors and technical resources in language that is appropriate for each group.
Fundamental understanding of distributed computing principles
Knowledge of application and data security concepts best practices and common vulnerabilities.
Conceptual understanding of one or more of the following disciplines preferred big data technologies and distributions metadata management products commercial ETL tools Bi and reporting tools messaging systems data warehousing Java language and run time environment major version control systems continuous integration/delivery tools infrastructure automation and virtualization tools major cloud or rest API design and development.
Job Type: Contract
Pay: $45.96 - $50.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Cincinnati, OH 45215: Reliably commute or planning to relocate before starting work (Required)
Experience:
data engineer: 10 years (Preferred)
Big data: 3 years (Preferred)
Work Location: Hybrid remote in Cincinnati, OH 45215",$47.98 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Fincons Group
4.0",4.0,Remote,Data Engineer – Databricks / Kafka / SQL,"Fincons Group is an IT business consulting company that has been designing the digital future of leading companies on international markets for 40 years. Fincons Group built its reputation on foreseeing and interpreting new business models and the rapid evolution of IT systems by building a complete range of services: from research and consulting to design and development, from system integration of leading vendor software solutions right up to application management, supporting clients step by step along their digital transformation journey. Fincons is a multinational with over 2,600 people and several offices worldwide (in Italy, Switzerland, Germany, France, the United Kingdom, Belgium and the USA), but above all a Family Company where the founders play a strategic role with commitment and passion, grounding the company in the same principles of a united and caring family.
We firmly believe in the value of cooperation and in the contribution that every idea and intuition can bring. We believe that everyone’s effort can make a difference. Transforming knowledge into a strategic resource is our company mission, and we pursue it with the help of our most strategic asset: our people.

Job Summary
Looking for an experienced data engineer with hands on experience in developing solutions based on Databricks. If you are a curious team-player with passion to learn and working for different clients to develop modern data streaming platforms, please reach out to us!

Work location: any US location might be considered
Employment type: freelance / contractor
Start date: about July 1th
Duration: about 6 months with possible renewal

Responsibilities and Duties
Design, develop and implement real time data ingestion pipelines from multiple sources using Databricks
Develop high-quality, scalable and re-usable frameworks for ingesting high volume and large data sets
Apply best practices to Integrate and ingest various big data formats like Protobuf, Parquet, AVRO and JSON
Work with event based / streaming technologies like Kafka, or Kinesis, to ingest and transform the data using KafkaStreams or KSQL
Working with other members of the project team to support delivery of additional project components (API interfaces, Search)
Work to load test the pipelines for quality and better performance using JMeter, or Gatling etc

Qualifications and skills
Bachelor’s degree and first experience designing, developing, deploying and/or supporting data pipelines using Databricks
Experience in designing and implementing Medallion architecture is preferred
Expertise in designing and deploying data applications on cloud solutions, such as Azure or AWS
Hands on experience in performance tuning and optimizing code running in Databricks environment
Proficient in programming languages like Pyspark and Python
Good understanding of SQL, T-SQL and/or PL/SQL
Demonstrated analytical and problem-solving skills particularly those that apply to a big data environment
Experience with agile development methodologies.
Ability to work effectively both independently and in a group or team environment

Authorization to Work
Applicants for employment in the US must be authorized to work in the US. Fincons Group will not sponsor applicants for work visas.
All qualified applicants will receive consideration for employment without regard to race, colour, religion, sex, sexual orientation, gender identity, national origin, age, disability, or protected veteran status, or any other legally protected basis, in accordance with applicable law.
The data will be processed and stored exclusively for the purposes of this or future selections, guaranteeing the rights referred to in art. 13 Legislative Decree 196/03 and EU regulation 679/2016 (GDPR)
2yrKJdtT13",#N/A,201 to 500 Employees,Company - Private,Financial Services,Accounting & Tax,#N/A,Unknown / Non-Applicable
"Vedainfo
4.1",4.1,"Peoria, IL",Data Engineer,"Job Description
· Create and develop optimal data pipeline which fetches data from raw and apply business transformation and load derived.
· Build the AWS Fargate components required for optimal extraction, transformation, and loading of data-to-data lake.
· Develop data pipeline setup between base and derive values based on Caterpillar specification.
· Implement quality validation rules and follow process defined by stakeholder.
· Develop business events as defined by the solution team to generate CDC events for consumption layer (Kinesis)
· Create re-usable and configurable components wherever applicable.
· Create and execute unit, integration, and regression tests
· Ensure the code is developed satisfying all business rules, perform unit testing where applicable.
· Data pipelines development
· Python Development
· AWS cloud services
· Data quality validation rules
· Java (preferred)
· Implement CDC
· Transformation rules creation
· Message broker tools like Kinesis
· Azure DevOps – CI/CD pipelines
· Automated and Manual Testing experience
Job Type: Contract
Pay: Up to $60.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Peoria, IL 61602: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
This position is remote initially and you will be asked to work any of the client location once offer is made. Please confirm your current location.
Experience:
Data Engineer: 1 year (Preferred)
Python Development: 1 year (Preferred)
AWS: 1 year (Preferred)
Message broker tools: 1 year (Preferred)
Work Location: In person",$60.00 /hr (est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$1 to $5 million (USD)
Enterprise Minds,#N/A,Remote,Data Engineer,"Job Role: SQL Data Engineer
Location: Remote Opportunity - Need to work in PST hours
Visa: US Citizen Only
Duration: Long term
Roles and Responsibilities:
3+ years of experience with a complex build system in AWS using Starburst.
5+ years of experience with installation, maintenance, and administration of Oracle databases- PL/SQL and ANSI SQL
Oracle DBA + Oracle SQL to ANSI SQL conversion, performance/query tuning
3+ years of experience configuring, integrating, and securing multiple AWS database offerings (ex. RDS, Aurora, Casandra, and Dynamo)
2+ years of experience with supporting Linux systems engineering efforts in system design and evaluation, solution engineering, software development, or system administration.
Experience with Database Schema as Code tools
Ability to design and develop complex SQL queries and reports to retrieve required data.
Experience with database partitioning and microservices
Knowledge of Agile methodologies or the software development life cycle (SDLC)
US Citizenship is Mandatory.
Job Type: Full-time
Experience:
Data Engineer: 10 years (Preferred)
AWS using Starburst: 4 years (Preferred)
PL/SQL and ANSI SQL: 2 years (Preferred)
AWS RDS: 1 year (Preferred)
Work Location: Remote",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
shreetek,#N/A,"Charlotte, NC",Senior Data Engineer,"Job Title : Data Engineer
ONLY USC
On W2 only
Job Location : Newjersey OR Charlotte,NC
Collect, manage, and convert raw data accurately and reliably
Organize data systems for subgroup access and analyses
Configure and sustain data cloud structures
Able to work with structured and unstructured data.
Validate outputs of data pipelines
Degree in Data Engineering preferred.
Two Programmers –
Convert existing SAS code to python/pyspark code for model operation in the cloud.
Create and sustain policy analysis models in the cloud.
Troubleshoot user interfaces in the cloud.
Create and sustain intuitive user interfaces in the cloud.
A degree in Computer Science is preferred.
Overall across resources keep following stack as reference
Visualization : Tableau
Data Modeling/Science : Python / SAS
Engg : AWS cloud native services, security, data pipeline
Job Type: Contract
Salary: $50.00 - $60.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Charlotte, NC 28202: Reliably commute or planning to relocate before starting work (Required)
Work Location: One location",$55.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Zllius Inc.,#N/A,"Chicago, IL",Data Engineer,"Hi,
Greetings from Zllius Inc!
We have an urgent opening for the role Data Engineer. If interested, revert me back with your Updated resume, along with contact details, work authorization.
Job Role: Data Engineer
Duration: Long-Term
Position Type: W2/ 1099
Visa: Any
Job Description:
Technical/Functional Skills
Successful candidates will have:
Hands on experience in Dremio and Scripting
Hands on Scripting experience in Python and Flask
Hands on experience in Integrating data from multiple source systems (PostgreSQL and CSV)
Experience with Apache Superset
Ability to write complex SQL queries
Experience in building data pipelines using PostgreSQL, Oracle database, Dremio, AWS Open Search
Experience with Docker Containerization of Code
Work as a part of a scrum team in Agile methodology
ETL, Data Pipelines, PostgreSQL, SQL Queries, Scripting
Job Types: Full-time, Contract
Salary: $111,076.11 - $133,769.08 per year
Schedule:
8 hour shift
Day shift
Monday to Friday
Work Location: On the road","$122,423 /yr (est.)",Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
NLB Technology Services,#N/A,"Dallas, TX",CRM data Analytics Engineer,"MUST HAVE:
o Tableau CRMA & Einstein Discovery Certification
o Experience building Dashboards using CRMA (aka Tableau CRM and Einstein Analytics)
o Design, build, and support datasets, data recipes, dataflows, and dashboards using Salesforce and non-Salesforce data sources
o Troubleshoot functional, data, and performance issues with CRMA Dashboards
o Skilled in SAQL, JSON
o Develop and deploy AI/ML solutions using Einstein Discovery and deploy AI/ML Models built outside Salesforce
o Experience working with large data (1 B+ rows) and supporting large user base (10,000+)
o Mentor and coordinate the development efforts of other CRMA DevelopersAT&T
Job Type: Contract
Salary: $65.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Vision insurance
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
CRM ANalytics: 8 years (Required)
einstein: 1 year (Required)
Salesforce: 1 year (Required)
Work Location: One location",$70.00 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Lightcast
4.4",4.4,"Moscow, ID",Senior Data Engineer,"A Senior Data Engineer will architect big data analytical frameworks, and translate complex functional and technical requirements into detailed architecture design and high-performing software. A Senior Data Engineer will select data solution software and define hardware requirements. The position will be responsible for developing and implementing standards and processes for integration projects and initiatives.

Major Responsibilities:
Architect software applications, and test and build automated tools.
Select data solution software and define hardware requirements
Develop standards and processes for integration projects and initiatives.
Lead the design/development of software applications, testing, and building tools
Ensure database changes are reviewed and approved to standards
Lead and communicate to leadership on solution design
Provide technical assistance to junior members and to colleagues across the company
Skills & Abilities:
Analytical mind, personality and aptitude for working with data
Ability to look at the numbers, trends, and data to derive conclusions based on findings
Work closely with management to prioritize business and information needs
Proven ability to work effectively to meet goals and deadlines with minimal supervision
Highly creative problem-solving skills and an ability to tailor efforts based on the importance of the issue being addressed.
A hands-on, detail-oriented mindset with the ability to look beyond the box
Expert knowledge of object-oriented design, data structures, and algorithms
Demonstrated experience with agile or other rapid application development methods, object-oriented design, coding, testing patterns with a variety of languages
Significant knowledge of data structures, algorithms, data modeling and disaster recovery of data systems
Education & Experience:
Requires 5 years of data/software engineering/science experience
Bachelor's degree in computer science or other technical disciplines (masters preferred)
About Lightcast:
Lightcast is a global leader in labor market insights with headquarters in Moscow (ID) and Boston (MA) and offices in the United Kingdom, Europe, and India. We work with partners across six continents to help drive economic prosperity and mobility by providing the insights needed to build and develop our people, our institutions and companies, and our communities.

Lightcast is proud to be an equal opportunity workplace and is committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. Lightcast has always been, and always will be, committed to diversity, equity and inclusion. We seek dynamic professionals from all backgrounds to join our teams, and we encourage our employees to bring their authentic, original, and best selves to work.

#LI-SS1","$118,643 /yr (est.)",1 to 50 Employees,Company - Private,Government & Public Administration,State & Regional Agencies,2011,Unknown / Non-Applicable
"DiamondPick
4.5",4.5,Remote,GCP Data Engineer,"Position: GCP Data Engineer (8+years)
Location: Remote
Type: Contract
Job Details:
· Relevant Industry Work Experience (6+ for Developer, preferred 9+ years)
· Experience extracting data from a variety of sources, and a desire to expand those skills (Excellent knowledge in SQL and Spark is mandatory)
· Strong knowledge of Google BigQuery and architecting data pipelines from on-prem to GCP.
· Experience building applications using Google Cloud Platform related frameworks such as DataProc and GCS at the minimum.
· Excellent Communication Skills to Understand and Pass on Requirements.
· Excellent Data Analysis skills. Must be comfortable with querying and analyzing large amount of data on Hadoop HDFS using Hive and Spark.
· Professional experience with a cloud hosting platform (GCP preferred)
· GCP certification is preferred.
Job Type: Contract
Salary: Up to $65.00 per hour
Experience level:
10 years
8 years
9 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
Google BigQuery: 4 years (Preferred)
DataProc: 2 years (Preferred)
Google Cloud Platform: 9 years (Required)
Spark: 8 years (Required)
SQL: 8 years (Required)
License/Certification:
Google Cloud Platform(GCP) Certification (Required)
Work Location: Remote",$65.00 /hr (est.),1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,#N/A,Unknown / Non-Applicable
TekValue IT Solutions,#N/A,"Houston, TX",Data Engineer with Migrating,"Required Skills:
8-10 Required Experince on Data Engineer and Revalent Skills
Experience with NoSQL (MongoDB)
Experience with API'S
Good Knowledge on Data Migration like Oracle to Mongo db
Experience with Python Programming
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Houston, TX 77002: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 8 years (Required)
NoSQL: 6 years (Required)
MongoDB: 6 years (Required)
Migrating: 5 years (Required)
Work Location: One location
Speak with the employer
+91 7328323606",$72.50 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Infinity Quest
3.9",3.9,"Cleveland, OH",Data Center Engineer,"Required Skillsets:
1. Datacenter Management
2. Hardware Rack & Stack Cabling
3. Knowledge and operational of Diesel Generators, Cooling & Chillers
4. Inventory Asset management
5. DC Monitoring DC availability
6. Handling of CISCO UCS HW and VX RAIL, POWER FLEX
Activities:
1. Operate and Manage both routine and emergency service on a variety of state-of-the-art critical systems such as:
a. Medium voltage switchgear,
b. diesel generators,
c. UPS systems
d. Power distribution equipment, chillers, cooling towers, computer room air handlers,
e. Fire detection / suppression; building monitoring systems (BMS), building Automated Systems (BAS) ; etc
2. Supervise the on-site management of sub-contractors and vendors, ensuring that all work is performed according to established practices and procedures.
3. Manage local client relationship and act as the point of contact for the company at this site.
4. Establish performance benchmarks, conduct analyses and prepare reports on all aspects of the critical facility operations and maintenance.
5. Work with IT managers and other business leaders to coordinate projects, manage capacity and optimize plant safety, performance, reliability and efficiency.
6. Create, utilize and administer MOPs , SOPs, and Preventative Maintenance Procedures for all work on critical data center facility equipment.
7. Schedule work activities, within specified change control / management protocol.
8. Maintain a constant state of readiness in support of the mission goal of 99.999% uptime
9. Racking, Stacking of Infrastructure
a. Cabling Management
b. Patching, Network port swapping
c. Hardware reboots
d. Vendor coordination
e. Project Coordination
· Inventory asset management, Physical access management
Monitoring of Datacenter availability & update respective teams accordingly
Job Type: Contract
Salary: From $80.00 per year
Schedule:
8 hour shift
Ability to commute/relocate:
Cleveland, OH: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data center: 10 years (Preferred)
CISCO UCS: 10 years (Preferred)
MOPS: 10 years (Preferred)
SOP: 10 years (Preferred)
Cabling Management: 10 years (Preferred)
Network protocols: 10 years (Preferred)
Hardware reb: 10 years (Preferred)
VENDOR COORDINATION: 10 years (Preferred)
Project coordination: 10 years (Preferred)
Security clearance:
Confidential (Preferred)
Speak with the employer
+91 8838059965",#N/A,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,Unknown / Non-Applicable
"E-Business International INC
3.6",3.6,"Alexandria, VA",AWS Data Engineer,"Role: AWS Data Engineers
Location: Alexandria, VA
Full-Time/Permanent
AWS Engineers
We are seeking TWO (2) AWS Engineers to join our growing team. The qualified applicants will become part of the Enterprise Data Analytics Services (EDAS) program for a large federal agency in Alexandria, VA. Hybrid work options are available.
The AWS Engineer is responsible for the development and support of cloud-based solutions. Solutions include moving assets from on-premise to cloud environment, evaluation, and optimization. AWS Engineer will collaborate with the team effectively to ensure the team's success.
Responsibilities/Duties:
Work on automating the migration process in AWS from development to production.
Create CFT deploying/updating cloud resources.
Advise the engineering and software engineering team as they migrate from on-premise to cloud infrastructure.
Optimize cloud workloads for cost, scalability, availability, governance, compliance, etc.
Guide and/or provide hands-on support to administer production, staging, and deployment environments.
Partner with multi-disciplinary teams to understand requirements and plan architecture and solutions.
Apply insight and expertise across AWS services.
Apply knowledge of scripting and automation using tools like PowerShell, Python, Bash, Ruby, Perl, etc.
Experience and Qualifications:
Good working knowledge of AWS Services, including Compute, Storage Networking, Database, Management tools, Security, Identity, and Compliance.
3 years’ experience in AWS (EC2, Glue, S3, Redshift, Python programming).
Experience implementing and debugging workloads in AWS.
Good knowledge of creating CFTs.
Experience building infrastructure inside AWS via code. Familiarity with tools such as Terraform or CloudFormation.
Ability to proactively communicate, interact, and solve critical customer problems.
Ability to think and act strategically and proactively.
3 years of experience in software development tools and methodologies.
3 years of experience in infrastructure architecture, database architecture, and networking.
3 years of experience architecting/deploying operating solutions built on AWS.
Education/Certifications:
Bachelor's Degree in Computer Science, Information Technology, Engineering, or associated discipline.
AWS certification is a plus!
Clearance Requirements:
Ability to attain and maintain a Public Trust.
Job Type: Full-time
Salary: From $100,000.00 per year
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Alexandria, VA 20598: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Expected Salary Range (Must)?
Visa Status/ Work Authorization (Must)?
Experience:
AWS: 4 years (Required)
Work Location: One location","$100,000 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,1992,Unknown / Non-Applicable
"Amazee Global Ventures Inc
5.0",5.0,Remote,Data Engineer,"(W2 ONLY)
Job Title: Data Engineer
Duration : 12 months
Location: Remote
6-7 years of experience
Healthcare experience is must
Design, develop, and maintain Tableau dashboards and reports
Develop and maintain data models and ETL processes
Analyze data to identify trends and patterns
Create and maintain data dictionaries and data models
Develop and maintain data pipelines
Collaborate with stakeholders to understand data requirements
Develop and maintain data quality processes
Monitor data quality and performance
Troubleshoot data issues
Develop and maintain documentation
Requirements:
Bachelor’s degree in Computer Science, Information Systems, or related field
4+ years of experience in data engineering and analysis
Expertise in Tableau
Experience with SQL and data modeling
Knowledge of ETL processes
Knowledge of data warehousing
Excellent problem-solving and communication skills
Ability to work independently and in a team environment\
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Type: Full-time
Salary: $40.33 per hour
Work Location: Remote",$40.33 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,2019,Less than $1 million (USD)
"BigLynx Computer Software
4.9",4.9,"Redmond, WA",Databricks Data Engineer,"BigLynx, Inc is an American multinational technology corporation headquartered in Seattle, Washington, with operations in the United States, Canada, and India. The company began in 2016, as a product development company specializing in AI/ML Data Engineering in the Retail vertical space with its products warehouse & fast. Post Pandemic in 2022, BigLynx added a business division of boutique technology consulting, specializing inData Engineering, Full Stack , and Microsoft Dynamics helping clients build the next generation data platform and big data pipelines.
Data pipeline development: Design, develop, and maintain scalable and efficient data pipelines using Databricks to ingest, transform, and load data from various sources. This includes data extraction, data cleansing, data transformation, and data loading processes.
Data modeling and schema design: Design and implement data models, database schemas, and data structures on Databricks. Optimize data models for performance, scalability, and ease of use.
ETL processes: Develop and maintain ETL (Extract, Transform, Load) processes using Databricks to transform and cleanse data. Implement efficient data integration and transformation logic using languages such as Python, SQL, or Scala.
Data integration: Integrate data from multiple systems and sources, ensuring data consistency, accuracy, and quality. Develop and maintain data connectors, APIs, and data ingestion processes.
Performance optimization: Identify and address performance bottlenecks in data pipelines and data models. Optimize query performance, data loading, and data processing capabilities on Databricks.
Data governance and security: Implement data governance practices, data privacy measures, and security controls on Databricks. Ensure compliance with data governance policies and regulations.
Monitoring and troubleshooting: Monitor the health and performance of Databricks data infrastructure, data pipelines, and data processing jobs. Troubleshoot issues and provide timely resolutions.
Collaboration and teamwork: Collaborate with cross-functional teams, including data scientists, data analysts, and business stakeholders, to understand data requirements, provide data engineering expertise, and support their data-related needs.
Qualifications:
Databricks expertise: Strong knowledge and hands-on experience with the Databricks platform, including Databricks notebooks, Databricks runtime, and Databricks clusters.
Data engineering skills: Proficiency in data engineering principles, ETL processes, data modeling, and data integration techniques. Experience with programming languages such as Python, SQL, or Scala.
Big data technologies: Experience with big data technologies, such as Apache Spark, Apache Hadoop, or related frameworks. Familiarity with distributed computing and data processing concepts.
Cloud platforms: Experience working with cloud platforms, preferably Azure Databricks, AWS Databricks, or Google Cloud Databricks. Knowledge of cloud storage, compute, and networking services.
Database and data warehouse concepts: Understanding of relational databases, data warehousing concepts, and SQL. Familiarity with data warehousing best practices and dimensional modeling.
Performance optimization: Strong skills in optimizing Spark jobs and queries on Databricks. Ability to identify and resolve performance bottlenecks.
Problem-solving skills: Strong analytical and problem-solving abilities to tackle complex data engineering challenges and troubleshoot issues.
Collaboration and communication: Excellent collaboration and communication skills to work effectively with cross-functional teams and stakeholders, translating business requirements into technical solutions and providing technical guidance.
Education: A bachelor's or master's degree in computer science, data engineering, or a related field is typically required. Relevant certifications, such as Databricks Certified Developer or similar, are h
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Experience level:
7 years
Schedule:
Monday to Friday
Work Location: In person",$75.00 /hr (est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Wallero
4.4",4.4,"Seattle, WA",Data Engineer,"Hi,
Pretty much interested in your resume, we have an urgent opening for Data Engineer role with one of our clients based in Seattle WA.
Position: Data Engineer
Location: Seattle WA - Remote (100%)
Description:
Good Experience in reporting space including PowerBI, ADF, and possibly standing up a data warehouse/data mart.
The main scope centering is around reporting.
Experience with SQL, and agile framework including requirement gathering and story/feature writing skills.
Need someone strong who is comfortable leading and guiding teams in that capacity.
Migrating data from third party to Azure Cloud
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Seattle, WA 98101: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",$75.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2015,$1 to $5 million (USD)
"JPMorgan Chase Bank, N.A.
3.8",3.8,"Wilmington, DE",Data Engineer III,"As a Data Engineer III under Consumer and Community Banking within the ADE Data Governance team at JPMorgan Chase, you serve as a seasoned member of an agile team to design and deliver trusted data collection, storage, access, and analytics solutions in a secure, stable, and scalable way. You are responsible for developing, testing, and maintaining critical data pipelines and architectures across multiple technical areas within various business functions in support of the firm's business objectives.

Job responsibilities
Supports review of controls to ensure sufficient protection of enterprise data
Responsible for advising and making custom configuration changes in one to two tools to generate a product at the business or customer request
Updates logical or physical data models based on new use cases
Frequently uses SQL and understands NoSQL databases and their niche in the marketplace
Adds to team culture of diversity, equity, inclusion, and respect

Required qualifications, capabilities, and skills
Experience across the data lifecycle
Advanced at SQL (e.g., joins and aggregations)
Working understanding of NoSQL databases
Proficiency with JSON/Java, Erwin, Oracle, Snowflake, AWS
Significant experience with statistical data analysis and ability to determine appropriate tools and data patterns to perform analysis, including data modeling
Experience customizing changes in a tool to generate product
Executes data architecture solutions and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions and break down problems
Produces data architecture and design artifacts for complex applications while being accountable for ensuring design constraints are met by software code development
Gathers, analyzes, synthesizes, and develops visualizations and reporting from large, diverse data sets in service of continuous improvement of data frameworks, applications, and systems that proactively identifies hidden problems and patterns in data and uses these insights to drive improvements to coding hygiene and system architecture

Preferred qualifications, capabilities, and skills
Familiarity with modern technologies
Exposure to cloud technologies
Chase is a leading financial services firm, helping nearly half of America's households and small businesses achieve their financial goals through a broad range of financial products. Our mission is to create engaged, lifelong relationships and put our customers at the heart of everything we do. We also help small businesses, nonprofits and cities grow, delivering solutions to solve all their financial needs.

We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.
The health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the ""WELL Health-Safety Rating"" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.
As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm's current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm's vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.
We offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, discretionary incentive compensation which may be awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.
Equal Opportunity Employer/Disability/Veterans
Base Pay/Salary
Jersey City,NJ $97,850.00 - $150,000.00 / year","$123,925 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Banking & Lending,1799,$10+ billion (USD)
Cloudburst Technologies,#N/A,Remote,Data Collection Engineer,"About Us
Cloudburst is a seed-funded New York-based remote-first company helping customers with detection, prevention, and investigation into cryptocurrency market manipulation and fraud.
Cloudburst provides regulators, financial institutions, trading platforms, and others with access to a real-time, machine-readable crypto market monitoring tool, enabling advanced levels of diligence and customer/market protection. The Cloudburst team has multiple years of experience working together, plus it has worked directly with international law enforcement agencies and policymakers on cybercrime, terrorism, and high-level financial fraud.
What We’re Looking For
The ideal candidate
is an empathetic teammate who loves to help others, to bring order to chaos, and to document their path for others to follow
focuses primarily on web scraping, API integration, and collection of raw data
is capable of becoming fluent in Layer 7 protocols, blockchains, and emerging protocols
has high moral and ethical standards
is proficient in production-level Python
has worked in a threat intel or security environment
What You Will Do
Your primary responsibility will be to become expert at collecting open source data from the web and from other online protocols.
You will be comfortable building tools to bring in data from random files, varying degrees of quality of web sites, and new communications tools. You will have the opportunity to creatively apply your knowledge of software automation towards scaling data collection.
You will understand popular chat sites and how fraud actors use them. You will understand emerging technologies and will actively evaluate them for potential opportunities to collect data.
Why Cloudburst?
At Cloudburst we want to minimize our software engineers working on pixel-pushing and to maximize time spent understanding our domain and building products to help our customers investigate and prevent fraud.
We care about developing and sponsoring our engineers internally. The department adheres to these principles:
Rapid deployment of innovative new techniques for signal generation and attribution
KISS architecture that doesn’t get in developers’ or R&D’s way; we want code to be easy to understand, less abstract, easy to test, fast to deploy, and reducible to automation or simple manual processes
Building a diverse, unconventional team that cross-trains and grows together without ego or sacrifice to work-life balance
What You Will Be Using
Python
Cloud-based infrastructure
3rd party vendor integrations
Novel techniques for data collection using home-grown or discovered tools and frameworks
Hiring Process
Initial email
Initial phone screen with the hiring manager
Take-home test OR link to previous code
Panel interview
Call with CEO
Offer
Investors
Strategic Cyber Ventures
Coinbase Ventures
Bloccelerate
More Info:
https://www.crunchbase.com/organization/cloudburst-technologies-0a3e
https://www.prnewswire.com/news-releases/cloudburst-technologies-raises-3m-in-seed-funding-led-by-strategic-cyber-ventures-joined-by-coinbase-ventures-and-bloccelerate-301817742.html
https://burst.cloud/
https://open.spotify.com/episode/2cv1Is77s8jGLtHPPYRLPU
Job Type: Full-time
Pay: $115,000.00 - $150,000.00 per year
Benefits:
Dental insurance
Flexible schedule
Health insurance
Paid time off
Vision insurance
Compensation package:
Stock options
Experience level:
3 years
Schedule:
Choose your own hours
No nights
No weekends
Work Location: Remote","$132,500 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Stark Dev, LLC",#N/A,"Plano, TX",Data Engineer/Analyst / W2 /USC or GC (GC EAD) or H4 holders,"This position is open for United States Citizens or Green Card holders (GC EAD)/H4 EAD ONLY.
This is an on-site position in Plano/Dallas
CONTRACT W2
Top Skills Details
1) Experience working on a data migration project as a Data Analyst.
- This person will be working on their Permitting, Planning, and Inspection System Project. They are moving from a legacy permitting system, TRAKIT, to a completely new Salesforce application, Clariti. (TRAKIT and Clariti experience not required)
2) Experience doing data discovery, classification, verifying data, mapping rules
- On this project this team will be moving all of the historical data from the old application, TRAKIT, to the new application Clariti.
3) Proficient with SQL and writing SQL queries
\* Interpret data, analyze results using statistical techniques and provide ongoing reports
\* Develop and implement databases, data collection systems, data analytics and other strategies that optimize statistical efficiency and quality
\* Acquire data from primary or secondary data sources and maintain databases/data systems
\* Identify, analyze, and interpret trends or patterns in complex data sets
\* Filter and “clean” data by reviewing computer reports, printouts, and performance indicators to locate and correct code problems
\* Work with management to prioritize business and information needs
\* Locate and define new process improvement opportunities
Drug Test Required
false
Go To Work
false
Workplace Type
On-site
Experience Level
Expert Level
Job Type: Contract
Pay: $55.00 - $60.00 per hour
Schedule:
Monday to Friday
Application Question(s):
What is your visa status?
Work Location: In person",$57.50 /hr (est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Fresh Consulting
3.9",3.9,"Menlo Park, CA",Data Engineer,"Fresh Consulting is a design-led, software development and hardware engineering company, offering end-to-end digital services to help companies innovate. We bring together amazing UX designers, sophisticated developers, digital strategists, and top-notch engineers to help companies create fresh experiences that connect humans, systems, and machines. We’ve been growing fast and need someone to help us continue to manage the delivery of high-quality work in a fast-paced environment.
See more at freshconsulting.com Visit freshconsulting.com/portfolio to see our project work across several industries.
View and apply to all jobs - https://freshconsulting.applytojob.com/apply/ or visit freshconsulting.com/careers
Title: Data Engineer
Duration: 6 months with possible extension
Location: Onsite Menlo Park, CA
Benefits: Employee benefits at 100% including Medical, PTO, Holiday Pay, 401K Plan, and much more!
Hours: Minimum 40 Hours/Week
Role:
Design, construct, install, test, and maintain highly scalable data management systems.
Ensure systems meet business requirements and industry practices.
Design, implement, automate, and maintain large-scale enterprise data ETL processes.
Build high-performance algorithms, prototypes, predictive models, and proof of concepts.
Skills:
5+ years of work experience as a Data Engineer.
Ability to work as part of a team, as well as work independently or with minimal direction.
Excellent written, presentation, and verbal communication skills.
Collaborate with data architects, modelers, and IT team members on project goals.
Strong PC skills including knowledge of Microsoft SharePoint.
Process certification, such as Six Sigma, CBPP, BPM, ISO 20000, ITIL, and CMMI.
Education: BSCSE or related.
FRESH-
Work on engineering and research assignments with F500 companies and startups.
The relationships that we have created with our clients are one of a kind.
We help solve problems in many technologies focusing on R&D, product development, and manufacturing.
We work with the most cutting-edge and latest technologies from AR/VR to Autonomous technologies.
Closely working with our clients, we believe that long-term investments are extremely important to maintain the culture we together have created.
We’re a handpicked team of Engineers, digital strategists, designers, and developers united together in creating a fresh experience. Whether we are strategizing, designing, developing, or analyzing, our integrated team works as an extension of yours to improve your impact, your usability, and your customer conversion. In the process, we collaborate with you to get to know your business, understand your industry, and incorporate your big ideas into memorable experiences that keep your customers coming back for more.
Equal employment opportunity: All qualified persons will be considered for employment without regard to race, color, religion, sex, national origin, age, marital status, familial status, gender identity, sexual orientation, disability for which a reasonable accommodation can be made or any other status protected by law. Assistance will be gladly provided upon request for any applicant with sensory or non-sensory disabilities.
Fresh Consulting is a participating E-Verify company.
freshconsulting.com
Compensation offered will be determined by factors such as location, level, job-related knowledge, skills, and experience. Range $70/hr - $80/hr.
ylM5WIC1Wr",$75.00 /hr (est.),201 to 500 Employees,Company - Private,Management & Consulting,Business Consulting,2007,$25 to $100 million (USD)
NAVA TECH LLC,#N/A,Remote,Azure Data Engineer,"Azure Data Engineer
Remote
Long Term
Note: Need 10+ years of experience
Sound skills and hands on experience with Azure Data Lake, Azure Data Factory, SQL Data Warehouse Azure Blob, Azure Storage Explorer
Proficient in creating Data Factory pipelines for on-cloud ETL processing; copy activity, custom Azure development etc.
Knowledge of Azure Data Catalog, Event Grid, Service Bus, SQL and Synapse
Experience using Azure Databricks platform.
Experience with Python programming.
Experience using pandas and numpy for data engineering and data cleansing;
Ability to troubleshoot the job scheduler and compute clusters.
Experience with databricks CLI and secrets module.
Experience with Azure Blob storage access configuration.
p l e a s e s h a r e r e s u m e s t o m o h a m m e d (a t) n a v a t e c h (d o t) u s O R 4 4 3 5 3 7 9 8 3 5
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$62.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Maven Workforce
4.1",4.1,"Alpharetta, GA",Data Engineer,"Must Have skills:
Building Data Pipeline exp is a must
ETL Tools – SSIS, Alteryx
Data modelling
SQL Server development
Microsoft SQL Stack
Responsibilities:
Data acquisition and ingestion - Identify data sources and build pipelines using various ETL tools such as but not limited to, SSIS, and Alteryx. Data sources including but not limited to SQL Server, SAP, Teradata, Hadoop\Hive, PostgreSQL, Oracle and flat files.
Identifying ways to improve data reliability, efficiency and quality by various data solution techniques.
Expertise in Data project management with JIRA stories. Work as a liaison between business and IT to ensure successful and timely completion of the projects.
Assist in defining the data architecture framework, standards and principles, including modeling, metadata, security and reference data.
Create and optimize data models to support various business applications.
Reviewing modifications of existing data systems for cross-compatibility.
Automate and support workflows to ensure timely delivery.
Must Have:
5+ years of SQL Server development experience.
3+ years ALTERYX Admin /User management experience and advance workflow management.
5+ years data modeling experience.
5+ years of ETL experience.
5+ years of experience in working on more than one database technologies Microsoft SQL server, Teradata.
2+ years big data experience, Hadoop, Hive, Spark
Expert knowledge of data warehousing.
DESIRED SKILLS:
BI Lifecycle management
Working understanding of Microsoft VBA, HTML, Python
Data model development using ERWIN
Job Type: Contract
Salary: $50.00 per hour
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Building Data Pipeline: 8 years (Required)
ETL Tools – SSIS, Alteryx: 7 years (Required)
Data modelling: 6 years (Required)
SQL Server development: 8 years (Required)
Microsoft SQL Stack: 8 years (Required)
Work Location: One location
Speak with the employer
+91 7328456015",$50.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2008,Unknown / Non-Applicable
"Procore Technologies
4.5",4.5,Oregon,Staff Data Engineer,"Job Description

What if you could use your technology skills to develop a product that impacts the way communities' hospitals, homes, sports stadiums, and schools across the world are built? Construction impacts the lives of nearly everyone in the world, and yet it's also one of the world's least digitized industries, not to mention one of the most dangerous. That's why we're looking for a talented Staff Data Engineer to join Procore's journey to revolutionize a historically underserved industry.
As a Staff Data Engineer, you'll design and develop data products for Procore Data Platform data management area. You'll be part of the high-performance team of Data Engineers and will collaborate with platform engineers and product leaders.
This position will report to our Senior Manager of Data Engineering, and can be based remotely from any US location. We're looking for someone to join our team immediately.
What you'll do:
Lead the design and development of big data predictive analytics using object-oriented analysis, design and programming skills, and design patterns
Implement ETL workflows for data matching, data cleansing, data integration, and management
Maintain existing data pipelines and develop new data pipelines using big data technologies
Develop and maintain tables and data models in SQL, abstracting multiple sources and historical data across varied schemas to a format suitable for further analysis
Responsible for leading the effort to continuously improve the reliability, scalability, and stability of the enterprise data platform
Contribute to and lead the continuous improvement of the software development framework and processes by collaborating with Quality Assurance engineers
Deliver observable, reliable, and secure software, embracing the ""you build it, you run it"" mentality, focusing on automation and GitOps
Participate in daily standups, team meetings, sprint planning, and demo/retrospectives while working cross-functionality with other teams to drive the innovation of our products
Apply data governance framework, including the management of data, data compliance operating model, data policies, and standards
What we're looking for:
BS degree in Computer Science, a similar technical field of study, or equivalent practical experience; MS or Ph.D. degree in Computer Science or a related field is preferred
5+ years of experience in a Data Engineering position
Strong expertise with 3+ years of experience building enterprise techniques for large-scale distributed system design and data processing, including:
Building data pipelines with Databricks as the source
Building and maintaining data warehouses in support of BI tools (Snowflake, dbt, Tableau)
Building data pipeline framework for data workflow to process large data sets and Real-Time & Batch Data Pipeline development
Experience in processing structured and unstructured data into a form suitable for analysis and reporting with integration with a variety of data metrics providers ranging from advertising, web analytics, and consumer devices
Desire to be actively hands-on with code, using Java, Python (80%), and SQL, along with willingness and passion for mentoring junior engineers and performing code reviews
Possess familiarity with AWS-managed services for data (Glue, Athena, Data Pipeline, Flink, Spark) and Snowflake

Additional Information

Base Pay Range $147,200-$202,400. Eligible for Bonus Incentive Compensation. Eligible for Equity Compensation. Procore is committed to offering competitive, fair, and commensurate compensation, and has provided an estimated pay range for this role. Actual compensation will be based on a candidate’s job-related skills, experience, education or training, and location.
Perks & Benefits
At Procore, we invest in our employees and provide a full range of benefits and perks to help you grow and thrive. From generous paid time off and healthcare coverage to career enrichment and development programs, learn more details about what we offer and how we empower you to be your best.
About Us
Procore Technologies is building the software that builds the world. We provide cloud-based construction management software that helps clients more efficiently build skyscrapers, hospitals, retail centers, airports, housing complexes, and more. At Procore, we have worked hard to create and maintain a culture where you can own your work and are encouraged and given resources to try new ideas. Check us out on Glassdoor to see what others are saying about working at Procore.
We are an equal-opportunity employer and welcome builders of all backgrounds. We thrive in a diverse, dynamic, and inclusive environment. We do not tolerate discrimination against employees on the basis of age, color, disability, gender, gender identity or expression, marital status, national origin, political affiliation, race, religion, sexual orientation, veteran status, or any other classification protected by law.
If you'd like to stay in touch and be the first to hear about new roles at Procore, join our Talent Community.","$174,800 /yr (est.)",1001 to 5000 Employees,Company - Public,Information Technology,Enterprise Software & Network Solutions,2002,Unknown / Non-Applicable
"Deako
4.9",4.9,"Seattle, WA",Data Engineer II,"About Deako
Deako is delivering a revolutionary plug-n-play smart home platform with a laser focus on the untapped new home construction market. We make smart lighting so easy to upgrade that even those who would have never considered smart lighting are making it happen. We've built a company based on trust; where forming personal relationships is key to our success.
Software at Deako
The software team is a tight-knit team of smart, dedicated people. We are customer focused, we ship often, and are constantly asking “why.” We are passionate about the software we write. We work closely with other Deako teams. Developers at Deako are always looking for a better way and understand that no code is perfect.
Data at Deako
We believe that Data and Software go hand in hand. That's why the Data and Software teams work closely together to ensure we're collecting the right data, and getting it to the right people. Data is new at Deako, and we are working to make it a cornerstone of our company. We want every team at Deako to have a data-driven mindset.
Day to Day Expectations:
Participate in the code review process in our DBT repository
Build maintainable, testable SQL queries for various dashboards
Help maintain, contribute to and improve our data infrastructure and existing pipelines
Help maintain and improve our CI/CD pipelines
Responsible for obtaining, cleaning, and munging data and getting it into a form that our data analysts can access and analyze
Participate in meetings with stakeholders around data collection and cleaning

Must Have:
Demonstrable knowledge in SQL + Some Programming Language (Typescript, Python, Ect)
Demonstrates a high level autonomy and willingness to learn
1+ years of experience working as a Data Analyst/Engineer or Comparable field.
Experience with git, dbt
Nice to Have:
Experience in Snowflake, Fivetran, Hightouch
Experience with Gitlab CI
Experience with geospatial data
Stock Options
Hybrid work environment (Office + Remote)
Medical/Dental/Vision/Life/401K
Unlimited PTO
Free Snacks/Coffee/Drinks (Non-Alcoholic and Alcoholic)
Quarterly Company Parties","$115,000 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,2015,$5 to $25 million (USD)
"PRIMUS Global Services, Inc
4.1",4.1,"Austin, TX","Data Engineer – Snowflake, SQL – REMOTE WORK 43357","We have an immediate long-term opportunity with one of our key clients for a position of Senior Data Engineer, to work on a remote basis.

The ideal applicant must have extensive experience with Snowflake, SQL and Multiple ETL tools.

**ALL successful candidates for this position are required to work directly for PRIMUS. No agencies please**

For immediate consideration, please contact:

Tanya Khatri
PRIMUS Global Services
Direct: 972-200-4514
Phone No: 972-753-6500 Ext: 258
Email: jobs@primusglobal.com","$89,729 /yr (est.)",501 to 1000 Employees,Private Practice / Firm,Information Technology,Information Technology Support Services,2002,$5 to $25 million (USD)
"Orange County's Credit Union
4.0",4.0,"Santa Ana, CA",Data Engineer,"Great Opportunity At Orange County's Credit Union
Must reside in the state of CA, AZ, NV or TX.
Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk.
Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture.
Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential.
Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY!
More about Orange County's Credit Union:
Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace.
As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service.
Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference!
Overview:
Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization.
Essential Functions:
Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making.
Translate business requirements to technical solutions by applying technical knowledge and strong business acumen.
Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services.
Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform.
Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc).
Technical Must Haves for this Role:
5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment.
3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases).
3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.).
2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores.
2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS).
Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.

The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.

We perform thorough background checks and credit checks. EOE.","$92,145 /yr (est.)",201 to 500 Employees,Nonprofit Organization,Financial Services,Banking & Lending,1938,$25 to $100 million (USD)
Sconcept,#N/A,Remote,Azure Data Engineer,"** W2 LONG TERM CONTRACT BASED....**(NO C2C)
JOB TYPE:- Long Term Contract(6 months)Extendable
JOB LOCATION:-USA
About Sconcept LLC:-Software Concepts Consulting was established in 2008. With our “hands-on” approach we are very successful in delivering high performance, secure IT solutions and IT services. With headquarters in Carrollton, Texas we are specialized in providing business solutions to the Fortune 500 clients. Our clients include Software, Banking, Financial, Healthcare, Retail, Consumer goods and Insurance sectors. We deliver candidates based on client requirements.
Required Skills:-
Must have worked on several (5+) projects building solutions for Data Pipelines, Data warehousing, Data Modelling on Azure Data Platform (AWS, Google Cloud Platform - good to have)
Expertise in design and development of Data pipelines and ETL using ADF, Databricks to move data from relational/structured/unstructured data from source to data lake to data warehouse, on Azure
Creating Pipelines and integrations of multiple data sources
Data processing and transformation using Databricks (Scala/Python)
Expertise in optimizing cost and performance of data pipelines
Experience working with Data Lake, Synapse on Azure
Designing Data Lake storage layer (Landing/staging, raw, trusted/curated zones)
Designing data storage DB schema, data models and processing on Azure Synapse DW
Experience with Synapse Spark Pool
Experience in Data Modelling
Building Data Models from structured and semi-structured data (Data Model, Common Data Model etc.)
Performance optimization of data model for high performance analytical and reporting workloads
Expertise with SQL Server and extensive experience with SQL programming
Programming and optimizing DB objects Views, Stored Procs, Functions
Good experience in defining data access policy and security for azure data services
Experience with Oracle EDW
Knowledge/Experience migrating from Oracle EDW to any cloud DW
Experience integrating AI skills/ML models with Data and reporting solutions
Experience in building API layer for downstream consumption
Job Type: Contract
Pay: $45.00 - $50.00 per hour
Benefits:
Health insurance
Compensation package:
Bonus pay
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$47.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"VedaInfo Inc
4.1",4.1,"Phoenix, AZ",Big Data Engineer,"Hi,
I hope this note finds you well
I am reaching out to you regarding your career opportunity with us for “Big Data Engineer” for a “6+ Months Contract” @Phoenix, AZ
Please find the below requirement details.
Job Title: Big Data Engineer
Location: Phoenix, AZ (Onsite)
Duration: 6+ months
Rate: $60/hr C2C
Responsibilities:
Design, implement, and maintain big data systems handling large volumes of data.
Utilize Hadoop, Hive, and Spark for efficient data processing and analysis.
Collaborate with cross-functional teams to understand data requirements.
Develop and optimize data ingestion, storage, and transformation processes.
Build scalable data pipelines for seamless data processing and analysis.
Monitor and troubleshoot data processing and performance issues.
Stay updated with emerging big data technologies.
Requirements:
Bachelor's/Master's degree in CS, Engineering, or related field
7+ years of experience as a Big Data Engineer
Strong proficiency in Hadoop, Hive, and Spark
Extensive experience with AWS or Azure
Solid understanding of data ingestion, storage, and transformation
Excellent problem-solving and communication skills
Proactive and self-motivated
This is a 6+ month onsite contract position in Phoenix, AZ, with the possibility of extension. Compensation is $60/hr on C2C basis.
Please submit your updated resume highlighting relevant experience.
Thank you for your interest!
Thanks & Regards
Mohammed ZAIN
Job Type: Contract
Pay: $50.00 - $60.00 per hour
Experience level:
7 years
Ability to commute/relocate:
Phoenix, AZ: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
To improve your chances of getting Shortlisted, make sure to tailor your Profile to the Job Requirements, Highlight your Relevant Skills and Experience, and then upload your Resume.
Experience:
AWS (Amazon Web Services): 7 years (Required)
Azure: 7 years (Required)
Big data: 7 years (Required)
Hadoop: 7 years (Required)
Apache Hive: 7 years (Required)
Spark: 7 years (Required)
Work Location: One location",$55.00 /hr (est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$1 to $5 million (USD)
"Myticas Consulting
3.9",3.9,Arizona,BHJOB15656_20058 - Data Engineer,"Myticas's direct client based out of Phoenix, AZ is currently seeking a Data Engineer for a 100% remote contract position.
Pay Rate: Up to $34/hr

Job Description:
A Data Engineer can confidently understand, write, troubleshoot, optimize, and peer review SQL code. They understand business requirements and create code and reporting that exceeds specifications through anticipating underlying needs. They also understand relational database design, table structure, data types, and data models. They are experienced with and confident in development in reporting visualizations in Excel, PowerBI, Tableau, QuickSight, SSRS, or other data products. They can create reporting dashboards and products that inform the business. They easily grasp and retain complex data concepts and are able to explain them to others. Looking for engineers who are driven to innovate, use cutting edge technologies to solve problems and think outside the box.
What You’ll Do:
Digital Transformation into AWS
Data Warehouse related work (SQL, ETL, Replication, Databases)
Understand, write, troubleshoot, optimize, and peer review SQL code.
Understand business requirements and create code and reporting that exceeds specifications through anticipating underlying needs.
Understand relational database design, table structure, data types, and data models. 6.Experienced with and confidence in development in reporting visualizations in Excel, PowerBI, Tableau, QuickSight, SSRS, or other data products.
Data Warehouse related work (SQL, ETL, Replication, Databases
Perform other duties as assigned or apparent.
MINIMUM EDUCATION AND RELATED WORK EXPERIENCE:
Bachelor's degree in a Computer Science field and 2 years' work experience
2 years of experience in most phases of IT systems deployments in one or more of the following areas: design and deployment of cloud services, data migration to AWS cloud, Digital Transformation into AWS, and Data Warehouse related work
INDCHI
INDREM",$34.00 /hr (est.),1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"MARVEL TECHNOLOGIES INC
3.7",3.7,"Cincinnati, OH",Data Engineer,"Job Description
The data engineer designs and builds platforms tools and solutions that help the bank manage secure and generate value from its data. The person in this role creates scalable and reusable solutions for gathering collecting storing processing and serving data on both small and very large i.e. Big Data scales. These solutions can include on-premise and cloud-based data platforms and solutions in any of the following domains ETL business intelligence analytics persistence relational NoSQL data lakes search messaging data warehousing stream processing and machine learning.
Responsible and accountable for risk by openly exchanging ideas and opinions elevating concerns and personally following policies and procedures as defined. Accountable for always doing the right thing for customers and colleagues and ensures that actions and behaviors drive a positive customer experience. While operating within the Bank's risk appetite achieves results by consistently identifying assessing managing monitoring and reporting risks of all types.
ESSENTIAL DUTIES AND RESPONSIBILITIES:
Responsible for design Development and Support of data solutions APIs tools and processes to enable rapid delivery of business capabilities.
Work closely with IT application teams Enterprise architecture infrastructure information security and LOB stakeholders to translate business and technical strategies into data-driven solutions for the Bank.
Act as a technical Expert addressing problems related to system and application design performance integration security etc.
Conduct research and Development based on current trends and technologies related to the banking industry data engineering and architecture data security and related topics.
Work with developers to Build CI/CD pipelines Self-service Build tools and automated deployment processes.
Evaluate software products and Provide documented recommendations as needed.
Provide Support and troubleshooting for data platforms. Must be willing to Provide escalated on-call Support for complicated and/or critical incidents.
Participate in the planning process for hardware and software.
Plan and work on internal projects as needed including legacy system replacement Monitoring and analytics improvements tool Development and technical documentation.
Provide technical guidance and mentoring for other team members.
Manage and prioritize multiple assignments.
MINIMUM KNOWLEDGE SKILLS AND ABILITIES REQUIRED:
Bachelor's degree in Computer Science/Information Systems or equivalent combination of education and experience.
Must be able to communicate ideas both verbally and in writing to management business and IT sponsors and technical resources in language that is appropriate for each group.
Fundamental understanding of distributed computing principles
Knowledge of application and data security concepts best practices and common vulnerabilities.
Conceptual understanding of one or more of the following disciplines preferred big data technologies and distributions metadata management products commercial ETL tools Bi and reporting tools messaging systems data warehousing Java language and run time environment major version control systems continuous integration/delivery tools infrastructure automation and virtualization tools major cloud or rest API design and development.
Job Types: Full-time, Contract
Pay: $60.00 - $62.00 per hour
Compensation package:
Hourly pay
Yearly pay
Experience level:
9 years
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",$61.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,#N/A,$5 to $25 million (USD)
"ProGrad
4.2",4.2,Remote,Data Engineer,"We are looking for data engineers that are excited about building and maintaining data pipelines for a variety of interests. The pipelines will ingest data from a variety of sources ranging from internal change data streams to external integrations with various SaaS platforms through webhooks and events. The ideal candidate is self driven and looking to grow as a data engineer while having an outsized impact on the future of data at the company. A love for database architecture and optimisation is a must as is the ability to work across diverse teams.
Requirements/Qualifications:
Bachelor's or Master's degree in Data Science (specialization)
Software development experience coding in a general-purpose programming language such as C, C++, Java, JavaScript, Golang, or Python
Experience working with data structures or algorithms (i. e., data structures /algorithms class, coursework/projects, research, internships, or other practical experience in/outside of school or work (including open source hobby coding)
Proficiency in SQL, and NoSQL Databases
Experience working with large datasets
Experience writing and debugging complex SQL queries
A strong understanding of cloud database best practices and performance optimisation
Experience building and maintaining data pipelines, coupled with knowledge of good programming practices such as unit and integration testing
. A strong understanding of cloud database best practices and performance optimisation
Good to have skills:
. Experience building and maintaining data pipelines, coupled with knowledge of good programming practices such as unit and integration testing
Job Types: Full-time, Permanent
Salary: Up to $1,200,000.00 per year
Experience level:
1 year
Under 1 year
Schedule:
Day shift
Work Location: Remote
Speak with the employer
+91 9057202434",#N/A,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
META FORCE IT LLC,#N/A,"Cincinnati, OH",Data Engineer,"Job Description
The data engineer designs and builds platforms tools and solutions that help the bank manage secure and generate value from its data. The person in this role creates scalable and reusable solutions for gathering collecting storing processing and serving data on both small and very large i.e. Big Data scales. These solutions can include on-premise and cloud-based data platforms and solutions in any of the following domains ETL business intelligence analytics persistence relational NoSQL data lakes search messaging data warehousing stream processing and machine learning.
Responsible and accountable for risk by openly exchanging ideas and opinions elevating concerns and personally following policies and procedures as defined. Accountable for always doing the right thing for customers and colleagues and ensures that actions and behaviors drive a positive customer experience. While operating within the Bank's risk appetite achieves results by consistently identifying assessing managing monitoring and reporting risks of all types.
ESSENTIAL DUTIES AND RESPONSIBILITIES:
Responsible for design Development and Support of data solutions APIs tools and processes to enable rapid delivery of business capabilities.
Work closely with IT application teams Enterprise architecture infrastructure information security and LOB stakeholders to translate business and technical strategies into data-driven solutions for the Bank.
Act as a technical Expert addressing problems related to system and application design performance integration security etc.
Conduct research and Development based on current trends and technologies related to the banking industry data engineering and architecture data security and related topics.
Work with developers to Build CI/CD pipelines Self-service Build tools and automated deployment processes.
Evaluate software products and Provide documented recommendations as needed.
Provide Support and troubleshooting for data platforms. Must be willing to Provide escalated on-call Support for complicated and/or critical incidents.
Participate in the planning process for hardware and software.
Plan and work on internal projects as needed including legacy system replacement Monitoring and analytics improvements tool Development and technical documentation.
Provide technical guidance and mentoring for other team members.
Manage and prioritize multiple assignments.
MINIMUM KNOWLEDGE SKILLS AND ABILITIES REQUIRED:
Bachelor's degree in Computer Science/Information Systems or equivalent combination of education and experience.
Must be able to communicate ideas both verbally and in writing to management business and IT sponsors and technical resources in language that is appropriate for each group.
Fundamental understanding of distributed computing principles
Knowledge of application and data security concepts best practices and common vulnerabilities.
Conceptual understanding of one or more of the following disciplines preferred big data technologies and distributions metadata management products commercial ETL tools Bi and reporting tools messaging systems data warehousing Java language and run time environment major version control systems continuous integration/delivery tools infrastructure automation and virtualization tools major cloud or rest API design and development.
Job Type: Contract
Pay: $45.96 - $50.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Cincinnati, OH 45215: Reliably commute or planning to relocate before starting work (Required)
Experience:
data engineer: 10 years (Preferred)
Big data: 3 years (Preferred)
Work Location: Hybrid remote in Cincinnati, OH 45215",$47.98 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Fincons Group
4.0",4.0,Remote,Data Engineer – Databricks / Kafka / SQL,"Fincons Group is an IT business consulting company that has been designing the digital future of leading companies on international markets for 40 years. Fincons Group built its reputation on foreseeing and interpreting new business models and the rapid evolution of IT systems by building a complete range of services: from research and consulting to design and development, from system integration of leading vendor software solutions right up to application management, supporting clients step by step along their digital transformation journey. Fincons is a multinational with over 2,600 people and several offices worldwide (in Italy, Switzerland, Germany, France, the United Kingdom, Belgium and the USA), but above all a Family Company where the founders play a strategic role with commitment and passion, grounding the company in the same principles of a united and caring family.
We firmly believe in the value of cooperation and in the contribution that every idea and intuition can bring. We believe that everyone’s effort can make a difference. Transforming knowledge into a strategic resource is our company mission, and we pursue it with the help of our most strategic asset: our people.

Job Summary
Looking for an experienced data engineer with hands on experience in developing solutions based on Databricks. If you are a curious team-player with passion to learn and working for different clients to develop modern data streaming platforms, please reach out to us!

Work location: any US location might be considered
Employment type: freelance / contractor
Start date: about July 1th
Duration: about 6 months with possible renewal

Responsibilities and Duties
Design, develop and implement real time data ingestion pipelines from multiple sources using Databricks
Develop high-quality, scalable and re-usable frameworks for ingesting high volume and large data sets
Apply best practices to Integrate and ingest various big data formats like Protobuf, Parquet, AVRO and JSON
Work with event based / streaming technologies like Kafka, or Kinesis, to ingest and transform the data using KafkaStreams or KSQL
Working with other members of the project team to support delivery of additional project components (API interfaces, Search)
Work to load test the pipelines for quality and better performance using JMeter, or Gatling etc

Qualifications and skills
Bachelor’s degree and first experience designing, developing, deploying and/or supporting data pipelines using Databricks
Experience in designing and implementing Medallion architecture is preferred
Expertise in designing and deploying data applications on cloud solutions, such as Azure or AWS
Hands on experience in performance tuning and optimizing code running in Databricks environment
Proficient in programming languages like Pyspark and Python
Good understanding of SQL, T-SQL and/or PL/SQL
Demonstrated analytical and problem-solving skills particularly those that apply to a big data environment
Experience with agile development methodologies.
Ability to work effectively both independently and in a group or team environment

Authorization to Work
Applicants for employment in the US must be authorized to work in the US. Fincons Group will not sponsor applicants for work visas.
All qualified applicants will receive consideration for employment without regard to race, colour, religion, sex, sexual orientation, gender identity, national origin, age, disability, or protected veteran status, or any other legally protected basis, in accordance with applicable law.
The data will be processed and stored exclusively for the purposes of this or future selections, guaranteeing the rights referred to in art. 13 Legislative Decree 196/03 and EU regulation 679/2016 (GDPR)
2yrKJdtT13",#N/A,201 to 500 Employees,Company - Private,Financial Services,Accounting & Tax,#N/A,Unknown / Non-Applicable
"Vedainfo
4.1",4.1,"Peoria, IL",Data Engineer,"Job Description
· Create and develop optimal data pipeline which fetches data from raw and apply business transformation and load derived.
· Build the AWS Fargate components required for optimal extraction, transformation, and loading of data-to-data lake.
· Develop data pipeline setup between base and derive values based on Caterpillar specification.
· Implement quality validation rules and follow process defined by stakeholder.
· Develop business events as defined by the solution team to generate CDC events for consumption layer (Kinesis)
· Create re-usable and configurable components wherever applicable.
· Create and execute unit, integration, and regression tests
· Ensure the code is developed satisfying all business rules, perform unit testing where applicable.
· Data pipelines development
· Python Development
· AWS cloud services
· Data quality validation rules
· Java (preferred)
· Implement CDC
· Transformation rules creation
· Message broker tools like Kinesis
· Azure DevOps – CI/CD pipelines
· Automated and Manual Testing experience
Job Type: Contract
Pay: Up to $60.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Peoria, IL 61602: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
This position is remote initially and you will be asked to work any of the client location once offer is made. Please confirm your current location.
Experience:
Data Engineer: 1 year (Preferred)
Python Development: 1 year (Preferred)
AWS: 1 year (Preferred)
Message broker tools: 1 year (Preferred)
Work Location: In person",$60.00 /hr (est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$1 to $5 million (USD)
Enterprise Minds,#N/A,Remote,Data Engineer,"Job Role: SQL Data Engineer
Location: Remote Opportunity - Need to work in PST hours
Visa: US Citizen Only
Duration: Long term
Roles and Responsibilities:
3+ years of experience with a complex build system in AWS using Starburst.
5+ years of experience with installation, maintenance, and administration of Oracle databases- PL/SQL and ANSI SQL
Oracle DBA + Oracle SQL to ANSI SQL conversion, performance/query tuning
3+ years of experience configuring, integrating, and securing multiple AWS database offerings (ex. RDS, Aurora, Casandra, and Dynamo)
2+ years of experience with supporting Linux systems engineering efforts in system design and evaluation, solution engineering, software development, or system administration.
Experience with Database Schema as Code tools
Ability to design and develop complex SQL queries and reports to retrieve required data.
Experience with database partitioning and microservices
Knowledge of Agile methodologies or the software development life cycle (SDLC)
US Citizenship is Mandatory.
Job Type: Full-time
Experience:
Data Engineer: 10 years (Preferred)
AWS using Starburst: 4 years (Preferred)
PL/SQL and ANSI SQL: 2 years (Preferred)
AWS RDS: 1 year (Preferred)
Work Location: Remote",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
shreetek,#N/A,"Charlotte, NC",Senior Data Engineer,"Job Title : Data Engineer
ONLY USC
On W2 only
Job Location : Newjersey OR Charlotte,NC
Collect, manage, and convert raw data accurately and reliably
Organize data systems for subgroup access and analyses
Configure and sustain data cloud structures
Able to work with structured and unstructured data.
Validate outputs of data pipelines
Degree in Data Engineering preferred.
Two Programmers –
Convert existing SAS code to python/pyspark code for model operation in the cloud.
Create and sustain policy analysis models in the cloud.
Troubleshoot user interfaces in the cloud.
Create and sustain intuitive user interfaces in the cloud.
A degree in Computer Science is preferred.
Overall across resources keep following stack as reference
Visualization : Tableau
Data Modeling/Science : Python / SAS
Engg : AWS cloud native services, security, data pipeline
Job Type: Contract
Salary: $50.00 - $60.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Charlotte, NC 28202: Reliably commute or planning to relocate before starting work (Required)
Work Location: One location",$55.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Zllius Inc.,#N/A,"Chicago, IL",Data Engineer,"Hi,
Greetings from Zllius Inc!
We have an urgent opening for the role Data Engineer. If interested, revert me back with your Updated resume, along with contact details, work authorization.
Job Role: Data Engineer
Duration: Long-Term
Position Type: W2/ 1099
Visa: Any
Job Description:
Technical/Functional Skills
Successful candidates will have:
Hands on experience in Dremio and Scripting
Hands on Scripting experience in Python and Flask
Hands on experience in Integrating data from multiple source systems (PostgreSQL and CSV)
Experience with Apache Superset
Ability to write complex SQL queries
Experience in building data pipelines using PostgreSQL, Oracle database, Dremio, AWS Open Search
Experience with Docker Containerization of Code
Work as a part of a scrum team in Agile methodology
ETL, Data Pipelines, PostgreSQL, SQL Queries, Scripting
Job Types: Full-time, Contract
Salary: $111,076.11 - $133,769.08 per year
Schedule:
8 hour shift
Day shift
Monday to Friday
Work Location: On the road","$122,423 /yr (est.)",Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
NLB Technology Services,#N/A,"Dallas, TX",CRM data Analytics Engineer,"MUST HAVE:
o Tableau CRMA & Einstein Discovery Certification
o Experience building Dashboards using CRMA (aka Tableau CRM and Einstein Analytics)
o Design, build, and support datasets, data recipes, dataflows, and dashboards using Salesforce and non-Salesforce data sources
o Troubleshoot functional, data, and performance issues with CRMA Dashboards
o Skilled in SAQL, JSON
o Develop and deploy AI/ML solutions using Einstein Discovery and deploy AI/ML Models built outside Salesforce
o Experience working with large data (1 B+ rows) and supporting large user base (10,000+)
o Mentor and coordinate the development efforts of other CRMA DevelopersAT&T
Job Type: Contract
Salary: $65.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Vision insurance
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
CRM ANalytics: 8 years (Required)
einstein: 1 year (Required)
Salesforce: 1 year (Required)
Work Location: One location",$70.00 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Lightcast
4.4",4.4,"Moscow, ID",Senior Data Engineer,"A Senior Data Engineer will architect big data analytical frameworks, and translate complex functional and technical requirements into detailed architecture design and high-performing software. A Senior Data Engineer will select data solution software and define hardware requirements. The position will be responsible for developing and implementing standards and processes for integration projects and initiatives.

Major Responsibilities:
Architect software applications, and test and build automated tools.
Select data solution software and define hardware requirements
Develop standards and processes for integration projects and initiatives.
Lead the design/development of software applications, testing, and building tools
Ensure database changes are reviewed and approved to standards
Lead and communicate to leadership on solution design
Provide technical assistance to junior members and to colleagues across the company
Skills & Abilities:
Analytical mind, personality and aptitude for working with data
Ability to look at the numbers, trends, and data to derive conclusions based on findings
Work closely with management to prioritize business and information needs
Proven ability to work effectively to meet goals and deadlines with minimal supervision
Highly creative problem-solving skills and an ability to tailor efforts based on the importance of the issue being addressed.
A hands-on, detail-oriented mindset with the ability to look beyond the box
Expert knowledge of object-oriented design, data structures, and algorithms
Demonstrated experience with agile or other rapid application development methods, object-oriented design, coding, testing patterns with a variety of languages
Significant knowledge of data structures, algorithms, data modeling and disaster recovery of data systems
Education & Experience:
Requires 5 years of data/software engineering/science experience
Bachelor's degree in computer science or other technical disciplines (masters preferred)
About Lightcast:
Lightcast is a global leader in labor market insights with headquarters in Moscow (ID) and Boston (MA) and offices in the United Kingdom, Europe, and India. We work with partners across six continents to help drive economic prosperity and mobility by providing the insights needed to build and develop our people, our institutions and companies, and our communities.

Lightcast is proud to be an equal opportunity workplace and is committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. Lightcast has always been, and always will be, committed to diversity, equity and inclusion. We seek dynamic professionals from all backgrounds to join our teams, and we encourage our employees to bring their authentic, original, and best selves to work.

#LI-SS1","$118,643 /yr (est.)",1 to 50 Employees,Company - Private,Government & Public Administration,State & Regional Agencies,2011,Unknown / Non-Applicable
"DiamondPick
4.5",4.5,Remote,GCP Data Engineer,"Position: GCP Data Engineer (8+years)
Location: Remote
Type: Contract
Job Details:
· Relevant Industry Work Experience (6+ for Developer, preferred 9+ years)
· Experience extracting data from a variety of sources, and a desire to expand those skills (Excellent knowledge in SQL and Spark is mandatory)
· Strong knowledge of Google BigQuery and architecting data pipelines from on-prem to GCP.
· Experience building applications using Google Cloud Platform related frameworks such as DataProc and GCS at the minimum.
· Excellent Communication Skills to Understand and Pass on Requirements.
· Excellent Data Analysis skills. Must be comfortable with querying and analyzing large amount of data on Hadoop HDFS using Hive and Spark.
· Professional experience with a cloud hosting platform (GCP preferred)
· GCP certification is preferred.
Job Type: Contract
Salary: Up to $65.00 per hour
Experience level:
10 years
8 years
9 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
Google BigQuery: 4 years (Preferred)
DataProc: 2 years (Preferred)
Google Cloud Platform: 9 years (Required)
Spark: 8 years (Required)
SQL: 8 years (Required)
License/Certification:
Google Cloud Platform(GCP) Certification (Required)
Work Location: Remote",$65.00 /hr (est.),1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,#N/A,Unknown / Non-Applicable
TekValue IT Solutions,#N/A,"Houston, TX",Data Engineer with Migrating,"Required Skills:
8-10 Required Experince on Data Engineer and Revalent Skills
Experience with NoSQL (MongoDB)
Experience with API'S
Good Knowledge on Data Migration like Oracle to Mongo db
Experience with Python Programming
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Houston, TX 77002: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 8 years (Required)
NoSQL: 6 years (Required)
MongoDB: 6 years (Required)
Migrating: 5 years (Required)
Work Location: One location
Speak with the employer
+91 7328323606",$72.50 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Infinity Quest
3.9",3.9,"Cleveland, OH",Data Center Engineer,"Required Skillsets:
1. Datacenter Management
2. Hardware Rack & Stack Cabling
3. Knowledge and operational of Diesel Generators, Cooling & Chillers
4. Inventory Asset management
5. DC Monitoring DC availability
6. Handling of CISCO UCS HW and VX RAIL, POWER FLEX
Activities:
1. Operate and Manage both routine and emergency service on a variety of state-of-the-art critical systems such as:
a. Medium voltage switchgear,
b. diesel generators,
c. UPS systems
d. Power distribution equipment, chillers, cooling towers, computer room air handlers,
e. Fire detection / suppression; building monitoring systems (BMS), building Automated Systems (BAS) ; etc
2. Supervise the on-site management of sub-contractors and vendors, ensuring that all work is performed according to established practices and procedures.
3. Manage local client relationship and act as the point of contact for the company at this site.
4. Establish performance benchmarks, conduct analyses and prepare reports on all aspects of the critical facility operations and maintenance.
5. Work with IT managers and other business leaders to coordinate projects, manage capacity and optimize plant safety, performance, reliability and efficiency.
6. Create, utilize and administer MOPs , SOPs, and Preventative Maintenance Procedures for all work on critical data center facility equipment.
7. Schedule work activities, within specified change control / management protocol.
8. Maintain a constant state of readiness in support of the mission goal of 99.999% uptime
9. Racking, Stacking of Infrastructure
a. Cabling Management
b. Patching, Network port swapping
c. Hardware reboots
d. Vendor coordination
e. Project Coordination
· Inventory asset management, Physical access management
Monitoring of Datacenter availability & update respective teams accordingly
Job Type: Contract
Salary: From $80.00 per year
Schedule:
8 hour shift
Ability to commute/relocate:
Cleveland, OH: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data center: 10 years (Preferred)
CISCO UCS: 10 years (Preferred)
MOPS: 10 years (Preferred)
SOP: 10 years (Preferred)
Cabling Management: 10 years (Preferred)
Network protocols: 10 years (Preferred)
Hardware reb: 10 years (Preferred)
VENDOR COORDINATION: 10 years (Preferred)
Project coordination: 10 years (Preferred)
Security clearance:
Confidential (Preferred)
Speak with the employer
+91 8838059965",#N/A,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,Unknown / Non-Applicable
"E-Business International INC
3.6",3.6,"Alexandria, VA",AWS Data Engineer,"Role: AWS Data Engineers
Location: Alexandria, VA
Full-Time/Permanent
AWS Engineers
We are seeking TWO (2) AWS Engineers to join our growing team. The qualified applicants will become part of the Enterprise Data Analytics Services (EDAS) program for a large federal agency in Alexandria, VA. Hybrid work options are available.
The AWS Engineer is responsible for the development and support of cloud-based solutions. Solutions include moving assets from on-premise to cloud environment, evaluation, and optimization. AWS Engineer will collaborate with the team effectively to ensure the team's success.
Responsibilities/Duties:
Work on automating the migration process in AWS from development to production.
Create CFT deploying/updating cloud resources.
Advise the engineering and software engineering team as they migrate from on-premise to cloud infrastructure.
Optimize cloud workloads for cost, scalability, availability, governance, compliance, etc.
Guide and/or provide hands-on support to administer production, staging, and deployment environments.
Partner with multi-disciplinary teams to understand requirements and plan architecture and solutions.
Apply insight and expertise across AWS services.
Apply knowledge of scripting and automation using tools like PowerShell, Python, Bash, Ruby, Perl, etc.
Experience and Qualifications:
Good working knowledge of AWS Services, including Compute, Storage Networking, Database, Management tools, Security, Identity, and Compliance.
3 years’ experience in AWS (EC2, Glue, S3, Redshift, Python programming).
Experience implementing and debugging workloads in AWS.
Good knowledge of creating CFTs.
Experience building infrastructure inside AWS via code. Familiarity with tools such as Terraform or CloudFormation.
Ability to proactively communicate, interact, and solve critical customer problems.
Ability to think and act strategically and proactively.
3 years of experience in software development tools and methodologies.
3 years of experience in infrastructure architecture, database architecture, and networking.
3 years of experience architecting/deploying operating solutions built on AWS.
Education/Certifications:
Bachelor's Degree in Computer Science, Information Technology, Engineering, or associated discipline.
AWS certification is a plus!
Clearance Requirements:
Ability to attain and maintain a Public Trust.
Job Type: Full-time
Salary: From $100,000.00 per year
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Alexandria, VA 20598: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Expected Salary Range (Must)?
Visa Status/ Work Authorization (Must)?
Experience:
AWS: 4 years (Required)
Work Location: One location","$100,000 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,1992,Unknown / Non-Applicable
"Amazee Global Ventures Inc
5.0",5.0,Remote,Data Engineer,"(W2 ONLY)
Job Title: Data Engineer
Duration : 12 months
Location: Remote
6-7 years of experience
Healthcare experience is must
Design, develop, and maintain Tableau dashboards and reports
Develop and maintain data models and ETL processes
Analyze data to identify trends and patterns
Create and maintain data dictionaries and data models
Develop and maintain data pipelines
Collaborate with stakeholders to understand data requirements
Develop and maintain data quality processes
Monitor data quality and performance
Troubleshoot data issues
Develop and maintain documentation
Requirements:
Bachelor’s degree in Computer Science, Information Systems, or related field
4+ years of experience in data engineering and analysis
Expertise in Tableau
Experience with SQL and data modeling
Knowledge of ETL processes
Knowledge of data warehousing
Excellent problem-solving and communication skills
Ability to work independently and in a team environment\
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Type: Full-time
Salary: $40.33 per hour
Work Location: Remote",$40.33 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,2019,Less than $1 million (USD)
"BigLynx Computer Software
4.9",4.9,"Redmond, WA",Databricks Data Engineer,"BigLynx, Inc is an American multinational technology corporation headquartered in Seattle, Washington, with operations in the United States, Canada, and India. The company began in 2016, as a product development company specializing in AI/ML Data Engineering in the Retail vertical space with its products warehouse & fast. Post Pandemic in 2022, BigLynx added a business division of boutique technology consulting, specializing inData Engineering, Full Stack , and Microsoft Dynamics helping clients build the next generation data platform and big data pipelines.
Data pipeline development: Design, develop, and maintain scalable and efficient data pipelines using Databricks to ingest, transform, and load data from various sources. This includes data extraction, data cleansing, data transformation, and data loading processes.
Data modeling and schema design: Design and implement data models, database schemas, and data structures on Databricks. Optimize data models for performance, scalability, and ease of use.
ETL processes: Develop and maintain ETL (Extract, Transform, Load) processes using Databricks to transform and cleanse data. Implement efficient data integration and transformation logic using languages such as Python, SQL, or Scala.
Data integration: Integrate data from multiple systems and sources, ensuring data consistency, accuracy, and quality. Develop and maintain data connectors, APIs, and data ingestion processes.
Performance optimization: Identify and address performance bottlenecks in data pipelines and data models. Optimize query performance, data loading, and data processing capabilities on Databricks.
Data governance and security: Implement data governance practices, data privacy measures, and security controls on Databricks. Ensure compliance with data governance policies and regulations.
Monitoring and troubleshooting: Monitor the health and performance of Databricks data infrastructure, data pipelines, and data processing jobs. Troubleshoot issues and provide timely resolutions.
Collaboration and teamwork: Collaborate with cross-functional teams, including data scientists, data analysts, and business stakeholders, to understand data requirements, provide data engineering expertise, and support their data-related needs.
Qualifications:
Databricks expertise: Strong knowledge and hands-on experience with the Databricks platform, including Databricks notebooks, Databricks runtime, and Databricks clusters.
Data engineering skills: Proficiency in data engineering principles, ETL processes, data modeling, and data integration techniques. Experience with programming languages such as Python, SQL, or Scala.
Big data technologies: Experience with big data technologies, such as Apache Spark, Apache Hadoop, or related frameworks. Familiarity with distributed computing and data processing concepts.
Cloud platforms: Experience working with cloud platforms, preferably Azure Databricks, AWS Databricks, or Google Cloud Databricks. Knowledge of cloud storage, compute, and networking services.
Database and data warehouse concepts: Understanding of relational databases, data warehousing concepts, and SQL. Familiarity with data warehousing best practices and dimensional modeling.
Performance optimization: Strong skills in optimizing Spark jobs and queries on Databricks. Ability to identify and resolve performance bottlenecks.
Problem-solving skills: Strong analytical and problem-solving abilities to tackle complex data engineering challenges and troubleshoot issues.
Collaboration and communication: Excellent collaboration and communication skills to work effectively with cross-functional teams and stakeholders, translating business requirements into technical solutions and providing technical guidance.
Education: A bachelor's or master's degree in computer science, data engineering, or a related field is typically required. Relevant certifications, such as Databricks Certified Developer or similar, are h
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Experience level:
7 years
Schedule:
Monday to Friday
Work Location: In person",$75.00 /hr (est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Wallero
4.4",4.4,"Seattle, WA",Data Engineer,"Hi,
Pretty much interested in your resume, we have an urgent opening for Data Engineer role with one of our clients based in Seattle WA.
Position: Data Engineer
Location: Seattle WA - Remote (100%)
Description:
Good Experience in reporting space including PowerBI, ADF, and possibly standing up a data warehouse/data mart.
The main scope centering is around reporting.
Experience with SQL, and agile framework including requirement gathering and story/feature writing skills.
Need someone strong who is comfortable leading and guiding teams in that capacity.
Migrating data from third party to Azure Cloud
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Seattle, WA 98101: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",$75.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2015,$1 to $5 million (USD)
"JPMorgan Chase Bank, N.A.
3.8",3.8,"Wilmington, DE",Data Engineer III,"As a Data Engineer III under Consumer and Community Banking within the ADE Data Governance team at JPMorgan Chase, you serve as a seasoned member of an agile team to design and deliver trusted data collection, storage, access, and analytics solutions in a secure, stable, and scalable way. You are responsible for developing, testing, and maintaining critical data pipelines and architectures across multiple technical areas within various business functions in support of the firm's business objectives.

Job responsibilities
Supports review of controls to ensure sufficient protection of enterprise data
Responsible for advising and making custom configuration changes in one to two tools to generate a product at the business or customer request
Updates logical or physical data models based on new use cases
Frequently uses SQL and understands NoSQL databases and their niche in the marketplace
Adds to team culture of diversity, equity, inclusion, and respect

Required qualifications, capabilities, and skills
Experience across the data lifecycle
Advanced at SQL (e.g., joins and aggregations)
Working understanding of NoSQL databases
Proficiency with JSON/Java, Erwin, Oracle, Snowflake, AWS
Significant experience with statistical data analysis and ability to determine appropriate tools and data patterns to perform analysis, including data modeling
Experience customizing changes in a tool to generate product
Executes data architecture solutions and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions and break down problems
Produces data architecture and design artifacts for complex applications while being accountable for ensuring design constraints are met by software code development
Gathers, analyzes, synthesizes, and develops visualizations and reporting from large, diverse data sets in service of continuous improvement of data frameworks, applications, and systems that proactively identifies hidden problems and patterns in data and uses these insights to drive improvements to coding hygiene and system architecture

Preferred qualifications, capabilities, and skills
Familiarity with modern technologies
Exposure to cloud technologies
Chase is a leading financial services firm, helping nearly half of America's households and small businesses achieve their financial goals through a broad range of financial products. Our mission is to create engaged, lifelong relationships and put our customers at the heart of everything we do. We also help small businesses, nonprofits and cities grow, delivering solutions to solve all their financial needs.

We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.
The health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the ""WELL Health-Safety Rating"" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.
As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm's current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm's vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.
We offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, discretionary incentive compensation which may be awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.
Equal Opportunity Employer/Disability/Veterans
Base Pay/Salary
Jersey City,NJ $97,850.00 - $150,000.00 / year","$123,925 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Banking & Lending,1799,$10+ billion (USD)
Cloudburst Technologies,#N/A,Remote,Data Collection Engineer,"About Us
Cloudburst is a seed-funded New York-based remote-first company helping customers with detection, prevention, and investigation into cryptocurrency market manipulation and fraud.
Cloudburst provides regulators, financial institutions, trading platforms, and others with access to a real-time, machine-readable crypto market monitoring tool, enabling advanced levels of diligence and customer/market protection. The Cloudburst team has multiple years of experience working together, plus it has worked directly with international law enforcement agencies and policymakers on cybercrime, terrorism, and high-level financial fraud.
What We’re Looking For
The ideal candidate
is an empathetic teammate who loves to help others, to bring order to chaos, and to document their path for others to follow
focuses primarily on web scraping, API integration, and collection of raw data
is capable of becoming fluent in Layer 7 protocols, blockchains, and emerging protocols
has high moral and ethical standards
is proficient in production-level Python
has worked in a threat intel or security environment
What You Will Do
Your primary responsibility will be to become expert at collecting open source data from the web and from other online protocols.
You will be comfortable building tools to bring in data from random files, varying degrees of quality of web sites, and new communications tools. You will have the opportunity to creatively apply your knowledge of software automation towards scaling data collection.
You will understand popular chat sites and how fraud actors use them. You will understand emerging technologies and will actively evaluate them for potential opportunities to collect data.
Why Cloudburst?
At Cloudburst we want to minimize our software engineers working on pixel-pushing and to maximize time spent understanding our domain and building products to help our customers investigate and prevent fraud.
We care about developing and sponsoring our engineers internally. The department adheres to these principles:
Rapid deployment of innovative new techniques for signal generation and attribution
KISS architecture that doesn’t get in developers’ or R&D’s way; we want code to be easy to understand, less abstract, easy to test, fast to deploy, and reducible to automation or simple manual processes
Building a diverse, unconventional team that cross-trains and grows together without ego or sacrifice to work-life balance
What You Will Be Using
Python
Cloud-based infrastructure
3rd party vendor integrations
Novel techniques for data collection using home-grown or discovered tools and frameworks
Hiring Process
Initial email
Initial phone screen with the hiring manager
Take-home test OR link to previous code
Panel interview
Call with CEO
Offer
Investors
Strategic Cyber Ventures
Coinbase Ventures
Bloccelerate
More Info:
https://www.crunchbase.com/organization/cloudburst-technologies-0a3e
https://www.prnewswire.com/news-releases/cloudburst-technologies-raises-3m-in-seed-funding-led-by-strategic-cyber-ventures-joined-by-coinbase-ventures-and-bloccelerate-301817742.html
https://burst.cloud/
https://open.spotify.com/episode/2cv1Is77s8jGLtHPPYRLPU
Job Type: Full-time
Pay: $115,000.00 - $150,000.00 per year
Benefits:
Dental insurance
Flexible schedule
Health insurance
Paid time off
Vision insurance
Compensation package:
Stock options
Experience level:
3 years
Schedule:
Choose your own hours
No nights
No weekends
Work Location: Remote","$132,500 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Stark Dev, LLC",#N/A,"Plano, TX",Data Engineer/Analyst / W2 /USC or GC (GC EAD) or H4 holders,"This position is open for United States Citizens or Green Card holders (GC EAD)/H4 EAD ONLY.
This is an on-site position in Plano/Dallas
CONTRACT W2
Top Skills Details
1) Experience working on a data migration project as a Data Analyst.
- This person will be working on their Permitting, Planning, and Inspection System Project. They are moving from a legacy permitting system, TRAKIT, to a completely new Salesforce application, Clariti. (TRAKIT and Clariti experience not required)
2) Experience doing data discovery, classification, verifying data, mapping rules
- On this project this team will be moving all of the historical data from the old application, TRAKIT, to the new application Clariti.
3) Proficient with SQL and writing SQL queries
\* Interpret data, analyze results using statistical techniques and provide ongoing reports
\* Develop and implement databases, data collection systems, data analytics and other strategies that optimize statistical efficiency and quality
\* Acquire data from primary or secondary data sources and maintain databases/data systems
\* Identify, analyze, and interpret trends or patterns in complex data sets
\* Filter and “clean” data by reviewing computer reports, printouts, and performance indicators to locate and correct code problems
\* Work with management to prioritize business and information needs
\* Locate and define new process improvement opportunities
Drug Test Required
false
Go To Work
false
Workplace Type
On-site
Experience Level
Expert Level
Job Type: Contract
Pay: $55.00 - $60.00 per hour
Schedule:
Monday to Friday
Application Question(s):
What is your visa status?
Work Location: In person",$57.50 /hr (est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Fresh Consulting
3.9",3.9,"Menlo Park, CA",Data Engineer,"Fresh Consulting is a design-led, software development and hardware engineering company, offering end-to-end digital services to help companies innovate. We bring together amazing UX designers, sophisticated developers, digital strategists, and top-notch engineers to help companies create fresh experiences that connect humans, systems, and machines. We’ve been growing fast and need someone to help us continue to manage the delivery of high-quality work in a fast-paced environment.
See more at freshconsulting.com Visit freshconsulting.com/portfolio to see our project work across several industries.
View and apply to all jobs - https://freshconsulting.applytojob.com/apply/ or visit freshconsulting.com/careers
Title: Data Engineer
Duration: 6 months with possible extension
Location: Onsite Menlo Park, CA
Benefits: Employee benefits at 100% including Medical, PTO, Holiday Pay, 401K Plan, and much more!
Hours: Minimum 40 Hours/Week
Role:
Design, construct, install, test, and maintain highly scalable data management systems.
Ensure systems meet business requirements and industry practices.
Design, implement, automate, and maintain large-scale enterprise data ETL processes.
Build high-performance algorithms, prototypes, predictive models, and proof of concepts.
Skills:
5+ years of work experience as a Data Engineer.
Ability to work as part of a team, as well as work independently or with minimal direction.
Excellent written, presentation, and verbal communication skills.
Collaborate with data architects, modelers, and IT team members on project goals.
Strong PC skills including knowledge of Microsoft SharePoint.
Process certification, such as Six Sigma, CBPP, BPM, ISO 20000, ITIL, and CMMI.
Education: BSCSE or related.
FRESH-
Work on engineering and research assignments with F500 companies and startups.
The relationships that we have created with our clients are one of a kind.
We help solve problems in many technologies focusing on R&D, product development, and manufacturing.
We work with the most cutting-edge and latest technologies from AR/VR to Autonomous technologies.
Closely working with our clients, we believe that long-term investments are extremely important to maintain the culture we together have created.
We’re a handpicked team of Engineers, digital strategists, designers, and developers united together in creating a fresh experience. Whether we are strategizing, designing, developing, or analyzing, our integrated team works as an extension of yours to improve your impact, your usability, and your customer conversion. In the process, we collaborate with you to get to know your business, understand your industry, and incorporate your big ideas into memorable experiences that keep your customers coming back for more.
Equal employment opportunity: All qualified persons will be considered for employment without regard to race, color, religion, sex, national origin, age, marital status, familial status, gender identity, sexual orientation, disability for which a reasonable accommodation can be made or any other status protected by law. Assistance will be gladly provided upon request for any applicant with sensory or non-sensory disabilities.
Fresh Consulting is a participating E-Verify company.
freshconsulting.com
Compensation offered will be determined by factors such as location, level, job-related knowledge, skills, and experience. Range $70/hr - $80/hr.
ylM5WIC1Wr",$75.00 /hr (est.),201 to 500 Employees,Company - Private,Management & Consulting,Business Consulting,2007,$25 to $100 million (USD)
NAVA TECH LLC,#N/A,Remote,Azure Data Engineer,"Azure Data Engineer
Remote
Long Term
Note: Need 10+ years of experience
Sound skills and hands on experience with Azure Data Lake, Azure Data Factory, SQL Data Warehouse Azure Blob, Azure Storage Explorer
Proficient in creating Data Factory pipelines for on-cloud ETL processing; copy activity, custom Azure development etc.
Knowledge of Azure Data Catalog, Event Grid, Service Bus, SQL and Synapse
Experience using Azure Databricks platform.
Experience with Python programming.
Experience using pandas and numpy for data engineering and data cleansing;
Ability to troubleshoot the job scheduler and compute clusters.
Experience with databricks CLI and secrets module.
Experience with Azure Blob storage access configuration.
p l e a s e s h a r e r e s u m e s t o m o h a m m e d (a t) n a v a t e c h (d o t) u s O R 4 4 3 5 3 7 9 8 3 5
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$62.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Maven Workforce
4.1",4.1,"Alpharetta, GA",Data Engineer,"Must Have skills:
Building Data Pipeline exp is a must
ETL Tools – SSIS, Alteryx
Data modelling
SQL Server development
Microsoft SQL Stack
Responsibilities:
Data acquisition and ingestion - Identify data sources and build pipelines using various ETL tools such as but not limited to, SSIS, and Alteryx. Data sources including but not limited to SQL Server, SAP, Teradata, Hadoop\Hive, PostgreSQL, Oracle and flat files.
Identifying ways to improve data reliability, efficiency and quality by various data solution techniques.
Expertise in Data project management with JIRA stories. Work as a liaison between business and IT to ensure successful and timely completion of the projects.
Assist in defining the data architecture framework, standards and principles, including modeling, metadata, security and reference data.
Create and optimize data models to support various business applications.
Reviewing modifications of existing data systems for cross-compatibility.
Automate and support workflows to ensure timely delivery.
Must Have:
5+ years of SQL Server development experience.
3+ years ALTERYX Admin /User management experience and advance workflow management.
5+ years data modeling experience.
5+ years of ETL experience.
5+ years of experience in working on more than one database technologies Microsoft SQL server, Teradata.
2+ years big data experience, Hadoop, Hive, Spark
Expert knowledge of data warehousing.
DESIRED SKILLS:
BI Lifecycle management
Working understanding of Microsoft VBA, HTML, Python
Data model development using ERWIN
Job Type: Contract
Salary: $50.00 per hour
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Building Data Pipeline: 8 years (Required)
ETL Tools – SSIS, Alteryx: 7 years (Required)
Data modelling: 6 years (Required)
SQL Server development: 8 years (Required)
Microsoft SQL Stack: 8 years (Required)
Work Location: One location
Speak with the employer
+91 7328456015",$50.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2008,Unknown / Non-Applicable
"Procore Technologies
4.5",4.5,Oregon,Staff Data Engineer,"Job Description

What if you could use your technology skills to develop a product that impacts the way communities' hospitals, homes, sports stadiums, and schools across the world are built? Construction impacts the lives of nearly everyone in the world, and yet it's also one of the world's least digitized industries, not to mention one of the most dangerous. That's why we're looking for a talented Staff Data Engineer to join Procore's journey to revolutionize a historically underserved industry.
As a Staff Data Engineer, you'll design and develop data products for Procore Data Platform data management area. You'll be part of the high-performance team of Data Engineers and will collaborate with platform engineers and product leaders.
This position will report to our Senior Manager of Data Engineering, and can be based remotely from any US location. We're looking for someone to join our team immediately.
What you'll do:
Lead the design and development of big data predictive analytics using object-oriented analysis, design and programming skills, and design patterns
Implement ETL workflows for data matching, data cleansing, data integration, and management
Maintain existing data pipelines and develop new data pipelines using big data technologies
Develop and maintain tables and data models in SQL, abstracting multiple sources and historical data across varied schemas to a format suitable for further analysis
Responsible for leading the effort to continuously improve the reliability, scalability, and stability of the enterprise data platform
Contribute to and lead the continuous improvement of the software development framework and processes by collaborating with Quality Assurance engineers
Deliver observable, reliable, and secure software, embracing the ""you build it, you run it"" mentality, focusing on automation and GitOps
Participate in daily standups, team meetings, sprint planning, and demo/retrospectives while working cross-functionality with other teams to drive the innovation of our products
Apply data governance framework, including the management of data, data compliance operating model, data policies, and standards
What we're looking for:
BS degree in Computer Science, a similar technical field of study, or equivalent practical experience; MS or Ph.D. degree in Computer Science or a related field is preferred
5+ years of experience in a Data Engineering position
Strong expertise with 3+ years of experience building enterprise techniques for large-scale distributed system design and data processing, including:
Building data pipelines with Databricks as the source
Building and maintaining data warehouses in support of BI tools (Snowflake, dbt, Tableau)
Building data pipeline framework for data workflow to process large data sets and Real-Time & Batch Data Pipeline development
Experience in processing structured and unstructured data into a form suitable for analysis and reporting with integration with a variety of data metrics providers ranging from advertising, web analytics, and consumer devices
Desire to be actively hands-on with code, using Java, Python (80%), and SQL, along with willingness and passion for mentoring junior engineers and performing code reviews
Possess familiarity with AWS-managed services for data (Glue, Athena, Data Pipeline, Flink, Spark) and Snowflake

Additional Information

Base Pay Range $147,200-$202,400. Eligible for Bonus Incentive Compensation. Eligible for Equity Compensation. Procore is committed to offering competitive, fair, and commensurate compensation, and has provided an estimated pay range for this role. Actual compensation will be based on a candidate’s job-related skills, experience, education or training, and location.
Perks & Benefits
At Procore, we invest in our employees and provide a full range of benefits and perks to help you grow and thrive. From generous paid time off and healthcare coverage to career enrichment and development programs, learn more details about what we offer and how we empower you to be your best.
About Us
Procore Technologies is building the software that builds the world. We provide cloud-based construction management software that helps clients more efficiently build skyscrapers, hospitals, retail centers, airports, housing complexes, and more. At Procore, we have worked hard to create and maintain a culture where you can own your work and are encouraged and given resources to try new ideas. Check us out on Glassdoor to see what others are saying about working at Procore.
We are an equal-opportunity employer and welcome builders of all backgrounds. We thrive in a diverse, dynamic, and inclusive environment. We do not tolerate discrimination against employees on the basis of age, color, disability, gender, gender identity or expression, marital status, national origin, political affiliation, race, religion, sexual orientation, veteran status, or any other classification protected by law.
If you'd like to stay in touch and be the first to hear about new roles at Procore, join our Talent Community.","$174,800 /yr (est.)",1001 to 5000 Employees,Company - Public,Information Technology,Enterprise Software & Network Solutions,2002,Unknown / Non-Applicable
"Deako
4.9",4.9,"Seattle, WA",Data Engineer II,"About Deako
Deako is delivering a revolutionary plug-n-play smart home platform with a laser focus on the untapped new home construction market. We make smart lighting so easy to upgrade that even those who would have never considered smart lighting are making it happen. We've built a company based on trust; where forming personal relationships is key to our success.
Software at Deako
The software team is a tight-knit team of smart, dedicated people. We are customer focused, we ship often, and are constantly asking “why.” We are passionate about the software we write. We work closely with other Deako teams. Developers at Deako are always looking for a better way and understand that no code is perfect.
Data at Deako
We believe that Data and Software go hand in hand. That's why the Data and Software teams work closely together to ensure we're collecting the right data, and getting it to the right people. Data is new at Deako, and we are working to make it a cornerstone of our company. We want every team at Deako to have a data-driven mindset.
Day to Day Expectations:
Participate in the code review process in our DBT repository
Build maintainable, testable SQL queries for various dashboards
Help maintain, contribute to and improve our data infrastructure and existing pipelines
Help maintain and improve our CI/CD pipelines
Responsible for obtaining, cleaning, and munging data and getting it into a form that our data analysts can access and analyze
Participate in meetings with stakeholders around data collection and cleaning

Must Have:
Demonstrable knowledge in SQL + Some Programming Language (Typescript, Python, Ect)
Demonstrates a high level autonomy and willingness to learn
1+ years of experience working as a Data Analyst/Engineer or Comparable field.
Experience with git, dbt
Nice to Have:
Experience in Snowflake, Fivetran, Hightouch
Experience with Gitlab CI
Experience with geospatial data
Stock Options
Hybrid work environment (Office + Remote)
Medical/Dental/Vision/Life/401K
Unlimited PTO
Free Snacks/Coffee/Drinks (Non-Alcoholic and Alcoholic)
Quarterly Company Parties","$115,000 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,2015,$5 to $25 million (USD)
"PRIMUS Global Services, Inc
4.1",4.1,"Austin, TX","Data Engineer – Snowflake, SQL – REMOTE WORK 43357","We have an immediate long-term opportunity with one of our key clients for a position of Senior Data Engineer, to work on a remote basis.

The ideal applicant must have extensive experience with Snowflake, SQL and Multiple ETL tools.

**ALL successful candidates for this position are required to work directly for PRIMUS. No agencies please**

For immediate consideration, please contact:

Tanya Khatri
PRIMUS Global Services
Direct: 972-200-4514
Phone No: 972-753-6500 Ext: 258
Email: jobs@primusglobal.com","$89,729 /yr (est.)",501 to 1000 Employees,Private Practice / Firm,Information Technology,Information Technology Support Services,2002,$5 to $25 million (USD)
"Orange County's Credit Union
4.0",4.0,"Santa Ana, CA",Data Engineer,"Great Opportunity At Orange County's Credit Union
Must reside in the state of CA, AZ, NV or TX.
Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk.
Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture.
Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential.
Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY!
More about Orange County's Credit Union:
Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace.
As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service.
Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference!
Overview:
Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization.
Essential Functions:
Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making.
Translate business requirements to technical solutions by applying technical knowledge and strong business acumen.
Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services.
Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform.
Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc).
Technical Must Haves for this Role:
5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment.
3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases).
3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.).
2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores.
2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS).
Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.

The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.

We perform thorough background checks and credit checks. EOE.","$92,145 /yr (est.)",201 to 500 Employees,Nonprofit Organization,Financial Services,Banking & Lending,1938,$25 to $100 million (USD)
